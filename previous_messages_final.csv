,thread_id,email_id,email_code,email_body,previous_email_id,previous_email_code,previous_email_body,concat_body
0,152625,156564,uncivil,"How do you plan to handle the external references? For example, the following LWN articles has a link this file. And changing the name and/or location will break that link, AFAIK.",156500,uncivil,"So I hate this rst crap with a passion, so NAK from me.","So I hate this rst crap with a passion, so NAK from me. How do you plan to handle the external references? For example, the following LWN articles has a link this file. And changing the name and/or location will break that link, AFAIK."
1,152625,156500,uncivil,"So I hate this rst crap with a passion, so NAK from me.",152625,technical,"Let PDF & HTML's be created out of memory-barriers Text by reStructuring.  reStructuring done were, 1. Section headers modification, lower header case except start 2. Removal of manual index(contents section), since it now gets created    automatically for html/pdf 3. Internal cross reference for easy navigation 4. Alignment adjustments 5. Strong emphasis made wherever there was emphasis earlier (through    other ways), strong was chosen as normal emphasis showed in italics,    which was felt to be not enough & strong showed it in bold 6. ASCII text & code snippets in literal blocks 7. Backquotes for inline instances in the paragraph's where they are    expressed not in English, but in C, pseudo-code, file path etc. 8. Notes section created out of the earlier notes 9. Manual numbering replaced by auto-numbering 10.Bibliography (References section) made such that it can be cross-linked.   Hi,  With this change, pdf & html could be generated. There certainly are improvements to be made, but thought of first knowing whether migrating memory-barriers from txt to rst is welcome.  The location chosen is Documentation/kernel-hacking"", i was unsure where this should reside & there was no .rst file in top-level directory ""Documentation"", so put it into one of the existing folder that seemed to me as not that unsuitable.  Other files refer to memory-barrier.txt, those also needs to be adjusted based on where .rst can reside.   ","Let PDF & HTML's be created out of memory-barriers Text by reStructuring.  reStructuring done were, 1. Section headers modification, lower header case except start 2. Removal of manual index(contents section), since it now gets created    automatically for html/pdf 3. Internal cross reference for easy navigation 4. Alignment adjustments 5. Strong emphasis made wherever there was emphasis earlier (through    other ways), strong was chosen as normal emphasis showed in italics,    which was felt to be not enough & strong showed it in bold 6. ASCII text & code snippets in literal blocks 7. Backquotes for inline instances in the paragraph's where they are    expressed not in English, but in C, pseudo-code, file path etc. 8. Notes section created out of the earlier notes 9. Manual numbering replaced by auto-numbering 10.Bibliography (References section) made such that it can be cross-linked.   Hi,  With this change, pdf & html could be generated. There certainly are improvements to be made, but thought of first knowing whether migrating memory-barriers from txt to rst is welcome.  The location chosen is Documentation/kernel-hacking"", i was unsure where this should reside & there was no .rst file in top-level directory ""Documentation"", so put it into one of the existing folder that seemed to me as not that unsuitable.  Other files refer to memory-barrier.txt, those also needs to be adjusted based on where .rst can reside.    So I hate this rst crap with a passion, so NAK from me."
2,152625,156871,uncivil,"IMO symlinks are mostly ending in a mess, URLs are never stable. There is a link to handle such requirements. Take a look at intersphinx to see how it works:  Each Sphinx HTML build creates a file named objects.inv that contains a mapping from object names to URIs relative to the HTML set's root. This means articles from external (like lwn articles) has to be recompiled. Not perfect, but a first solution. I really like them, factually valuable comments .. please express your concern so that we have a chance to move on. I think that's a pity.",156614,technical,"If necessary to handle these, symlink might help here i believe.  Upon trying to understand memory-barriers.txt, i felt that it might be better to have it in PDF/HTML format, thus attempted to convert it to rst. And i see it not being welcomed, hence shelving the conversion.","If necessary to handle these, symlink might help here i believe.  Upon trying to understand memory-barriers.txt, i felt that it might be better to have it in PDF/HTML format, thus attempted to convert it to rst. And i see it not being welcomed, hence shelving the conversion. IMO symlinks are mostly ending in a mess, URLs are never stable. There is a link to handle such requirements. Take a look at intersphinx to see how it works:  Each Sphinx HTML build creates a file named objects.inv that contains a mapping from object names to URIs relative to the HTML set's root. This means articles from external (like lwn articles) has to be recompiled. Not perfect, but a first solution. I really like them, factually valuable comments .. please express your concern so that we have a chance to move on. I think that's a pity."
3,152625,157636,civil,"Thanks for the details. Initially i was sceptical of rst & once instead of hitting the fly, hit ""make htmldocs"" on the keyboard :), and the opinion about it was changed. It was easy to navigate through various docs & the realized that various topics (& many) were present (yes, it was there earlier also, but had to dive inside Documentation & search, while viewing the toplevel index.html made them standout). It was like earlier you had to go after docs, but now it was docs coming after you, that is my opinion. 
Later while fighting with memory-barriers.txt, felt that it might be good for it as well as to be in that company. And the readability as a text is not hurt as well. It was thought that rst conversion could be done quickly, but since this was my first attempt with rst, had to put some effort to get a not so bad output, even if this patch dies, i am happy to have learnt rst conversion to some extent. When one of the author of the original document objected, i felt it is better to backoff. But if there is a consensus, i will proceed.",156871,uncivil,"IMO symlinks are mostly ending in a mess, URLs are never stable. There is an object to handle such requirements. Take a look at intersphinx to see how it works:  Each Sphinx HTML build creates a file named objects.inv that contains a mapping from object names to URIs relative to the HTML sets root.  This means articles from external (like lwn articles) has to be recompiled. Not perfect, but a first solution. I really like them, factually valuable comments .. please express your concern so that we have a chance to move on. I think that's a pity. ","IMO symlinks are mostly ending in a mess, URLs are never stable. There is an object to handle such requirements. Take a look at intersphinx to see how it works:  Each Sphinx HTML build creates a file named objects.inv that contains a mapping from object names to URIs relative to the HTML sets root.  This means articles from external (like lwn articles) has to be recompiled. Not perfect, but a first solution. I really like them, factually valuable comments .. please express your concern so that we have a chance to move on. I think that's a pity.  Thanks for the details. Initially i was sceptical of rst & once instead of hitting the fly, hit ""make htmldocs"" on the keyboard :), and the opinion about it was changed. It was easy to navigate through various docs & the realized that various topics (& many) were present (yes, it was there earlier also, but had to dive inside Documentation & search, while viewing the toplevel index.html made them standout). It was like earlier you had to go after docs, but now it was docs coming after you, that is my opinion. 
Later while fighting with memory-barriers.txt, felt that it might be good for it as well as to be in that company. And the readability as a text is not hurt as well. It was thought that rst conversion could be done quickly, but since this was my first attempt with rst, had to put some effort to get a not so bad output, even if this patch dies, i am happy to have learnt rst conversion to some extent. When one of the author of the original document objected, i felt it is better to backoff. But if there is a consensus, i will proceed."
4,159308,177720,uncivil,"A version of this patch has been queued by Catalin. Now that the cpufeature bits are queued, I think this can be split up into two separate series for v4.16-rc1, one to tackle NOTIFY_SEI and the associated plumbing. The second for the KVM 'make SError pending' API. I didn't sign-off this patch. If you pick some bits from another version and want to credit someone else you can 'CC:' them or just mention it in the commit-message. Irrelevant-Nit: sys-regs usually have a 'SYS_' prefix, and are in instruction encoding order lower down the file. (These PSTATE PAN things are a bit odd as they were used to generate and instruction before the fancy {read,write}_sysreg() helpers were added). Bits of this are spread between patches 5 and 6. If you put them in the other order this wouldn't happen. (but after a rebase most of this patch should disappear) So this writes an impdef ESR, because its the existing code-path in KVM. And then you overwrite it. Which is a bit odd as there is a helper to do both in one go. How come you don't use this in kvm_arm_set_sei_esr()?",177719,technical,"After this patch user-space can trigger an SError in the guest. If it wants to migrate the guest, how does the pending SError get migrated?  I think we need to fix migration first. Andrew Jones suggested using this:  Given KVM uses kvm_inject_vabt() on v8.0 hardware too, we should cover systems without the v8.2 RAS Extensions with the same API. I think this means a bit to read/write whether SError is pending, and another to indicate the ESR should be set/read. CPUs without the v8.2 RAS Extensions can reject pending-SError that had an ESR.  user-space can then use the 'for migration' calls to make a 'new' SError pending.  Now that the cpufeature bits are queued, I think this can be split up into two separate series for v4.16-rc1, one to tackle NOTIFY_SEI and the associated plumbing. The second for the KVM 'make SError pending' API.    Does nothing in the patch that adds the support? This is a bit odd. (oh, its hiding in patch 6...)","After this patch user-space can trigger an SError in the guest. If it wants to migrate the guest, how does the pending SError get migrated?  I think we need to fix migration first. Andrew Jones suggested using this:  Given KVM this function on v8.0 hardware too, we should cover systems without the v8.2 RAS Extensions with the same API. I think this means a bit to read/write whether SError is pending, and another to indicate the ESR should be set/read. CPUs without the v8.2 RAS Extensions can reject pending-SError that had an ESR.  user-space can then use the 'for migration' calls to make a 'new' SError pending.  Now that the cpufeature bits are queued, I think this can be split up into two separate series for this version, one to tackle NOTIFY_SEI and the associated plumbing. The second for the KVM 'make SError pending' API.    Does nothing in the patch that adds the support? This is a bit odd. (oh, its hiding in patch 6...) A version of this patch has been queued by Catalin. Now that the cpufeature bits are queued, I think this can be split up into two separate series for this version, one to tackle NOTIFY_SEI and the associated plumbing. The second for the KVM 'make SError pending' API. I didn't sign-off this patch. If you pick some bits from another version and want to credit someone else you can 'CC:' them or just mention it in the commit-message. Irrelevant-Nit: sys-regs usually have a 'SYS_' prefix, and are in instruction encoding order lower down the file. (These PSTATE PAN things are a bit odd as they were used to generate and instruction before the fancy helpers were added). Bits of this are spread between patches 5 and 6. If you put them in the other order this wouldn't happen. (but after a rebase most of this patch should disappear) So this writes an impdef ESR, because its the existing code-path in KVM. And then you overwrite it. Which is a bit odd as there is a helper to do both in one go. How come you don't use this in this function?"
5,161354,161783,uncivil,"For 2500Base-X, do you report a speed of 2500Mbps through ethtool, or are you reporting 1000Mbps?  I don't see any code in this patch that deals with that.",161722,technical,Thanks for adding the comment. ,"Thanks for adding the comment.  For 2500Base-X, do you report a speed of 2500Mbps through ethtool, or are you reporting 1000Mbps?  I don't see any code in this patch that deals with that."
6,165231,165261,civil,"According to my comment on the other thread, this stands true in case the child is managed by runtime PM as well. Otherwise this looks good to me. How about adding an additional patch on top taking into account the ignore_children flag and folding that into the series, kind of as you also suggested? My point is, we might as well take the opportunity to fix this right away, don't you think?",165230,technical,"One of the limitations of pm_runtime_force_suspend/resume() is that if a parent driver wants to use these functions, all of its child drivers generally have to do that too because of the parent usage counter manipulations necessary to get the correct state of the parent during system-wide transitions to the working state (system resume). However, that limitation turns out to be artificial, so remove it.  Namely, pm_runtime_force_suspend() only needs to update the children counter of its parent (if there's is a parent) when the device can stay in suspend after the subsequent system resume transition, as that counter is correct already otherwise.  Now, if the parent's children counter is not updated, it is not necessary to increment the parent's usage counter in that case any more, as long as the children counters of devices are checked along with their usage counters in order to decide whether or not the devices may be left in suspend after the subsequent system resume transition.  Accordingly, modify pm_runtime_force_suspend() to only call pm_runtime_set_suspended() for devices whose usage and children counters are at the no references"" level (the runtime PM status of the device needs to be updated to ""suspended"" anyway in case this function is called once again for the same device during the transition under way), drop the parent usage counter incrementation from it and update pm_runtime_force_resume() to compensate for these changes. ","One of the limitations of the resume function is that if a parent driver wants to use these functions, all of its child drivers generally have to do that too because of the parent usage counter manipulations necessary to get the correct state of the parent during system-wide transitions to the working state (system resume). However, that limitation turns out to be artificial, so remove it.  Namely, pm_runtime_force_suspend() only needs to update the children counter of its parent (if there's is a parent) when the device can stay in suspend after the subsequent system resume transition, as that counter is correct already otherwise.  Now, if the parent's children counter is not updated, it is not necessary to increment the parent's usage counter in that case any more, as long as the children counters of devices are checked along with their usage counters in order to decide whether or not the devices may be left in suspend after the subsequent system resume transition.  Accordingly, the suspend function to only call this function for devices whose usage and children counters are at the no references"" level (the runtime PM status of the device needs to be updated to ""suspended"" anyway in case this function is called once again for the same device during the transition under way), drop the parent usage counter incrementation from it and update the resume function to compensate for these changes.  According to my comment on the other thread, this stands true in case the child is managed by runtime PM as well. Otherwise this looks good to me. How about adding an additional patch on top taking into account the ignore_children flag and folding that into the series, kind of as you also suggested? My point is, we might as well take the opportunity to fix this right away, don't you think?"
7,165657,169349,uncivil,"Can you please use a consistent name space? retpoline_ ... or such? I really don't like fiddling with that variable. That's just hackery. The variable reflects the actual enabled mitigation state of the kernel proper. That'll break once we get other mitigation variants. These newlines are there to separate stuff for readability sake. This really can be done in a cleaner way. That only needs one function and that one can take care of setting a
variable in the spectre code which then influences the sysfs output. And that output should not be ""Vulnerable"" like you force with the hack above. It actually should tell WHY it is vulnerable despite having had protection in place before the module was loaded.",165657,technical," There's a risk that a kernel that has full retpoline mitigations becomes vulnerable when a module gets loaded that hasn't been compiled with the right compiler or the right option.  We cannot fix it, but should at least warn the user when that happens.  When the a module hasn't been compiled with a retpoline aware compiler, print a warning and change the SPECTRE_V2 mitigation mode to show the system is vulnerable now.  For modules it is checked at compile time, however it cannot check assembler or other non compiled objects used in the module link.  v2: Change warning message v3.","There's a risk that a kernel that has full retpoline mitigations becomes vulnerable when a module gets loaded that hasn't been compiled with the right compiler or the right option.  We cannot fix it, but should at least warn the user when that happens.  When the a module hasn't been compiled with a retpoline aware compiler, print a warning and change the SPECTRE_V2 mitigation mode to show the system is vulnerable now.  For modules it is checked at compile time, however it cannot check assembler or other non compiled objects used in the module link.  v2: Change warning message v3. Can you please use a consistent name space? retpoline ... or such? I really don't like fiddling with that variable. That's just hackery. The variable reflects the actual enabled mitigation state of the kernel proper. That'll break once we get other mitigation variants. These newlines are there to separate stuff for readability sake. This really can be done in a cleaner way. That only needs one function and that one can take care of setting a
variable in the spectre code which then influences the sysfs output. And that output should not be ""Vulnerable"" like you force with the hack above. It actually should tell WHY it is vulnerable despite having had protection in place before the module was loaded."
8,166193,168161,uncivil,I didn't get any response to a comment I've written about the point above during the previous patch iteration.,168038,technical,tested v3... ,tested v3...  I didn't get any response to a comment I've written about the point above during the previous patch iteration.
9,167856,174594,civil,"No problem, at the beginning, I only wanted to enable the strict. Doing this involves that I have to remove pinctrl nodes for the pins which are going to be request through the gpiolib to avoid conflicts. These pins were configured with bias-pull-up. That's why I try to add the bias support. Thanks for the detailed answer about what you have in mind. Well, yes and not! As a consequence of enabling strict mode, I have to find another way to configure the pins. Yes, I have noticed this issue. Right, I have spotted some drivers to fix. I will try to handle the ones related to the platforms I am using.",174357,technical,"I think we need to think over what is a good way to share ownership of a pin.  Russell pointed me to a similar problem incidentally and I briefly looked into it: there are cases when several devices may need to hold the same pin.  Can't we just look up the associated gpio_chip from the GPIO range, and in case the pin is connected between the pin controller and the GPIO chip, then we allow the gpiochip to also take a reference?  I.e. in that case you just allow gpio_owner to proceed and take the pin just like with a non-strict controller. ","I think we need to think over what is a good way to share ownership of a pin.  Russell pointed me to a similar problem incidentally and I briefly looked into it: there are cases when several devices may need to hold the same pin.  Can't we just look up the associated gpio_chip from the GPIO range, and in case the pin is connected between the pin controller and the GPIO chip, then we allow the gpiochip to also take a reference?  I.e. in that case you just allow gpio_owner to proceed and take the pin just like with a non-strict controller.  No problem, at the beginning, I only wanted to enable the strict. Doing this involves that I have to remove pinctrl nodes for the pins which are going to be request through the gpiolib to avoid conflicts. These pins were configured with bias-pull-up. That's why I try to add the bias support. Thanks for the detailed answer about what you have in mind. Well, yes and not! As a consequence of enabling strict mode, I have to find another way to configure the pins. Yes, I have noticed this issue. Right, I have spotted some drivers to fix. I will try to handle the ones related to the platforms I am using."
10,168060,169219,uncivil,This does not make sense vs. the documentation. This should say: And I really have to ask whether this should be named GLOBAL instead of SHARED. Hmm?,168063,technical,Test the new MEMBARRIER_CMD_PRIVATE_EXPEDITED_SYNC_CORE and MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED_SYNC_CORE commands.,Test the new commands. This does not make sense vs. the documentation. This should say: And I really have to ask whether this should be named GLOBAL instead of SHARED. Hmm?
11,168169,173259,civil,"Thanks a lot for all this work! It was long overdue and it is nice to see the project finally getting to an end, after passing into so many hands!
I am not sure I understand the purpose of this level here. As far as I understand, you only have per-engine control whether you want to enable CG or not. What you call BLCG and SLCG levels just mean ""don't use the boot values, but rather use our values (taken from nvidia)"". 
Now, here comes the nasty part: NVIDIA only ever validated the boot values (I guess they are extremely safe ones), and the optimised values (the ones coming from your patch 2, 3, and 4 along with the level 3. I think introducing a single parameter that controls both CG, PG, and automatic reclocking would be safer. For CG and PG, it would be a all-or-nothing (either boot values, or everything like nvidia). This message is a bit odd, whether we keep the notion of levels or not. Can you get rid of the mention of powergating given that this is not part of this patchset? If you agree with having a single enable bit for CG, then a simple: ""Clockgating status: (boot|optimized)"" would work perfectly. All this time, I thought these parameters were for power gating... I also did not expect that clock gating had to be disabled before we could program them.

Great find! Why introduce it? I can't find references to it in this patch (outside of the function below) or in the following patches. As you even export this function, it looks like you used to use this function in an earlier revision of this series.
Aside from all these nitpicks, the approach is quite self contained and I like the following patches. Well done! Once we settle on the configuration parameter, I can give you my R-b :smile:",173260,technical,"FWIW, SLCG stands for second level clock gating","FWIW, SLCG stands for second level clock gating Thanks a lot for all this work! It was long overdue and it is nice to see the project finally getting to an end, after passing into so many hands!
I am not sure I understand the purpose of this level here. As far as I understand, you only have per-engine control whether you want to enable CG or not. What you call BLCG and SLCG levels just mean ""don't use the boot values, but rather use our values (taken from nvidia)"". 
Now, here comes the nasty part: NVIDIA only ever validated the boot values (I guess they are extremely safe ones), and the optimised values (the ones coming from your patch 2, 3, and 4 along with the level 3. I think introducing a single parameter that controls both CG, PG, and automatic reclocking would be safer. For CG and PG, it would be a all-or-nothing (either boot values, or everything like nvidia). This message is a bit odd, whether we keep the notion of levels or not. Can you get rid of the mention of powergating given that this is not part of this patchset? If you agree with having a single enable bit for CG, then a simple: ""Clockgating status: (boot|optimized)"" would work perfectly. All this time, I thought these parameters were for power gating... I also did not expect that clock gating had to be disabled before we could program them.
Great find! Why introduce it? I can't find references to it in this patch (outside of the function below) or in the following patches. As you even export this function, it looks like you used to use this function in an earlier revision of this series.
Aside from all these nitpicks, the approach is quite self contained and I like the following patches. Well done! Once we settle on the configuration parameter, I can give you my R-b :smile:"
12,168668,168727,uncivil,"Again, 'boutside' protection  Other than that. Reviewed.",168726,technical,"boutside protection'?  In general I'd rather have the tracepoints when actually submitting the request, with this tracepoint we might be getting a trace which doesn't really indicate if the command was submitted at all.","boutside protection'?  In general I'd rather have the tracepoints when actually submitting the request, with this tracepoint we might be getting a trace which doesn't really indicate if the command was submitted at all. Again, 'boutside' protection. Other than that. Reviewed."
13,169133,177084,uncivil,"The timer is supposed to restart the protocol again, that's how this whole thing is designed to work. I think you are making changes to the symptom rather than the true because of the problems you are seeing. Sorry, I will not apply this until the exact issue is better understood.",169133,technical,"In drivers/net/wan/hdlc_ppp.c, some noise on physical line can cause the carrier detect still ok, but the protocol will fail. So if carrier detect ok, don't turn off protocol negotiation  This patch is against the kernel version Linux 4.15-rc8","In this file, some noise on physical line can cause the carrier detect still ok, but the protocol will fail. So if carrier detect ok, don't turn off protocol negotiation  This patch is against the kernel version Linux 4.15-rc8 The timer is supposed to restart the protocol again, that's how this whole thing is designed to work. I think you are making changes to the symptom rather than the true because of the problems you are seeing. Sorry, I will not apply this until the exact issue is better understood."
14,169133,177642,uncivil,"Ok, I check the source code again. It have nothing to do with the interrupts, it is related how the hdlc.c is implemented. My case is the carrier always good, but protocol will fail due to perfect noise, and this issue was found and complained by our customers. So it is not my theory guessing, it is a real problem.",177133,technical,"The timer is supposed to be triggered by carrier detect interrupt. After remove the line noise, the carrier detect interrupt is never triggered again, because the carrier is always ok and it only trigger the timer once, Since the protocol was terminated and no new interrupts happen, the link will never be back. So the case here is that the line noise is good and just good to make the carrier detect still good  but the protocol fail, the timer will be never triggered again.  Of course, if you increase the noise and make even the carrier detect fail, then remove the noise, the link will be up, Because the carrier down and up again and then trigger the timer to restart. The timer is supposed to restart the protocol again, that's how this whole thing is designed to work.  I think you are making changes to the symptom rather than the true cause of the problems you are seeing.  Sorry, I will not apply this until the exact issue is better understood.","The timer is supposed to be triggered by carrier detect interrupt. After remove the line noise, the carrier detect interrupt is never triggered again, because the carrier is always ok and it only trigger the timer once, Since the protocol was terminated and no new interrupts happen, the link will never be back. So the case here is that the line noise is good and just good to make the carrier detect still good  but the protocol fail, the timer will be never triggered again.  Of course, if you increase the noise and make even the carrier detect fail, then remove the noise, the link will be up, Because the carrier down and up again and then trigger the timer to restart. The timer is supposed to restart the protocol again, that's how this whole thing is designed to work.  I think you are making changes to the symptom rather than the true cause of the problems you are seeing.  Sorry, I will not apply this until the exact issue is better understood. Ok, I check the source code again. It have nothing to do with the interrupts, it is related how the hdlc.c is implemented. My case is the carrier always good, but protocol will fail due to perfect noise, and this issue was found and complained by our customers. So it is not my theory guessing, it is a real problem."
15,169133,179802,civil,"Sorry about being late, just returned home and am trying to get all the backlogs under control. I remember the PPP standard is a bit cloudy about the possible issue, but the latter indeed exists (the PPP state machine was written directly to STD-51). There is related (more visible in practice, though we aren't affected) issue of ""active"" vs ""passive"" mode (hdlc_ppp.c is ""active"", and two ""passives"" wouldn't negotiate at all). 
Anyway the problem is real (though not very visible in practice, especially on relatively modern links rather than 300 or 1200 bps dialup connections) and should be fixed. Looking at the patch, my first impression is it makes the code differ from STD-51 a little bit. On the other hand, perhaps applying it as is and forgetting about the issue is the way to go. Ideally, I think the negotiation failure should end up (optionally, in addition to the current behavior) in some configurable sleep, then the negotiation should restart. If it's worth the effort at this point, I don't know. Perhaps I could look at this later, but no promises (this requires pulling on and setting up some legacy hardware). Anyway, since the patch is safe and can solve an existing problem.",177642,uncivil,"Ok, I check the source code again. It have nothing to do with the interrupts, it is related how the hdlc.c is implemented. My case is the carrier always good, but protocol will fail due to perfect noise, and this issue was found and complained by our customers. So it is not my theory guessing, it is a real problem.","Ok, I check the source code again. It have nothing to do with the interrupts, it is related how the hdlc.c is implemented. My case is the carrier always good, but protocol will fail due to perfect noise, and this issue was found and complained by our customers. So it is not my theory guessing, it is a real problem. Sorry about being late, just returned home and am trying to get all the backlogs under control. I remember the PPP standard is a bit cloudy about the possible issue, but the latter indeed exists (the PPP state machine was written directly to STD-51). There is related (more visible in practice, though we aren't affected) issue of ""active"" vs ""passive"" mode (this file is ""active"", and two ""passives"" wouldn't negotiate at all). 
Anyway the problem is real (though not very visible in practice, especially on relatively modern links rather than 300 or 1200 bps dialup connections) and should be fixed. Looking at the patch, my first impression is it makes the code differ from STD-51 a little bit. On the other hand, perhaps applying it as is and forgetting about the issue is the way to go. Ideally, I think the negotiation failure should end up (optionally, in addition to the current behavior) in some configurable sleep, then the negotiation should restart. If it's worth the effort at this point, I don't know. Perhaps I could look at this later, but no promises (this requires pulling on and setting up some legacy hardware). Anyway, since the patch is safe and can solve an existing problem."
16,169133,193114,civil,"How  do you think my patch? As you see, Krzysztof  think my patch is ok to be accepted. But if you have a better idea to fix it,I am glad to see it. Anyway, this issue have to be fixed.
",179802,civil,"Sorry about being late, just returned home and am trying to get all the backlogs under control.  I remember the PPP standard is a bit cloudy about the possible issue, but the latter indeed exists (the PPP state machine was written directly to STD-51). There is related (more visible in practice, though we aren't affected) issue of active"" vs ""passive"" mode (hdlc_ppp.c is ""active"", and two ""passives"" wouldn't negotiate at all).  Anyway the problem is real (though not very visible in practice, especially on relatively modern links rather than 300 or 1200 bps dialup connections) and should be fixed. Looking at the patch, my first impression is it makes the code differ from STD-51 a little bit. On the other hand, perhaps applying it as is and forgetting about the issue is the way to go.  Ideally, I think the negotiation failure should end up (optionally, in addition to the current behavior) in some configurable sleep, then the negotiation should restart. If it's worth the effort at this point, I don't know.  Perhaps I could look at this later, but no promises (this requires pulling on and setting up some legacy hardware).  Anyway, since the patch is safe and can solve an existing problem: ","Sorry about being late, just returned home and am trying to get all the backlogs under control.  I remember the PPP standard is a bit cloudy about the possible issue, but the latter indeed exists (the PPP state machine was written directly to STD-51). There is related (more visible in practice, though we aren't affected) issue of active"" vs ""passive"" mode (this file is ""active"", and two ""passives"" wouldn't negotiate at all).  Anyway the problem is real (though not very visible in practice, especially on relatively modern links rather than 300 or 1200 bps dialup connections) and should be fixed. Looking at the patch, my first impression is it makes the code differ from STD-51 a little bit. On the other hand, perhaps applying it as is and forgetting about the issue is the way to go.  Ideally, I think the negotiation failure should end up (optionally, in addition to the current behavior) in some configurable sleep, then the negotiation should restart. If it's worth the effort at this point, I don't know.  Perhaps I could look at this later, but no promises (this requires pulling on and setting up some legacy hardware).  Anyway, since the patch is safe and can solve an existing problem:  How  do you think my patch? As you see, Krzysztof  think my patch is ok to be accepted. But if you have a better idea to fix it, I am glad to see it. Anyway, this issue have to be fixed.
"
17,169133,204633,uncivil,"I cannot apply a patch which has been corrupted by your email client like
this.

Please send it properly again, plain ASCII text, and no trasnformations
by your email client.
You should send the patch to yourself and try to apply the patch you receive, do not send to the list until you can pass the test properly. Do not use attachments to fix this problem, the patch must be inline after your commit message and signoffs. Please read these for more information.",203117,technical,"How  is your thinking about this patch? Subject: netdev: carrier detect ok, don't turn off negotiation   Sometimes when physical lines have a just good noise to make the protocol  handshaking fail, but the carrier detect still good. Then after remove of  the noise, nobody will trigger this protocol to be start again to cause  the link to never come back. The fix is when the carrier is still on, not  terminate the protocol handshaking.  Ok, I submit it  again.   In drivers/net/wan/hdlc_ppp.c, some noise on physical line can cause the carrier detect still ok, but the protocol will fail. So if carrier detect ok, don't turn off protocol negotiation  This patch is against the kernel version Linux 4.15-rc8 Please resubmit it and I'll think about it again, thank you.","How  is your thinking about this patch? Subject: netdev: carrier detect ok, don't turn off negotiation   Sometimes when physical lines have a just good noise to make the protocol  handshaking fail, but the carrier detect still good. Then after remove of  the noise, nobody will trigger this protocol to be start again to cause  the link to never come back. The fix is when the carrier is still on, not  terminate the protocol handshaking.  Ok, I submit it  again.   In this file, some noise on physical line can cause the carrier detect still ok, but the protocol will fail. So if carrier detect ok, don't turn off protocol negotiation  This patch is against the kernel version Linux 4.15-rc8 Please resubmit it and I'll think about it again, thank you. I cannot apply a patch which has been corrupted by your email client like this. Please send it properly again, plain ASCII text, and no transformations by your email client.
You should send the patch to yourself and try to apply the patch you receive, do not send to the list until you can pass the test properly. Do not use attachments to fix this problem, the patch must be inline after your commit message and signoffs. Please read these for more information."
18,170193,170197,uncivil,"Please don't put plain-text files into core-api - that's a directory full
of RST documents.  Your document is 99.9% RST already, better to just
finish the job and tie it into the rest of the kernel docs.


We might as well put the SPDX tag here, it's a new file.


This is all good information, but I'd suggest it belongs more in the 0/n
patch posting than here.  The introduction of *this* document should say what it actually covers. This seems like a relevant and important aspect of the API that shouldn't be buried in the middle of a section talking about random things. So one gets this far, but has no actual idea of how to do these things. Which leads me to wonder: what is this document for?  Who are you expecting to read it? You could improve things a lot by (once again) going to RST and using directives to bring in the kerneldoc comments from the source (which, I note, do exist).  But I'd suggest rethinking this document and its audience.  Most of the people reading it are likely wanting to learn how to *use* this API; I think it would be best to not leave them frustrated.",170198,technical,Add basic self-test functionality for pmalloc.,"Add basic self-test functionality for pmalloc. Please don't put plain-text files into core-api - that's a directory full of RST documents.  Your document is 99.9% RST already, better to just finish the job and tie it into the rest of the kernel docs. We might as well put the SPDX tag here, it's a new file.

This is all good information, but I'd suggest it belongs more in the 0/n patch posting than here.  The introduction of *this* document should say what it actually covers. This seems like a relevant and important aspect of the API that shouldn't be buried in the middle of a section talking about random things. So one gets this far, but has no actual idea of how to do these things. Which leads me to wonder: what is this document for?  Who are you expecting to read it? You could improve things a lot by (once again) going to RST and using directives to bring in the kerneldoc comments from the source (which, I note, do exist).  But I'd suggest rethinking this document and its audience.  Most of the people reading it are likely wanting to learn how to *use* this API; I think it would be best to not leave them frustrated."
19,170193,188751,uncivil,"Relatively significant? I do not object to your comment, but in practice i see that:
- vmalloc is used relatively little
- allocations do not seem to be huge
- there seem to be way larger overheads in the handling of virtual pages (see my proposal for the LFS/m summit, about collapsing struct vm_struct and struct vmap_area)
Can you please point me to this function/macro? I don't seem to be able to find it, at least not in 4.15. During hardened user copy permission check, I need to confirm if the memory range that would be exposed to userspace is a legitimate sub-range of a pmalloc allocation. So, I start with the pair (address, size) and I must end up to something I can compare it against. The idea here is to pass through struct_page and then the related vmap area, which already has the information about the specific chunk of virtual memory. I cannot comment on your proposal because I do not know where to find the reference you made, or maybe I do not understand what you mean :sad:",170195,technical,Well this introduces significant overhead for large sized allocation. Does this not matter because the areas are small?  Would it not be better to use compound page allocations here? page_head(whatever) gets you the head page where you can store all sorts of information about the chunk of memory.,"Well this introduces significant overhead for large sized allocation. Does this not matter because the areas are small?  Would it not be better to use compound page allocations here? page_head(whatever) gets you the head page where you can store all sorts of information about the chunk of memory. Relatively significant? I do not object to your comment, but in practice i see that:
- vmalloc is used relatively little
- allocations do not seem to be huge
- there seem to be way larger overheads in the handling of virtual pages (see my proposal for the LFS/m summit, about collapsing struct vm_struct and struct vmap_area)
Can you please point me to this function/macro? I don't seem to be able to find it, at least not in 4.15. During hardened user copy permission check, I need to confirm if the memory range that would be exposed to userspace is a legitimate sub-range of a pmalloc allocation. So, I start with the pair (address, size) and I must end up to something I can compare it against. The idea here is to pass through struct_page and then the related vmap area, which already has the information about the specific chunk of virtual memory. I cannot comment on your proposal because I do not know where to find the reference you made, or maybe I do not understand what you mean :sad:"
20,170193,188745,civil,"ok. ok, this is all new stuff to me ... I suppose I should do it also for all the other new files I create. But what is the license for the documentation? It's not code, so GPL seems wrong. Creative commons? I just noticed a patch for checkpatch.pl about SPDX and asked the same question there. ok. I'll move it to the Use section. I will add a reference to the selftest file. In practice it can also work as example. ok, the example route should be more explicative. thanks again for the review.",190492,technical,Thank you for the patch! Yet something to improve:,"Thank you for the patch! Yet something to improve: ok. ok, this is all new stuff to me ... I suppose I should do it also for all the other new files I create. But what is the license for the documentation? It's not code, so GPL seems wrong. Creative commons? I just noticed a patch for checkpatch.pl about SPDX and asked the same question there. ok. I'll move it to the Use section. I will add a reference to the selftest file. In practice it can also work as example. ok, the example route should be more explicative. thanks again for the review."
21,173287,173288,uncivil,You can't do it this simply as it will cause deadlock due to nested locking of the buf_lock. To share the lock you will need to provide unlocked versions of the read and write functions and use those if the lock has already been taken.,173287,technical,This is to be used only by the IIO core for protecting device mode changes between INDIO_DIRECT and INDIO_BUFFER.  This patch replaces the use of mlock with the already established buf_lock mutex. ,This is to be used only by the IIO core for protecting device mode changes between INDIO_DIRECT and INDIO_BUFFER.  This patch replaces the use of mlock with the already established buf_lock mutex.  You can't do it this simply as it will cause deadlock due to nested locking of the buf_lock. To share the lock you will need to provide unlocked versions of the read and write functions and use those if the lock has already been taken.
22,174463,174508,uncivil,Ok. I've looked at your patch for way too long now and still don't see how you've shown it to be correct. Shouldn't there be a at least a comment to explain why zero is an appropriate initialization value in that case?,174487,technical, I already sent a fix for this., I already sent a fix for this. Ok. I've looked at your patch for way too long now and still don't see how you've shown it to be correct. Shouldn't there be a at least a comment to explain why zero is an appropriate initialization value in that case?
23,174735,174850,uncivil,Are you moving checks from the core subsystem to drivers ? This looks really nonsensical and the commit message doesn't explain the rationale for that at all.,174736,technical,"Export and import are mandatory in async hash. As drivers were rewritten, drop empty wrappers and correct init of ahash transformation. ","Export and import are mandatory in async hash. As drivers were rewritten, drop empty wrappers and correct init of ahash transformation.  Are you moving checks from the core subsystem to drivers ? This looks really nonsensical and the commit message doesn't explain the rationale for that at all."
24,174735,199553,uncivil,"This makes no sense, cfr my comment on 5/5. Seems like if the driver doesn't implement those, the core can easily detect that and perform the necessary action. Moving the checks out of core seems like the wrong thing to do, rather you should enhance the checks in core if they're insufficient in my opinion.",199556,technical,All applied.  Thanks. ,"All applied.  Thanks.  This makes no sense, cfr my comment on 5/5. Seems like if the driver doesn't implement those, the core can easily detect that and perform the necessary action. Moving the checks out of core seems like the wrong thing to do, rather you should enhance the checks in core if they're insufficient in my opinion."
25,174735,199600,uncivil,"The core can very well check if these functions are not populated and
return ENOSYS. So you remove all NULL pointer checks ? Esp. in security-sensitive code? What is the impact of this non-critical path code on performance? Come on ... You can very well impose that in the core, except you don't duplicate the code.",199597,technical,"The bug can only be in driver which will not implement those two functions, but we already had all drivers with those due to patches 1..4 All other drivers do have them.  Additionally, with crypto we want minimize code and run as fast as possible.  Moving checks out of core will impose on driver author need for implement those functions, or declare them empty, but in case of empty ones  crypto will not work properly with such driver. ","The bug can only be in driver which will not implement those two functions, but we already had all drivers with those due to patches 1..4 All other drivers do have them.  Additionally, with crypto we want minimize code and run as fast as possible.  Moving checks out of core will impose on driver author need for implement those functions, or declare them empty, but in case of empty ones  crypto will not work properly with such driver.  The core can very well check if these functions are not populated and return ENOSYS. So you remove all NULL pointer checks ? Esp. in security-sensitive code? What is the impact of this non-critical path code on performance? Come on ... You can very well impose that in the core, except you don't duplicate the code."
26,174735,199673,uncivil,"Why you want checks for something that not exist ? Those without them will not work and will do Oops in crypto testmgr, so such drivers should not be used nor accepted in drivers/crypto. Ask yourself why crypto do not check for NULL in ahash digest or other required ahash functions. Now size of crypto core is reduced.",199600,uncivil,"The core can very well check if these functions are not populated and return ENOSYS. So you remove all NULL pointer checks ? Esp. in security-sensitive code? What is the impact of this non-critical path code on performance?  Come on ...   You can very well impose that in the core, except you don't duplicate the code. ","The core can very well check if these functions are not populated and return ENOSYS. So you remove all NULL pointer checks ? Esp. in security-sensitive code? What is the impact of this non-critical path code on performance?  Come on ...   You can very well impose that in the core, except you don't duplicate the code.  Why you want checks for something that not exist ? Those without them will not work and will do Oops in crypto testmgr, so such drivers should not be used nor accepted in drivers/crypto. Ask yourself why crypto do not check for NULL in ahash digest or other required ahash functions. Now size of crypto core is reduced."
27,174735,199701,uncivil,"Are you suggesting that the kernel code should NOT perform NULL pointer checks? Are you suggesting each driver should implement every single callback available and if it is not implemented, return -ENOSYS ? This looks like a MASSIVE code duplication. You implemented the same code thrice, it surely is not reduced.",199673,uncivil,"Why you want checks for something that not exist ? Those without them will not work and will do Oops in crypto testmgr, so such drivers should not be used nor accepted in drivers/crypto  Ask yourself why crypto do not check for NULL in ahash digest or other required ahash functions.   Now size of crypto core is reduced. ","Why you want checks for something that not exist ? Those without them will not work and will do Oops in crypto testmgr, so such drivers should not be used nor accepted in drivers/crypto  Ask yourself why crypto do not check for NULL in ahash digest or other required ahash functions.   Now size of crypto core is reduced.  Are you suggesting that the kernel code should NOT perform NULL pointer checks? Are you suggesting each driver should implement every single callback available and if it is not implemented, return -ENOSYS ? This looks like a MASSIVE code duplication. You implemented the same code thrice, it surely is not reduced."
28,174735,200015,uncivil,"You can compile kernel with generic config and at that point you have all the duplicated code stored on your machine. But this discussion is moving away from the point I was concerned about -- that this patchset _increases_ code duplication and I find this wrong. It does NOT reduce the binary size, just try compiling all the drivers in and it will make the kernel bigger.",199976,uncivil,"It is source code duplication. One do not load all crypto drivers at once, simple because one board has only one crypto HW (or few closely related), and if one even try, almost none of them will initialize on given hardware. E.g. on Exynos board only exynos drivers will load, on board with omap crypto only omap crypto will load. As I said above, it reduces binary size at cost of more source code in few drivers. ","It is source code duplication. One do not load all crypto drivers at once, simple because one board has only one crypto HW (or few closely related), and if one even try, almost none of them will initialize on given hardware. E.g. on Exynos board only exynos drivers will load, on board with omap crypto only omap crypto will load. As I said above, it reduces binary size at cost of more source code in few drivers.  You can compile kernel with generic config and at that point you have all the duplicated code stored on your machine. But this discussion is moving away from the point I was concerned about -- that this patchset _increases_ code duplication and I find this wrong. It does NOT reduce the binary size, just try compiling all the drivers in and it will make the kernel bigger."
29,174735,199976,uncivil,"It is source code duplication. One do not load all crypto drivers at once, simple because one board has only one crypto HW (or few closely related), and if one even try, almost none of them will initialize on given hardware. E.g. on Exynos board only exynos drivers will load, on board with omap crypto only omap crypto will load. As I said above, it reduces binary size at cost of more source code in few drive",199701,uncivil,"Are you suggesting that the kernel code should NOT perform NULL pointer checks ?  Are you suggesting each driver should implement every single callback available and if it is not implemented, return -ENOSYS ? This looks like a MASSIVE code duplication.   You implemented the same code thrice, it surely is not reduced. ","Are you suggesting that the kernel code should NOT perform NULL pointer checks ?  Are you suggesting each driver should implement every single callback available and if it is not implemented, return -ENOSYS ? This looks like a MASSIVE code duplication.   You implemented the same code thrice, it surely is not reduced.  It is source code duplication. One do not load all crypto drivers at once, simple because one board has only one crypto HW (or few closely related), and if one even try, almost none of them will initialize on given hardware. E.g. on Exynos board only exynos drivers will load, on board with omap crypto only omap crypto will load. As I said above, it reduces binary size at cost of more source code in few drive"
30,180488,180837,civil,"Then, I am wondering why we are holding mmap_sem when calling migrate_pages() in existing code. Sorry, I missed that. If mmap_sem is not needed for migrate_pages(), please ignore this patch.",180546,technical,"This doesn't make much sense to me, to be honest. We are holding mmap_sem for _read_ so we allow parallel updates like page faults or unmaps. Therefore we are isolating pages prior to the migration.  The sole purpose of the mmap_sem in add_page_for_migration is to protect from vma going away _while_ need it to get the proper page.  Moving the lock up is just wrong because it allows caller to hold the lock for way too long if a lot of pages is migrated. Not only that, it is even incorrect because we are doing get_user() (aka page fault) and while read lock recursion is OK, we might block and deadlock when there is a writer pending. I haven't checked the current implementation of semaphores but I believe we do not allow recursive locking.","This doesn't make much sense to me, to be honest. We are holding this so we allow parallel updates like page faults or unmaps. Therefore we are isolating pages prior to the migration.  The sole purpose of this in add_page_for_migration is to protect from vma going away _while_ need it to get the proper page.  Moving the lock up is just wrong because it allows caller to hold the lock for way too long if a lot of pages is migrated. Not only that, it is even incorrect because we are getting the user (aka page fault) and while read lock recursion is OK, we might block and deadlock when there is a writer pending. I haven't checked the current implementation of semaphores but I believe we do not allow recursive locking. Then, I am wondering why we are holding this when calling migrate_pages in existing code. Sorry, I missed that. If it is not needed for migrate_pages, please ignore this patch."
31,183468,183472,civil,Thanks for testing this and letting me know.,183473,technical,"stable: 62 boots: 0 failed, 58 passed with 2 offline, 2 untried/unknown Full Boot Summary here. Full build summary here. For more info write here.","stable: 62 boots: 0 failed, 58 passed with 2 offline, 2 untried/unknown Full Boot Summary here. Full build summary here. For more info write here. Thanks for testing this and letting me know."
32,202437,202446,uncivil,"Use normal patch styles.
Fix your tools before you send any more patches.",202444,technical,"The variables dssdev"" and ""vid_dev"" will eventually be set to appropriate pointers a bit later. Thus omit the explicit initialisations at the beginning.  ","The variables dssdev"" and ""vid_dev"" will eventually be set to appropriate pointers a bit later. Thus omit the explicit initialisations at the beginning.   Use normal patch styles.
Fix your tools before you send any more patches."
33,202437,505099,uncivil,"While we do not mind cleanup patches, the way you post them (one fix per file) is really annoying and takes us too much time to review. I'll take the ""Fix a possible null pointer"" patch since it is an actual bug fix, but will reject the others, not just this driver but all of them that are currently pending in our patchwork (https://patchwork.linuxtv.org). Feel free to repost, but only if you organize the patch as either fixing the same type of issue for a whole subdirectory (media/usb, media/pci, etc) or fixing all issues for a single driver. Actual bug fixes (like the null pointer patch in this series) can still be posted as separate patches, but cleanups shouldn't. So in this particular case I would expect two omap_vout patches: one for the bug fix, one for the cleanups. Just so you know, I'll reject any future patch series that do not follow these rules. Just use common sense when posting these things in the future. I would also suggest that your time might be spent more productively if you would work on some more useful projects. There is more than enough to do. However, that's up to you.",202446,uncivil,Use normal patch styles. Fix your tools before you send any more patches.,"Use normal patch styles. Fix your tools before you send any more patches. While we do not mind cleanup patches, the way you post them (one fix per file) is really annoying and takes us too much time to review. I'll take the ""Fix a possible null pointer"" patch since it is an actual bug fix, but will reject the others, not just this driver but all of them that are currently pending in our patchwork. Feel free to repost, but only if you organize the patch as either fixing the same type of issue for a whole subdirectory (media/usb, media/pci, etc) or fixing all issues for a single driver. Actual bug fixes (like the null pointer patch in this series) can still be posted as separate patches, but cleanups shouldn't. So in this particular case I would expect two omap_vout patches: one for the bug fix, one for the cleanups. Just so you know, I'll reject any future patch series that do not follow these rules. Just use common sense when posting these things in the future. I would also suggest that your time might be spent more productively if you would work on some more useful projects. There is more than enough to do. However, that's up to you."
34,202437,505195,uncivil,"??? I did that: either one patch per directory with the same type of change,
or one patch per driver combining all the changes for that driver. Yes, and you were told not to do it like that again.",505193,uncivil,"Interesting... Would you like to share any more information from this meeting?    I would appreciate further indications for a corresponding change acceptance.  I found a feedback by Mauro Carvalho Chehab more constructive. Cleanup fixes. This time, I was nice and I took some time doing this.","Interesting... Would you like to share any more information from this meeting? I would appreciate further indications for a corresponding change acceptance.  I found a feedback by Mauro Carvalho Chehab more constructive. Cleanup fixes. This time, I was nice and I took some time doing this. ??? I did that: either one patch per directory with the same type of change, or one patch per driver combining all the changes for that driver. Yes, and you were told not to do it like that again."
35,202437,505193,uncivil,Interesting  Would you like to share any more information from this meeting? I would appreciate further indications for a corresponding change acceptance. I found a feedback by Mauro Carvalho Chehab more constructive.,505158,technical,For the record: this only applies to drivers/media. We discussed what do to with series like this during our media summit last Friday and this was the conclusion of that. Obviously I can't talk about other subsystems.,For the record: this only applies to drivers/media. We discussed what do to with series like this during our media summit last Friday and this was the conclusion of that. Obviously I can't talk about other subsystems. Interesting  Would you like to share any more information from this meeting? I would appreciate further indications for a corresponding change acceptance. I found a feedback by Mauro Carvalho Chehab more constructive.
36,202437,513970,uncivil,"I find it very surprising that you rejected 146 useful update suggestions
so easily. What does this software area make it so special in comparison to
other Linux subsystems? Have you taken any other solution approaches into account than a quick rejection? Could your reaction have been different if the remarkable number of change possibilities were sent by different authors (and not only me)? How should possibly remaining disagreements about affected implementation details be resolved now? Are you looking for further improvements around development tools like patchwork and quilt? Will you accept increasing risks because of bigger patch sizes? Can such an information lead to differences in the preferred patch granularity? How do you think about this detail?
How would you ever like to clean up stuff in affected source files which was accumulated (or preserved somehow) over years? I guess that this handling will trigger more communication challenges. Our common sense seems to be occasionally different in significant ways. I distribute my software development capacity over several areas. Does your wording indicate a questionable signal for further contributions?",505225,technical,I have got an other impression.    I continued with the presentation of suggestions from my selection of change possibilities. It seems that there are very different expectations for the preferred patch granularity.  Can it happen again that you are going to use a development tool like quilt (as a maintainer) for the desired recombination of possible update steps?,"I have got an other impression.    I continued with the presentation of suggestions from my selection of change possibilities. It seems that there are very different expectations for the preferred patch granularity.  Can it happen again that you are going to use a development tool like quilt (as a maintainer) for the desired recombination of possible update steps? I find it very surprising that you rejected 146 useful update suggestions
so easily. What does this software area make it so special in comparison to other Linux subsystems? Have you taken any other solution approaches into account than a quick rejection? Could your reaction have been different if the remarkable number of change possibilities were sent by different authors (and not only me)? How should possibly remaining disagreements about affected implementation details be resolved now? Are you looking for further improvements around development tools like patchwork and quilt? Will you accept increasing risks because of bigger patch sizes? Can such an information lead to differences in the preferred patch granularity? How do you think about this detail?
How would you ever like to clean up stuff in affected source files which was accumulated (or preserved somehow) over years? I guess that this handling will trigger more communication challenges. Our common sense seems to be occasionally different in significant ways. I distribute my software development capacity over several areas. Does your wording indicate a questionable signal for further contributions?"
37,202437,745390,uncivil,Would you like to answer my still remaining questions in any more constructive ways?,513970,uncivil,I find it very surprising that you rejected 146 useful update suggestions so easily. What does this software area make it so special in comparison to other Linux subsystems? Have you taken any other solution approaches into account than a quick rejection? Could your reaction have been different if the remarkable number of  change possibilities were sent by different authors (and not only me)? How should possibly remaining disagreements about affected implementation   details be resolved now? Are you looking for further improvements around development tools   like patchwork and quilt?  Will you accept increasing risks because of bigger patch sizes?    Can such an information lead to differences in the preferred patch granularity? How do you think about this detail? How would you ever like to clean up stuff in affected source files which was accumulated (or preserved somehow) over years? I guess that this handling will trigger more communication challenges. Our common sense seems to be occasionally different in significant ways.  I distribute my software development capacity over several areas. Does your wording indicate a questionable signal for further contributions?,I find it very surprising that you rejected 146 useful update suggestions so easily. What does this software area make it so special in comparison to other Linux subsystems? Have you taken any other solution approaches into account than a quick rejection? Could your reaction have been different if the remarkable number of  change possibilities were sent by different authors (and not only me)? How should possibly remaining disagreements about affected implementation   details be resolved now? Are you looking for further improvements around development tools   like patchwork and quilt?  Will you accept increasing risks because of bigger patch sizes?    Can such an information lead to differences in the preferred patch granularity? How do you think about this detail? How would you ever like to clean up stuff in affected source files which was accumulated (or preserved somehow) over years? I guess that this handling will trigger more communication challenges. Our common sense seems to be occasionally different in significant ways.  I distribute my software development capacity over several areas. Does your wording indicate a questionable signal for further contributions? Would you like to answer my still remaining questions in any more constructive ways?
38,202437,160227,uncivil,Are you going to answer any of my remaining questions in a more constructive way?,745390,uncivil,Would you like to answer my still remaining questions in any more constructive ways?,Would you like to answer my still remaining questions in any more constructive ways? Are you going to answer any of my remaining questions in a more constructive way?
39,202437,191235,uncivil,Do any contributors get into the mood to take another look at software updates from my selection of change possibilities in a more constructive way? Do you need any additional development resources?,160227,uncivil,Are you going to answer any of my remaining questions in a more constructive way?,Are you going to answer any of my remaining questions in a more constructive way? Do any contributors get into the mood to take another look at software updates from my selection of change possibilities in a more constructive way? Do you need any additional development resources?
40,202437,191241,uncivil,"One last time: either post per-driver patches with all the cleanups for a driver in a single patch, or a per-directory patch (drivers/media/pci, usb, etc) doing the same cleanup for all drivers in that directory. I prefer the first approach, but it's up to you. We don't have the time to wade through dozens of one-liner cleanup patches. I don't understand what is so difficult about this.",191235,uncivil,Do any contributors get into the mood to take another look at software updates from my selection of change possibilities in a more constructive way?  Do you need any additional development resources?,"Do any contributors get into the mood to take another look at software updates from my selection of change possibilities in a more constructive way?  Do you need any additional development resources? One last time: either post per-driver patches with all the cleanups for a driver in a single patch, or a per-directory patch (drivers/media/pci, usb, etc) doing the same cleanup for all drivers in that directory. I prefer the first approach, but it's up to you. We don't have the time to wade through dozens of one-liner cleanup patches. I don't understand what is so difficult about this."
41,202437,191275,uncivil,"I preferred to offer source code adjustments according to specific transformation patterns mostly for each software module separately (also in small patch series). I am curious if bigger patch packages would be easier to get accepted. Or would you get frightened still by any other change combination? We have got different preferences for a safe patch granularity. I imagine that there are more development factors involved. It is usual that integration of update suggestions will take some time. How would the situation change if I would dare to regroup possible update steps? There are communication difficulties to consider since your terse information from your conference meeting. If you would insist on patch squashing, would you dare to use a development tool like quilt fold also on your own once more?",191241,uncivil,"One last time: either post per-driver patches with all the cleanups for a driver in a single patch, or a per-directory patch (drivers/media/pci, usb, etc) doing the same cleanup for all drivers in that directory.  I prefer the first approach, but it's up to you.  We don't have the time to wade through dozens of one-liner cleanup patches.  I don't understand what is so difficult about this.","One last time: either post per-driver patches with all the cleanups for a driver in a single patch, or a per-directory patch (drivers/media/pci, usb, etc) doing the same cleanup for all drivers in that directory.  I prefer the first approach, but it's up to you.  We don't have the time to wade through dozens of one-liner cleanup patches.  I don't understand what is so difficult about this. I preferred to offer source code adjustments according to specific transformation patterns mostly for each software module separately (also in small patch series). I am curious if bigger patch packages would be easier to get accepted. Or would you get frightened still by any other change combination? We have got different preferences for a safe patch granularity. I imagine that there are more development factors involved. It is usual that integration of update suggestions will take some time. How would the situation change if I would dare to regroup possible update steps? There are communication difficulties to consider since your terse information from your conference meeting. If you would insist on patch squashing, would you dare to use a development tool like quilt fold also on your own once more?"
42,202437,196318,uncivil,I find such a change combination unsafe. Would you dare to apply any (of my) scripts for the semantic patch language directly on the whole directory for multi-media software? Can you handle bigger patches really better than similar patch series? Are there any further possibilities to consider around consequences from a general change resistance? Will any development (or management) tools like quilt fold make the regrouping of possible update steps more convenient and safer? ,191275,uncivil,"I preferred to offer source code adjustments according to specific transformation patterns mostly for each software module separately (also in small patch series).    I am curious if bigger patch packages would be easier to get accepted.  Or would you get frightened still by any other change combination?     We have got different preferences for a safe patch granularity.    I imagine that there are more development factors involved.    It is usual that integration of update suggestions will take some time. How would the situation change if I would dare to regroup possible update steps?    There are communication difficulties to consider since your terse information from your conference meeting.  If you would insist on patch squashing, would you dare to use a development tool like quilt fold also on your own once more?","I preferred to offer source code adjustments according to specific transformation patterns mostly for each software module separately (also in small patch series). I am curious if bigger patch packages would be easier to get accepted.  Or would you get frightened still by any other change combination? We have got different preferences for a safe patch granularity. I imagine that there are more development factors involved. It is usual that integration of update suggestions will take some time. How would the situation change if I would dare to regroup possible update steps? There are communication difficulties to consider since your terse information from your conference meeting.  If you would insist on patch squashing, would you dare to use a development tool like quilt fold also on your own once more? I find such a change combination unsafe. Would you dare to apply any (of my) scripts for the semantic patch language directly on the whole directory for multi-media software? Can you handle bigger patches really better than similar patch series? Are there any further possibilities to consider around consequences from a general change resistance? Will any development (or management) tools like quilt fold make the regrouping of possible update steps more convenient and safer? "
43,207400,207934,civil,"Cool. I would say this is done right. How about writing an i2c bus driver which sits directly on top of another i2c bus? Basically a one port i2c mux. The current mux code does not seem to directly allow it, since it calls i2c_transfer() directly on the parent, where as you want it to call your own i2c_transfer function. But maybe you could expended the core mux code to allow the i2c_mux_core structure to contain a transfer function?",207917,technical,"You didn't miss it - I probably need to explain it better.  The 'emulated devices' do respond on different slave addresses (which match one of or the only the slave addresses those parts support). For example the device-tree for the GW54xx has the following which are all from the GSC which is the only thing on i2c1 One issue I'm trying to figure out the best way to deal with is the fact that the GSC can 'NAK' transactions occasionally which is why I override the regmap read/write functions and provide retries. This resolves the issue for the mfd core driver and sub-module drivers but doesn't resolve the issue with these 'emulated devices' which have their own stand-alone drivers. I'm not sure how to best deal with that yet. I tried to add retires to the i2c adapter but that wasn't accepted upstream because it was too generic and I was told I need to work around it in device-drivers. I'm guessing I need to write my own sub-module drivers that will largely duplicate whats in the stand-alone drivers.""","You didn't miss it - I probably need to explain it better.  The 'emulated devices' do respond on different slave addresses (which match one of or the only the slave addresses those parts support). For example the device-tree for the GW54xx has the following which are all from the GSC which is the only thing on i2c1 One issue I'm trying to figure out the best way to deal with is the fact that the GSC can 'NAK' transactions occasionally which is why I override the regmap read/write functions and provide retries. This resolves the issue for the mfd core driver and sub-module drivers but doesn't resolve the issue with these 'emulated devices' which have their own stand-alone drivers. I'm not sure how to best deal with that yet. I tried to add retires to the i2c adapter but that wasn't accepted upstream because it was too generic and I was told I need to work around it in device-drivers. I'm guessing I need to write my own sub-module drivers that will largely duplicate whats in the stand-alone drivers."" Cool. I would say this is done right. How about writing an i2c bus driver which sits directly on top of another i2c bus? Basically a one port i2c mux. The current mux code does not seem to directly allow it, since it calls i2c_transfer() directly on the parent, where as you want it to call your own i2c_transfer function. But maybe you could expended the core mux code to allow the i2c_mux_core structure to contain a transfer function?"
44,207400,208102,civil,"thanks for the review! Ok. ok - will have this in v2. oops, did not mean to submit that. it was for original debugging and not needed - will remove this. Yes, that makes sense. I'll propose something like the following in v2.",208059,technical,There appears to be a few spaces vs tabs issues in this file.,"There appears to be a few spaces vs tabs issues in this file. thanks for the review! Ok. ok - will have this in v2. oops, did not mean to submit that. it was for original debugging and not needed - will remove this. Yes, that makes sense. I'll propose something like the following in v2."
45,207400,208141,civil,"No, at this point it requires both I2C and OF. I may add platform data to support an older non-device-tree family of boards but it still would require I2C. I will remove this. Thanks for catching that.",208102,civil," thanks for the review!   ok   ok - will have this in v2. oops, did not mean to submit that   it was for original debugging and not needed - will remove   Yes, that makes sense. I'll propose something like the following in. right - thanks!   I am using this with thread_fn with thread_fn (vs handler).  Do you mean why use a work procedure? I guess I don't need that and can call input_report_key directly from the irq.   ok. can you point me to an example dts/driver? "," thanks for the review!   ok   ok - will have this in v2. oops, did not mean to submit that   it was for original debugging and not needed - will remove   Yes, that makes sense. I'll propose something like the following in. right - thanks!   I am using this with thread_fn with thread_fn (vs handler).  Do you mean why use a work procedure? I guess I don't need that and can call input_report_key directly from the irq.   ok. can you point me to an example dts/driver?  No, at this point it requires both I2C and OF. I may add platform data to support an older non-device-tree family of boards but it still would require I2C. I will remove this. Thanks for catching that."
46,207400,208159,civil,"Thanks for the review! oops - left that in by mistake. It has 16x ADC channels where some can be temperatures and others can be voltage inputs (based on device tree). understood - a much cleaner pattern. right - removed. yikes - thanks for catching that. ok. yes, that static arrays are not very forward-thinking and yes my arrays are not consistent. I'll convert to dynamically allocating the channels for v2. right - certainly an issue. will do. will add validation. ok. Do you mean stuffing a u32 into a u8? will fix. will fix. will do this. It could also return -EINVAL but not with the args I'm passing in so I'll change it to this. Thanks!",208144,technical,Thanks. I'll run through checkpatch prior to v2.,"Thanks. I'll run through checkpatch prior to v2. Thanks for the review! oops - left that in by mistake. It has 16x ADC channels where some can be temperatures and others can be voltage inputs (based on device tree). understood - a much cleaner pattern. right - removed. yikes - thanks for catching that. ok. yes, that static arrays are not very forward-thinking and yes my arrays are not consistent. I'll convert to dynamically allocating the channels for v2. right - certainly an issue. will do. will add validation. ok. Do you mean stuffing a u32 into a u8? will fix. will fix. will do this. It could also return -EINVAL but not with the args I'm passing in so I'll change it to this. Thanks!"
47,210458,210459,uncivil,"Wait, what?  Why would it do that, because it thinks dereferencing NULL is undefined behaviour and it can just do whatever it wants to? That feels crazy, as for these calls we ""know"" it will never be NULL because the previous call to debugfs_file_get() will always ensure it will be correct. So this is a case of the compiler trying to be smarter than it really is, and getting things totally wrong :sad:. Has anyone reported this to the clang developers? Papering over compiler foolishness is not something I like to do in kernel code if at all possible...",210458,technical,"debugfs_real_fops() returns a NULL pointer when it is invoked without a prior call to debugfs_file_get(). In code paths including this call it is not strictly necessary to check the return value of debugfs_real_fops(). However clang inlines debugfs_real_fops(), detects the invalid dereferencing of the NULL pointer and drops the code path. This leads to a bunch of objtool warnings when building with clang,  Check the pointer returned by debugfs_real_fops() in all code paths to make clang and objtool happy.  ","This function returns a NULL pointer when it is invoked without a prior call to the get function. In code paths including this call it is not strictly necessary to check the return value of the function. However clang inlines the function, detects the invalid dereferencing of the NULL pointer and drops the code path. This leads to a bunch of objtool warnings when building with clang,  Check the pointer returned by debugfs_real_fops() in all code paths to make clang and objtool happy.   Wait, what?  Why would it do that, because it thinks dereferencing NULL is undefined behaviour and it can just do whatever it wants to? That feels crazy, as for these calls we ""know"" it will never be NULL because the previous call to debugfs_file_get() will always ensure it will be correct. So this is a case of the compiler trying to be smarter than it really is, and getting things totally wrong :sad:. Has anyone reported this to the clang developers? Papering over compiler foolishness is not something I like to do in kernel code if at all possible..."
48,210458,210461,uncivil,"Why is top-posting such a bad thing? Because it messes up the order in which people normally read text. What is the most annoying thing in e-mail? Top-posting. Should I include quotations after my reply? No. Then fix the tool, the C code is correct :) Then tell clang not to do that, like we tell gcc not to do that as that is a foolish thing for a compiler to do when building the kernel.",210467,technical,"Or, as the case may be, oopsing at the point of failure.    --","Or, as the case may be, oopsing at the point of failure.    -- Why is top-posting such a bad thing? Because it messes up the order in which people normally read text. What is the most annoying thing in e-mail? Top-posting. Should I include quotations after my reply? No. Then fix the tool, the C code is correct :smile: Then tell clang not to do that, like we tell gcc not to do that as that is a foolish thing for a compiler to do when building the kernel."
49,210458,210463,uncivil,"Wait, clang does not have that?  That's crazy, how has this not been hit yet when building the kernel? Confused.",210462,technical," Thanks all for your input, we'll try to get -fno-delete-null-pointer-checks or a similar flag to be added to clang."," Thanks all for your input, we'll try to get -fno-delete-null-pointer-checks or a similar flag to be added to clang. Wait, clang does not have that?  That's crazy, how has this not been hit yet when building the kernel? Confused."
50,216128,216133,civil,"Sorry for being dense. What tree is this against? I can't find mention of amdcz in Linus's tree nor linux-next. Where does that get used where it isn't defined? (i.e. why is the #ifdef needed here?) Otherwise, sure, sounds good. :smile:",216131,technical,"The old_serial_port global array in 8250_core is supposed to hold an entry for each serial port on the system that cannot be discovered via a standard enumeration mechanism (aka these).  The array is populated at compile-time from the value specified in the SERIAL_PORT_DFNS macro. This macro is defined in arch/serial.h.  For x86, this macro is currently unconditionally initialized to supply four ioport UARTs.  However, not all x86 CPUs have these four ioport UARTs.  For example, the UARTs on AMD Carrizo and later are separate memory mapped Designware IP blocks.  Fairly early in boot the console_initcall univ8250_console_init iterates over this array and installs these old UARTs into the global array serial8250_ports.  Further, it attempts to register them for use as the console.  In other words, if, for example, the kernel commandline has console=ttyS0, the console will be switched over to one of these non-existent UARTs.  Only later, when the real UART drivers are probed and their devices are instantiated will the console switch back over to the proper UART.  This is noticeable when using earlycon, since part of the serial console log will appear to disappear (when the bogus old takes over) and then re-appear (when the real UART finally gets registered for the console).  The problem is even more noticable when *not* using earlycon, since in this case the entire console output is missing, having been incorrectly played back to the non-existing serial port.  Create a global variable to allow skipping old serial port initialization and wire it up to the quirk and the special amdcz earlycon setup handler.  ","The old serial port global array in 8250_core is supposed to hold an entry for each serial port on the system that cannot be discovered via a standard enumeration mechanism (aka these).  The array is populated at compile-time from the value specified in the serial port macro. This macro is defined in arch/serial.h.  For x86, this macro is currently unconditionally initialized to supply four ioport UARTs.  However, not all x86 CPUs have these four ioport UARTs.  For example, the UARTs on AMD Carrizo and later are separate memory mapped Designware IP blocks.  Fairly early in boot the console init call universal console iterates over this array and installs these old UARTs into the global array serial ports.  Further, it attempts to register them for use as the console.  In other words, if, for example, the kernel commandline has this condition, the console will be switched over to one of these non-existent UARTs.  Only later, when the real UART drivers are probed and their devices are instantiated will the console switch back over to the proper UART.  This is noticeable when using earlycon, since part of the serial console log will appear to disappear (when the bogus old takes over) and then re-appear (when the real UART finally gets registered for the console).  The problem is even more noticable when *not* using earlycon, since in this case the entire console output is missing, having been incorrectly played back to the non-existing serial port.  Create a global variable to allow skipping old serial port initialization and wire it up to the quirk and the special amdcz earlycon setup handler.   Sorry for being dense. What tree is this against? I can't find mention of amdcz in Linus's tree nor linux-next. Where does that get used where it isn't defined? (i.e. why is the #ifdef needed here?) Otherwise, sure, sounds good. :smile:"
51,216128,216130,civil,"Can you possibly send the entire series again and CC all patches to linux-acpi
and fix the kbuild warnings if the are relevant for that matter?",216132,technical,"Thank you for the patch! Yet something to improve:  if your patch is applied to the wrong git tree, please drop us a note to help improve the system.","Thank you for the patch! Yet something to improve:  if your patch is applied to the wrong git tree, please drop us a note to help improve the system. Can you possibly send the entire series again and CC all patches to linux-acpi and fix the kbuild warnings if the are relevant for that matter?"
52,221804,221853,uncivil,Choose one of those two. Better to keep in order. Ditto. What's wrong with dev_info()? Hmm... Can't you use devm_ioremap_resources() to get the virtual address for I/O ? When you use explicit casting in printf() you are doing in 99.9% cases something wrong. Noise.,221804,technical,Add Amiga Gayle PATA controller driver. It enables libata support for the on-board IDE interfaces on some Amiga models and also for IDE interfaces on the Zorro expansion bus (M-Tech E-Matrix 530 expansion card).  Thanks to John Paul Adrian Glaubitz and Michael Schmitz for help with testing the driver. ,Add Amiga Gayle PATA controller driver. It enables libata support for the on-board IDE interfaces on some Amiga models and also for IDE interfaces on the Zorro expansion bus (M-Tech E-Matrix 530 expansion card).  Thanks to John Paul Adrian Glaubitz and Michael Schmitz for help with testing the driver.  Choose one of those two. Better to keep in order. Ditto. What's wrong with dev_info? Hmm... Can't you use the resources function to get the virtual address for I/O ? When you use explicit casting in printf() you are doing in 99.9% cases something wrong. Noise.
53,222860,223021,uncivil,"...wait a second...this looks like it's a u-boot driver. There's a surprising amount of similarity between U-boot and Linux drivers (no coincidence I'm sure), including <linux/...> headers. Since when do U-Boot patches go to LKML and dri-devel? Anyway, I'll try my best to ignore this series.",223039,uncivil,"Hi,    How many times are we going to allow copy-and-pasting the same driver? Last time we wanted to modify the Rockchip driver, we were told consolidate"", because ST had already forked our driver. This nearly halted all progress. I'm going to be real disappointed if we see another fork get merged.  (IOW, I would say ""over my dead body,"" but I have no power here.)  And why can't you use DRM?""","How many times are we going to allow copy-and-pasting the same driver? Last time we wanted to modify the Rockchip driver, we were told consolidate"", because ST had already forked our driver. This nearly halted all progress. I'm going to be real disappointed if we see another fork get merged.  (IOW, I would say ""over my dead body,"" but I have no power here.)  And why can't you use DRM?"" ...wait a second...this looks like it's a u-boot driver. There's a surprising amount of similarity between U-boot and Linux drivers (no coincidence I'm sure), including headers. Since when do U-Boot patches go to LKML and dri-devel? Anyway, I'll try my best to ignore this series."
54,222860,230472,civil,"Can you add a commit message explaining why you add a specific defconfig for this board. FYI, previously, the same defconfig was used for all STM32F7 boards. You will also need to resync with the last master branch regarding defconfig content.",224687,technical,Does this use driver model? I cannot see it.,"Does this use driver model? I cannot see it. Can you add a commit message explaining why you add a specific defconfig for this board. FYI, previously, the same defconfig was used for all boards. You will also need to resync with the last master branch regarding defconfig content."
55,230843,231058,civil,"This is indeed guaranteed. For FTRACE use case. If it's being called from FTRACE in run time, this would mean there were long calls in this module section, which in turn means, get_module_plt() was called at least once for this module and this section. This doesn't hold in general, though. In any case, if you insist, I can try to rework the whole stuff implementing module_finalize().",231041,technical,"Right, ok. That's a problem.  This means that you are relying on get_module_plt() being called at least once at module load time, which is not guaranteed.  Instead, you should set this member (and perhaps the entire prealloc routine) in a module_finalize() implementation.","Right, ok. That's a problem.  This means that you are relying on get_module being called at least once at module load time, which is not guaranteed.  Instead, you should set this member (and perhaps the entire prealloc routine) in a module_finalize implementation. This is indeed guaranteed. For FTRACE use case. If it's being called from FTRACE in run time, this would mean there were long calls in this module section, which in turn means, get_module was called at least once for this module and this section. This doesn't hold in general, though. In any case, if you insist, I can try to rework the whole stuff implementing this function."
56,231231,232204,civil,So make it fit by returning an unsigned int.,232201,technical,"There is no limit on CPU equivalence table length in this patch series like it was in the previous version.  The maximum possible value returned by install_equiv_cpu_table() of comes from the maximum value of this 'size' variable (that is UINT_MAX) plus the header length of CONTAINER_HDR_SZ. This won't fit in 'int' type, hence this patch.  That's because this functions tells its caller how much bytes to skip from the beginning of a microcode container file to the first patch section contained in it.","There is no limit on CPU equivalence table length in this patch series like it was in the previous version.  The maximum possible value returned by install_equiv_cpu_table of comes from the maximum value of this 'size' variable (that is UINT_MAX) plus the header length of the container. This won't fit in 'int' type, hence this patch.  That's because this functions tells its caller how much bytes to skip from the beginning of a microcode container file to the first patch section contained in it. So make it fit by returning an unsigned int."
57,232313,234089,uncivil,"meta comment (i.e., not about the merits of the patch itself). You'll need to send the patch to someone if you want it to be merged. Maintainers don't mine mailing lists for patches to apply.",232313,technical,"there are 2 reasons for no need to wait device probe  reason 1: mount root device is very late in kernel initial stage. all initcalls are finished. that means most of probe functions are returned.  and deferred probe are also finished by late_initcall. only async probe driver are possible in  probing.  no block devices, device-mapper or nfs are use async probe. so no need to wait device probe.  reason 2: let's check dd.c, probe_count is increased and decreased only in really_probe, and when really_probe returns, probe_count always be 0.  when someone really wants to wait device-B probe. but code looks like this.","there are 2 reasons for no need to wait device probe  reason 1: mount root device is very late in kernel initial stage. all initcalls are finished. that means most of probe functions are returned.  and deferred probe are also finished by late_initcall. only async probe driver are possible in  probing.  no block devices, device-mapper or nfs are use async probe. so no need to wait device probe.  reason 2: let's check this file, probe_count is increased and decreased only in really_probe, and when really_probe returns, probe_count always be 0.  when someone really wants to wait device-B probe. but code looks like this. meta comment (i.e., not about the merits of the patch itself). You'll need to send the patch to someone if you want it to be merged. Maintainers don't mine mailing lists for patches to apply."
58,239101,240255,uncivil,"
They've been dropped.  BUT please do note that the patches I pushed to
linux-dm.git were rebased ontop of the 'check_at_most_once' patch.
I never did get an answer about how the sg array is free'd in certain
error paths (see ""FIXME:"" in the 2nd patch). Also, I fixed some issues I saw in error paths, and lots of formatting. I'll be pretty frustrated if you submit v2 that is blind to the kinds of
changes I made. I'll send you a private copy of the patches just so you have them for
your reference.",240077,technical,I will run veritysetup test on next version of these patches and contact you about verity-compat-test testsuits.,"I will run veritysetup test on next version of these patches and contact you about verity-compat-test testsuits. 
They've been dropped.  BUT please do note that the patches I pushed were rebased ontop of the 'check_at_most_once' patch.
I never did get an answer about how the sg array is free'd in certain error paths (see ""FIXME:"" in the 2nd patch). Also, I fixed some issues I saw in error paths, and lots of formatting. I'll be pretty frustrated if you submit v2 that is blind to the kinds of changes I made. I'll send you a private copy of the patches just so you have them for your reference."
59,239101,240572,civil,"Thank you so much for many style, formatting and other issues fixes and also for integration of 'check_at_most_once' patch, it saved me several review iterations. Regarding free of sg in two error paths, you were correct. I fixed it by placing several error labels to differentiate each handling. I also noted that reqdata_arr[b].req was not released properly, this is also fixed. following is a diff of my fix based on your modifications. (I can send it in a patch format, but it doesn't include a fix for Eric Biggers comments) I took your modifications and working upon it.",240255,uncivil,"They've been dropped.  BUT please do note that the patches I pushed to linux-dm.git were rebased ontop of the 'check_at_most_once' patch.  I never did get an answer about how the sg array is free'd in certain error paths (see FIXME:"" in the 2nd patch).  Also, I fixed some issues I saw in error paths, and lots of formatting.  I'll be pretty frustrated if you submit v2 that is blind to the kinds of changes I made.  I'll send you a private copy of the patches just so you have them for your reference.""","They've been dropped.  BUT please do note that the patches I pushed to linux-dm.git were rebased ontop of the 'check_at_most_once' patch.  I never did get an answer about how the sg array is free'd in certain error paths (see FIXME:"" in the 2nd patch).  Also, I fixed some issues I saw in error paths, and lots of formatting.  I'll be pretty frustrated if you submit v2 that is blind to the kinds of changes I made.  I'll send you a private copy of the patches just so you have them for your reference."" Thank you so much for many style, formatting and other issues fixes and also for integration of 'check_at_most_once' patch, it saved me several review iterations. Regarding free of sg in two error paths, you were correct. I fixed it by placing several error labels to differentiate each handling. I also noted that this was not released properly, this is also fixed. following is a diff of my fix based on your modifications. (I can send it in a patch format, but it doesn't include a fix for Eric Biggers comments) I took your modifications and working upon it."
60,245912,245922,uncivil,"Is this include needed ? Please use bool. I am quite completely missing how the two functions above are different. There is a lot of duplication in those functions. Would it be possible to find common code and use functions for it instead of duplicating everything several times? 
What if nothing is found? FWIW, it might be better to pass channel as parameter. What if it didn't find a core? This attribute should not exist. It is this, and this above is tjmax - tcontrol ? How does this make sense? Am I missing something, or is the same temperature reported several time?
tjmax is also reported as temp_crit cputemp_read_die(), for example. There is again a lot of duplication in those functions. Can this be made less magic with some defines ? Does this mean there will be an error message for each non-supported CPU? Why ? This is not an error, and should not result in an error message. Besides, the error can also be propagated from peci core code, and may well be something else. Then what ? Shouldn't this result in probe deferral or something more useful instead of just being ignored ? FWIW, this should be two separate patches. Needed ? It might make sense to provide the duplicate functions in a core file. This again looks like duplicate code. Please handle error cases first. More duplicate code. One set of brackets is unnecessary on each side of the expression. Why is this ""invalid"", and why does it warrant an error message? Is this guaranteed to be this? Or the peci command failed?",245963,technical,This commit adds a maintainer information for the PECI subsystem.,"This commit adds a maintainer information for the PECI subsystem. Is this include needed ? Please use bool. I am quite completely missing how the two functions above are different. There is a lot of duplication in those functions. Would it be possible to find common code and use functions for it instead of duplicating everything several times? 
What if nothing is found? FWIW, it might be better to pass channel as parameter. What if it didn't find a core? This attribute should not exist. It is this, and this above is control ? How does this make sense? Am I missing something, or is the same temperature reported several time?
tjmax is also reported as this function, for example. There is again a lot of duplication in those functions. Can this be made less magic with some defines ? Does this mean there will be an error message for each non-supported CPU? Why ? This is not an error, and should not result in an error message. Besides, the error can also be propagated from peci core code, and may well be something else. Then what ? Shouldn't this result in probe deferral or something more useful instead of just being ignored ? FWIW, this should be two separate patches. Needed ? It might make sense to provide the duplicate functions in a core file. This again looks like duplicate code. Please handle error cases first. More duplicate code. One set of brackets is unnecessary on each side of the expression. Why is this ""invalid"", and why does it warrant an error message? Is this guaranteed to be this? Or the peci command failed?"
61,245912,245953,civil,"The driver is looking good!

It looks like you've done some kind of review that we weren't allowed
to see, which is a double edged sword - I might be asking about things
that you've already spoken about with someone else.

I'm only just learning about PECI, but I do have some general comments below.


I think just saying ASPEED PECI support is enough. That way if the
next ASPEED SoC happens to have PECI we don't need to update all of
the help text :)


Nit: we use ASPEED instead of AST in the upstream kernel to distingush
from the aspeed sdk drivers. If you feel strongly about this then I
won't insist you change.


I know these come from the ASPEED sdk driver. Do we need them all?


Could the above use regmap_read_poll_timeout instead?


That looks like an endian swap. Can we do something like this?

 regmap_write(map, reg, cpu_to_be32p((void *)msg->tx_buff))


Having #defines is frowned upon. I think print_hex_dump_debug will do
what you want here.


I find this hard to read. Use a few more lines to make it clear what
your code is doing.

Actually, the entire for loop is cryptic. I understand what it's doing now. Can you rework it to make it more readable? You follow a similar pattern above in the write case. 
Given the regmap_read is always going to be a memory read on the aspeed, I can't think of a situation where the read will fail.
On that note, is there a reason you are using regmap and not just accessing the hardware directly? regmap imposes a number of pointer lookups and tests each time you do a read or write. 
Again, a memory mapped read won't fail. How about we check that the regmap is working once in your _probe() function, and assume it will continue working from there (or remove the regmap abstraction all together). All of this code is for debugging only. Do you want to put it behind some kind of conditional? We have a framework for doing clocks in the kernel. Would it make sense to write a driver for this clock and add it to this?
The property is optional so I suggest we don't print a message if it's not present. We certainly don't want to print a message saying ""invalid"". The same comment applies to the other optional properties below. Can we probe in parallel? If not, putting a sleep in the _probe will hold up the rest of drivers from being able to do anything, and hold up boot. 
If you decide that you do need to probe here, please add a comment. (This is the wait for the clock to be stable?). This interrupt is only for the peci device. Why is it marked as shared?",245922,uncivil,"Is this include needed?   Please use bool.  I am quite completely missing how the two functions above are different.   There is a lot of duplication in those functions. Would it be possible to find common code and use functions for it instead of duplicating everything several times ?   What if nothing is found ?   FWIW, it might be better to pass channel - DEFAULT_CHANNEL_NUMS as parameter.  What if find_core_index() returns this, ie if it didn't find a core ?   This attribute should not exist.   lcrit is tcontrol - tjmax, and crit_hyst above is tjmax - tcontrol ? How does this make sense ?   Am I missing something, or is the same temperature reported several times ? tjmax is also reported as temp_crit cputemp_read_die(), for example.   There is again a lot of duplication in those functions.   Can this be made less magic with some defines ?   Does this mean there will be an error message for each non-supported CPU ? Why ?   -ENODEV is not an error, and should not result in an error message. Besides, the error can also be propagated from peci core code, and may well be something else.   Then what ? Shouldn't this result in probe deferral or something more useful instead of just being ignored ?   FWIW, this should be two separate patches.   Needed?  It might make sense to provide the duplicate functions in a core file.   This again looks like duplicate code.   Please handle error cases first.   More duplicate code.   One set of brackets is unnecessary on each side of the expression.   Why is this invalid"", and why does it warrant an error message ?   Is priv->addr guaranteed to be this?  Or the peci command failed.?","Is this include needed?   Please use bool.  I am quite completely missing how the two functions above are different.   There is a lot of duplication in those functions. Would it be possible to find common code and use functions for it instead of duplicating everything several times ?   What if nothing is found ?   FWIW, it might be better to pass channel as parameter.  What if find_core_index returns this, ie if it didn't find a core ?   This attribute should not exist.  How does this make sense ?   Am I missing something, or is the same temperature reported several times ? tjmax is also reported as temp_crit cputemp_read_die(), for example.   There is again a lot of duplication in those functions.   Can this be made less magic with some defines ?   Does this mean there will be an error message for each non-supported CPU ? Why ?   -ENODEV is not an error, and should not result in an error message. Besides, the error can also be propagated from peci core code, and may well be something else. Then what ? Shouldn't this result in probe deferral or something more useful instead of just being ignored ?   FWIW, this should be two separate patches. Needed?  It might make sense to provide the duplicate functions in a core file.   This again looks like duplicate code.   Please handle error cases first. More duplicate code. One set of brackets is unnecessary on each side of the expression.   Why is this invalid"", and why does it warrant an error message?   Is this guaranteed to be this?  Or the peci command failed.? The driver is looking good!
It looks like you've done some kind of review that we weren't allowed to see, which is a double edged sword - I might be asking about things that you've already spoken about with someone else.
I'm only just learning about PECI, but I do have some general comments below. I think just saying ASPEED PECI support is enough. That way if the next ASPEED SoC happens to have PECI we don't need to update all of the help text :smile:
we use ASPEED instead of AST in the upstream kernel to distingush from the aspeed sdk drivers. If you feel strongly about this then I won't insist you change.
I know these come from the ASPEED sdk driver. Do we need them all? Could the above use timeout instead? That looks like an endian swap. Can we do something like this? Having #defines is frowned upon. I think debug will do.
what you want here. I find this hard to read. Use a few more lines to make it clear what your code is doing.
Actually, the entire for loop is cryptic. I understand what it's doing now. Can you rework it to make it more readable? You follow a similar pattern above in the write case.  Given the regmap_read is always going to be a memory read on the aspeed, I can't think of a situation where the read will fail.
On that note, is there a reason you are using regmap and not just accessing the hardware directly? regmap imposes a number of pointer lookups and tests each time you do a read or write. 
Again, a memory mapped read won't fail. How about we check that the regmap is working once in your _probe() function, and assume it will continue working from there (or remove the regmap abstraction all together). All of this code is for debugging only. Do you want to put it behind some kind of conditional? We have a framework for doing clocks in the kernel. Would it make sense to write a driver for this clock and add it to this?
The property is optional so I suggest we don't print a message if it's not present. We certainly don't want to print a message saying ""invalid"". The same comment applies to the other optional properties below. Can we probe in parallel? If not, putting a sleep in the _probe will hold up the rest of drivers from being able to do anything, and hold up boot. 
If you decide that you do need to probe here, please add a comment. (This is the wait for the clock to be stable?). This interrupt is only for the peci device. Why is it marked as shared?"
62,245912,245916,uncivil,"As per the in-kernel documentation, I am now allowed to make fun of you. You are trying to ""out smart"" the kernel by getting rid of a warning message that was explicitly put there for you to do something.  To think that by just providing an ""empty"" function you are somehow fulfilling the API requirement is quite bold, don't you think? This has to be fixed.  I didn't put that warning in there for no good reason.  Please go read the documentation again...",245939,technical,"Additionally, I need to explain that there is one and only bus host (adapter) and multiple clients on a PECI bus, and PECI spec doesn't allow multiple originators so only the host device can originate message. In this implementation, all message transactions on a bus from client driver modules and user space will be serialized well in the PECI core bus driver so bus occupation and traffic arbitration will be managed well in the PECI core bus driver even in case of a bus has 2  client drivers at the same address. I'm sure that this implementation  doesn't make that kind of problem to OS.","Additionally, I need to explain that there is one and only bus host (adapter) and multiple clients on a PECI bus, and PECI spec doesn't allow multiple originators so only the host device can originate message. In this implementation, all message transactions on a bus from client driver modules and user space will be serialized well in the PECI core bus driver so bus occupation and traffic arbitration will be managed well in the PECI core bus driver even in case of a bus has 2  client drivers at the same address. I'm sure that this implementation  doesn't make that kind of problem to OS. As per the in-kernel documentation, I am now allowed to make fun of you. You are trying to ""out smart"" the kernel by getting rid of a warning message that was explicitly put there for you to do something.  To think that by just providing an ""empty"" function you are somehow fulfilling the API requirement is quite bold, don't you think? This has to be fixed.  I didn't put that warning in there for no good reason.  Please go read the documentation again..."
63,254985,255085,uncivil,"Please do not repost with such a small changes. It is much more important to sort out the big picture first and only then deal with minor implementation details. The more versions you post the more fragmented and messy the discussion will become.
You will have to be patient because this is a rather big change and it will take _quite_ some time to get sorted.",254985,technical,"Page replacement is handled in the Linux Kernel in one of two ways:  1) Asynchronously via kswapd 2) Synchronously, via direct reclaim  At page allocation time the allocating task is immediately given a page from the zone free list allowing it to go right back to work doing whatever it was doing, Probably directly or indirectly executing business logic.  Just prior to satisfying the allocation, free pages is checked to see if it has reached the zone low watermark and if so, kswapd is awakened. Kswapd will start scanning pages looking for inactive pages to evict to make room for new page allocations. The work of kswapd allows tasks to continue allocating memory from their respective zone free list without incurring any delay.  When the demand for free pages exceeds the rate that kswapd tasks can supply them, page allocation works differently. Once the allocating task finds that the number of free pages is at or below the zone min watermark, the task will no longer pull pages from the free list. Instead, the task will run the same CPU-bound routines as kswapd to satisfy its own allocation by scanning and evicting pages. This is called a direct reclaim.  The time spent performing a direct reclaim can be substantial, often taking tens to hundreds of milliseconds for small order0 allocations to half a second or more for order9 huge-page allocations. In fact, kswapd is not actually required on a linux system. It exists for the sole purpose of optimizing performance by preventing direct reclaims.  When memory shortfall is sufficient to trigger direct reclaims, they can occur in any task that is running on the system. A single aggressive memory allocating task can set the stage for collateral damage to occur in small tasks that rarely allocate additional memory. Consider the impact of injecting an additional 100ms of latency when nscd allocates memory to facilitate caching of a DNS query.  The presence of direct reclaims 10 years ago was a fairly reliable indicator that too much was being asked of a Linux system. Kswapd was likely wasting time scanning pages that were ineligible for eviction. Adding RAM or reducing the working set size would usually make the problem go away. Since then hardware has evolved to bring a new struggle for kswapd. Storage speeds have increased by orders of magnitude while CPU clock speeds stayed the same or even slowed down in exchange for more cores per package. This presents a throughput problem for a single threaded kswapd that will get worse with each generation of new hardware.  Test Details  NOTE: The tests below were run with shadow entries disabled. See the associated patch and cover letter for details  The tests below were designed with the assumption that a kswapd bottleneck is best demonstrated using filesystem reads. This way, the inactive list will be full of clean pages, simplifying the analysis and allowing kswapd to achieve the highest possible steal rate. Maximum steal rates for kswapd are likely to be the same or lower for any other mix of page types on the system.  Tests were run on a 2U Oracle X7-2L with 52 Intel Xeon Skylake 2GHz cores, 756GB of RAM and 8 x 3.6 TB NVMe Solid State Disk drives. Each drive has an XFS file system mounted separately as /d0 through /d7. SSD drives require multiple concurrent streams to show their potential, so I created 11 250GB zero-filled files on each drive so that I could test with parallel reads.  The test script runs in multiple stages. At each stage, the number of dd tasks run concurrently is increased by 2. I did not include all of the test output for brevity.  During each stage dd tasks are launched to read from each drive in a round robin fashion until the specified number of tasks for the stage has been reached. Then iostat, vmstat and top are started in the background with 10 second intervals. After five minutes, all of the dd tasks are killed and the iostat, vmstat and top output is parsed in order to report the following:  CPU consumption - sy - aggregate kernel mode CPU consumption from vmstat output. The value doesn't tend to fluctuate much so I just grab the highest value. Each sample is averaged over 10 seconds - dd_cpu - for all of the dd tasks averaged across the top samples since there is a lot of variation.  Throughput - in Kbytes - Command is total  This first test performs reads using O_DIRECT in order to show the maximum throughput that can be obtained using these drives. It also demonstrates how rapidly throughput scales as the number of dd tasks are increased.  The dd command for this test looks like this> Throughput was close to peak with only 22 dd tasks. Very little system CPU was consumed as expected as the drives DMA directly into the user address space when using direct IO.  In this next test, the iflag=direct option is removed and we only run the test until the pgscan_kswapd from /proc/vmstat starts to increment. At that point metrics are parsed and reported and the pagecache contents are dropped prior to the next test. Lather, rinse, repeat.  Each read has to pause after the buffer in kernel space is populated while those pages are added to the pagecache and copied into the user address space. For this reason, more parallel streams are required to achieve peak throughput. The copy operation consumes substantially more CPU than direct IO as expected.  The next test measures throughput after kswapd starts running. This is the same test only we wait for kswapd to wake up before we start collecting metrics. The script actually keeps track of a few things that were not mentioned earlier. It tracks direct reclaims and page scans by watching the metrics in /proc/vmstat. CPU consumption for kswapd is tracked the same way it is tracked for dd.  Since the test is 100% reads, you can assume that the page steal rate for kswapd and direct reclaims is almost identical to the scan rate. In the previous test where kswapd was not involved, the system-wide kernel mode CPU consumption with 90 dd tasks was 16%. In this test CPU consumption with 90 tasks is at 43%. With 52 cores, and two kswapd tasks (one per NUMA node), kswapd can only be responsible for a little over 4% of the increase. The rest is likely caused by 51,618 direct reclaims that scanned 1.2 billion pages over the five minute time period of the test.  Same test, more kswapd tasks:  By increasing the number of kswapd threads, throughput increased by ~50% while kernel mode CPU utilization decreased or stayed the same, likely due to a decrease in the number of parallel tasks at any given time doing page replacement. allows you to control the number of kswapd threads per node running on the system. This provides the ability to devote additional CPU resources toward proactive page replacement with the goal of reducing direct reclaims. When direct reclaims are prevented, the CPU consumed by them is prevented as well. Depending on the workload, the result can cause aggregate CPU usage on the system to go up, down or stay the same.  More aggressive page replacement can reduce direct reclaims which cause latency for tasks and decrease throughput when doing filesystem IO through the pagecache. Direct reclaims are recorded using the allocstall counter this. The default value is 1 and the range of acceptible values are 1-16. Always start with lower values in the 2-6 range. Higher values should be justified with testing. If direct reclaims occur in spite of high values, the cost of direct reclaims (in latency) that occur can be higher due to increased lock contention. ","Page replacement is handled in the Linux Kernel in one of two ways:  1) Asynchronously via kswapd 2) Synchronously, via direct reclaim  At page allocation time the allocating task is immediately given a page from the zone free list allowing it to go right back to work doing whatever it was doing, Probably directly or indirectly executing business logic.  Just prior to satisfying the allocation, free pages is checked to see if it has reached the zone low watermark and if so, kswapd is awakened. Kswapd will start scanning pages looking for inactive pages to evict to make room for new page allocations. The work of kswapd allows tasks to continue allocating memory from their respective zone free list without incurring any delay.  When the demand for free pages exceeds the rate that kswapd tasks can supply them, page allocation works differently. Once the allocating task finds that the number of free pages is at or below the zone min watermark, the task will no longer pull pages from the free list. Instead, the task will run the same CPU-bound routines as kswapd to satisfy its own allocation by scanning and evicting pages. This is called a direct reclaim.  The time spent performing a direct reclaim can be substantial, often taking tens to hundreds of milliseconds for small order0 allocations to half a second or more for order9 huge-page allocations. In fact, kswapd is not actually required on a linux system. It exists for the sole purpose of optimizing performance by preventing direct reclaims.  When memory shortfall is sufficient to trigger direct reclaims, they can occur in any task that is running on the system. A single aggressive memory allocating task can set the stage for collateral damage to occur in small tasks that rarely allocate additional memory. Consider the impact of injecting an additional 100ms of latency when nscd allocates memory to facilitate caching of a DNS query.  The presence of direct reclaims 10 years ago was a fairly reliable indicator that too much was being asked of a Linux system. Kswapd was likely wasting time scanning pages that were ineligible for eviction. Adding RAM or reducing the working set size would usually make the problem go away. Since then hardware has evolved to bring a new struggle for kswapd. Storage speeds have increased by orders of magnitude while CPU clock speeds stayed the same or even slowed down in exchange for more cores per package. This presents a throughput problem for a single threaded kswapd that will get worse with each generation of new hardware.  Test Details  NOTE: The tests below were run with shadow entries disabled. See the associated patch and cover letter for details  The tests below were designed with the assumption that a kswapd bottleneck is best demonstrated using filesystem reads. This way, the inactive list will be full of clean pages, simplifying the analysis and allowing kswapd to achieve the highest possible steal rate. Maximum steal rates for kswapd are likely to be the same or lower for any other mix of page types on the system.  Tests were run on a 2U Oracle X7-2L with 52 Intel Xeon Skylake 2GHz cores, 756GB of RAM and 8 x 3.6 TB NVMe Solid State Disk drives. Each drive has an XFS file system mounted separately as /d0 through /d7. SSD drives require multiple concurrent streams to show their potential, so I created 11 250GB zero-filled files on each drive so that I could test with parallel reads.  The test script runs in multiple stages. At each stage, the number of dd tasks run concurrently is increased by 2. I did not include all of the test output for brevity.  During each stage dd tasks are launched to read from each drive in a round robin fashion until the specified number of tasks for the stage has been reached. Then iostat, vmstat and top are started in the background with 10 second intervals. After five minutes, all of the dd tasks are killed and the iostat, vmstat and top output is parsed in order to report the following:  CPU consumption - sy - aggregate kernel mode CPU consumption from vmstat output. The value doesn't tend to fluctuate much so I just grab the highest value. Each sample is averaged over 10 seconds for all of the dd tasks averaged across the top samples since there is a lot of variation.  Throughput - in Kbytes - Command is total  This first test performs reads using O_DIRECT in order to show the maximum throughput that can be obtained using these drives. It also demonstrates how rapidly throughput scales as the number of dd tasks are increased.  The dd command for this test looks like this. Throughput was close to peak with only 22 dd tasks. Very little system CPU was consumed as expected as the drives DMA directly into the user address space when using direct IO.  In this next test, the iflag=direct option is removed and we only run the test until this starts to increment. At that point metrics are parsed and reported and the pagecache contents are dropped prior to the next test. Lather, rinse, repeat.  Each read has to pause after the buffer in kernel space is populated while those pages are added to the pagecache and copied into the user address space. For this reason, more parallel streams are required to achieve peak throughput. The copy operation consumes substantially more CPU than direct IO as expected.  The next test measures throughput after kswapd starts running. This is the same test only we wait for kswapd to wake up before we start collecting metrics. The script actually keeps track of a few things that were not mentioned earlier. It tracks direct reclaims and page scans by watching the metrics in /proc/vmstat. CPU consumption for kswapd is tracked the same way it is tracked for dd.  Since the test is 100% reads, you can assume that the page steal rate for kswapd and direct reclaims is almost identical to the scan rate. In the previous test where kswapd was not involved, the system-wide kernel mode CPU consumption with 90 dd tasks was 16%. In this test CPU consumption with 90 tasks is at 43%. With 52 cores, and two kswapd tasks (one per NUMA node), kswapd can only be responsible for a little over 4% of the increase. The rest is likely caused by 51,618 direct reclaims that scanned 1.2 billion pages over the five minute time period of the test.  Same test, more kswapd tasks:  By increasing the number of kswapd threads, throughput increased by about 50% while kernel mode CPU utilization decreased or stayed the same, likely due to a decrease in the number of parallel tasks at any given time doing page replacement. allows you to control the number of kswapd threads per node running on the system. This provides the ability to devote additional CPU resources toward proactive page replacement with the goal of reducing direct reclaims. When direct reclaims are prevented, the CPU consumed by them is prevented as well. Depending on the workload, the result can cause aggregate CPU usage on the system to go up, down or stay the same.  More aggressive page replacement can reduce direct reclaims which cause latency for tasks and decrease throughput when doing filesystem IO through the pagecache. Direct reclaims are recorded using the allocstall counter this. The default value is 1 and the range of acceptible values are 1-16. Always start with lower values in the 2-6 range. Higher values should be justified with testing. If direct reclaims occur in spite of high values, the cost of direct reclaims (in latency) that occur can be higher due to increased lock contention.  Please do not repost with such a small changes. It is much more important to sort out the big picture first and only then deal with minor implementation details. The more versions you post the more fragmented and messy the discussion will become.
You will have to be patient because this is a rather big change and it will take _quite_ some time to get sorted."
64,258997,259457,uncivil,"Pulled, and then immediately unpulled again. The code causes new compiler warnings, and the warnings are valid. If people don't care enough about their code to even check the warnings, I'm not going to waste one second pulling the resulting garbage. It's that simple.",258997,technical,Please pull from here  to receive the latest Thermal Management updates with top-most commit?  Merge branches 'thermal-core' and 'thermal-soc' into next on top of commit: Specifics:  Fix race condition in imx_thermal_probe().  Add cooling device's statistics in sysfs.  add support for i.MX7 thermal sensor in imx_thermal driver. add support for MT7622 SoC in mtk_thermal driver. Remove unused min/max cpu cooling DT property. A series of fixes on exynos driver. ,"Please pull from here  to receive the latest Thermal Management updates with top-most commit?  Merge branches 'thermal-core' and 'thermal-soc' into next on top of commit: Specifics:  Fix race condition in thermal_probe.  Add cooling device's statistics in sysfs.  add support for i.MX7 thermal sensor in imx_thermal driver. add support for MT7622 SoC in thermal driver. Remove unused min/max cpu cooling DT property. A series of fixes on exynos driver.  Pulled, and then immediately unpulled again. The code causes new compiler warnings, and the warnings are valid. If people don't care enough about their code to even check the warnings, I'm not going to waste one second pulling the resulting garbage. It's that simple."
65,258997,259513,civil,I'm really sorry for this. could you please illustrate me what the kconfig & warning is? I didn't get such warnings from 0-day.,259457,uncivil,"Pulled, and then immediately unpulled again.  The code causes new compiler warnings, and the warnings are valid.  If people don't care enough about their code to even check the warnings, I'm not going to waste one second pulling the resulting garbage. It's that simple.","Pulled, and then immediately unpulled again.  The code causes new compiler warnings, and the warnings are valid.  If people don't care enough about their code to even check the warnings, I'm not going to waste one second pulling the resulting garbage. It's that simple. I'm really sorry for this. could you please illustrate me what the kconfig & warning is? I didn't get such warnings from 0-day."
66,258997,260177,uncivil,Could you please just merge the obvious fix from Arnd instead? [ it was posted two weeks ago and ACKed by me ],260171,technical,Since this condition cannot happen (the driver makes sure of this during probe) I would prefer much simpler fix from Arnd. (I've already ACKed it two weeks ago).   ditto,Since this condition cannot happen (the driver makes sure of this during probe) I would prefer much simpler fix from Arnd. (I've already ACKed it two weeks ago). ditto. Could you please just merge the obvious fix from Arnd instead? [ it was posted two weeks ago and ACKed by me ]
67,258997,260189,uncivil,The init function is making sure cal_type is one or another. Can you fix it correctly by replacing the 'switch' by a 'if' instead of adding dead branches to please gcc?,260184,technical,It is okay to return 0 because this code-path (the default one) will be never hit by the driver (probe makes sure of it) - the default case is here is just to silence compilation errors..,It is okay to return 0 because this code-path (the default one) will be never hit by the driver (probe makes sure of it) - the default case is here is just to silence compilation errors.. The init function is making sure cal_type is one or another. Can you fix it correctly by replacing the 'switch' by a 'if' instead of adding dead branches to please gcc?
68,258997,260193,civil,I'm not the one that added this switch statement (it has been there since 2011) and I would be happy to remove it.  However could we please defer this to v4.17 and merge the current set of Exynos thermal fixes/cleanups (they simplify the driver a lot and make ground for future changes)?,260189,uncivil,The init function is making sure cal_type is one or another. Can you fix it correctly by replacing the 'switch' by a 'if' instead of adding dead branches to please gcc?,The init function is making sure cal_type is one or another. Can you fix it correctly by replacing the 'switch' by a 'if' instead of adding dead branches to please gcc? I'm not the one that added this switch statement (it has been there since 2011) and I would be happy to remove it.  However could we please defer this to v4.17 and merge the current set of Exynos thermal fixes/cleanups (they simplify the driver a lot and make ground for future changes)?
69,259276,259299,uncivil,"Did you actually test this?  The usual reason for wanting m/udelay is that the timing must be exact.  The driver is filled with mdelay()s for this reason.  The one you've picked on is in the init path so it won't affect the runtime in any way.  I also don't think we have the hrtimer machinery for usleep_range() to work properly on parisc, so I don't think the replacement works.",259276,technical,"de4x5_hw_init() is never called in atomic context.  de4x5_hw_init() is only called by de4x5_pci_probe(), which is only  set as .probe"" in struct pci_driver.  Despite never getting called from atomic context, de4x5_hw_init()  calls mdelay() to busily wait. This is not necessary and can be replaced with usleep_range() to  avoid busy waiting.  This is found by a static analysis tool named DCNS written by myself. And I also manually check it.","This function is never called in atomic context.  It is only called by this function, which is only  set as .probe"" in struct pci_driver.  Despite never getting called from atomic context, it calls mdelay() to busily wait. This is not necessary and can be replaced with sleep_range() to  avoid busy waiting.  This is found by a static analysis tool named DCNS written by myself. And I also manually check it. Did you actually test this?  The usual reason for wanting m/udelay is that the timing must be exact.  The driver is filled with mdelay()s for this reason.  The one you've picked on is in the init path so it won't affect the runtime in any way.  I also don't think we have the hrtimer machinery for usleep_range() to work properly on parisc, so I don't think the replacement works."
70,259276,259489,civil,"Thanks for your reply :smile:
I admit I am not familiar with this driver. I did not know this driver is only loaded during system boot-up time, I thought this driver can be loaded as a kernel module (like many drivers) after system booting. After knowing this, I admit my patch is not proper, sorry...",259487,technical,"James is right, You have added all usleep_range() during system boot-up  time. During boot-up system will run as single threaded. Where this change will not make much sense. System first priority is match the exact timing on each and every boot-up.  ","James is right, You have added all usleep_range() during system boot-up  time. During boot-up system will run as single threaded. Where this change will not make much sense. System first priority is match the exact timing on each and every boot-up.   Thanks for your reply :smile:
I admit I am not familiar with this driver. I did not know this driver is only loaded during system boot-up time, I thought this driver can be loaded as a kernel module (like many drivers) after system booting. After knowing this, I admit my patch is not proper, sorry..."
71,261377,261389,uncivil,"This doesn't have to be on separate lines; as written, it just causes confusion. Good find, but your patch is corrupted to the point where any attenpt to fix it up on my side failed. Please resend without corruption, and please provide a Fixes: line.",261377,technical,Upstream commit  Make calibration register value fixed  makes ina2xx_set_shunt() call mutex_lock on an un-initialized mutex. Initialize it prior so we don't get a NULL pointer dereference error  ,"Upstream commit. Make calibration register value fixed  makes this functiion call mutex_lock on an un-initialized mutex. Initialize it prior so we don't get a NULL pointer dereference error. This doesn't have to be on separate lines; as written, it just causes confusion. Good find, but your patch is corrupted to the point where any attenpt to fix it up on my side failed. Please resend without corruption, and please provide a Fixes: line."
72,261866,263843,uncivil,"Sorry, but this is a hack to *try* to make multi-slot work and this isn't sufficient. There were good reasons to why the earlier non-working multi slot support was removed from dw_mmc.
Let me elaborate a bit for your understanding. The core uses a host lock to serialize operations and commands, as to confirm to the SD/SDIO/(e)MMC specs. The above changes gives no guarantees for this. To make that work, we would need a ""mmc bus lock"" to be managed by the core. However, inventing a ""mmc bus lock"" would lead to other problems related to I/O scheduling for upper layers - it simply breaks. For example, I/O requests for one card/slot can then starve I/O requests reaching another card/slot.",261868,technical,This patch adds missing stuff to support multislot mode in DesignWare MMC driver.  The main changes:  Add missing slot switch to __dw_mci_start_request() function. Refactor set_ios function:    a) Calculate common clock which is suitable for all slots instead of directly use clock value provided by mmc core. We calculate common clock as the minimum among each used slot clocks. This clock is calculated in       dw_mci_calc_common_clock() function which is called from set_ios(). b) Disable clock only if no other slots are ON.  c) Setup clock directly in set_ios() only if no other slots       are ON. Otherwise adjust clock in __dw_mci_start_request() function before slot switch.    d) Move timings and bus_width setup to separate funcions.  Use timing field in each slot structure instead of common field in    host structure.  Add locks to serialize access to registers.  NOTE: this patch is based off of v4.17-rc1  NOTE: as of today I tested this changes (in singleslot and multislot    modes) only on Synopsys HSDK board. But I will get ODROID-XU4 board (with Exynos5422 which has DW MMC controller) the next week so I will test it on this board too to catch any regressions. ,"This patch adds missing stuff to support multislot mode in DesignWare MMC driver.  The main changes:  Add missing slot switch to this function. Refactor set_ios function:    a) Calculate common clock which is suitable for all slots instead of directly use clock value provided by mmc core. We calculate common clock as the minimum among each used slot clocks. This clock is calculated in this function which is called from this b) Disable clock only if no other slots are ON.  c) Setup clock directly in this only if no other slots are ON. Otherwise adjust clock in this function before slot switch. d) Move timings and bus_width setup to separate funcions.  Use timing field in each slot structure instead of common field in    host structure.  Add locks to serialize access to registers.  NOTE: this patch is based off of this version NOTE: as of today I tested this changes (in singleslot and multislot    modes) only on Synopsys HSDK board. But I will get ODROID-XU4 board (with Exynos5422 which has DW MMC controller) the next week so I will test it on this board too to catch any regressions.  Sorry, but this is a hack to *try* to make multi-slot work and this isn't sufficient. There were good reasons to why the earlier non-working multi slot support was removed from this.
Let me elaborate a bit for your understanding. The core uses a host lock to serialize operations and commands, as to confirm to the specs. The above changes gives no guarantees for this. To make that work, we would need a ""mmc bus lock"" to be managed by the core. However, inventing a ""mmc bus lock"" would lead to other problems related to I/O scheduling for upper layers - it simply breaks. For example, I/O requests for one card/slot can then starve I/O requests reaching another card/slot."
73,266017,266025,uncivil,What actually took so long?  Could you analyze further instead of blindly putting the flag?,266017,technical,"On an ASRock E350M1, with Linux 4.17-rc1 according to `initcall_debug` calling `azx_driver_init` takes sometimes more than a few milliseconds, and up to 200 ms.  returned 0 after 49195 usecs ```  Trying to execute the Linux kernel in less than 500 ms, this is quite a hold-up, and therefore request the probe from an async task.  With this change, the test shows, that the function returns earlier.  The same behavior is visible on a Dell OptiPlex 7010. The longer times seem to happen, when the module *e1000e* is probed during the same time.  ","On an ASRock E350M1, with Linux 4.17-rc1 according to `initcall_debug` calling this function takes sometimes more than a few milliseconds, and up to 200 ms.  returned 0 after 49195 usecs. Trying to execute the Linux kernel in less than 500 ms, this is quite a hold-up, and therefore request the probe from an async task.  With this change, the test shows, that the function returns earlier.  The same behavior is visible on a Dell OptiPlex 7010. The longer times seem to happen, when the module *e1000e* is probed during the same time.   What actually took so long?  Could you analyze further instead of blindly putting the flag?"
74,266017,266029,civil,"Well, I am not sure. Could you please give me hints, how to debug this further? Is there some debug flag? I am only aware of the Ftrace framework, but in my experience it also skews the timings quite a bit, so might not be the best choice.",266025,uncivil,What actually took so long?  Could you analyze further instead of blindly putting the flag?,"What actually took so long?  Could you analyze further instead of blindly putting the flag? Well, I am not sure. Could you please give me hints, how to debug this further? Is there some debug flag? I am only aware of the Ftrace framework, but in my experience it also skews the timings quite a bit, so might not be the best choice."
75,266279,266313,uncivil,"Please enlighten me: how do you think this could be exploited? When an application calls VIDIOC_ENUM_FMT from a video0 device, it will just enumerate a hardware functionality, with is constant for a given hardware piece. The way it works is that userspace do something like this in order to read an entire const table. Usually, it doesn't require any special privilege to call this ioctl, but, even if someone changes its permission to 0x400, a simple lsusb output is enough to know what hardware model is there. A lsmod or cat also tells that the tm6000 module was loaded, with is a very good hint that the tm6000 is there or was there in the past.

In the specific case of tm6000, all hardware supports exactly the same formats, as this is usually defined per-driver. So, a quick look at the driver is enough to know exactly what the ioctl would answer.  Also, the net is full of other resources that would allow anyone to get the supported formats for a piece of hardware. Even assuming that the OS doesn't have lsusb, that /proc is not mounted, that video require special permissions, that the potential attacker doesn't have physical access to the equipment (in order to see if an USB board is plugged), etc... What possible harm he could do by identifying a hardware feature? Similar notes for the other patches to drivers/media in this series: let's not just start adding bloatware where not needed. Please notice that I'm fine if you want to submit potential Spectre variant 1 fixups, but if you're willing to do so, please provide an explanation about the potential threat scenarios that you're identifying at the code. It probably makes sense to have somewhere at smatch a place where we could explicitly mark the false-positives, in order to avoid use to receive patches that would just add an extra delay where it is not needed. ",266296,technical,"code->index can be controlled by user-space, hence leading to a potential exploitation of the Spectre variant 1 vulnerability.  Smatch warning: Fix this by sanitizing code->index before using it to index codes.  Notice that given that speculation windows are large, the policy is to kill the speculation on the first load and not worry if it can be completed with a dependent load/store","this index can be controlled by user-space, hence leading to a potential exploitation of the Spectre variant 1 vulnerability.  Smatch warning: Fix this by sanitizing it before using it to index codes.  Notice that given that speculation windows are large, the policy is to kill the speculation on the first load and not worry if it can be completed with a dependent load/store Please enlighten me: how do you think this could be exploited? When an application calls this from a video0 device, it will just enumerate a hardware functionality, with is constant for a given hardware piece. The way it works is that userspace do something like this in order to read an entire const table. Usually, it doesn't require any special privilege to call this ioctl, but, even if someone changes its permission to 0x400, a simple lsusb output is enough to know what hardware model is there. A lsmod or cat also tells that the tm6000 module was loaded, with is a very good hint that the tm6000 is there or was there in the past.

In the specific case of tm6000, all hardware supports exactly the same formats, as this is usually defined per-driver. So, a quick look at the driver is enough to know exactly what the ioctl would answer.  Also, the net is full of other resources that would allow anyone to get the supported formats for a piece of hardware. Even assuming that the OS doesn't have lsusb, that /proc is not mounted, that video require special permissions, that the potential attacker doesn't have physical access to the equipment (in order to see if an USB board is plugged), etc... What possible harm he could do by identifying a hardware feature? Similar notes for the other patches to drivers/media in this series: let's not just start adding bloatware where not needed. Please notice that I'm fine if you want to submit potential Spectre variant 1 fixups, but if you're willing to do so, please provide an explanation about the potential threat scenarios that you're identifying at the code. It probably makes sense to have somewhere at smatch a place where we could explicitly mark the false-positives, in order to avoid use to receive patches that would just add an extra delay where it is not needed. "
76,266279,266331,civil,I see I've missed some obvious things that you've pointed out here. I'll mark these warnings as False Positives and take your points into account for the analysis of the rest of the Spectre issues reported by Smatch. Sorry for the noise and thanks for the feedback.,266313,uncivil,"Please enlighten me: how do you think this could be exploited?  When an application calls this from a video0 device, it will just enumerate a hardware functionality, with is constant for a given hardware piece.  The way it works is that userspace do something like:   in order to read an entire const table.  Usually, it doesn't require any special privilege to call this ioctl, but, even if someone changes its permission to 0x400, a simple lsusb output is enough to know what hardware model is there. A lsmod or cat /proc/modules) also tells that the tm6000 module was loaded, with is a very good hint that the tm6000 is there or was there in the past.  In the specific case of tm6000, all hardware supports exactly the same formats, as this is usually defined per-driver. So, a quick look at the driver is enough to know exactly what the ioctl would answer.  Also, the net is full of other resources that would allow anyone to get the supported formats for a piece of hardware.  Even assuming that the OS doesn't have lsusb, that /proc is not mounted, that video0 require special permissions, that the potential attacker doesn't have physical access to the equipment (in order to see if an USB board is plugged), etc... What possible harm he could do by identifying a hardware feature?  Similar notes for the other patches to drivers/media in this series: let's not just start adding bloatware where not needed.  Please notice that I'm fine if you want to submit potential Spectre variant 1 fixups, but if you're willing to do so, please provide an explanation about the potential threat scenarios that you're identifying at the code.  Dan,  It probably makes sense to have somewhere at smatch a place where we could explicitly mark the false-positives, in order to avoid use to receive patches that would just add an extra delay where it is not needed.""","Please enlighten me: how do you think this could be exploited?  When an application calls this from a video0 device, it will just enumerate a hardware functionality, with is constant for a given hardware piece.  The way it works is that userspace do something like:   in order to read an entire const table.  Usually, it doesn't require any special privilege to call this ioctl, but, even if someone changes its permission to 0x400, a simple lsusb output is enough to know what hardware model is there. A lsmod or cat /proc/modules) also tells that the tm6000 module was loaded, with is a very good hint that the tm6000 is there or was there in the past.  In the specific case of tm6000, all hardware supports exactly the same formats, as this is usually defined per-driver. So, a quick look at the driver is enough to know exactly what the ioctl would answer.  Also, the net is full of other resources that would allow anyone to get the supported formats for a piece of hardware.  Even assuming that the OS doesn't have lsusb, that /proc is not mounted, that video0 require special permissions, that the potential attacker doesn't have physical access to the equipment (in order to see if an USB board is plugged), etc... What possible harm he could do by identifying a hardware feature?  Similar notes for the other patches to drivers/media in this series: let's not just start adding bloatware where not needed.  Please notice that I'm fine if you want to submit potential Spectre variant 1 fixups, but if you're willing to do so, please provide an explanation about the potential threat scenarios that you're identifying at the code.  Dan,  It probably makes sense to have somewhere at smatch a place where we could explicitly mark the false-positives, in order to avoid use to receive patches that would just add an extra delay where it is not needed."" I see I've missed some obvious things that you've pointed out here. I'll mark these warnings as False Positives and take your points into account for the analysis of the rest of the Spectre issues reported by Smatch. Sorry for the noise and thanks for the feedback."
77,266279,266334,civil,"Please, drop this series. Further analysis is required as it seems all these are False Positives. Sorry for the noise.",266331,civil,I see I've missed some obvious things that you've pointed out here. I'll  mark these warnings as False Positives and take your points into account  for the analysis of the rest of the Spectre issues reported by Smatch.  Sorry for the noise and thanks for the feedback.,"I see I've missed some obvious things that you've pointed out here. I'll  mark these warnings as False Positives and take your points into account  for the analysis of the rest of the Spectre issues reported by Smatch.  Sorry for the noise and thanks for the feedback. Please, drop this series. Further analysis is required as it seems all these are False Positives. Sorry for the noise."
78,266279,267299,civil,"Thanks for a comprehensive explanation about that. It now makes more sense to me. Yeah, better to apply a fix to avoid the issue with VIDIOC_ENUM_FMT.  Btw, on almost all media drivers, the implementation for enumerating the supported formats are the same (and we have a few other VIDOC_ENUM_foo ioctls that usually do similar stuff): the V4L2 core calls a driver, with looks into an array, returning the results to the core.
So, a fix like that should likely go to almost all media drivers (there are a lot of them!), and, for every new one, to take care to avoid introducing it again during patch review process.
So, I'm wondering if are there any way to mitigate it inside the core itself, instead of doing it on every driver, e. g. changing v4l_enum_fmt() implementation at v4l2-ioctl. Ok, a ""poor man"" approach would be to pass the array directly to the core and let the implementation there to implement the array fetch logic, calling array_index_nospec() there, but I wonder if are there any other way that won't require too much code churn.",266764,technical,"Just had a better look at v4l_fill_fmtdesc() and actually read the comment. The code cannot be compiled as a array because it is big and sparse. But the log(n) condition tree is a prime candidate for the branchscope side-channel, which would be able to reconstruct a significant number of bits of the original value. A denser tree gives more bits etc.","Just had a better look at the function and actually read the comment. The code cannot be compiled as a array because it is big and sparse. But the log(n) condition tree is a prime candidate for the branchscope side-channel, which would be able to reconstruct a significant number of bits of the original value. A denser tree gives more bits etc. Thanks for a comprehensive explanation about that. It now makes more sense to me. Yeah, better to apply a fix to avoid the issue with this.  Btw, on almost all media drivers, the implementation for enumerating the supported formats are the same (and we have a few other functions that usually do similar stuff): the V4L2 core calls a driver, with looks into an array, returning the results to the core.
So, a fix like that should likely go to almost all media drivers (there are a lot of them!), and, for every new one, to take care to avoid introducing it again during patch review process.
So, I'm wondering if are there any way to mitigate it inside the core itself, instead of doing it on every driver, e. g. changing v4l_enum_fmt() implementation at v4l2-ioctl. Ok, a ""poor man"" approach would be to pass the array directly to the core and let the implementation there to implement the array fetch logic, calling array_index_nospec() there, but I wonder if are there any other way that won't require too much code churn."
79,266918,267111,civil,"please don't submit such a huge number of patches all at one time. Also, please fix the indentation of the functions whose arguments span multiple lines as has been pointed out to you in patch feedback. Finally, make this a true patch series.  It is so much easier for maintainers to work with a set of changes all doing the same thing if you make them a proper patch series with an appropriate ""[PATCH 0/N] ..."" header posting.",266918,technical,"The method ndo_start_xmit() is defined as returning an 'netdev_tx_t', which is a typedef for an enum type, but the implementation in this driver returns an 'int'.  Fix this by returning 'netdev_tx_t' in this driver too.","The method is defined as returning this, which is a typedef for an enum type, but the implementation in this driver returns an 'int'.  Fix this by returning this in this driver too. please don't submit such a huge number of patches all at one time. Also, please fix the indentation of the functions whose arguments span multiple lines as has been pointed out to you in patch feedback. Finally, make this a true patch series.  It is so much easier for maintainers to work with a set of changes all doing the same thing if you make them a proper patch series with an appropriate patch header posting."
80,266918,269136,uncivil,"I suppose these sort of patches are as much a PITA for the sender
than for the receivers. I hesitated between a single patch, a series or separated patches. In a sense, the single patch would have been the easier for both sides but I guessed it would not have been very well welcomed. Since for a series, you're supposed to CC the whole series to everyone involved, it would have been, or at least at thought so, maximaly noisy for no good reasons. Finally, as all of these patches are totally independent, I thought it would be the best to send them as separated patches, each drivers maintainers being then free to accept, reject or ignore the patch(es) concerning him/her. It seems it was a bad guess, and yes, I see the point of having a series for this. I'll remember all this for the next time (if next time there is, of course, I was already quite hesitant to spend time to prepare and send patches for these issues with enum/integer mix-up). Sorry for the annoyance.",267111,civil,"please don't submit such a huge number of patches all at one time.  Also, please fix the indentation of the functions whose arguments span multiple lines as has been pointed out to you in patch feedback.  Finally, make this a true patch series.  It is so much easier for maintainers to work with a set of changes all doing the same thing if you make them a proper patch series with an appropriate patch ..."" header posting.""","please don't submit such a huge number of patches all at one time.  Also, please fix the indentation of the functions whose arguments span multiple lines as has been pointed out to you in patch feedback.  Finally, make this a true patch series.  It is so much easier for maintainers to work with a set of changes all doing the same thing if you make them a proper patch series with an appropriate patch ..."" header posting."" I suppose these sort of patches are as much a PITA for the sender
than for the receivers. I hesitated between a single patch, a series or separated patches. In a sense, the single patch would have been the easier for both sides but I guessed it would not have been very well welcomed. Since for a series, you're supposed to CC the whole series to everyone involved, it would have been, or at least at thought so, maximaly noisy for no good reasons. Finally, as all of these patches are totally independent, I thought it would be the best to send them as separated patches, each drivers maintainers being then free to accept, reject or ignore the patch(es) concerning him/her. It seems it was a bad guess, and yes, I see the point of having a series for this. I'll remember all this for the next time (if next time there is, of course, I was already quite hesitant to spend time to prepare and send patches for these issues with enum/integer mix-up). Sorry for the annoyance."
81,281311,281957,uncivil,"Either it does exist, or it doesn't. If it exists, it needs to be fixed.  If it doesn't exist, nothing
needs to be done. Which is the case?",281311,technical,"The write operation to hotplug->enabled"" is protected by the lock on line 1760, but the read operation to this data on line 1755 is not protected by the lock. Thus, there may exist a data race for ""hotplug->enabled"".  To fix this data race, the read operation to ""hotplug->enabled"" is  also protected by the lock.","The write operation to this is protected by the lock on line 1760, but the read operation to this data on line 1755 is not protected by the lock. Thus, there may exist a data race for this.  To fix this data race, the read operation to it is  also protected by the lock. Either it does exist, or it doesn't. If it exists, it needs to be fixed.  If it doesn't exist, nothing needs to be done. Which is the case?"
82,281311,283024,uncivil,It looks like you are not actually sure what you are doing then.,282919,technical,"I only read the code and find this possible data race. It is not found in real driver execution. I am not sure of it, so I use may"" and ""possible"" here.""","I only read the code and find this possible data race. It is not found in real driver execution. I am not sure of it, so I use may"" and ""possible"" here."" It looks like you are not actually sure what you are doing then."
83,289026,291846,civil,"One of the basic questions/concerns I have is accounting for surplus huge pages in the default memory resource controller.  The existing huegtlb resource controller already takes hugetlbfs huge pages into account, including surplus pages.  This series would allow surplus pages to be accounted for in the default  memory controller, or the hugetlb controller or both. 
I understand that current mechanisms do not meet the needs of the above use case.  The question is whether this is an appropriate way to approach the issue.  My cgroup experience and knowledge is extremely limited, but it does not appear that any other resource can be controlled by multiple controllers.  Therefore, I am concerned that this may be going against basic cgroup design philosophy.
It would be good to get comments from people more cgroup knowledgeable, and especially from those involved in the decision to do separate hugetlb control.",291684,technical,That looks a lot better. Thanks for giving it a go.,"That looks a lot better. Thanks for giving it a go. One of the basic questions/concerns I have is accounting for surplus huge pages in the default memory resource controller.  The existing huegtlb resource controller already takes hugetlbfs huge pages into account, including surplus pages.  This series would allow surplus pages to be accounted for in the default  memory controller, or the hugetlb controller or both. 
I understand that current mechanisms do not meet the needs of the above use case.  The question is whether this is an appropriate way to approach the issue.  My cgroup experience and knowledge is extremely limited, but it does not appear that any other resource can be controlled by multiple controllers.  Therefore, I am concerned that this may be going against basic cgroup design philosophy.
It would be good to get comments from people more cgroup knowledgeable, and especially from those involved in the decision to do separate hugetlb control."
84,289026,292549,civil,"I am sorry that I didn't join the discussion for the previous version but time just didn't allow that. So sorry if I am repeating something already sorted out. There was a deliberate decision to keep hugetlb and ""normal"" memory cgroup controllers separate. Mostly because hugetlb memory is an artificial memory subsystem on its own and it doesn't fit into the rest of memcg accounted memory very well. I believe we want to keep that status quo.
Well such a usecase requires an explicit configuration already. Either by using special wrappers or modifying the code. So I would argue that you have quite a good knowlege of the setup. If you need a greater flexibility then just do not use hugetlb at all and rely on THP. I do not really think this is a good idea. We really do not want to make the current hugetlb code more complex than it is already. The current hugetlb cgroup controller is simple and works at least somehow. I would not add more on top unless there is a _really_ strong usecase behind. Please make sure to describe such a usecase in details before we even start considering the code.
Well, then I would argue that you shouldn't use 64kB pages for your setup or allow THP for smaller sizes. Really hugetlb pages are by no means a substitute here.",292515,technical,"Thank you for your feedback. That makes sense, surplus hugepages are charged to both memcg and hugetlb cgroup, which may be contrary to cgroup design philosophy.  Based on the above advice, I have considered the following improvements, what do you think about?  The 'charge_surplus_hugepages' of v2 patch-set was an option to switch whether to charge memcg in addition to hugetlb cgroup"", but it will be abolished. Instead, change to ""switch only to memcg instead of hugetlb cgroup"" option. This is called 'surplus_charge_to_memcg'.  The surplus_charge_to_memcg option is created in per hugetlb cgroup. If it is false(default), charge destination cgroup of various page types is the same as the current kernel version. If it become true, hugetlb cgroup stops accounting for surplus hugepages, and memcg starts accounting instead.  A table showing which cgroups are charged.  I stared at the commit log of mm/hugetlb_cgroup.c, but it did not seem to have specially considered of surplus hugepages. Later, I will send a mail to hugetlb cgroup's committer to ask about surplus hugepages charge specifications. ","Thank you for your feedback. That makes sense, surplus hugepages are charged to both memcg and hugetlb cgroup, which may be contrary to cgroup design philosophy.  Based on the above advice, I have considered the following improvements, what do you think about?  The 'charge_surplus_hugepages' of v2 patch-set was an option to switch whether to charge memcg in addition to hugetlb cgroup"", but it will be abolished. Instead, change to ""switch only to memcg instead of hugetlb cgroup"" option. This is called 'surplus_charge'.  The surplus_charge option is created in per hugetlb cgroup. If it is false(default), charge destination cgroup of various page types is the same as the current kernel version. If it become true, hugetlb cgroup stops accounting for surplus hugepages, and memcg starts accounting instead.  A table showing which cgroups are charged.  I stared at the commit log of this file, but it did not seem to have specially considered of surplus hugepages. Later, I will send a mail to hugetlb cgroup's committer to ask about surplus hugepages charge specifications.  I am sorry that I didn't join the discussion for the previous version but time just didn't allow that. So sorry if I am repeating something already sorted out. There was a deliberate decision to keep hugetlb and ""normal"" memory cgroup controllers separate. Mostly because hugetlb memory is an artificial memory subsystem on its own and it doesn't fit into the rest of memcg accounted memory very well. I believe we want to keep that status quo.
Well such a usecase requires an explicit configuration already. Either by using special wrappers or modifying the code. So I would argue that you have quite a good knowlege of the setup. If you need a greater flexibility then just do not use hugetlb at all and rely on THP. I do not really think this is a good idea. We really do not want to make the current hugetlb code more complex than it is already. The current hugetlb cgroup controller is simple and works at least somehow. I would not add more on top unless there is a _really_ strong usecase behind. Please make sure to describe such a usecase in details before we even start considering the code.
Well, then I would argue that you shouldn't use 64kB pages for your setup or allow THP for smaller sizes. Really hugetlb pages are by no means a substitute here."
85,289026,292839,uncivil,"I do share your view Mike! This all looks so hackish and ad-hoc that I would be tempted to give it an outright nack, but let's here more about why do we need this fiddling at all. I've asked in other email so I guess I will get an answer there but let me just emphasize again that I absolutely detest a possibility to put hugetlb pages into the memcg mix. They just do not belong there. Try to look at previous discussions why it has been decided to have a separate hugetlb pages at all. I am also quite confused why you keep distinguishing surplus hugetlb pages from regular preallocated ones. Being a surplus page is an implementation detail that we use for an internal accounting rather than something to exhibit to the userspace even more than we do currently. Just look at what [sw]hould when you need to adjust accounting - e.g. due to the pool resize. Are you going to uncharge those surplus pages from memcg to reflect their persistence?",292549,civil,"I am sorry that I didn't join the discussion for the previous version but time just didn't allow that. So sorry if I am repeating something already sorted out.   There was a deliberate decision to keep hugetlb and normal"" memory cgroup controllers separate. Mostly because hugetlb memory is an artificial memory subsystem on its own and it doesn't fit into the rest of memcg accounted memory very well. I believe we want to keep that status quo.   Well such a usecase requires an explicit configuration already. Either by using special wrappers or modifying the code. So I would argue that you have quite a good knowlege of the setup. If you need a greater flexibility then just do not use hugetlb at all and rely on THP. [...]   I do not really think this is a good idea. We really do not want to make the current hugetlb code more complex than it is already. The current hugetlb cgroup controller is simple and works at least somehow. I would not add more on top unless there is a _really_ strong usecase behind. Please make sure to describe such a usecase in details before we even start considering the code.   Well, then I would argue that you shouldn't use 64kB pages for your setup or allow THP for smaller sizes. Really hugetlb pages are by no means a substitute here.","I am sorry that I didn't join the discussion for the previous version but time just didn't allow that. So sorry if I am repeating something already sorted out.   There was a deliberate decision to keep hugetlb and normal"" memory cgroup controllers separate. Mostly because hugetlb memory is an artificial memory subsystem on its own and it doesn't fit into the rest of memcg accounted memory very well. I believe we want to keep that status quo.   Well such a usecase requires an explicit configuration already. Either by using special wrappers or modifying the code. So I would argue that you have quite a good knowlege of the setup. If you need a greater flexibility then just do not use hugetlb at all and rely on THP. I do not really think this is a good idea. We really do not want to make the current hugetlb code more complex than it is already. The current hugetlb cgroup controller is simple and works at least somehow. I would not add more on top unless there is a _really_ strong usecase behind. Please make sure to describe such a usecase in details before we even start considering the code.   Well, then I would argue that you shouldn't use 64kB pages for your setup or allow THP for smaller sizes. Really hugetlb pages are by no means a substitute here. I do share your view Mike! This all looks so hackish and ad-hoc that I would be tempted to give it an outright nack, but let's here more about why do we need this fiddling at all. I've asked in other email so I guess I will get an answer there but let me just emphasize again that I absolutely detest a possibility to put hugetlb pages into the memcg mix. They just do not belong there. Try to look at previous discussions why it has been decided to have a separate hugetlb pages at all. I am also quite confused why you keep distinguishing surplus hugetlb pages from regular preallocated ones. Being a surplus page is an implementation detail that we use for an internal accounting rather than something to exhibit to the userspace even more than we do currently. Just look at what [sw]hould when you need to adjust accounting - e.g. due to the pool resize. Are you going to uncharge those surplus pages from memcg to reflect their persistence?"
86,289026,294078,civil,"I apologize for having confused. The hugetlb pages obtained from the pool do not waste the buddy pool. On the other hand, surplus hugetlb pages waste the buddy pool. Due to this difference in property, I thought it could be distinguished.
Although my memcg knowledge is extremely limited, memcg is accounting for various kinds of pages obtained from the buddy pool by the task belonging to it. I would like to argue that surplus hugepage has specificity in terms of obtaining from the buddy pool, and that it is specially permitted charge requirements for memcg.
It seems very strange that charge hugetlb page to memcg, but essentially it only charges the usage of the compound page obtained from the buddy pool, and even if that page is used as hugetlb page after that, memcg is not interested in that.
I will completely apologize if my way of thinking is wrong. It would be greatly appreciated if you could mention why we can not charge surplus hugepages to memcg. I could not understand the intention of this question, sorry. When resize the pool, I think that the number of surplus hugepages in use does not change. Could you explain what you were concerned about?",294072,technical,"Thank you for your time.  I do not know if it is really a strong use case, but I will explain my motive in detail. English is not my native language, so please pardon my poor English.  I am one of the developers for software that managing the resource used from user job at HPC-Cluster with Linux. The resource is memory mainly. The HPC-Cluster may be shared by multiple people and used. Therefore, the memory used by each user must be strictly controlled, otherwise the user's job will runaway, not only will it hamper the other users, it will crash the entire system in OOM.  Some users of HPC are very nervous about performance. Jobs are executed while synchronizing with MPI communication using multiple compute nodes. Since CPU wait time will occur when synchronizing, they want to minimize the variation in execution time at each node to reduce waiting times as much as possible. We call this variation a noise.  THP does not guarantee to use the Huge Page, but may use the normal page. This mechanism is one cause of variation(noise).  The users who know this mechanism will be hesitant to use THP. However, the users also know the benefits of the Huge Page's TLB hit rate performance, and the Huge Page seems to be attractive. It seems natural that these users are interested in HugeTLBfs, I do not know at all whether it is the right approach or not.  At the very least, our HPC system is pursuing high versatility and we have to consider whether we can provide it if users want to use HugeTLBfs.  In order to use HugeTLBfs we need to create a persistent pool, but in our use case sharing nodes, it would be impossible to create, delete or resize the pool.  One of the answers I have reached is to use HugeTLBfs by overcommitting without creating a pool(this is the surplus hugepage).  Surplus hugepages is hugetlb page, but I think at least that consuming buddy pool is a decisive difference from hugetlb page of persistent pool. If nr_overcommit_hugepages is assumed to be infinite, allocating pages for surplus hugepages from buddy pool is all unlimited even if being limited by memcg. In extreme cases, overcommitment will allow users to exhaust the entire memory of the system. Of course, this can be prevented by the hugetlb cgroup, but even if we set the limit for memcg and hugetlb cgroup respectively, as I asked in the first mail(set limit to 10GB), the control will not work.  I thought I could charge surplus hugepages to memcg, but maybe I did not have enough knowledge about memcg. I would like to reply to another mail for details.   Actually, I am opposed to the 64KB page, but the proposal to change the page size is expected to be dismissed as a problem. ","Thank you for your time.  I do not know if it is really a strong use case, but I will explain my motive in detail. English is not my native language, so please pardon my poor English.  I am one of the developers for software that managing the resource used from user job at HPC-Cluster with Linux. The resource is memory mainly. The HPC-Cluster may be shared by multiple people and used. Therefore, the memory used by each user must be strictly controlled, otherwise the user's job will runaway, not only will it hamper the other users, it will crash the entire system in OOM.  Some users of HPC are very nervous about performance. Jobs are executed while synchronizing with MPI communication using multiple compute nodes. Since CPU wait time will occur when synchronizing, they want to minimize the variation in execution time at each node to reduce waiting times as much as possible. We call this variation a noise.  THP does not guarantee to use the Huge Page, but may use the normal page. This mechanism is one cause of variation(noise).  The users who know this mechanism will be hesitant to use THP. However, the users also know the benefits of the Huge Page's TLB hit rate performance, and the Huge Page seems to be attractive. It seems natural that these users are interested in HugeTLBfs, I do not know at all whether it is the right approach or not.  At the very least, our HPC system is pursuing high versatility and we have to consider whether we can provide it if users want to use HugeTLBfs.  In order to use HugeTLBfs we need to create a persistent pool, but in our use case sharing nodes, it would be impossible to create, delete or resize the pool.  One of the answers I have reached is to use HugeTLBfs by overcommitting without creating a pool(this is the surplus hugepage).  Surplus hugepages is hugetlb page, but I think at least that consuming buddy pool is a decisive difference from hugetlb page of persistent pool. If number of overcommited hugepages is assumed to be infinite, allocating pages for surplus hugepages from buddy pool is all unlimited even if being limited by memcg. In extreme cases, overcommitment will allow users to exhaust the entire memory of the system. Of course, this can be prevented by the hugetlb cgroup, but even if we set the limit for memcg and hugetlb cgroup respectively, as I asked in the first mail(set limit to 10GB), the control will not work.  I thought I could charge surplus hugepages to memcg, but maybe I did not have enough knowledge about memcg. I would like to reply to another mail for details.   Actually, I am opposed to the 64KB page, but the proposal to change the page size is expected to be dismissed as a problem.  I apologize for having confused. The hugetlb pages obtained from the pool do not waste the buddy pool. On the other hand, surplus hugetlb pages waste the buddy pool. Due to this difference in property, I thought it could be distinguished.
Although my memcg knowledge is extremely limited, memcg is accounting for various kinds of pages obtained from the buddy pool by the task belonging to it. I would like to argue that surplus hugepage has specificity in terms of obtaining from the buddy pool, and that it is specially permitted charge requirements for memcg.
It seems very strange that charge hugetlb page to memcg, but essentially it only charges the usage of the compound page obtained from the buddy pool, and even if that page is used as hugetlb page after that, memcg is not interested in that.
I will completely apologize if my way of thinking is wrong. It would be greatly appreciated if you could mention why we can not charge surplus hugepages to memcg. I could not understand the intention of this question, sorry. When resize the pool, I think that the number of surplus hugepages in use does not change. Could you explain what you were concerned about?"
87,289026,271213,civil,"As you said, my patch did not consider handling when manipulating the pool. And even if that handling is done well, it will not be a valid reason to charge surplus hugepage to memcg. I understood the concept of memcg. As you said, it must be an alien. Thanks to the interaction up to here, I understood that my solution is inappropriate. I will look for another way. Thank you for your kind explanation.",294650,technical,"Note.  You do not want to use THP because THP does not guarantee"".   Using hugetlbfs overcommit also does not provide a guarantee.  Without doing much research, I would say the failure rate for obtaining a huge page via THP and hugetlbfs overcommit is about the same.  The most difficult issue in both cases will be obtaining a ""huge page"" number of pages from the buddy allocator.  I really do not think hugetlbfs overcommit will provide any benefit over THP for your use case.  Also, new user space code is required to ""fall back"" to normal pages in the case of hugetlbfs page allocation failure.  This is not needed in the THP case. ","Note.  You do not want to use THP because THP does not guarantee"".   Using hugetlbfs overcommit also does not provide a guarantee.  Without doing much research, I would say the failure rate for obtaining a huge page via THP and hugetlbfs overcommit is about the same.  The most difficult issue in both cases will be obtaining a ""huge page"" number of pages from the buddy allocator.  I really do not think hugetlbfs overcommit will provide any benefit over THP for your use case.  Also, new user space code is required to ""fall back"" to normal pages in the case of hugetlbfs page allocation failure.  This is not needed in the THP case.  As you said, my patch did not consider handling when manipulating the pool. And even if that handling is done well, it will not be a valid reason to charge surplus hugepage to memcg. I understood the concept of memcg. As you said, it must be an alien. Thanks to the interaction up to here, I understood that my solution is inappropriate. I will look for another way. Thank you for your kind explanation."
88,289431,289688,civil,"I'm not sure I understand what you intend here. If __sync_blockdev fails, then the error should have already been marked in this (via patch #6). We wouldn't want to record that again at syncfs time. Note that __sync_blockdev will return errors based on the legacy flags. We really do need to record it in the superblock as soon as possible after an error occurs. If we want to allow userland to eventually be able to scrape this value out of the kernel (as we discussed at LSF/MM) then we can't assume that it'll be doing any sort of syncfs call beforehand. The main reason to push this down into the filesystems is to allow them control over whether to report errors at syncfs time via the superblock errseq_t or not. If we don't really care about allowing this to be an opt-in thing, then we could just take the patch that I sent on April 17th: track per-sb writeback errors and report them to syncfs. We'd also want patch #6 from this series, I think, but that's more or less enough to implement this over all filesystems, assuming they use mapping_set_error to record writeback errors. I'm fine with either approach.",271302,technical,"XFS never uses the block device mapping for anything, so this is not needed.   The proper name for this would be vfs_sync_fs.  And I don't think it warrants an inline.","XFS never uses the block device mapping for anything, so this is not needed.   The proper name for this would be this.  And I don't think it warrants an inline. I'm not sure I understand what you intend here. If this fails, then the error should have already been marked in this (via patch #6). We wouldn't want to record that again at syncfs time. Note that it will return errors based on the legacy flags. We really do need to record it in the superblock as soon as possible after an error occurs. If we want to allow userland to eventually be able to scrape this value out of the kernel (as we discussed at LSF/MM) then we can't assume that it'll be doing any sort of syncfs call beforehand. The main reason to push this down into the filesystems is to allow them control over whether to report errors at syncfs time via the superblock errseq_t or not. If we don't really care about allowing this to be an opt-in thing, then we could just take the patch that I sent on April 17th: track per-sb writeback errors and report them to syncfs. We'd also want patch #6 from this series, I think, but that's more or less enough to implement this over all filesystems, assuming they use mapping_set_error to record writeback errors. I'm fine with either approach."
89,289431,292003,uncivil,"Where did this come from? XFS doesn't use the underlying blockdev address space, so this does nothing at all and should not be here. So to return errors correctly it needs to capture errors from the log force (i.e. metadata errors such as filesystem shutdowns, journal IO errors, etc), then check for pending data IO errors.",271303,technical,"Thanks, I wasn't sure about xfs. I'll drop this hunk.  FWIW, I think pushing this call down into the sync_fs routines is still probably the right thing to do, regardless of the state of the later patches.   I patterned the name after the call_mmap (and now-defunct call_fsync) helpers. I'll rename it and change it to be non-inlined.","Thanks, I wasn't sure about xfs. I'll drop this hunk.  FWIW, I think pushing this call down into the sync_fs routines is still probably the right thing to do, regardless of the state of the later patches.   I patterned the name after the call_mmap (and now-defunct call_fsync) helpers. I'll rename it and change it to be non-inlined. Where did this come from? XFS doesn't use the underlying blockdev address space, so this does nothing at all and should not be here. So to return errors correctly it needs to capture errors from the log force (i.e. metadata errors such as filesystem shutdowns, journal IO errors, etc), then check for pending data IO errors."
90,303621,305468,uncivil,The SPDX header is explicitly here to remove the license text and create a tag that is in a indirect reference to the license text in LICENSES. It's not going away. I never said we were perfect reviewers. Feel free to help in the process.,304654,technical,"The X11 license notice states explicitly that the notice has to be included in the file.  Wouldn't removing it be a violation of the license?    FYI, this was copied from another .dts file.  All of the other brightness-levels settings in sun files follow similar patterns:","The X11 license notice states explicitly that the notice has to be included in the file.  Wouldn't removing it be a violation of the license?    FYI, this was copied from another .dts file.  All of the other brightness-levels settings in sun files follow similar patterns: The SPDX header is explicitly here to remove the license text and create a tag that is in a indirect reference to the license text in LICENSES. It's not going away. I never said we were perfect reviewers. Feel free to help in the process."
91,306145,317171,civil,"Please don't top post. And wrap your lines at around 75 characters. Look closely at the two implementations. Look at what
mmd_phy_indirect() does. I _think_ these are identical. So don't add your own helper, please use the core code.",317094,technical,"thanks for the hint. But actually I cannot confirm - or I don't see it yet.     Without having tested, just from the code, the struct phy_driver instance for thos in micrel.c does not have a .write_mmd function assigned, thus phy_write_mmd should evaluate to its else-clause (see below) and not to mdiobus_write (as in phy_write).    Also the  function which I have added uses the same principle as already existing HW-specific functions in micrel.c for simular reasons.  They use phy_write all over the place in that file and never phy_write_mmd - for whatever reason they had.  Thus I thought it would be a good idea ...    PHY link failure after cable connect. This looks a lot like phy_write_mmd(). ","thanks for the hint. But actually I cannot confirm - or I don't see it yet.     Without having tested, just from the code, the struct phy_driver instance for thos in micrel.c does not have a .write_mmd function assigned, thus phy_write_mmd should evaluate to its else-clause (see below) and not to mdiobus_write (as in phy_write).    Also the  function which I have added uses the same principle as already existing HW-specific functions in micrel.c for simular reasons.  They use phy_write all over the place in that file and never phy_write_mmd - for whatever reason they had.  Thus I thought it would be a good idea ...    PHY link failure after cable connect. This looks a lot like phy_write_mmd().  Please don't top post. And wrap your lines at around 75 characters. Look closely at the two implementations. Look at what this function does. I _think_ these are identical. So don't add your own helper, please use the core code."
92,306145,357688,uncivil,But you did it again.... Your email client should not be forcing you to top post. So please don't.,357674,technical,"thanks for your feedback. I was on holiday, thus just delayed, not forgotten... Sorry for top-posting - odd company default mail setup. I checked again phy_write_mmd(), you are right !  Patch with changed implementation will follow.  PHY link failure after cable connect. Please don't top post. And wrap your lines at around 75 characters. Look closely at the two implementations. Look at what mmd_phy_indirect() does. I _think_ these are identical. So don't add your own helper, please use the core code.","thanks for your feedback. I was on holiday, thus just delayed, not forgotten... Sorry for top-posting - odd company default mail setup. I checked again that function, you are right !  Patch with changed implementation will follow.  PHY link failure after cable connect. Please don't top post. And wrap your lines at around 75 characters. Look closely at the two implementations. Look at what this function does. I _think_ these are identical. So don't add your own helper, please use the core code. But you did it again.... Your email client should not be forcing you to top post. So please don't."
93,307410,307390,civil,"Oops, sorry, I double posted patch 5. Please disregard the second one.",307830,technical,"When remote files are counted in get_files_count, without using SSH, the code returns 0 because there is a colon prepended to LOC. VPATH should have been used instead of LOC. (NTB: test)","When remote files are counted in get_files_count, without using SSH, the code returns 0 because there is a colon prepended to LOC. VPATH should have been used instead of LOC. (NTB: test) Oops, sorry, I double posted patch 5. Please disregard the second one."
94,307410,311549,civil,"Well, clients not checking the error code made this harder to debug for sure, but removing the error code is a side effect and not what is happening here (in fact someone should probably still go back and add error checking because these functions can still return errors but that's not really something I have time to do). After the next couple patches, the clients will use this change to detect that there are no port numbers and handle things similarly to the way they did before they were broken by the multiport changes. This is the opposite of what I've ever heard before. Having a commit message that explains what led up to this commit is a good thing and allows people debugging in the future to better understand the decisions made. People debugging commits will never find the 0/X cover letter which is just intended to introduce the series to reviewers and describe changes if the series is posted multiple times. No this is not a feature request. This is fixing a regression that broke previously working code in the only sensible way I can come up with. If you have a better way to fix this, I'd be glad to hear it. But this should *not* be treated as a feature request.",311538,technical,"I disagree strongly. You can tell it's not device specific seeing we have 4 devices that need the exact same code. In fact, there is nothing device specific in those lines of code as the device specific part comes when a driver sets the PCI parent device's DMA mask. These lines just initialize the dma_mask for the new NTB device with its parent's mask. This is just sensible given that nothing now works if it is not done and trusting driver writers to get it right is not a good idea seeing we already screwed it up once. Furthermore, it violates DRY (do not repeat yourself).  If there is something driver specific that must be done (although I can't actually imagine what this would be) the drivers are free to change the mask after calling ntb_register but getting the common case setup in common code is just good design.","I disagree strongly. You can tell it's not device specific seeing we have 4 devices that need the exact same code. In fact, there is nothing device specific in those lines of code as the device specific part comes when a driver sets the PCI parent device's DMA mask. These lines just initialize the dma_mask for the new NTB device with its parent's mask. This is just sensible given that nothing now works if it is not done and trusting driver writers to get it right is not a good idea seeing we already screwed it up once. Furthermore, it violates DRY (do not repeat yourself).  If there is something driver specific that must be done (although I can't actually imagine what this would be) the drivers are free to change the mask after calling ntb_register but getting the common case setup in common code is just good design. Well, clients not checking the error code made this harder to debug for sure, but removing the error code is a side effect and not what is happening here (in fact someone should probably still go back and add error checking because these functions can still return errors but that's not really something I have time to do). After the next couple patches, the clients will use this change to detect that there are no port numbers and handle things similarly to the way they did before they were broken by the multiport changes. This is the opposite of what I've ever heard before. Having a commit message that explains what led up to this commit is a good thing and allows people debugging in the future to better understand the decisions made. People debugging commits will never find the 0/X cover letter which is just intended to introduce the series to reviewers and describe changes if the series is posted multiple times. No this is not a feature request. This is fixing a regression that broke previously working code in the only sensible way I can come up with. If you have a better way to fix this, I'd be glad to hear it. But this should *not* be treated as a feature request."
95,307410,349339,uncivil,I took a closer look at this and it's not necessary. (Note: I do the majority of my testing in a looped-back setup). What you didn't notice is that split_remote() separates the colon whether there is a host or not. It's not passed to ssh or cat (or whatever) directly. So the change you propose will actually break the how it was designed.,314425,technical,Thanks. It would be good to get Acked-bys or Reviewed-bys on the patches you approved.,Thanks. It would be good to get Acked-bys or Reviewed-bys on the patches you approved. I took a closer look at this and it's not necessary. (Note: I do the majority of my testing in a looped-back setup). What you didn't notice is that split_remote() separates the colon whether there is a host or not. It's not passed to ssh or cat (or whatever) directly. So the change you propose will actually break the how it was designed.
96,310967,311378,uncivil,There are no unexpected results. Making a non-fatal error fatal doesn't serve a useful purpose,310967,technical,"When i2c_new_dummy fails, the lack of error-handling code may cause unexpected results.  This patch adds error-handling code after calling i2c_new_dummy.  ","When the new dummy fails, the lack of error-handling code may cause unexpected results.  This patch adds error-handling code after calling the new dummy. There are no unexpected results. Making a non-fatal error fatal doesn't serve a useful purpose"
97,314071,314224,civil,"The commit description is not quite correct.  What the NO_HIDE_STALE flag does is allow a discard request for those block devices which do not have the DISCARD_ZEROES_DATA flag. I will note that the FALLOC_FL_NO_HIDE_STALE flag is a bit controversial in linux-fsdevel.  I have a similar patch in the VFS in Google's internal data center kernel, as well as an internal patch which implements support for this flag in ext4.  However, the patches are out of tree, because pretty much all of the file system developers who work for enterpise distributions were against this functionality. I know of one other major cloud provider (in China) using the functionality as an out-of-tree patch, but with no one else speaking in favor of it, and everyone else NAK'ing the patch and enterprise distro's saying they would revert the patch in their distro kernels, the compromise we came to was that the code point for NO_HIDE_STALE_FL would be reserved so that users of the out-of-tree patches wouldn't collide with future fallocate flags; and I would stop trying to push the patches upstream. I have no idea how Darrick was able to get this commit upstream, but I guess it was less controversial for block devices than for file systems. So I'm certainly in favor of this patch landing in mainline, but you should be aware that there may be some opposition to it.",314071,technical,"The flag must be set if user want to issue a discard request for block devices. But vfs_fallocate() will return with an error indicating lack of support if this flag is set.  fix it by allowing  flag in vfs_fallocate  Fixes: (block: implement (some of) fallocate for block devices"") 	","The flag must be set if user want to issue a discard request for block devices. But it will return with an error indicating lack of support if this flag is set.  fix it by allowing  flag in vfs_fallocate  Fixes: (block: implement (some of) fallocate for block devices""). The commit description is not quite correct.  What the NO_HIDE_STALE flag does is allow a discard request for those block devices which do not have the DISCARD_ZEROES_DATA flag. I will note that the FALLOC_FL_NO_HIDE_STALE flag is a bit controversial in linux-fsdevel.  I have a similar patch in the VFS in Google's internal data center kernel, as well as an internal patch which implements support for this flag in ext4.  However, the patches are out of tree, because pretty much all of the file system developers who work for enterpise distributions were against this functionality. I know of one other major cloud provider (in China) using the functionality as an out-of-tree patch, but with no one else speaking in favor of it, and everyone else NAK'ing the patch and enterprise distro's saying they would revert the patch in their distro kernels, the compromise we came to was that the code point for NO_HIDE_STALE_FL would be reserved so that users of the out-of-tree patches wouldn't collide with future fallocate flags; and I would stop trying to push the patches upstream. I have no idea how Darrick was able to get this commit upstream, but I guess it was less controversial for block devices than for file systems. So I'm certainly in favor of this patch landing in mainline, but you should be aware that there may be some opposition to it."
98,331266,331355,uncivil,"What is this crazy union for?  Why are you messing around with ""raw"" kobject attributes?  This is a device, you should never have to mess with sysfs calls or kobject calls or structures directly.  If you do, that's a huge hint something is wrong here. You aren't ""adding"" any attributes here, you are only setting them up (in an odd way, see below...). That's an oddly-hard-coded array size for no good reason :sad:. This works?  You normally have to manually initialize a dynamic attribute.  Why are you doing it this way and not using an attribute group? Why are you using a custom device class for a single device? you need to document the heck out of this in the changelog to help explain all of these odd design decisions.",331349,technical,Same problem here :(,"Same problem here :sad: What is this crazy union for?  Why are you messing around with ""raw"" kobject attributes?  This is a device, you should never have to mess with sysfs calls or kobject calls or structures directly.  If you do, that's a huge hint something is wrong here. You aren't ""adding"" any attributes here, you are only setting them up (in an odd way, see below...). That's an oddly-hard-coded array size for no good reason :sad:. This works?  You normally have to manually initialize a dynamic attribute.  Why are you doing it this way and not using an attribute group? Why are you using a custom device class for a single device? you need to document the heck out of this in the changelog to help explain all of these odd design decisions."
99,331266,331694,uncivil,"Greg (and replying to your other comments as well)... This is an RFC series, it's not meant for you to take at this point, it's about discussing the overall approach to exposing BMC random ""tunables"" as explained in patch 0 of the series. Yes the individual patches aren't yet at the level of polish for a formal submission, we (naively ?) thought that's what the whole RFC tag is about :-)",331356,technical,No changelog :(,"No changelog :sad: (and replying to your other comments as well)... This is an RFC series, it's not meant for you to take at this point, it's about discussing the overall approach to exposing BMC random ""tunables"" as explained in patch 0 of the series. Yes the individual patches aren't yet at the level of polish for a formal submission, we (naively ?) thought that's what the whole RFC tag is about :-)"
100,331266,331847,uncivil,"Well, it adds documentation :-) You can just read the patch which is ... the documentation :) Yes, you did that's fine. Thanks.",331710,uncivil,"Oh come on, putting a basic here is what this patch does"" comment should be part of every patch, otherwise what is there to comment on if we don't know what is going on in the patch itself?  Anyway, I provided a bunch of feedback to the ""real"" patch in this series...""","Oh come on, putting a basic here is what this patch does"" comment should be part of every patch, otherwise what is there to comment on if we don't know what is going on in the patch itself?  Anyway, I provided a bunch of feedback to the ""real"" patch in this series..."" Well, it adds documentation :smile: You can just read the patch which is ... the documentation :smile: Yes, you did that's fine. Thanks."
101,331266,331710,uncivil,"Oh come on, putting a basic ""here is what this patch does"" comment should be part of every patch, otherwise what is there to comment on if we don't know what is going on in the patch itself? Anyway, I provided a bunch of feedback to the ""real"" patch in this series...",331694,uncivil,"(and replying to your other comments as well)...  This is an RFC series, it's not meant for you to take at this point, it's about discussing the overall approach to exposing BMC random tunables"" as explained in patch 0 of the series.  Yes the individual patches aren't yet at the level of polish for a formal submission, we (naively ?) thought that's what the whole RFC tag is about :-)""","(and replying to your other comments as well)...  This is an RFC series, it's not meant for you to take at this point, it's about discussing the overall approach to exposing BMC random tunables"" as explained in patch 0 of the series.  Yes the individual patches aren't yet at the level of polish for a formal submission, we (naively ?) thought that's what the whole RFC tag is about :smile:"" Oh come on, putting a basic ""here is what this patch does"" comment should be part of every patch, otherwise what is there to comment on if we don't know what is going on in the patch itself? Anyway, I provided a bunch of feedback to the ""real"" patch in this series..."
102,336834,337035,uncivil,"Yeah, not going to happen. You grow that structure from 64 bytes to 96 bytes and with that grow the static footprint of the kernel by 512k (in the very best case) possibly again breaking things like sparc (which have a strict limit on the kernel image size). And all that for data that I've never needed and never even considered useful when looking at lockdep output.",336834,technical,"Analyzing the circular locking dependency splat [2], I can see the following skeleton/pattern:  is trying to acquire lock X. same has previously acquired lock Y.     lock Y depends on lock X (hence chance for deadlock).      Print lock dependency chain Print an example of potential scenario leading to real deadlock.     List all locks held by. Print backtrace (seems to be equal to stack dump). The following questions appeared in my mind when analyzing [2]: (A) What is the chronology of consuming the locks? Is it related to     the order seen in the ""dependency chain""? (B) All four locks are reported to be held by the same task.     Indeeed, based on available backtraces, 3 of 4 are acquired during the load by systemd-udevd. However,     there is a different syscall behind this which made we wonder if it is really systemd-udevd consuming this lock. If not, is it still correct to say that systemd-udevd holds all locks? (C) The example of potential unsafe scenario leading to deadlock     puts a special emphasis on the CPU on which the lock is held.     However, except for the last held lock  whose CPU can be extracted from  there is no CPU-related information for the other locks. doesn't match the stack backtrace   (there is no call in the backtrace).   Maybe I misinterpret the report? ","Analyzing the circular locking dependency splat [2], I can see the following skeleton/pattern  is trying to acquire lock X. same has previously acquired lock Y.     lock Y depends on lock X (hence chance for deadlock).      Print lock dependency chain Print an example of potential scenario leading to real deadlock.     List all locks held by. Print backtrace (seems to be equal to stack dump). The following questions appeared in my mind when analyzing [2]: (A) What is the chronology of consuming the locks? Is it related to     the order seen in the ""dependency chain""? (B) All four locks are reported to be held by the same task.     Indeeed, based on available backtraces, 3 of 4 are acquired during the load by systemd-udevd. However,     there is a different syscall behind this which made we wonder if it is really systemd-udevd consuming this lock. If not, is it still correct to say that systemd-udevd holds all locks? (C) The example of potential unsafe scenario leading to deadlock     puts a special emphasis on the CPU on which the lock is held.     However, except for the last held lock  whose CPU can be extracted from  there is no CPU-related information for the other locks. doesn't match the stack backtrace   (there is no call in the backtrace).   Maybe I misinterpret the report?  Yeah, not going to happen. You grow that structure from 64 bytes to 96 bytes and with that grow the static footprint of the kernel by 512k (in the very best case) possibly again breaking things like sparc (which have a strict limit on the kernel image size). And all that for data that I've never needed and never even considered useful when looking at lockdep output."
103,336834,337316,uncivil,"I  confirm that in case of x86_64, the bss size is increased by around 1 M with standard config. For sparc there seems to be a dedicated CONFIG_LOCKDEP_SMALL, which seems to downsize the lockdep implementation anyway. It's likely because you infer about certain aspects which are not clearly stated in the deadlock report. As example, the original report doesn't say that the process which holds this is different to the process which holds the other locks. On the contrary, it tells the user that all the locks are being held by the same task, which seems to be wrong. You likely also infer about the order of consuming the locks based on the contents of the stack dump associated to each lock. Without doing some mental diffs between the backtraces, it's not possible to see the chronological order of consuming the locks. Actually this only works for backtraces with common history, i.e. there is no clue what is the time/point of acquiring 'cpu_hotplug_lock.rw_sem' relative to the other locks. The patch mostly shares my personal experience of trying to make sense of lockdep output. It's OK if it doesn't reach mainline. I still hope that I can get some feedback from community regarding the actual cpufreq-related issue pointed out in the splat. I can also reproduce it on v4.14, so it appears to be in the kernel for quite some time. Thank you in advance.",337035,uncivil,"Yeah, not going to happen. You grow that structure from 64 bytes to 96 bytes and with that grow the static footprint of the kernel by 512k (in the very best case) possibly again breaking things like sparc (which have a strict limit on the kernel image size).  And all that for data that I've never needed and never even considered useful when looking at lockdep output.","Yeah, not going to happen. You grow that structure from 64 bytes to 96 bytes and with that grow the static footprint of the kernel by 512k (in the very best case) possibly again breaking things like sparc (which have a strict limit on the kernel image size).  And all that for data that I've never needed and never even considered useful when looking at lockdep output. I  confirm that in case of x86_64, the bss size is increased by around 1 M with standard config. For sparc there seems to be a dedicated CONFIG_LOCKDEP_SMALL, which seems to downsize the lockdep implementation anyway. It's likely because you infer about certain aspects which are not clearly stated in the deadlock report. As example, the original report doesn't say that the process which holds this is different to the process which holds the other locks. On the contrary, it tells the user that all the locks are being held by the same task, which seems to be wrong. You likely also infer about the order of consuming the locks based on the contents of the stack dump associated to each lock. Without doing some mental diffs between the backtraces, it's not possible to see the chronological order of consuming the locks. Actually this only works for backtraces with common history, i.e. there is no clue what is the time/point of acquiring it relative to the other locks. The patch mostly shares my personal experience of trying to make sense of lockdep output. It's OK if it doesn't reach mainline. I still hope that I can get some feedback from community regarding the actual cpufreq-related issue pointed out in the splat. I can also reproduce it on v4.14, so it appears to be in the kernel for quite some time. Thank you in advance."
104,346223,347267,civil,"As far as I can tell, the above is the whole reason for the patchset, yes?  To avoid confusing users. Is that sufficient?  Can we instead simplify their lives by providing better documentation or informative printks or better Kconfig text, etc? And who *are* the people who are performing this configuration?  Random system administrators?  Linux distro engineers?  If the latter then they presumably aren't easily confused!
In other words, I'm trying to understand how much benefit this patchset will provide to our users as a whole.",346227,technical,"For file loading, if this is 'true', the memory which is used to load kernel/initrd/purgatory is supposed to be allocated from top to down. This is what we have been doing all along in the old kexec loading interface and the kexec loading is still default setting in some distributions. However, the current kexec_file loading interface doesn't do like this. The function it calls ignores checking that, but calls walk_system_ram_res() directly to go through all resources of System RAM from bottom to up, to try to find memory region which can contain the specific kexec buffer, then call it to allocate memory in that found memory region from top to down. This brings confusion especially when KASLR is widely supported , users have to make clear why kexec/kdump kernel loading position is different between these two interfaces in order to exclude unnecessary noises. Hence these two interfaces need be unified on behaviour.  Here add checking if kexec_buf.top_down is 'true' in it, if yes, call the newly added function to find memory region from top to down to load kernel. ","For file loading, if this is 'true', the memory which is used to load purgatory is supposed to be allocated from top to down. This is what we have been doing all along in the old kexec loading interface and the kexec loading is still default setting in some distributions. However, the current kexec_file loading interface doesn't do like this. The function it calls ignores checking that, but calls this function directly to go through all resources of System RAM from bottom to up, to try to find memory region which can contain the specific kexec buffer, then call it to allocate memory in that found memory region from top to down. This brings confusion especially when KASLR is widely supported , users have to make clear why kexec/kdump kernel loading position is different between these two interfaces in order to exclude unnecessary noises. Hence these two interfaces need be unified on behaviour.  Here add checking if kexec_buf.top_down is 'true' in it, if yes, call the newly added function to find memory region from top to down to load kernel.  As far as I can tell, the above is the whole reason for the patchset, yes?  To avoid confusing users. Is that sufficient?  Can we instead simplify their lives by providing better documentation or informative printks or better Kconfig text, etc? And who *are* the people who are performing this configuration?  Random system administrators?  Linux distro engineers?  If the latter then they presumably aren't easily confused!
In other words, I'm trying to understand how much benefit this patchset will provide to our users as a whole."
105,347183,348381,civil,"Hopefully I'm not missing anything here, but this doesn't really make any sense. I'm not sure I explained myself as well as I thought I did. To be honest, I had to double check this about literally 20 times to make sure I was actually understanding this issue correctly. Turns out I was missing a couple of parts, so I'm going to try again at explaining this using a diagram that shows the various threads running concurrently. phew. that took a LONG time to come up with. Anyway-that's why your explanation doesn't make sense: the deadlock is happening because we're calling pm_runtime_get_sync(). If we were to make that call conditional (e.g. drm_kms_helper_is_poll_worker()), all that would mean is that we wouldn't grab any runtime power reference and the GPU would immediately suspend once the atomic commit finished, as the suspend request in Thread 5 would finally get unblocked and thus----suspend. Hopefully I explained that better this time, I'll definitely make sure to actually include that diagram in the patch. As for whether or not this patch is even the right solution, I will need to confirm that tommorrow (if you don't think it is still, please feel free to say so!) because it's getting late here.",347502,technical,"Hm, a runtime PM ref is already acquired in nouveau_connector_detect(). I'm wondering why that's not sufficient?","Hm, a runtime PM ref is already acquired in nouveau_connector_detect(). I'm wondering why that's not sufficient? Hopefully I'm not missing anything here, but this doesn't really make any sense. I'm not sure I explained myself as well as I thought I did. To be honest, I had to double check this about literally 20 times to make sure I was actually understanding this issue correctly. Turns out I was missing a couple of parts, so I'm going to try again at explaining this using a diagram that shows the various threads running concurrently. phew. that took a LONG time to come up with. Anyway-that's why your explanation doesn't make sense: the deadlock is happening because we're calling this function. If we were to make that call conditional (e.g. this function), all that would mean is that we wouldn't grab any runtime power reference and the GPU would immediately suspend once the atomic commit finished, as the suspend request in Thread 5 would finally get unblocked and thus----suspend. Hopefully I explained that better this time, I'll definitely make sure to actually include that diagram in the patch. As for whether or not this patch is even the right solution, I will need to confirm that tommorrow (if you don't think it is still, please feel free to say so!) because it's getting late here."
106,347183,349692,civil,"First of all, I was mistaken when I wrote above that a check for! It would solve the problem.  Sorry! It doesn't because the call to pm_runtime_get_sync() is not happening in this but in this.

Looking once more at the three stack traces you've provided, we've got:
- output_poll_execute() stuck waiting for fb_helper->lock which is held by drm_dp_mst_link_probe_work()
- this is stuck waiting for it to finish
- this is stuck waiting in here. For the moment we can ignore the first task, i.e. output_poll_execute(), and focus on the latter two. As said I'm unfamiliar with MST but browsing through this I notice that drm_dp_mst_link_probe_work() is the ->work element in this and is queued on HPD.  I further notice that the work item is flushed on runtime_suspend. And before the work item is flushed, the HPD source is quiesced.
So it looks like drm_dp_mst_link_probe_work() can only ever run while the GPU is runtime resumed, it never runs while the GPU is runtime suspended.  This means that you don't have to acquire any runtime PM references in or below drm_dp_mst_link_probe_work(). Au contraire, you must not acquire any because it will deadlock while the GPU is runtime suspending.  If there are functions which are called from drm_dp_mst_link_probe_work() as well as from other contexts, and those other contexts need a runtime PM ref to be acquired, you need to acquire the runtime PM ref conditionally on not being drm_dp_mst_link_probe_work() (using the current_work() technique). Alternatively, move acquisition of the runtime PM ref further up in the call chain to those other contexts. Right, that seems to be a bug nouveau_pmops_runtime_suspend().
If a display is plugged in while the GPU is about to runtime suspend, the display may be lit up by output_poll_execute() but the GPU will then nevertheless be powered off. 
I guess after calling drm_kms_helper_poll_disable() we should re-check if a crtc has been activated.  This should have bumped the runtime PM refcount and have_disp_power_ref should be true.  In that case, the nouveau_pmops_runtime_suspend() should return -EBUSY to abort the runtime_suspend.
The same check seems necessary after flushing drm_dp_mst_link_probe_work(): If the work item lit up a new display, all previous suspend steps need to be unwound and -EBUSY needs to be returned to the PM core.
Communication with an MST hub exceeding the autosuspend timeout is just one scenario where this bug manifests itself.
BTW, drm_kms_helper_poll_disable() seems to be called twice in the runtime_suspend code path, once in nouveau_pmops_runtime_suspend() and a second time in nouveau_display_fini(). A stupid question, I notice that this only if encoder_type is not equal to this. Why isn't that equal?",348395,technical,"As an additional note, I realized this might seem wrong but it isn't  this function calls down to nouveau's runtime idle callback, which does this. So, it doesn't actually synchronously suspend the GPU, it just starts up the autosuspend thread  Just wanted to make sure there wasn't any confusion :) ","As an additional note, I realized this might seem wrong but it isn't  this function calls down to nouveau's runtime idle callback, which does this. So, it doesn't actually synchronously suspend the GPU, it just starts up the autosuspend thread  Just wanted to make sure there wasn't any confusion :)  First of all, I was mistaken when I wrote above that a check for! It would solve the problem.  Sorry! It doesn't because the call to pm_runtime_get_sync() is not happening in this but in this.

Looking once more at the three stack traces you've provided, we've got:
- output_poll_execute stuck waiting for the lock which is held by this function
- this function is stuck waiting for it to finish
- this functiion is stuck waiting in here. For the moment we can ignore the first task, i.e. output_poll_execute(), and focus on the latter two. As said I'm unfamiliar with MST but browsing through this I notice that this is the function is the ->work element in this and is queued on HPD.  I further notice that the work item is flushed on runtime_suspend. And before the work item is flushed, the HPD source is quiesced.
So it looks like this function can only ever run while the GPU is runtime resumed, it never runs while the GPU is runtime suspended.  This means that you don't have to acquire any runtime PM references in or below this function. Au contraire, you must not acquire any because it will deadlock while the GPU is runtime suspending.  If there are functions which are called from this function as well as from other contexts, and those other contexts need a runtime PM ref to be acquired, you need to acquire the runtime PM ref conditionally on not being this function (using the current_work() technique). Alternatively, move acquisition of the runtime PM ref further up in the call chain to those other contexts. Right, that seems to be a bug function.
If a display is plugged in while the GPU is about to runtime suspend, the display may be lit up by output_poll_execute() but the GPU will then nevertheless be powered off. 
I guess after calling helper_poll_disable() we should re-check if a crtc has been activated.  This should have bumped the runtime PM refcount and have_disp_power_ref should be true.  In that case, the runtime_suspend() should return -EBUSY to abort the runtime_suspend.
The same check seems necessary after flushing link_probe_work(): If the work item lit up a new display, all previous suspend steps need to be unwound and -EBUSY needs to be returned to the PM core.
Communication with an MST hub exceeding the autosuspend timeout is just one scenario where this bug manifests itself.
BTW, poll_disable() seems to be called twice in the runtime_suspend code path, once in runtime_suspend() and a second time in this function. A stupid question, I notice that this only if encoder_type is not equal to this. Why isn't that equal?"
107,358863,358900,uncivil,"

These calling conventions are rather suboptimal.  First of all, none of actor callbacks will ever get called directly. There are only 4 callers.  3 of them (all in this) are of this form.  The fourth is this, which itself is an actor callback. So all these ""return -E..."" in the instances are completely pointless; we should just turn filldir_t into pointer-to-function-returning-bool and get rid of that boilerplate, rather than adding more to it. Furthermore, who the hell cares which callback has stepped into it? ""The first time it happened from getdents in a 32 bit process and that's all you'll ever get out of me"" seems to be less than helpful... And frankly, I would prefer this, making that thing return -EUCLEAN or 0.  Quite possibly - inlining it as well...",358863,technical,"When you e.g. run `find` on a directory for which getdents returns filenames"" that contain slashes, `find` passes those ""filenames"" back to the kernel, which then interprets them as paths. That could conceivably cause userspace to do something bad when accessing something like an untrusted USB stick, but I'm not aware of any specific example.  Instead of returning bogus filenames to userspace, return -EUCLEAN.  ","When you e.g. run `find` on a directory for which getdents returns filenames"" that contain slashes, `find` passes those ""filenames"" back to the kernel, which then interprets them as paths. That could conceivably cause userspace to do something bad when accessing something like an untrusted USB stick, but I'm not aware of any specific example.  Instead of returning bogus filenames to userspace, return -EUCLEAN.   

These calling conventions are rather suboptimal.  First of all, none of actor callbacks will ever get called directly. There are only 4 callers.  3 of them (all in this) are of this form.  The fourth is this, which itself is an actor callback. So all these ""return -E..."" in the instances are completely pointless; we should just turn filldir_t into pointer-to-function-returning-bool and get rid of that boilerplate, rather than adding more to it. Furthermore, who the hell cares which callback has stepped into it? ""The first time it happened from getdents in a 32 bit process and that's all you'll ever get out of me"" seems to be less than helpful... And frankly, I would prefer this, making that thing return -EUCLEAN or 0.  Quite possibly - inlining it as well..."
108,365796,367116,uncivil,Thanks for the review. The problem we have here is there is a potential to control 3 different LED string but only 2 sinks.  So control bank A can control 2 LED strings and control bank b can control 1 LED string. These values represent device level control and configuration of the LED strings to a specific control bank. I racked my brain trying to figure out how to configure the control banks and associated LED strings. These values are for the device configuration itself and the reg below indicates which control bank the LED node is assigned to. Don't see how you could compute this.  There is no easy way to give indication to the driver which LED node belongs to which control bank.  The control-bank-cfg is a device level property and the reg under the child is a LED string level property denoting the Class node to control bank mapping. Furthermore there are 2 device configurations that can be configured to only use 1 bank for all 3 LED strings. This will be answered in your comments in the code. This I can fix it should be a value between 1 and 6.,367099,technical,"Hi!  That's rather long and verbose way to describe a bitmap, right?   Is rbtree good idea? You don't have that many registers.  ....  No error checking required here?   This checks if we have just one bank, I see it. Should it also check the led actually uses the correct bank?   Is not this function done twice for non-error case?   The if is not needed here.   Misleading, this does nothing with regulators.  			","Hi!  That's rather long and verbose way to describe a bitmap, right?   Is rbtree good idea? You don't have that many registers.  ....  No error checking required here?   This checks if we have just one bank, I see it. Should it also check the led actually uses the correct bank?   Is not this function done twice for non-error case?   The if is not needed here.   Misleading, this does nothing with regulators.  			 Thanks for the review. The problem we have here is there is a potential to control 3 different LED string but only 2 sinks.  So control bank A can control 2 LED strings and control bank b can control 1 LED string. These values represent device level control and configuration of the LED strings to a specific control bank. I racked my brain trying to figure out how to configure the control banks and associated LED strings. These values are for the device configuration itself and the reg below indicates which control bank the LED node is assigned to. Don't see how you could compute this.  There is no easy way to give indication to the driver which LED node belongs to which control bank.  The control-bank-cfg is a device level property and the reg under the child is a LED string level property denoting the Class node to control bank mapping. Furthermore there are 2 device configurations that can be configured to only use 1 bank for all 3 LED strings. This will be answered in your comments in the code. This I can fix it should be a value between 1 and 6."
109,365796,367141,civil,"Just a blind shot, without going into details - could you please check if led-sources property documented in the common LED bindings couldn't help here?",367140,technical,"Yes, and LED strings are statically assigned to banks, right?  So why not simply forget about LED strings for sake of hw abstractions, and work just with banks? ","Yes, and LED strings are statically assigned to banks, right?  So why not simply forget about LED strings for sake of hw abstractions, and work just with banks?  Just a blind shot, without going into details - could you please check if led-sources property documented in the common LED bindings couldn't help here?"
110,365796,367690,civil,This is better than my proposal. Thanks!,367623,technical,"led-sources was designed for describing the topology where one LED can be connected to more then one output, see bindings of max77693-led (in Documentation).  Here the topology is a bit different - more than one LED (string) can be connected to a single bank, but this is accomplished inside the chip. Logically LEDs configured that way can be treated as a single LED (string) connected to two outputs, and what follows they should be described by a single DT child node.  led-sources will fit very well for this purpose. You could do the following mapping: Then, in the child DT nodes you would use these identifiers to describe the topology:  Following node would describe strings connected to the outputs HVLED1 and HVLED2 controlled by bank A.","led-sources was designed for describing the topology where one LED can be connected to more then one output, see bindings of max77693-led (in Documentation).  Here the topology is a bit different - more than one LED (string) can be connected to a single bank, but this is accomplished inside the chip. Logically LEDs configured that way can be treated as a single LED (string) connected to two outputs, and what follows they should be described by a single DT child node.  led-sources will fit very well for this purpose. You could do the following mapping: Then, in the child DT nodes you would use these identifiers to describe the topology:  Following node would describe strings connected to the outputs HVLED1 and HVLED2 controlled by bank A. This is better than my proposal. Thanks!"
111,365796,367764,uncivil,"It is better do add some complexity to the driver than to the user configurable settings like DT. Besides - you will only need to check if given led-source is already taken by another node. Some description will be needed for sure, but I don't expect it to be overwhelmingly lengthy. Your control-bank-cfg seemed like having much room for improvement, and it would for sure raise questions on why it was implemented that way. Documenting all available combinations of the configuration is seldom the best solution. It often obscures the issue. In your bindings device configuration is scattered among global control-bank-cfg property and child node's reg property. In my proposal each child node contains all the needed configuration, also in the form of two properties - led-sources and reg. IMHO having all the LED class device related configuration in one place simplifies the analysis.",367698,technical,"I agree to use the led-sources but I still believe this approach may be confusing to other sw devs and will lead to configuration issues by users.  This implementation requires the sw dev to know which strings are controlled by which bank. And this method may produce a misconfiguration like something below where HVLED2 is declared in both bank A and bank B.  The driver will need to be intelligent and declare a miss configuration on the above. Not saying this cannot be done but I am not sure why we want to add all of these extra LoC and intelligence in the kernel driver. The driver cannot make assumptions on the intention.  And the device tree documentation will need to pretty much need a lengthy explanation on how to configure the child nodes.  The implementation I suggested removes that ambiguity.  It is a simple integer that is written to the device as part of the device configuration, which the config is a setting for the device.  The child nodes denote which bank the exposed LED node will control.  Removing any need for the sw developers new or old to know the specific device configurations. ","I agree to use the led-sources but I still believe this approach may be confusing to other sw devs and will lead to configuration issues by users.  This implementation requires the sw dev to know which strings are controlled by which bank. And this method may produce a misconfiguration like something below where HVLED2 is declared in both bank A and bank B.  The driver will need to be intelligent and declare a miss configuration on the above. Not saying this cannot be done but I am not sure why we want to add all of these extra LoC and intelligence in the kernel driver. The driver cannot make assumptions on the intention.  And the device tree documentation will need to pretty much need a lengthy explanation on how to configure the child nodes.  The implementation I suggested removes that ambiguity.  It is a simple integer that is written to the device as part of the device configuration, which the config is a setting for the device.  The child nodes denote which bank the exposed LED node will control.  Removing any need for the sw developers new or old to know the specific device configurations.  It is better do add some complexity to the driver than to the user configurable settings like DT. Besides - you will only need to check if given led-source is already taken by another node. Some description will be needed for sure, but I don't expect it to be overwhelmingly lengthy. Your control-bank-cfg seemed like having much room for improvement, and it would for sure raise questions on why it was implemented that way. Documenting all available combinations of the configuration is seldom the best solution. It often obscures the issue. In your bindings device configuration is scattered among global control-bank-cfg property and child node's reg property. In my proposal each child node contains all the needed configuration, also in the form of two properties - led-sources and reg. IMHO having all the LED class device related configuration in one place simplifies the analysis."
112,368515,368789,civil,"I welcome this feature, been wanting it for some time now. There is simply not enough support in maps or smaps to get this information. This is important to improve code and data layouts.
I would like to see the following changes to your proposal: - call it PERF_SAMPLE_DATA_PAGE_SIZE
That would allow two things:
   1 - not tied to PERF_SAMPLE_ADDR
   2 - Allow PERF_SAMPLE_CODE_PAGE_SIZE to be added
In some measurements, you may just care about the distribution of accesses across page sizes. No need to use double the buffer space to save the address you will not use.
Layout is important for code as well, in fact, that's what most people want first. Having a CODE_PAGE_SIZE is therefore useful. I am happy adding it on top on your proposal. Note that PERF_SAMPLE_CODE_PAGE_SIZE would not have to be tied to PEBS unlike DATA_PAGE_SIZE. Thanks.",368520,technical,Extend sample-parsing test cases to support new sample type.,"Extend sample-parsing test cases to support new sample type. I welcome this feature, been wanting it for some time now. There is simply not enough support in maps or smaps to get this information. This is important to improve code and data layouts.
I would like to see the following changes to your proposal: - call it PERF_SAMPLE_DATA_PAGE_SIZE
That would allow two things:
   1 - not tied to PERF_SAMPLE_ADDR
   2 - Allow PERF_SAMPLE_CODE_PAGE_SIZE to be added
In some measurements, you may just care about the distribution of accesses across page sizes. No need to use double the buffer space to save the address you will not use.
Layout is important for code as well, in fact, that's what most people want first. Having a CODE_PAGE_SIZE is therefore useful. I am happy adding it on top on your proposal. Note that PERF_SAMPLE_CODE_PAGE_SIZE would not have to be tied to PEBS unlike DATA_PAGE_SIZE. Thanks."
113,372597,372959,uncivil,"This patch was corrupted by your email client, for example it turned TAB characters into sequences of spaces. Please fix this, email a test patch to yourself, and do not resend the patch to this mailing list until you can successfully extract and cleanly apply the test patch you email to yourself. Thank you.",372597,technical,"The tap_queue and the tap_dev"" are loosely coupled, not ""macvlan_dev"".  And I also change one rcu_read_lock's place, seems can reduce rcu critical section a little. ","The tap_queue and the tap_dev"" are loosely coupled, not ""macvlan_dev"".  And I also change one rcu_read_lock's place, seems can reduce rcu critical section a little.  This patch was corrupted by your email client, for example it turned TAB characters into sequences of spaces. Please fix this, email a test patch to yourself, and do not resend the patch to this mailing list until you can successfully extract and cleanly apply the test patch you email to yourself. Thank you."
114,374095,374559,civil,Please use your real name. 1st Signed-off-by and patch author should match. Good find! I'll queue this for the next fixes-pull-request.,374095,technical,"Even though we protect on-flash data by CRC checksums, we still don't trust the media. If lnum is not 0 or 1, access exceed array boundary can lead to bad situation.  ","Even though we protect on-flash data by CRC checksums, we still don't trust the media. If lnum is not 0 or 1, access exceed array boundary can lead to bad situation.   Please use your real name. 1st Signed-off-by and patch author should match. Good find! I'll queue this for the next fixes-pull-request."
115,377230,377461,civil,"Interesting - I don't see the grant head reservation code in any of my performance benchmark profiling, even when running at over a million transactions/s on a 2-socket 32-core 64-thread skylake system. I see other places in the transaction subsystem that are hot (e.g the CIL context lock), but not the space reservations. 
My initial suspect is that you have a tiny log on your test filesystem, so it's permanently out of space and so always hitting the slow path. Can you tell us what the storage is and it's configuration? At minimum, I need to see the output of the xfs_info command on your test filesystem. Fixing this may simply be using a larger log on your benchmark systems.
FWIW, can you post the actual profile you are seeing in the commit message? That helps us identify similar problems in the future, and it lets us know what paths are leading to the transaction reservation contention. i.e. this may not even be a problem with the transaction reservation code itself. How does this impact on the strict FIFO queue behaviour the grant queues currently have? The current code only wakes up enough waiters to consume the newly available space and it queues new waiters to the tail of the queue. If there ever is a spurious wakeup then the waiter that was woken from the head remains there until the next wakeup comes in. This is intentional - spurious wakeups are rare enough we can ignore them because a) this is the slow path, and b) correctness is far more important that performance in this path. The fast path is already lockless, and we've already given up peformance if we reach this slow path. hence we only care about correctness in this path, not performance optimisation.
AFAICT the patch changes the spurious wakeup behaviour - it requeues tasks to the tail of the queue if there wasn't space available when they are woken, rather than leaving them as them at the head.  They now have to wait for all the other reservations to make progress. This breaks the guarantees of ordered forward progress the grant queue provides permanent transaction reservations and hence opens us up to log space deadlocks because those transactions can't move their objects forward in the log to free up space in the log... Also, I note that wake_q_add() assumes that the wake queue is a local stack object and so not subject to concurrency - it explicitly states this in the code. That's not the case here - the wake queue is part of the grant head, and so is subject to extreme concurrency that is tempered by a spin lock.  Does the wake_q code work correctly (e.g. have all the necessary memory barriers, etc) when it's not a local stack object and instead protected from concurrency by a spin lock? At minimum, the wake_q infrastructure comments and documentation need updating to accommodate this new use case that wake queues are being used for.

This doesn't generally doesn't happen because the space accounting tends to prevent multiple wakeups. i.e. we only wake the tasks we have reservation space for, and log space being made available tends to arrive in discrete chunks (because IO is slow!) such that that pending wakeups have already been processed before the next chunk of available space comes in....
Yes, but they are very rare and we don't really care about this in the slow path. If you see lots of them, it's typically a sign of an inappropriately configured filesystem for the workload being run. On a correctly configured system, we should almost never use this slow path....

I'm betting that you'll get that and a whole lot more simply by increasing the log size and not running the slow path at all.
Where's the hunk context in your headers? You must be using a non-standard git option here.
Linux kernel specific includes go in fs/xfs/xfs_linux.h, not individual files.
Why do you need to delete the ticket from the queue here? This leads to landmines and incorrect non-FIFO behaviour...
.... here. This is a potential list corruption landmine because this function now has unbalanced list add and removal contexts. IOWs, we can't restart this loop without first having guaranteed the ticket is not already on the ticket queue. You need to document constraints like this in comments and explain what code needs to guarantee those constraints are met. [Because, as I noted at the end, you got this wrong for xlog_grant_head_wake_all()]

To maintian FIFO behaviour, the ticket needs to be left at the head of the grant head wait queue until it has space available to make progress, not get removed and requeued to the tail. Spurious wake ups are irrelevant here - forwards progress (i.e. correctness) requires FIFO ticket ordering behaviour be maintained.
This push is needed to make the necessary space we are waiting on available in the log. Hence leaving it out of the loop you put below will cause the journal to get stuck in the spurious wakeup loop below and be unable to make progress. This will lead to filesystem hangs.
That's a new nested loop. Please implement it as a loop. This is buggy  - i will lead to hangs if the filesystem is shut down and there is a spurious wakeup that triggers this to go back to sleep. The shutdown check needs to break the sleep loop.
That's racy. You can't drop the spin lock between xlog_grant_head_wake() and xlog_grant_head_wait(), because free_bytes is only valid while while the spinlock is held.  Same for the ""wake_all"" variable you added. i..e. while waking up the waiters, we could have run out of space again and had more tasks queued, or had the AIL tail move and now have space available. Either way, we can do the wrong thing because we dropped the lock and free_bytes and wake_all are now stale and potentially incorrect. That's another landmine. Just define the wakeq in the context where it is used rather than use a function wide variable that requires reinitialisation. Ok, what about xlog_grant_head_wake_all()? You didn't convert that to use wake queues, and so that won't remove tickets for the grant head waiter list, and so those tasks will never get out of the new inner loop you added to xlog_grant_head_wait(). That means filesystem shutdowns will just hang the filesystem and leave it unmountable. Did you run this through fstests? ",377229,technical,"Running the AIM7 fserver workload on a 2-socket 24-core 48-thread Broadwell system, it was found that there were severe spinlock contention in the XFS code. In particular, this function consumes 69.7% of cpu time. The check function call and its sub-function calls underneath it consumed 27.2% of the cpu time. This function tried to wake up tasks in the log space wait queue and then put itself into the wait queue if there is not enough log space left.  The process of waking up task can be time consuming and it is not really necessary to hold an XFS lock while doing the wakeups. So the wake function is modified to put the tasks to be waken up into a wake_q to be passed to wake_up_q() without holding the lock.  Corresponding changes are made in xlog_grant_head_wait() to dequeue the tasks from the wait queue after they are put into the wake_q. This avoids multiple wakeups of the same task from different log space waiters. Multiple wakeups seems to be a possibility in the existing code too.  With the use of the wake_q, the cpu time used by this function dropped to 39.6%. However, the performance of the AIM7 fserver workload increased from 91,485.51 jobs/min to 397,290.21 jobs/min which was more than 4X improvement.  ","Running the AIM7 fserver workload on a 2-socket 24-core 48-thread Broadwell system, it was found that there were severe spinlock contention in the XFS code. In particular, this function consumes 69.7% of cpu time. The check function call and its sub-function calls underneath it consumed 27.2% of the cpu time. This function tried to wake up tasks in the log space wait queue and then put itself into the wait queue if there is not enough log space left.  The process of waking up task can be time consuming and it is not really necessary to hold an XFS lock while doing the wakeups. So the wake function is modified to put the tasks to be waken up into a wake_q to be passed to wake_up_q() without holding the lock.  Corresponding changes are made in xlog_grant_head_wait() to dequeue the tasks from the wait queue after they are put into the wake_q. This avoids multiple wakeups of the same task from different log space waiters. Multiple wakeups seems to be a possibility in the existing code too.  With the use of the wake_q, the cpu time used by this function dropped to 39.6%. However, the performance of the AIM7 fserver workload increased from 91,485.51 jobs/min to 397,290.21 jobs/min which was more than 4X improvement.   Interesting - I don't see the grant head reservation code in any of my performance benchmark profiling, even when running at over a million transactions/s on a 2-socket 32-core 64-thread skylake system. I see other places in the transaction subsystem that are hot (e.g the CIL context lock), but not the space reservations. 
My initial suspect is that you have a tiny log on your test filesystem, so it's permanently out of space and so always hitting the slow path. Can you tell us what the storage is and it's configuration? At minimum, I need to see the output of the xfs_info command on your test filesystem. Fixing this may simply be using a larger log on your benchmark systems.
FWIW, can you post the actual profile you are seeing in the commit message? That helps us identify similar problems in the future, and it lets us know what paths are leading to the transaction reservation contention. i.e. this may not even be a problem with the transaction reservation code itself. How does this impact on the strict FIFO queue behaviour the grant queues currently have? The current code only wakes up enough waiters to consume the newly available space and it queues new waiters to the tail of the queue. If there ever is a spurious wakeup then the waiter that was woken from the head remains there until the next wakeup comes in. This is intentional - spurious wakeups are rare enough we can ignore them because a) this is the slow path, and b) correctness is far more important that performance in this path. The fast path is already lockless, and we've already given up peformance if we reach this slow path. hence we only care about correctness in this path, not performance optimisation.
AFAICT the patch changes the spurious wakeup behaviour - it requeues tasks to the tail of the queue if there wasn't space available when they are woken, rather than leaving them as them at the head.  They now have to wait for all the other reservations to make progress. This breaks the guarantees of ordered forward progress the grant queue provides permanent transaction reservations and hence opens us up to log space deadlocks because those transactions can't move their objects forward in the log to free up space in the log... Also, I note that wake_q_add() assumes that the wake queue is a local stack object and so not subject to concurrency - it explicitly states this in the code. That's not the case here - the wake queue is part of the grant head, and so is subject to extreme concurrency that is tempered by a spin lock.  Does the wake_q code work correctly (e.g. have all the necessary memory barriers, etc) when it's not a local stack object and instead protected from concurrency by a spin lock? At minimum, the wake_q infrastructure comments and documentation need updating to accommodate this new use case that wake queues are being used for.

This doesn't generally doesn't happen because the space accounting tends to prevent multiple wakeups. i.e. we only wake the tasks we have reservation space for, and log space being made available tends to arrive in discrete chunks (because IO is slow!) such that that pending wakeups have already been processed before the next chunk of available space comes in....
Yes, but they are very rare and we don't really care about this in the slow path. If you see lots of them, it's typically a sign of an inappropriately configured filesystem for the workload being run. On a correctly configured system, we should almost never use this slow path....

I'm betting that you'll get that and a whole lot more simply by increasing the log size and not running the slow path at all.
Where's the hunk context in your headers? You must be using a non-standard git option here.
Linux kernel specific includes go in this, not individual files.
Why do you need to delete the ticket from the queue here? This leads to landmines and incorrect non-FIFO behaviour...
.... here. This is a potential list corruption landmine because this function now has unbalanced list add and removal contexts. IOWs, we can't restart this loop without first having guaranteed the ticket is not already on the ticket queue. You need to document constraints like this in comments and explain what code needs to guarantee those constraints are met. [Because, as I noted at the end, you got this wrong for xlog_grant_head_wake_all()]

To maintian FIFO behaviour, the ticket needs to be left at the head of the grant head wait queue until it has space available to make progress, not get removed and requeued to the tail. Spurious wake ups are irrelevant here - forwards progress (i.e. correctness) requires FIFO ticket ordering behaviour be maintained.
This push is needed to make the necessary space we are waiting on available in the log. Hence leaving it out of the loop you put below will cause the journal to get stuck in the spurious wakeup loop below and be unable to make progress. This will lead to filesystem hangs.
That's a new nested loop. Please implement it as a loop. This is buggy  - i will lead to hangs if the filesystem is shut down and there is a spurious wakeup that triggers this to go back to sleep. The shutdown check needs to break the sleep loop.
That's racy. You can't drop the spin lock between xlog_grant_head_wake() and xlog_grant_head_wait(), because free_bytes is only valid while while the spinlock is held.  Same for the ""wake_all"" variable you added. i..e. while waking up the waiters, we could have run out of space again and had more tasks queued, or had the AIL tail move and now have space available. Either way, we can do the wrong thing because we dropped the lock and free_bytes and wake_all are now stale and potentially incorrect. That's another landmine. Just define the wakeq in the context where it is used rather than use a function wide variable that requires reinitialisation. Ok, what about xlog_grant_head_wake_all()? You didn't convert that to use wake queues, and so that won't remove tickets for the grant head waiter list, and so those tasks will never get out of the new inner loop you added to xlog_grant_head_wait(). That means filesystem shutdowns will just hang the filesystem and leave it unmountable. Did you run this through fstests? "
116,377230,378511,civil,"Thanks for your detailed review of the patch. I now have a better understanding of what should and shouldn't be done. I have sent out a more conservative v2 patchset which, hopefully, can address the concerns that you raised.",378176,technical,"You are right. The XFS filesystem was created on a small ramfs device and so the disk space was tiny. The microbenchmark that I used exposes some extreme cases that may not be normally observed.   Those were part of the perf trace that I marked down:   A spurious wakeup shouldn't change the behavior as the waiter should find that it is still being queued (list not empty) and so will sleep again. However, if the waiter is rightfully waken but find that there is not enough log space somehow, it will put itself back to the end of the queue. That is a change in behavior that I think could be an issue.  I am just wondering why the code doesn't reserve the actual log space at the time a task is being waken. Instead, it has to try to get it itself after being waken.  Also I think the current code allows consecutive waiters to come in and wake up the same set of tasks again and again. That may be where a part of the performance slowdown come from.   As I said above, spurious wakeup shouldn't cause problem. However, consecutive waiters will probably wake up more tasks than there is log space available. In this case, some of the tasks will be put back to the queue. Maybe a counter to record the log space temporarily reserved for the waken-up task can help to prevent over-subscription.    The wake_q should work correctly as it is used by mutexes and rwsems. We will see a lot of problem reports if that is not the case.   That is probably true.   I am using a old git (1.8). I should probably upgrade to a newer one.   OK   It is because the wake_q uses a field in the task structure for queuing. So a task can only be in one wake_q at a time. Leaving the ticket in the queue may cause the task to be put into multiple wake_q causing missed wakeup, perhaps.    Yes, you are right. The wake function need to be modified as well.   OK, I need more time to think about some of the questions that you raise.  Thanks for reviewing the patch.","You are right. The XFS filesystem was created on a small ramfs device and so the disk space was tiny. The microbenchmark that I used exposes some extreme cases that may not be normally observed.   Those were part of the perf trace that I marked down:   A spurious wakeup shouldn't change the behavior as the waiter should find that it is still being queued (list not empty) and so will sleep again. However, if the waiter is rightfully waken but find that there is not enough log space somehow, it will put itself back to the end of the queue. That is a change in behavior that I think could be an issue.  I am just wondering why the code doesn't reserve the actual log space at the time a task is being waken. Instead, it has to try to get it itself after being waken.  Also I think the current code allows consecutive waiters to come in and wake up the same set of tasks again and again. That may be where a part of the performance slowdown come from.   As I said above, spurious wakeup shouldn't cause problem. However, consecutive waiters will probably wake up more tasks than there is log space available. In this case, some of the tasks will be put back to the queue. Maybe a counter to record the log space temporarily reserved for the waken-up task can help to prevent over-subscription.    The wake_q should work correctly as it is used by mutexes and rwsems. We will see a lot of problem reports if that is not the case.   That is probably true.   I am using a old git (1.8). I should probably upgrade to a newer one.   OK   It is because the wake_q uses a field in the task structure for queuing. So a task can only be in one wake_q at a time. Leaving the ticket in the queue may cause the task to be put into multiple wake_q causing missed wakeup, perhaps.    Yes, you are right. The wake function need to be modified as well.   OK, I need more time to think about some of the questions that you raise.  Thanks for reviewing the patch. Thanks for your detailed review of the patch. I now have a better understanding of what should and shouldn't be done. I have sent out a more conservative v2 patchset which, hopefully, can address the concerns that you raised."
117,378505,378562,uncivil,"Again I'll ask: what is the performance when the log is made large enough that your benchmark is not hammering the slow path? i.e. does running instead of using the default tiny log on your tiny test filesystem make the problem go away? Without that information, we have no idea what the slow path impact on peformance actually is, and whether it is worth persuing optimising slow path behaviour that very, very few production environments see lock contention in....",378507,technical,"In the current log space reservation slowpath code, the log space waiters are waken up by an incoming waiter while holding the lock. As the process of waking up a task can be time consuming, doing it while holding the lock can make spinlock contention, if present, more severe.  This patch changes the slowpath code to use the wake_q for waking up tasks without holding the lock, thus improving performance and reducing spinlock contention level.  Running the AIM7 fserver workload on a 2-socket 24-core 48-thread Broadwell system with a small xfs filesystem on ramfs, the performance increased from 192,666 jobs/min to 285,221 with this change. ","In the current log space reservation slowpath code, the log space waiters are waken up by an incoming waiter while holding the lock. As the process of waking up a task can be time consuming, doing it while holding the lock can make spinlock contention, if present, more severe.  This patch changes the slowpath code to use the wake_q for waking up tasks without holding the lock, thus improving performance and reducing spinlock contention level.  Running the AIM7 fserver workload on a 2-socket 24-core 48-thread Broadwell system with a small xfs filesystem on ramfs, the performance increased from 192,666 jobs/min to 285,221 with this change.  Again I'll ask: what is the performance when the log is made large enough that your benchmark is not hammering the slow path? i.e. does running instead of using the default tiny log on your tiny test filesystem make the problem go away? Without that information, we have no idea what the slow path impact on peformance actually is, and whether it is worth persuing optimising slow path behaviour that very, very few production environments see lock contention in...."
118,378505,379417,civil,"Can you please re-run and report the results for each patch on the ramdisk setup? And, please, include the mkfs.xfs or xfs_info output for the ramdisk filesystem so I can see /exactly/ how much concurrency the filesystems are providing to the benchmark you are running. 50GB is tiny for XFS. Personally, I've been using ~1PB filesystems(*) for the performance testing I've been doing
recently... Yes, petabytes. Sparse image files on really fast SSDs are a wonderful thing.",379318,technical,"Yes, I realise that.  I am expecting that when it comes to optimising for pmem, we'll actually rewrite the journal to map pmem and memcpy() directly rather than go through the buffering and IO layers we currently do so we can minimise write latency and control concurrency ourselves. Hence I'm not really concerned by performance issues with pmem at this point - most of our still users have traditional storage and will for a long time to come....","Yes, I realise that.  I am expecting that when it comes to optimising for pmem, we'll actually rewrite the journal to map pmem and memcpy() directly rather than go through the buffering and IO layers we currently do so we can minimise write latency and control concurrency ourselves. Hence I'm not really concerned by performance issues with pmem at this point - most of our still users have traditional storage and will for a long time to come.... Can you please re-run and report the results for each patch on the ramdisk setup? And, please, include the mkfs.xfs or xfs_info output for the ramdisk filesystem so I can see /exactly/ how much concurrency the filesystems are providing to the benchmark you are running. 50GB is tiny for XFS. Personally, I've been using ~1PB filesystems(*) for the performance testing I've been doing recently... Yes, petabytes. Sparse image files on really fast SSDs are a wonderful thing."
119,379138,379657,civil,"I like the idea and I think it's good direction to go, but could you please share some from perf stat or whatever you used to meassure the new performance?",379138,technical,"Currently in record mode the tool implements trace writing serially.  The algorithm loops over mapped per-cpu data buffers and stores ready  data chunks into a trace file using write() system call.  At some circumstances the kernel may lack free space in a buffer  because the other buffer's half is not yet written to disk due to  some other buffer's data writing by the tool at the moment.  Thus serial trace writing implementation may cause the kernel  to loose profiling data and that is what observed when profiling  highly parallel CPU bound workloads on machines with big number  of cores.  Experiment with G123 profiling matrix multiplication code executing 128  threads on Intel Xeon Phi (KNM) with 272 cores, like below, demonstrates data loss metrics value of 98%: Data loss metrics is the ratio lost_time/elapsed_time where  lost_time is the sum of time intervals containing lost records and elapsed_time is the elapsed application run time  under profiling.  Applying asynchronous trace streaming thru Posix AIO API  lowers data loss  metrics value providing ~25% improvement in average.  ","Currently in record mode the tool implements trace writing serially.  The algorithm loops over mapped per-cpu data buffers and stores ready  data chunks into a trace file using write() system call.  At some circumstances the kernel may lack free space in a buffer  because the other buffer's half is not yet written to disk due to  some other buffer's data writing by the tool at the moment.  Thus serial trace writing implementation may cause the kernel  to loose profiling data and that is what observed when profiling  highly parallel CPU bound workloads on machines with big number  of cores.  Experiment with G123 profiling matrix multiplication code executing 128  threads on Intel Xeon Phi (KNM) with 272 cores, like below, demonstrates data loss metrics value of 98%: Data loss metrics is the ratio lost_time/elapsed_time where  lost_time is the sum of time intervals containing lost records and elapsed_time is the elapsed application run time  under profiling.  Applying asynchronous trace streaming thru Posix AIO API  lowers data loss  metrics value providing ~25% improvement in average.   I like the idea and I think it's good direction to go, but could you please share some from perf stat or whatever you used to meassure the new performance?"
120,383199,392690,uncivil,"Ick, this is still messy, just try making this. Yeah, it's over 80 columns, but it looks better and is easier to read, right? Also, all your patches have the whitespace turned from tabs into spaces, making them impossible to be applied even if I wanted to :)",383199,technical,Using checkpatch.pl I was able to find a multiline dereference which goes again the coding style for the kernel. I'm still working on my email client so the indentation looks bad here (in gmail) but the arguments for this should go just under the opening ,"Using checkpatch.pl I was able to find a multiline dereference which goes again the coding style for the kernel. I'm still working on my email client so the indentation looks bad here (in gmail) but the arguments for this should go just under the opening  Ick, this is still messy, just try making this. Yeah, it's over 80 columns, but it looks better and is easier to read, right? Also, all your patches have the whitespace turned from tabs into spaces, making them impossible to be applied even if I wanted to :)"
121,390394,390741,civil,"Sorry but I don't like imposing a run-time check on everybody when stack-based requests are the odd ones out.  If we're going to make this a run-time check (I'd much prefer a compile-time check, but I understand that this may involve too much churn), then please do it for stack-based request users only.",390396,technical,"Since the size is now fixed, there is no need to include the tfm argument. This removes it from the definition and callers.   --","Since the size is now fixed, there is no need to include the tfm argument. This removes it from the definition and callers.   -- Sorry but I don't like imposing a run-time check on everybody when stack-based requests are the odd ones out.  If we're going to make this a run-time check (I'd much prefer a compile-time check, but I understand that this may involve too much churn), then please do it for stack-based request users only."
122,391895,394633,civil,"[ I am not subscribed to LKML, please keep me CC'd on replies ] I tried a simple test with several VMs (in my initial test, I have 48
idle 1-cpu 512-mb VMs and 2 idle 2-cpu, 2-gb VMs) using libvirt, none pinned to any CPUs. When I tried to set all of the top-level libvirt cpu cgroups' to be co-scheduled. There are several moving parts there, so I tried narrowing it down, by only coscheduling one VM, and thing seemed fine. One thing that is not entirely obvious to me (but might be completely intentional) is that since by default the top-level libvirt cpu cgroups are empty. the result of this should be a no-op, right? [This becomes relevant below] Specifically, all of the threads of qemu are in sub-cgroups, which do not indicate they are co-scheduling. When I then try to coschedule the second VM, the machine hangs. On the console, I see the same backtraces I see when I try to set all of the VMs to be coscheduled. I am happy to do any further debugging I can do, or try patches on top of those posted on the mailing list.",392481,technical,"Hi, Please document both of these kernel parameters in Documentation.","Hi, Please document both of these kernel parameters in Documentation. [ I am not subscribed to LKML, please keep me CC'd on replies ] I tried a simple test with several VMs (in my initial test, I have 48
idle 1-cpu 512-mb VMs and 2 idle 2-cpu, 2-gb VMs) using libvirt, none pinned to any CPUs. When I tried to set all of the top-level libvirt cpu cgroups' to be co-scheduled. There are several moving parts there, so I tried narrowing it down, by only coscheduling one VM, and thing seemed fine. One thing that is not entirely obvious to me (but might be completely intentional) is that since by default the top-level libvirt cpu cgroups are empty. the result of this should be a no-op, right? [This becomes relevant below] Specifically, all of the threads of qemu are in sub-cgroups, which do not indicate they are co-scheduling. When I then try to coschedule the second VM, the machine hangs. On the console, I see the same backtraces I see when I try to set all of the VMs to be coscheduled. I am happy to do any further debugging I can do, or try patches on top of those posted on the mailing list."
123,391895,397297,uncivil,"I don't call this non-intrusive. I'll beg to differ; this isn't anywhere near something to consider merging. Also 'happened' suggests a certain stage of completeness, this again doesn't qualify. There are known scalability problems with the existing cgroup muck; you just made things a ton worse. The existing cgroup overhead is significant, you also made that many times worse. The cgroup stuff needs cleanups and optimization, not this. That is the whole and only reason you did this; and it doesn't even begin to cover the requirements for it. Not to mention I detest cgroups; for their inherent complixity and the performance costs associated with them.  _If_ we're going to do something for L1TF then I feel it should not depend on cgroups. It is after all, perfectly possible to run a kvm thingy without cgroups. 
Note that in order to avoid PLE and paravirt spinlocks and paravirt tlb-invalidate you have to gang-schedule the _entire_ VM, not just SMT siblings. Now explain to me how you're going to gang-schedule a VM with a good number of vCPU threads (say spanning a number of nodes) and preserving the rest of CFS without it turning into a massive trainwreck? Such things (gang scheduling VMs) _are_ possible, but not within the confines of something like CFS, they are also fairly inefficient because, as you do note, you will have to explicitly schedule idle time for idle vCPUs. Things like the Tableau scheduler are what come to mind; but I'm not sure how to integrate that with a general purpose scheduling scheme. You pretty much have to dedicate a set of CPUs to just scheduling VMs with such a scheduler. And that would call for cpuset-v2 integration along with a new scheduling class. And then people will complain again that partitioning a system isn't dynamic enough and we need magic :/ (and this too would be tricky to virtualize itself). You gloss over a ton of details here; many of which are non trivial and marked broken in your patches. Unless you have solid suggestions on how to deal with all of them, this is a complete non-starter. The per-cpu IRQ/steal time accounting for example. The task timeline isn't the same on every CPU because of those. You now basically require steal time and IRQ load to match between CPUs. That places very strict requirements and effectively breaks virt invariance. That is, the scheduler now behaves significantly different inside a VM than it does outside of it -- without the guest being gang scheduled itself and having physical pinning to reflect the same topology the coschedule=1 thing should not be exposed in a guest. And that is a mayor failing IMO. Also; I think you're sharing a cfs_rq between CPUs. that is broken, the virtual runtime stuff needs nontrivial modifications for multiple CPUs. And if you do that, I've no idea how you're dealing with SMP affinities. You don't even begin to outline how you preserve smp-nice fairness. IOW it's completely friggin useless for L1TF. Have you actually read your own code? What about that atrocious locking you sprinkle all over the place? 'some additional lock contention' doesn't even begin to describe that horror show. Hint: we're not going to increase the lockdep subclasses, and most certainly not for scheduler locking. All in all, I'm not inclined to consider this approach, it complicates an already overly complicated thing (cpu-cgroups) and has a ton of unresolved issues while at the same time it doesn't (and cannot) meet the goal it was made for.",396847,technical,"Here is an extra"" patch containing bug fixes and warning removals, that I have accumulated up to this point.  It goes on top of the other 60 patches. (When it is time for v2, these fixes will be integrated into the appropriate patches within the series.)  The changes are:  1. Avoid a hang with nested scheduled task groups. 2. Get rid of a lockdep warning. 3. Get rid of warnings about missed clock updates. 4. Get rid of ""untested code path"" warnings/reminders (after testing    said code paths).  This should make experimenting with this patch series a little less bumpy.  ","Here is an extra"" patch containing bug fixes and warning removals, that I have accumulated up to this point.  It goes on top of the other 60 patches. (When it is time for v2, these fixes will be integrated into the appropriate patches within the series.)  The changes are:  1. Avoid a hang with nested scheduled task groups. 2. Get rid of a lockdep warning. 3. Get rid of warnings about missed clock updates. 4. Get rid of ""untested code path"" warnings/reminders (after testing    said code paths).  This should make experimenting with this patch series a little less bumpy.   I don't call this non-intrusive. I'll beg to differ; this isn't anywhere near something to consider merging. Also 'happened' suggests a certain stage of completeness, this again doesn't qualify. There are known scalability problems with the existing cgroup muck; you just made things a ton worse. The existing cgroup overhead is significant, you also made that many times worse. The cgroup stuff needs cleanups and optimization, not this. That is the whole and only reason you did this; and it doesn't even begin to cover the requirements for it. Not to mention I detest cgroups; for their inherent complixity and the performance costs associated with them.  _If_ we're going to do something for L1TF then I feel it should not depend on cgroups. It is after all, perfectly possible to run a kvm thingy without cgroups. 
Note that in order to avoid PLE and paravirt spinlocks and paravirt tlb-invalidate you have to gang-schedule the _entire_ VM, not just SMT siblings. Now explain to me how you're going to gang-schedule a VM with a good number of vCPU threads (say spanning a number of nodes) and preserving the rest of CFS without it turning into a massive trainwreck? Such things (gang scheduling VMs) _are_ possible, but not within the confines of something like CFS, they are also fairly inefficient because, as you do note, you will have to explicitly schedule idle time for idle vCPUs. Things like the Tableau scheduler are what come to mind; but I'm not sure how to integrate that with a general purpose scheduling scheme. You pretty much have to dedicate a set of CPUs to just scheduling VMs with such a scheduler. And that would call for cpuset-v2 integration along with a new scheduling class. And then people will complain again that partitioning a system isn't dynamic enough and we need magic :/ (and this too would be tricky to virtualize itself). You gloss over a ton of details here; many of which are non trivial and marked broken in your patches. Unless you have solid suggestions on how to deal with all of them, this is a complete non-starter. The per-cpu IRQ/steal time accounting for example. The task timeline isn't the same on every CPU because of those. You now basically require steal time and IRQ load to match between CPUs. That places very strict requirements and effectively breaks virt invariance. That is, the scheduler now behaves significantly different inside a VM than it does outside of it -- without the guest being gang scheduled itself and having physical pinning to reflect the same topology the coschedule=1 thing should not be exposed in a guest. And that is a mayor failing IMO. Also; I think you're sharing a cfs_rq between CPUs. that is broken, the virtual runtime stuff needs nontrivial modifications for multiple CPUs. And if you do that, I've no idea how you're dealing with SMP affinities. You don't even begin to outline how you preserve smp-nice fairness. IOW it's completely friggin useless for L1TF. Have you actually read your own code? What about that atrocious locking you sprinkle all over the place? 'some additional lock contention' doesn't even begin to describe that horror show. Hint: we're not going to increase the lockdep subclasses, and most certainly not for scheduler locking. All in all, I'm not inclined to consider this approach, it complicates an already overly complicated thing (cpu-cgroups) and has a ton of unresolved issues while at the same time it doesn't (and cannot) meet the goal it was made for."
124,391895,397649,uncivil,"Mm... there is certainly room for interpretation. :) For example, it is still possible to set affinities, to use nice, and to tune all the other existing CFS knobs. That is, if you have tuned the scheduler to your workload or your workload depends on some CFS feature to work efficiently (whether on purpose or not), then running with this patch set should not change the behavior of said workload. This patch set should ""just"" give the user the additional ability to coordinate scheduling decisions across multiple CPUs. At least, that's my goal.

If someone doesn't need it, they don't have to use it. Just like task groups. But maybe, people start experimenting with coordinated scheduling decisions -- after all, there is a ton of research on what one *could* do, if there was coscheduling. I did look over much of that research. What I didn't like about many of them, is that evaluation is based on a ""prototype"", that -- while making the point that coscheduling might be beneficial for that use case -- totally screws over the scheduler for any other use case. Like coscheduling based on deterministic, timed context switches across all CPUs. Bye bye interactivity. That is, what I call intrusive.
As mentioned before, existing scheduler features, like preemption, (should) still work as before with this variant of coscheduling, with the same look and feel. And who knows, maybe someone will come up with a use case that moves coscheduling out of its niche; like the auto-grouping feature promoted the use of task groups. 
I agree, that this isn't ready to be merged. Still, the current state is good to start a discussion about the involved mechanics. Are you referring to cgroups in general, or task groups (aka. the cpu controller) specifically?
With respect to scalability: many coscheduling use cases don't require synchronization across the whole system. With this patch set, only those parts that are actually coscheduled are involved in synchronization. So, conceptually, this scales to larger systems from that point of view.
If coscheduling of a larger fraction of the system is required, costs increase. So what? It's a trade-off. It may *still* be beneficial for a use case. If it is, it might get adopted. If not, that particular use case may be considered impractical unless someone comes up with a better implementation of coscheduling.
With respect to the need of cleanups and optimizations: I agree, that task groups are a bit messy. For example, here's my current wish list off the top of my head: 
a) lazy scheduler operations; for example: when dequeuing a task, don't bother walking up the task group hierarchy to dequeue all the SEs -- do it lazily when encountering an empty CFS RQ during picking when we hold the lock anyway.
b) ability to move CFS RQs between CPUs: someone changed the affinity of a cpuset? No problem, just attach the runqueue with all the tasks elsewhere. No need to touch each and every task.
c) light-weight task groups: don't allocate a runqueue for every CPU in the system, when it is known that tasks in the task group will only ever run on at most two CPUs, or so. (And while there is of course a use case for VMs in this, another class of use cases are auxiliary tasks, see eg, [1-5].)
Is this the level of optimizations, you're thinking about? Or do you want to throw away the whole nested CFS RQ experience in the code? It really isn't. But as your mind seems made up, I'm not going to bother to argue. Yes it is. But, for example, you won't have group-based fairness between multiple kvm thingies. Assuming, there is a cgroup-less solution that can prevent simultaneous execution of tasks on a core, when they're not supposed to. How would you tell the scheduler, which tasks these are? You probably don't -- for the same reason, why it is a bad idea to give an endless loop realtime priority. It's just a bad idea. As I said in the text you quoted: coscheduling comes with its own set of advantages and disadvantages. Just because you find one example, where it is a bad idea, doesn't make it a bad thing in general. With gang scheduling as defined by Feitelson and Rudolph, you'd have to explicitly schedule idle time. With coscheduling as defined by Ousterhout, you don't. In this patch set, the scheduling of idle time is ""merely"" a quirk of the implementation. And even with this implementation, there's nothing stopping you from down-sizing the width of the coscheduled set to take out the idle vCPUs dynamically, cutting down on fragmentation. Hence my ""counter"" suggestion in the form of this patch set: Integrated into a general purpose scheduler, no need to partition off a part of the system, not tied to just VM use cases. Yes, I do. :) I wanted a summary, not a design document. Maybe I was a bit to eager in condensing the design to just a few paragraphs... Address them one by one. Probably do some of the optimizations you suggested to just get rid of some of them. It's work in progress. Though, at this stage I am also really interested in things that are broken, that I am not aware of yet. I'll have to read up some more code to make a qualified statement here. It is not shared per se. There's only one CPU (the leader) making the scheduling decision for that runqueue and if another CPU needs to modify the runqueue, it works like it does for CPU runqueues as well: the other CPU works with the leader's time. There are also no tasks in a runqueue when it is responsible for more than one CPU.

Assuming, that a runqueue is responsible for a core and there are runnable tasks within the task group on said core, then there will one SE enqueued in that runqueue, a so called SD-SE (scheduling domain SE, or synchronization domain SE). This SD-SE represents the per CPU runqueues of this core of this task group. (As opposed to a ""normal"" task group SE (TG-SE), which represents just one runqueue in a different task group.) Tasks are still only enqueued in the per CPU runqueues. Works as before (or will work as before): a coscheduled task group has its own set of per CPU runqueues that hold the tasks of this group (per CPU). The load balancer will work on this subset of runqueues as it does on the ""normal"" per CPU runqueues -- smp-nice fairness and all. Do you believe me now, that L1TF is not ""the whole and only reason"" I did this? :grinning:
Currently, there are more code paths than I like, that climb up the parent relation to the top. They need to go, if we want to coschedule larger parts of the system in a more efficient manner. Hence, parts of my wish list further up.
That said, it is not as bad as you make it sound for the following three reasons:
a) The amount of CPUs that compete for a lock is currently governed by the ""cosched_max_level"" command line argument, making it a conscious decision to increase the overall overhead. Hence, coscheduling at, e.g., core level does not have a too serious impact on lock contention.
b) The runqueue locks are usually only taken by the leader of said runqueue. Hence, there is often only one user per lock even at higher levels. The prominent exception at this stage of the patch set is that enqueue and dequeue operations walk up the hierarchy up to the ""cosched_max_level"". And even then, due to lock chaining, multiple enqueue/dequeue operations on different CPUs can bubble up the shared part of the hierarchy in parallel.
c) The scheduling decision does not cause any lock contention by itself. Each CPU only accesses runqueues, where itself is the leader. Hence, once you have a relatively stable situation, lock contention is not an issue.
That's fine. Due to the overhead of nesting cgroups that you mentioned earlier, that many levels in the runqueue hierarchy are likely to be impracticable anyway. For the future, I imagine a more dynamic variant of task groups/scheduling domains, that can provide all the flexibility one would want without that deep of a nesting. At this stage, it is just a way to experiment with larger systems without having to disable lockdep.
Of course, if you have a suggestion for a different locking scheme, we can discuss that as well. The current one, is what I considered most suitable among some alternatives under the premise I was working: integrate coscheduling in a scheduler as an additional feature (instead of, eg, write a scheduler capable of coscheduling). So, I probably haven't considered all alternatives. Even if you're not inclined -- at this stage, if I may be so bold :) -- your feedback is valuable. Thank you for that.",397297,uncivil,"I don't call this non-intrusive.   I'll beg to differ, this isn't anywhere near something to consider merging. Also 'happened' suggests a certain stage of completeness, this again doesn't qualify.   There are known scalability problems with the existing cgroup muck, you just made things a ton worse. The existing cgroup overhead is significant, you also made that many times worse.  The cgroup stuff needs cleanups and optimization, not this.    That is the whole and only reason you did this, and it doesn't even begin to cover the requirements for it.  Not to mention I detest cgroups, for their inherent complixity and the performance costs associated with them.  _If_ we're going to do something for L1TF then I feel it should not depend on cgroups.  It is after all, perfectly possible to run a kvm thingy without cgroups.   Note that in order to avoid PLE and paravirt spinlocks and paravirt tlb-invalidate you have to gang-schedule the _entire_ VM, not just SMT siblings.  Now explain to me how you're going to gang-schedule a VM with a good number of vCPU threads (say spanning a number of nodes) and preserving the rest of CFS without it turning into a massive trainwreck?  Such things (gang scheduling VMs) _are_ possible, but not within the confines of something like CFS, they are also fairly inefficient because, as you do note, you will have to explicitly schedule idle time for idle vCPUs.  Things like the Tableau scheduler are what come to mind, but I'm not sure how to integrate that with a general purpose scheduling scheme. You pretty much have to dedicate a set of CPUs to just scheduling VMs with such a scheduler.  And that would call for cpuset-v2 integration along with a new scheduling class.  And then people will complain again that partitioning a system isn't dynamic enough and we need magic :unhappy:  (and this too would be tricky to virtualize itself)   You gloss over a ton of details here, many of which are non trivial and marked broken in your patches. Unless you have solid suggestions on how to deal with all of them, this is a complete non-starter.  The per-cpu IRQ/steal time accounting for example. The task timeline isn't the same on every CPU because of those.  You now basically require steal time and IRQ load to match between CPUs. That places very strict requirements and effectively breaks virt invariance. That is, the scheduler now behaves significantly different inside a VM than it does outside of it -- without the guest being gang scheduled itself and having physical pinning to reflect the same topology the coschedule=1 thing should not be exposed in a guest. And that is a mayor failing IMO.  Also, I think you're sharing a cfs_rq between CPUs: that is broken, the virtual runtime stuff needs nontrivial modifications for multiple CPUs. And if you do that, I've no idea how you're dealing with SMP affinities.  You don't even begin to outline how you preserve smp-nice fairness.   IOW it's completely friggin useless for L1TF.   Have you actually read your own code?  What about that atrocious locking you sprinkle all over the place? 'some additional lock contention' doesn't even begin to describe that horror show.  Hint: we're not going to increase the lockdep subclasses, and most certainly not for scheduler locking.   All in all, I'm not inclined to consider this approach, it complicates an already overly complicated thing (cpu-cgroups) and has a ton of unresolved issues while at the same time it doesn't (and cannot) meet the goal it was made for.","I don't call this non-intrusive.   I'll beg to differ, this isn't anywhere near something to consider merging. Also 'happened' suggests a certain stage of completeness, this again doesn't qualify.   There are known scalability problems with the existing cgroup muck, you just made things a ton worse. The existing cgroup overhead is significant, you also made that many times worse.  The cgroup stuff needs cleanups and optimization, not this.    That is the whole and only reason you did this, and it doesn't even begin to cover the requirements for it.  Not to mention I detest cgroups, for their inherent complixity and the performance costs associated with them.  _If_ we're going to do something for L1TF then I feel it should not depend on cgroups.  It is after all, perfectly possible to run a kvm thingy without cgroups.   Note that in order to avoid PLE and paravirt spinlocks and paravirt tlb-invalidate you have to gang-schedule the _entire_ VM, not just SMT siblings.  Now explain to me how you're going to gang-schedule a VM with a good number of vCPU threads (say spanning a number of nodes) and preserving the rest of CFS without it turning into a massive trainwreck?  Such things (gang scheduling VMs) _are_ possible, but not within the confines of something like CFS, they are also fairly inefficient because, as you do note, you will have to explicitly schedule idle time for idle vCPUs.  Things like the Tableau scheduler are what come to mind, but I'm not sure how to integrate that with a general purpose scheduling scheme. You pretty much have to dedicate a set of CPUs to just scheduling VMs with such a scheduler.  And that would call for cpuset-v2 integration along with a new scheduling class.  And then people will complain again that partitioning a system isn't dynamic enough and we need magic :unhappy:  (and this too would be tricky to virtualize itself)   You gloss over a ton of details here, many of which are non trivial and marked broken in your patches. Unless you have solid suggestions on how to deal with all of them, this is a complete non-starter.  The per-cpu IRQ/steal time accounting for example. The task timeline isn't the same on every CPU because of those.  You now basically require steal time and IRQ load to match between CPUs. That places very strict requirements and effectively breaks virt invariance. That is, the scheduler now behaves significantly different inside a VM than it does outside of it -- without the guest being gang scheduled itself and having physical pinning to reflect the same topology the coschedule=1 thing should not be exposed in a guest. And that is a mayor failing IMO.  Also, I think you're sharing a cfs_rq between CPUs: that is broken, the virtual runtime stuff needs nontrivial modifications for multiple CPUs. And if you do that, I've no idea how you're dealing with SMP affinities.  You don't even begin to outline how you preserve smp-nice fairness.   IOW it's completely friggin useless for L1TF.   Have you actually read your own code?  What about that atrocious locking you sprinkle all over the place? 'some additional lock contention' doesn't even begin to describe that horror show.  Hint: we're not going to increase the lockdep subclasses, and most certainly not for scheduler locking.   All in all, I'm not inclined to consider this approach, it complicates an already overly complicated thing (cpu-cgroups) and has a ton of unresolved issues while at the same time it doesn't (and cannot) meet the goal it was made for. Mm... there is certainly room for interpretation. :) For example, it is still possible to set affinities, to use nice, and to tune all the other existing CFS knobs. That is, if you have tuned the scheduler to your workload or your workload depends on some CFS feature to work efficiently (whether on purpose or not), then running with this patch set should not change the behavior of said workload. This patch set should ""just"" give the user the additional ability to coordinate scheduling decisions across multiple CPUs. At least, that's my goal.

If someone doesn't need it, they don't have to use it. Just like task groups. But maybe, people start experimenting with coordinated scheduling decisions -- after all, there is a ton of research on what one *could* do, if there was coscheduling. I did look over much of that research. What I didn't like about many of them, is that evaluation is based on a ""prototype"", that -- while making the point that coscheduling might be beneficial for that use case -- totally screws over the scheduler for any other use case. Like coscheduling based on deterministic, timed context switches across all CPUs. Bye bye interactivity. That is, what I call intrusive.
As mentioned before, existing scheduler features, like preemption, (should) still work as before with this variant of coscheduling, with the same look and feel. And who knows, maybe someone will come up with a use case that moves coscheduling out of its niche; like the auto-grouping feature promoted the use of task groups. 
I agree, that this isn't ready to be merged. Still, the current state is good to start a discussion about the involved mechanics. Are you referring to cgroups in general, or task groups (aka. the cpu controller) specifically?
With respect to scalability: many coscheduling use cases don't require synchronization across the whole system. With this patch set, only those parts that are actually coscheduled are involved in synchronization. So, conceptually, this scales to larger systems from that point of view.
If coscheduling of a larger fraction of the system is required, costs increase. So what? It's a trade-off. It may *still* be beneficial for a use case. If it is, it might get adopted. If not, that particular use case may be considered impractical unless someone comes up with a better implementation of coscheduling.
With respect to the need of cleanups and optimizations: I agree, that task groups are a bit messy. For example, here's my current wish list off the top of my head: 
a) lazy scheduler operations; for example: when dequeuing a task, don't bother walking up the task group hierarchy to dequeue all the SEs -- do it lazily when encountering an empty CFS RQ during picking when we hold the lock anyway.
b) ability to move CFS RQs between CPUs: someone changed the affinity of a cpuset? No problem, just attach the runqueue with all the tasks elsewhere. No need to touch each and every task.
c) light-weight task groups: don't allocate a runqueue for every CPU in the system, when it is known that tasks in the task group will only ever run on at most two CPUs, or so. (And while there is of course a use case for VMs in this, another class of use cases are auxiliary tasks, see eg, [1-5].)
Is this the level of optimizations, you're thinking about? Or do you want to throw away the whole nested CFS RQ experience in the code? It really isn't. But as your mind seems made up, I'm not going to bother to argue. Yes it is. But, for example, you won't have group-based fairness between multiple kvm thingies. Assuming, there is a cgroup-less solution that can prevent simultaneous execution of tasks on a core, when they're not supposed to. How would you tell the scheduler, which tasks these are? You probably don't -- for the same reason, why it is a bad idea to give an endless loop realtime priority. It's just a bad idea. As I said in the text you quoted: coscheduling comes with its own set of advantages and disadvantages. Just because you find one example, where it is a bad idea, doesn't make it a bad thing in general. With gang scheduling as defined by Feitelson and Rudolph, you'd have to explicitly schedule idle time. With coscheduling as defined by Ousterhout, you don't. In this patch set, the scheduling of idle time is ""merely"" a quirk of the implementation. And even with this implementation, there's nothing stopping you from down-sizing the width of the coscheduled set to take out the idle vCPUs dynamically, cutting down on fragmentation. Hence my ""counter"" suggestion in the form of this patch set: Integrated into a general purpose scheduler, no need to partition off a part of the system, not tied to just VM use cases. Yes, I do. :) I wanted a summary, not a design document. Maybe I was a bit to eager in condensing the design to just a few paragraphs... Address them one by one. Probably do some of the optimizations you suggested to just get rid of some of them. It's work in progress. Though, at this stage I am also really interested in things that are broken, that I am not aware of yet. I'll have to read up some more code to make a qualified statement here. It is not shared per se. There's only one CPU (the leader) making the scheduling decision for that runqueue and if another CPU needs to modify the runqueue, it works like it does for CPU runqueues as well: the other CPU works with the leader's time. There are also no tasks in a runqueue when it is responsible for more than one CPU.

Assuming, that a runqueue is responsible for a core and there are runnable tasks within the task group on said core, then there will one SE enqueued in that runqueue, a so called SD-SE (scheduling domain SE, or synchronization domain SE). This SD-SE represents the per CPU runqueues of this core of this task group. (As opposed to a ""normal"" task group SE (TG-SE), which represents just one runqueue in a different task group.) Tasks are still only enqueued in the per CPU runqueues. Works as before (or will work as before): a coscheduled task group has its own set of per CPU runqueues that hold the tasks of this group (per CPU). The load balancer will work on this subset of runqueues as it does on the ""normal"" per CPU runqueues -- smp-nice fairness and all. Do you believe me now, that L1TF is not ""the whole and only reason"" I did this? :grinning:
Currently, there are more code paths than I like, that climb up the parent relation to the top. They need to go, if we want to coschedule larger parts of the system in a more efficient manner. Hence, parts of my wish list further up.
That said, it is not as bad as you make it sound for the following three reasons:
a) The amount of CPUs that compete for a lock is currently governed by the ""cosched_max_level"" command line argument, making it a conscious decision to increase the overall overhead. Hence, coscheduling at, e.g., core level does not have a too serious impact on lock contention.
b) The runqueue locks are usually only taken by the leader of said runqueue. Hence, there is often only one user per lock even at higher levels. The prominent exception at this stage of the patch set is that enqueue and dequeue operations walk up the hierarchy up to the ""cosched_max_level"". And even then, due to lock chaining, multiple enqueue/dequeue operations on different CPUs can bubble up the shared part of the hierarchy in parallel.
c) The scheduling decision does not cause any lock contention by itself. Each CPU only accesses runqueues, where itself is the leader. Hence, once you have a relatively stable situation, lock contention is not an issue.
That's fine. Due to the overhead of nesting cgroups that you mentioned earlier, that many levels in the runqueue hierarchy are likely to be impracticable anyway. For the future, I imagine a more dynamic variant of task groups/scheduling domains, that can provide all the flexibility one would want without that deep of a nesting. At this stage, it is just a way to experiment with larger systems without having to disable lockdep.
Of course, if you have a suggestion for a different locking scheme, we can discuss that as well. The current one, is what I considered most suitable among some alternatives under the premise I was working: integrate coscheduling in a scheduler as an additional feature (instead of, eg, write a scheduler capable of coscheduling). So, I probably haven't considered all alternatives. Even if you're not inclined -- at this stage, if I may be so bold :) -- your feedback is valuable. Thank you for that."
125,399771,400133,uncivil,"How can you set a shared variable with no synchronization? A bool is particularly dangerous here, at least on some arches.
",399771,technical,"When link status change needs to be committed and rtnl lock couldn't be taken, avoid redisplay of same link status change message.  ","When link status change needs to be committed and rtnl lock couldn't be taken, avoid redisplay of same link status change message.   How can you set a shared variable with no synchronization? A bool is particularly dangerous here, at least on some arches.
"
126,399771,401526,uncivil,"
If try lock can not grab RTNL, there is no way the current thread can set the  variable without a race, if the word including rtnl_needed is shared by other fields in the structure. Your patch adds a subtle possibility of future bugs, even if it runs fine today. Do not pave the way for future bugs, make your code robust, please.",401073,technical,"Thanks for reviewing the patch. rtnl_needed is not a shared variable, it is part of bonding structure, that is one per bonding driver instance. There can't be two parallel instances of bond_miimon_inspect for a single  bonding driver instance at any given point of time. and only bond_miimon_inspect updates it. That's why I think there is no need of any synchronization here.    Thank you for cautioning us on bool usage. even a u8 can meet our requirement.  we will change it.  but, if time permits can you share more on particularly dangerous here, at least on some arches.","Thanks for reviewing the patch. rtnl_needed is not a shared variable, it is part of bonding structure, that is one per bonding driver instance. There can't be two parallel instances of bond_miimon_inspect for a single  bonding driver instance at any given point of time. and only bond_miimon_inspect updates it. That's why I think there is no need of any synchronization here.    Thank you for cautioning us on bool usage. even a u8 can meet our requirement.  we will change it.  but, if time permits can you share more on particularly dangerous here, at least on some arches. 
If try lock can not grab RTNL, there is no way the current thread can set the  variable without a race, if the word including rtnl_needed is shared by other fields in the structure. Your patch adds a subtle possibility of future bugs, even if it runs fine today. Do not pave the way for future bugs, make your code robust, please."
127,402510,413538,uncivil,"It would be very helpful if you cc all involved people on the cover letter instead of just cc'ing your own pile of email addresses. CC'ed now. This is really not helpful. The cover letter and the change logs should contain a summary of that discussion and a proper justification of the proposed change. Just saying 'sysadmins might want to allow' is not useful at all, it's yet another 'I want a pony' thing.

I read through the previous thread and there was a clear request to involve security people into this. Especially those who are deeply involved with hardware side channels. I don't see anyone Cc'ed on the whole series. For the record, I'm not buying the handwavy 'more noise' argument at all. It wants a proper analysis and we need to come up with criteria which PMUs can be exposed at all. All of this want's a proper documentation clearly explaining the risks and scope of these knobs per PMU. Just throwing magic knobs at sysadmins and then saying 'its their problem to figure it out' is not acceptable.",413483,technical,There is a hunk a bit lower in the patch where in perf_pmu_register the  initial setting is assigned from the global sysctl.   Sure!,"There is a hunk a bit lower in the patch where in perf_pmu_register the  initial setting is assigned from the global sysctl.   Sure! It would be very helpful if you cc all involved people on the cover letter instead of just cc'ing your own pile of email addresses. CC'ed now. This is really not helpful. The cover letter and the change logs should contain a summary of that discussion and a proper justification of the proposed change. Just saying 'sysadmins might want to allow' is not useful at all, it's yet another 'I want a pony' thing.

I read through the previous thread and there was a clear request to involve security people into this. Especially those who are deeply involved with hardware side channels. I don't see anyone Cc'ed on the whole series. For the record, I'm not buying the handwavy 'more noise' argument at all. It wants a proper analysis and we need to come up with criteria which PMUs can be exposed at all. All of this want's a proper documentation clearly explaining the risks and scope of these knobs per PMU. Just throwing magic knobs at sysadmins and then saying 'its their problem to figure it out' is not acceptable."
128,402510,413650,uncivil,"I accept it was by bad to miss adding Cc's on the cover letter, but my own email addresses hopefully should not bother you. It is simply a question of what I have in .gitconfig vs what I forgot to do manually. Okay, for the next round I will expand the cover letter with at least one concrete example on how it is usable and summarize the discussion a bit. Who would you recommend I add? Because I really don't know..
Presumably you see adding fine grained control as diminishing the overall security rather than raising it? Could you explain why? Because incompetent sysadmin will turn it off for some PMU, while without having the fine-grained control they wouldn't turn it off globally? This feature was requested by the exact opposite concern, that in order to access the i915 PMU, one has to compromise the security of the entire system by allowing access to *all* PMU's. Making this ability fine-grained sounds like a logical solution for solving this weakening of security controls. Concrete example was that on video transcoding farms users want to monitor the utilization of GPU engines (like CPU cores) and they can do that via the i915 PMU. But for that to work today they have to dial down the global perf_event_paranoid setting. Obvious improvement was to allow them to only dial down the i915.perf_event_paranoid setting. As such, for this specific use case at least, the security is increased.",413538,uncivil,"It would be very helpful if you cc all involved people on the cover letter instead of just cc'ing your own pile of email addresses. CC'ed now.   This is really not helpful. The cover letter and the change logs should contain a summary of that discussion and a proper justification of the proposed change. Just saying 'sysadmins might want to allow' is not useful at all, it's yet another 'I want a pony' thing.  I read through the previous thread and there was a clear request to involve security people into this. Especially those who are deeply involved with hardware side channels. I don't see anyone Cc'ed on the whole series.  For the record, I'm not buying the handwavy 'more noise' argument at all. It wants a proper analysis and we need to come up with criteria which PMUs can be exposed at all.  All of this want's a proper documentation clearly explaining the risks and scope of these knobs per PMU. Just throwing magic knobs at sysadmins and then saying 'its their problem to figure it out' is not acceptable.","It would be very helpful if you cc all involved people on the cover letter instead of just cc'ing your own pile of email addresses. CC'ed now.   This is really not helpful. The cover letter and the change logs should contain a summary of that discussion and a proper justification of the proposed change. Just saying 'sysadmins might want to allow' is not useful at all, it's yet another 'I want a pony' thing.  I read through the previous thread and there was a clear request to involve security people into this. Especially those who are deeply involved with hardware side channels. I don't see anyone Cc'ed on the whole series.  For the record, I'm not buying the handwavy 'more noise' argument at all. It wants a proper analysis and we need to come up with criteria which PMUs can be exposed at all.  All of this want's a proper documentation clearly explaining the risks and scope of these knobs per PMU. Just throwing magic knobs at sysadmins and then saying 'its their problem to figure it out' is not acceptable. I accept it was by bad to miss adding Cc's on the cover letter, but my own email addresses hopefully should not bother you. It is simply a question of what I have in .gitconfig vs what I forgot to do manually. Okay, for the next round I will expand the cover letter with at least one concrete example on how it is usable and summarize the discussion a bit. Who would you recommend I add? Because I really don't know..
Presumably you see adding fine grained control as diminishing the overall security rather than raising it? Could you explain why? Because incompetent sysadmin will turn it off for some PMU, while without having the fine-grained control they wouldn't turn it off globally? This feature was requested by the exact opposite concern, that in order to access the i915 PMU, one has to compromise the security of the entire system by allowing access to *all* PMU's. Making this ability fine-grained sounds like a logical solution for solving this weakening of security controls. Concrete example was that on video transcoding farms users want to monitor the utilization of GPU engines (like CPU cores) and they can do that via the i915 PMU. But for that to work today they have to dial down the global perf_event_paranoid setting. Obvious improvement was to allow them to only dial down the i915.perf_event_paranoid setting. As such, for this specific use case at least, the security is increased."
129,402510,413698,uncivil,"The keyword in the above sentence is 'just'. You can add as many of yours
as you want as long as everybody else is cc'ed. Sure, and because you don't know you didn't bother to ask around and ignored the review request. I already added Kees and Jann. Please look for the SECCOMP folks in MAINTAINERS. I did not say at all that this might be diminishing security. And the argumentation with 'incompetent sysadmins' is just the wrong attitude.
What I was asking for is proper documentation and this proper documentation is meant for _competent_ sysadmins.
That documentation has to clearly describe what kind of information is accessible and what potential side effects security wise this might have. You cannot expect that even competent sysadmins know offhand what which PMU might expose. And telling them 'Use Google' is just not the right thing to do.
If you can't explain and document it, then providing the knob is just fulfilling somebodys 'I want a pony' request.
Sure, and this wants to be documented in the cover letter and the changelogs.
But this does also require a proper analysis and documentation why it is not a security risk to expose the i915 PMU or what potential security issues this can create, so that the competent sysadmin can make a judgement.
And the same is required for all other PMUs which can be enabled in the same way for unprivileged access. And we might as well come to the conclusion via analysis that for some PMUs unpriviledged access is just not a good idea and exclude them. I surely know a few which qualify for exclusion, so the right approach is to provide this knob only when the risk is analyzed and documented and the PMU has been flagged as candidate for unpriviledged exposure. I.e. opt in and not opt out.",413650,uncivil,"Hi, I accept it was by bad to miss adding Cc's on the cover letter, but my  own email addresses hopefully should not bother you. It is simply a  question of what I have in .gitconfig vs what I forgot to do manually.   Okay, for the next round I will expand the cover letter with at least  one concrete example on how it is usable and summarize the discussion a bit.   Who would you recommend I add? Because I really don't know..   Presumably you see adding fine grained control as diminishing the  overall security rather than raising it? Could you explain why? Because  incompetent sysadmin will turn it off for some PMU, while without having  the fine-grained control they wouldn't turn it off globally?  This feature was requested by the exact opposite concern, that in order  to access the i915 PMU, one has to compromise the security of the entire  system by allowing access to *all* PMU's.  Making this ability fine-grained sounds like a logical solution for  solving this weakening of security controls.  Concrete example was that on video transcoding farms users want to  monitor the utilization of GPU engines (like CPU cores) and they can do  that via the i915 PMU. But for that to work today they have to dial down  the global setting. Obvious improvement was to allow  them to only dial down the setting. As such,  for this specific use case at least, the security is increased.","Hi, I accept it was by bad to miss adding Cc's on the cover letter, but my  own email addresses hopefully should not bother you. It is simply a  question of what I have in .gitconfig vs what I forgot to do manually.   Okay, for the next round I will expand the cover letter with at least  one concrete example on how it is usable and summarize the discussion a bit.   Who would you recommend I add? Because I really don't know..   Presumably you see adding fine grained control as diminishing the  overall security rather than raising it? Could you explain why? Because  incompetent sysadmin will turn it off for some PMU, while without having  the fine-grained control they wouldn't turn it off globally?  This feature was requested by the exact opposite concern, that in order  to access the i915 PMU, one has to compromise the security of the entire  system by allowing access to *all* PMU's.  Making this ability fine-grained sounds like a logical solution for  solving this weakening of security controls.  Concrete example was that on video transcoding farms users want to  monitor the utilization of GPU engines (like CPU cores) and they can do  that via the i915 PMU. But for that to work today they have to dial down  the global setting. Obvious improvement was to allow  them to only dial down the setting. As such,  for this specific use case at least, the security is increased. The keyword in the above sentence is 'just'. You can add as many of yours
as you want as long as everybody else is cc'ed. Sure, and because you don't know you didn't bother to ask around and ignored the review request. I already added Kees and Jann. Please look for the SECCOMP folks in MAINTAINERS. I did not say at all that this might be diminishing security. And the argumentation with 'incompetent sysadmins' is just the wrong attitude.
What I was asking for is proper documentation and this proper documentation is meant for _competent_ sysadmins.
That documentation has to clearly describe what kind of information is accessible and what potential side effects security wise this might have. You cannot expect that even competent sysadmins know offhand what which PMU might expose. And telling them 'Use Google' is just not the right thing to do.
If you can't explain and document it, then providing the knob is just fulfilling somebodys 'I want a pony' request.
Sure, and this wants to be documented in the cover letter and the changelogs.
But this does also require a proper analysis and documentation why it is not a security risk to expose the i915 PMU or what potential security issues this can create, so that the competent sysadmin can make a judgement.
And the same is required for all other PMUs which can be enabled in the same way for unprivileged access. And we might as well come to the conclusion via analysis that for some PMUs unpriviledged access is just not a good idea and exclude them. I surely know a few which qualify for exclusion, so the right approach is to provide this knob only when the risk is analyzed and documented and the PMU has been flagged as candidate for unpriviledged exposure. I.e. opt in and not opt out."
130,402510,413768,uncivil,"Sure, but you also used the word ""pile"" and I would argue that made the rest of your sentence, after and including ""instead"", sound like it not only bothers you I forgot to Cc people on the cover letter, but it also bothers you I included a ""pile"" of my own addresses. If that wasn't your intention in the slightest then I apologise for misreading it. No, not because of that. You are assuming my actions and motivations and constructing a story. ""did not bother"" = negative connotations, ""ignored"" = negative connotations. Note instead the time lapse between this and previous posting of the series, and if you want to assume something, assume things can get missed and forgotten without intent or malice. Thanks! 
Wrong attitude what? I was trying to guess your reasoning (cues in ""presumably"" and a lot of question marks) since it wasn't clear to me why is your position what it is. I did not mention Google. Well it's not a pony, it is mechanism to avoid having to turn off all security. We can hopefully discuss it without ponies. I am happy to work on the mechanics of achieving this once the security guys and all PMU owners get involved. Even though I am not convinced the bar to allow fine-grained control should be evaluating all possible PMUs*, but if the security folks agree that is the case it is fine by me.
The part of my reply you did not quote explains how the fine-grained control improves security in existing deployments. The documentation I added refers to the existing perf_event_paranoid documentation for explanation of security concerns involved. Which is not much in itself. But essentially we both have a PMU and a knob already. I don't see why adding the same knob per-PMU needs much more stringent criteria to be accepted. But as said, that's for security people to decide.",413698,uncivil,"The keyword in the above sentence is 'just'. You can add as many of yours as you want as long as everybody else is cc'ed.   Sure, and because you don't know you didn't bother to ask around and ignored the review request.  I already added them. Please look for the SECCOMP folks in MAINTAINERS.   I did not say at all that this might be diminishing security. And the argumentation with 'incompetent sysadmins' is just the wrong attitude.  What I was asking for is proper documentation and this proper documentation is meant for _competent_ sysadmins.  That documentation has to clearly describe what kind of information is accessible and what potential side effects security wise this might have. You cannot expect that even competent sysadmins know offhand what which PMU might expose. And telling them 'Use Google' is just not the right thing to do.  If you can't explain and document it, then providing the knob is just fulfilling somebodys 'I want a pony' request.   Sure, and this wants to be documented in the cover letter and the changelogs.  But this does also require a proper analysis and documentation why it is not a security risk to expose the i915 PMU or what potential security issues this can create, so that the competent sysadmin can make a judgement.  And the same is required for all other PMUs which can be enabled in the same way for unprivileged access. And we might as well come to the conclusion via analysis that for some PMUs unpriviledged access is just not a good idea and exclude them. I surely know a few which qualify for exclusion, so the right approach is to provide this knob only when the risk is analyzed and documented and the PMU has been flagged as candidate for unpriviledged exposure. I.e. opt in and not opt out.","The keyword in the above sentence is 'just'. You can add as many of yours as you want as long as everybody else is cc'ed.   Sure, and because you don't know you didn't bother to ask around and ignored the review request.  I already added them. Please look for the SECCOMP folks in MAINTAINERS.   I did not say at all that this might be diminishing security. And the argumentation with 'incompetent sysadmins' is just the wrong attitude.  What I was asking for is proper documentation and this proper documentation is meant for _competent_ sysadmins.  That documentation has to clearly describe what kind of information is accessible and what potential side effects security wise this might have. You cannot expect that even competent sysadmins know offhand what which PMU might expose. And telling them 'Use Google' is just not the right thing to do.  If you can't explain and document it, then providing the knob is just fulfilling somebodys 'I want a pony' request.   Sure, and this wants to be documented in the cover letter and the changelogs.  But this does also require a proper analysis and documentation why it is not a security risk to expose the i915 PMU or what potential security issues this can create, so that the competent sysadmin can make a judgement.  And the same is required for all other PMUs which can be enabled in the same way for unprivileged access. And we might as well come to the conclusion via analysis that for some PMUs unpriviledged access is just not a good idea and exclude them. I surely know a few which qualify for exclusion, so the right approach is to provide this knob only when the risk is analyzed and documented and the PMU has been flagged as candidate for unpriviledged exposure. I.e. opt in and not opt out. Sure, but you also used the word ""pile"" and I would argue that made the rest of your sentence, after and including ""instead"", sound like it not only bothers you I forgot to Cc people on the cover letter, but it also bothers you I included a ""pile"" of my own addresses. If that wasn't your intention in the slightest then I apologise for misreading it. No, not because of that. You are assuming my actions and motivations and constructing a story. ""did not bother"" = negative connotations, ""ignored"" = negative connotations. Note instead the time lapse between this and previous posting of the series, and if you want to assume something, assume things can get missed and forgotten without intent or malice. Thanks! 
Wrong attitude what? I was trying to guess your reasoning (cues in ""presumably"" and a lot of question marks) since it wasn't clear to me why is your position what it is. I did not mention Google. Well it's not a pony, it is mechanism to avoid having to turn off all security. We can hopefully discuss it without ponies. I am happy to work on the mechanics of achieving this once the security guys and all PMU owners get involved. Even though I am not convinced the bar to allow fine-grained control should be evaluating all possible PMUs*, but if the security folks agree that is the case it is fine by me.
The part of my reply you did not quote explains how the fine-grained control improves security in existing deployments. The documentation I added refers to the existing perf_event_paranoid documentation for explanation of security concerns involved. Which is not much in itself. But essentially we both have a PMU and a knob already. I don't see why adding the same knob per-PMU needs much more stringent criteria to be accepted. But as said, that's for security people to decide."
131,402510,413778,uncivil,"Guessing my reasonings has nothing to do with you mentioning incompentent sysadmins. I did not say that you mentioned google. But what is a sysadmin supposed to do when there is no documentation aside of using google? And not having documentation is basically the same thing as telling them to use google. If you want to make a pettifogger contest out of this discussion, then we can stop right here. I explained it technically why just adding a knob without further explanation and analysis is not acceptable. Making the knob opt in per PMU does not need all PMU owners to be involved. It allows to add the opt in flag on a case by case basis. The fact, that the existing knob is poorly documented does make an excuse for adding more knobs without documentation. Quite the contrary, if we notice that the existing knob lacks proper documentation, then we should fix that first.",413776,technical,Which paranoia level would be used for the setting in such a case?  Perhaps also CC  on the next version.,"Which paranoia level would be used for the setting in such a case?  Perhaps also CC  on the next version. Guessing my reasonings has nothing to do with you mentioning incompentent sysadmins. I did not say that you mentioned google. But what is a sysadmin supposed to do when there is no documentation aside of using google? And not having documentation is basically the same thing as telling them to use google. If you want to make a pettifogger contest out of this discussion, then we can stop right here. I explained it technically why just adding a knob without further explanation and analysis is not acceptable. Making the knob opt in per PMU does not need all PMU owners to be involved. It allows to add the opt in flag on a case by case basis. The fact, that the existing knob is poorly documented does make an excuse for adding more knobs without documentation. Quite the contrary, if we notice that the existing knob lacks proper documentation, then we should fix that first."
132,402510,414000,uncivil,"Ah only if google could simply answer all our questions! It's not like there is or isn't a security risk and that you can say that it is or it isn't in a global way.
Essentially these are channels of information. The channels always exist in form of timing variances for any shared resource (like shared caches or shared memory/IO/interconnect bandwidth) that can be measured. Perfmon counters make the channels generally less noisy, but they do not cause
them. To really close them completely you would need to avoid sharing anything, or not allowing to measure time, neither of which is practical short of an air gap. There are reasonable assesments you can make either way and the answers will be different based on your requirements. There isn't a single answer that works for everyone. 
There are cases where it isn't a problem at all. If you don't have multiple users on the system your tolerance should be extremely high. For users who have multiple users there can be different tradeoffs. So there isn't a single answer, and that is why it is important that this if configurable. ",413936,technical,"Just to make it clear. I'm not against separate settings at all. But I'm against adding knobs for every PMU the kernel supports wholesale without analysis and documentation just because we can and somebody wants it.  Right now we have a single knob, which is poorly documented and that should be fixed first. But some googling gives you the information that allowing unprivilegded access is a security risk. So the security focussed sysadmin will deny access to the PMUs no matter what.  Now we add more knobs without documentation what the exposure and risk of each PMU is. The proposed patch set does this wholesale for every PMU supported by the kernel. So the GPU user will ask the sysadmin to allow him access. How can he make an informed decision? If he grants it then the next user comes around and wants it for something else arguing that the other got it for the GPU already. How can he make an informed decision about that one?  We provide the knobs, so it's also our responsibility towards our users to give them the information about their usage and scope. And every single PMU knob has a different scope.  The documentation of the gazillion of knobs in /proc and /sysfs is not really brilliant, but we should really not continue this bad practice especially not when these knobs have potentially security relevant implcations. Yes, I know, writing documentation is work, but it's valuable and is appreciated by our users.  To make this doable and not blocked by requiring every PMU to be analyzed and documented at once, I suggested to make this opt-in. Do analysis for a given PMU, decide whether it should be exposed at all. If so, document it proper and flip the bit. That way this can be done gradually as the need arises and we can exclude the riskier ones completely.  I don't think this is an unreasonable request as it does not require the i915 folks to look at PMUs they are not familiar with and does not get blocked by waiting on every PMU wizard on the planet to have time.  Start with something like Documentation/admin-guide/perf-security.rst or whatever name fits better and add a proper documentation for the existing knob. With the infrastructure for fine grained access control add the general explanation for fine grained access control. With each PMU which opt's in for the knob, add a section with guidance about scope and risk for this particular one.","Just to make it clear. I'm not against separate settings at all. But I'm against adding knobs for every PMU the kernel supports wholesale without analysis and documentation just because we can and somebody wants it.  Right now we have a single knob, which is poorly documented and that should be fixed first. But some googling gives you the information that allowing unprivilegded access is a security risk. So the security focussed sysadmin will deny access to the PMUs no matter what.  Now we add more knobs without documentation what the exposure and risk of each PMU is. The proposed patch set does this wholesale for every PMU supported by the kernel. So the GPU user will ask the sysadmin to allow him access. How can he make an informed decision? If he grants it then the next user comes around and wants it for something else arguing that the other got it for the GPU already. How can he make an informed decision about that one?  We provide the knobs, so it's also our responsibility towards our users to give them the information about their usage and scope. And every single PMU knob has a different scope.  The documentation of the gazillion of knobs in /proc and /sysfs is not really brilliant, but we should really not continue this bad practice especially not when these knobs have potentially security relevant implcations. Yes, I know, writing documentation is work, but it's valuable and is appreciated by our users.  To make this doable and not blocked by requiring every PMU to be analyzed and documented at once, I suggested to make this opt-in. Do analysis for a given PMU, decide whether it should be exposed at all. If so, document it proper and flip the bit. That way this can be done gradually as the need arises and we can exclude the riskier ones completely.  I don't think this is an unreasonable request as it does not require the i915 folks to look at PMUs they are not familiar with and does not get blocked by waiting on every PMU wizard on the planet to have time.  Start with something like Documentation/admin-guide/perf-security.rst or whatever name fits better and add a proper documentation for the existing knob. With the infrastructure for fine grained access control add the general explanation for fine grained access control. With each PMU which opt's in for the knob, add a section with guidance about scope and risk for this particular one. Ah only if google could simply answer all our questions! It's not like there is or isn't a security risk and that you can say that it is or it isn't in a global way.
Essentially these are channels of information. The channels always exist in form of timing variances for any shared resource (like shared caches or shared memory/IO/interconnect bandwidth) that can be measured. Perfmon counters make the channels generally less noisy, but they do not cause
them. To really close them completely you would need to avoid sharing anything, or not allowing to measure time, neither of which is practical short of an air gap. There are reasonable assesments you can make either way and the answers will be different based on your requirements. There isn't a single answer that works for everyone. 
There are cases where it isn't a problem at all. If you don't have multiple users on the system your tolerance should be extremely high. For users who have multiple users there can be different tradeoffs. So there isn't a single answer, and that is why it is important that this if configurable. "
133,402510,414266,uncivil,"I said clearly that I'm not opposed against making it configurable. But because there is no single answer, it's even more important to have proper documentation. And that's all I'm asking for aside of making it opt-in instead of a wholesale expose everything approach.",414076,technical,"Ah, I guess the answer is 0, since you want to see data about what other users are doing.  Does the i915 PMU expose sampling events, counting events, or both? The thing about sampling events is that they AFAIK always let the user pick arbitrary data to collect - like register contents, or userspace stack memory -, and independent of the performance counter being monitored, this kind of access should not be permitted to other contexts. (But it might be that I misunderstand how perf works - I'm not super familiar with its API.)","Ah, I guess the answer is 0, since you want to see data about what other users are doing.  Does the i915 PMU expose sampling events, counting events, or both? The thing about sampling events is that they AFAIK always let the user pick arbitrary data to collect - like register contents, or userspace stack memory -, and independent of the performance counter being monitored, this kind of access should not be permitted to other contexts. (But it might be that I misunderstand how perf works - I'm not super familiar with its API.) I said clearly that I'm not opposed against making it configurable. But because there is no single answer, it's even more important to have proper documentation. And that's all I'm asking for aside of making it opt-in instead of a wholesale expose everything approach."
134,410867,414489,uncivil,"Even though the return type of ndo_start_xmit is netdev_tx_t, negative error codes are
still allowed I believe. Look, reviewing these are pretty stressful for me, because you  aren't documenting your changes and in many cases the transformations look incorrect. I'm tossing the rest of your changes in this area for now, sorry. Please double check your work and resubmit this at some time in the not-too-near future. Thank you.",410867,technical,"The method is defined as returning this, which is a typedef for an enum type, so make sure the implementation in this driver has returns this value, and change the function return type to this  Found by coccinelle","The method is defined as returning this, which is a typedef for an enum type, so make sure the implementation in this driver has returns this value, and change the function return type to this  Found by coccinelle Even though the return type of ndo_start_xmit is netdev_tx_t, negative error codes are
still allowed I believe. Look, reviewing these are pretty stressful for me, because you  aren't documenting your changes and in many cases the transformations look incorrect. I'm tossing the rest of your changes in this area for now, sorry. Please double check your work and resubmit this at some time in the not-too-near future. Thank you."
135,422694,423291,uncivil,"The way I see it, it is pretty well marked up as is. So, this paragraph is not describing the change. What is not ""proper"" about the existing comment? Yes yes, I *know* that GCC is not very intelligent about it and requires hand-holding, but blaming the existing comment for not *properly* marking an intentional fall through is ... rich. Adding some more context here. Considering the above added context, I have to say that this mindless change is not an improvement, as you have just destroyed the continued sentence from the previous comment. You must have noticed that this was the end of a continued sentence, as you even quoted it in the commit message. The big question is why you did not stop to think and consider the context?
Yes, I'm annoyed by mindless changes. Especially mindless changes aimed at improving readability while in fact making things less readable.
TL;DR, if you are desperate to fix ""the problem"" with this fall through comment, please do so in a way that preserves overall readability. And it would be nice to not blame the existing code for brain damage in GCC and various other static analyzers.",422694,technical,"In preparation to enabling -Wimplicit-fallthrough, mark switch cases where we are expecting to fall through.  Notice that in this particular case, I replaced ...and fall through."" with a proper ""fall through"", which is what GCC is expecting to find.  ","In preparation to enabling -Wimplicit-fallthrough, mark switch cases where we are expecting to fall through.  Notice that in this particular case, I replaced ...and fall through."" with a proper ""fall through"", which is what GCC is expecting to find.   The way I see it, it is pretty well marked up as is. So, this paragraph is not describing the change. What is not ""proper"" about the existing comment? Yes yes, I *know* that GCC is not very intelligent about it and requires hand-holding, but blaming the existing comment for not *properly* marking an intentional fall through is ... rich. Adding some more context here. Considering the above added context, I have to say that this mindless change is not an improvement, as you have just destroyed the continued sentence from the previous comment. You must have noticed that this was the end of a continued sentence, as you even quoted it in the commit message. The big question is why you did not stop to think and consider the context?
Yes, I'm annoyed by mindless changes. Especially mindless changes aimed at improving readability while in fact making things less readable.
TL;DR, if you are desperate to fix ""the problem"" with this fall through comment, please do so in a way that preserves overall readability. And it would be nice to not blame the existing code for brain damage in GCC and various other static analyzers."
136,422694,428645,uncivil,"I still object. It would have been so damn easy and it does not take a whole
lot of imagination to quiet down GCC while keeping the comments readable. Just
move the ""and"" to the previous comment, like this. Or add a sentence, like this (which is a bit more fun IMO).",428602,technical,"Thanks. Below are some examples of cases in which the fall-through warning turned out to be an actual bug:  So, yeah. This effort is worth it.","Thanks. Below are some examples of cases in which the fall-through warning turned out to be an actual bug:  So, yeah. This effort is worth it. I still object. It would have been so damn easy and it does not take a whole
lot of imagination to quiet down GCC while keeping the comments readable. Just
move the ""and"" to the previous comment, like this. Or add a sentence, like this (which is a bit more fun IMO)."
137,424089,424518,uncivil,"Which CPU architecture?  Most important architectures appear to define
__HAVE_ARCH_MEMCMP. What the heck does __visible do? This is going to do bad things if the incoming addresses aren't suitably aligned.
Certainly, byte-at-a-time is a pretty lame implementation when the addresses are suitably aligned.  A fallback to the lame version when there is misalignment will be simple to do.  And presumably there will be decent benefits to whoever is actually using this code.  But I'm wondering who is actually using this code!",424089,technical," During testing, I have configured 128 md/raid1's and, while under heavy IO, I started a check on each of them.The CPU utilization went through the ceiling and when looking for the cause (with 'perf top'). I've discovered that ~50% of the time was spend in memcmp() called from process_checks().  With this patch applied, it drops to 4% - 10%."," During testing, I have configured 128 md/raid1's and, while under heavy IO, I started a check on each of them.The CPU utilization went through the ceiling and when looking for the cause (with 'perf top'). I've discovered that ~50% of the time was spend in memcmp() called from process_checks().  With this patch applied, it drops to 4% - 10%. Which CPU architecture?  Most important architectures appear to define
__HAVE_ARCH_MEMCMP. What the heck does __visible do? This is going to do bad things if the incoming addresses aren't suitably aligned.
Certainly, byte-at-a-time is a pretty lame implementation when the addresses are suitably aligned.  A fallback to the lame version when there is misalignment will be simple to do.  And presumably there will be decent benefits to whoever is actually using this code.  But I'm wondering who is actually using this code!"
138,467091,471421,uncivil,"The major feature close to USB is this one and it can be found in others protocols (standardization process). Just to close this topic I3C vs USB, IMO it's wrong to pass the message that the I3C is closer to USB than I2C even more because I3C support the I2C on the fly. Sorry, with the proliferation of sensors I cannot see a multi master sensor network based on USB. Yes, we already talked about secondary master support. I would bet to do something like in i2c, we don't need the same level of complexity found in USB. I agree with the controller folder but not with prefix. Please check what is already in the kernel. In this case and taking what is already in the kernel it will be drivers/i3c/{master, slave, dwc, other with the same architecture as dwc}. I miss to mention PCI but since the beginning refer the slave and the common part. Splitting the driver is something that soon or later I will have to do. If you prefer later I'm ok with that. I think this discussion is starting to be counterproductive with arguing of both parts. Unfortunately I don't see anyone given their inputs too. To be clear, the subsystem is nice and I working with daily. As I said this is something that I dealing now and I'm telling what I think that is not correct.",470763,technical,"My point is, I don't get the relationship between your patch and secondary-master/slave-mode support.   I maintain that functionally, I3C is closer to USB than I2C. That does not mean that it's competing with USB performance-wise, it just means that the SW stack is likely to resemble the USB one (probably a bit simpler, at least at the beginning).    Look at the bus discovery mechanism, the notion of DCR (close to the concept of USB class), or the fact that each dev has a unique manufacturerPID pair (which resemble the product/vendor ID concept) that allows us to easily bind a dev to a driver without requiring a static description.   That's called USB OTG. Okay, to be fair, it's not exactly the same, and the mastership handover in I3C is probably more complex than what we have with USB OTG (I'm not a USB expert, so I might be wrong here).   Maybe.   So you have such a dual-role controller? I mean, Cadence IP has a dummy GPIO mode in its Master IP when is operating in slave mode (secondary master, or main master after it's released the bus), but I'm not sure this was designed for anything else but testing.  What I call a slave controller would be something that lets you reply to SDR/DDR transactions or fill a generic regmap that would be exposed to other masters on the bus. This way we could implement generic slave drivers in Linux (the same way we have gadget drivers). Anything else is likely to be to specific to be exposed as a generic framework.   Hm, not sure I like this idea either. So I see 2 options:  1/ put all controller drivers (both master and slave ones) in a common  directory as you suggest, and prefix them    correctly  2/ place them in separate directories I'm fine either way.   I think I understand your concerns now, but only because you started to mention a few things that were not clearly stated before (at least, I didn't understand it this way), like the fact that your controller (and probably others too) supports dual-role, or the fact that you plan to expose your IP through the PCI bus.","My point is, I don't get the relationship between your patch and secondary-master/slave-mode support.   I maintain that functionally, I3C is closer to USB than I2C. That does not mean that it's competing with USB performance-wise, it just means that the SW stack is likely to resemble the USB one (probably a bit simpler, at least at the beginning).    Look at the bus discovery mechanism, the notion of DCR (close to the concept of USB class), or the fact that each dev has a unique manufacturerPID pair (which resemble the product/vendor ID concept) that allows us to easily bind a dev to a driver without requiring a static description.   That's called USB OTG. Okay, to be fair, it's not exactly the same, and the mastership handover in I3C is probably more complex than what we have with USB OTG (I'm not a USB expert, so I might be wrong here).   Maybe.   So you have such a dual-role controller? I mean, Cadence IP has a dummy GPIO mode in its Master IP when is operating in slave mode (secondary master, or main master after it's released the bus), but I'm not sure this was designed for anything else but testing.  What I call a slave controller would be something that lets you reply to SDR/DDR transactions or fill a generic regmap that would be exposed to other masters on the bus. This way we could implement generic slave drivers in Linux (the same way we have gadget drivers). Anything else is likely to be to specific to be exposed as a generic framework.   Hm, not sure I like this idea either. So I see 2 options:  1/ put all controller drivers (both master and slave ones) in a common  directory as you suggest, and prefix them    correctly  2/ place them in separate directories I'm fine either way.   I think I understand your concerns now, but only because you started to mention a few things that were not clearly stated before (at least, I didn't understand it this way), like the fact that your controller (and probably others too) supports dual-role, or the fact that you plan to expose your IP through the PCI bus. The major feature close to USB is this one and it can be found in others protocols (standardization process). Just to close this topic I3C vs USB, IMO it's wrong to pass the message that the I3C is closer to USB than I2C even more because I3C support the I2C on the fly. Sorry, with the proliferation of sensors I cannot see a multi master sensor network based on USB. Yes, we already talked about secondary master support. I would bet to do something like in i2c, we don't need the same level of complexity found in USB. I agree with the controller folder but not with prefix. Please check what is already in the kernel. In this case and taking what is already in the kernel it will be drivers/i3c/{master, slave, dwc, other with the same architecture as dwc}. I miss to mention PCI but since the beginning refer the slave and the common part. Splitting the driver is something that soon or later I will have to do. If you prefer later I'm ok with that. I think this discussion is starting to be counterproductive with arguing of both parts. Unfortunately I don't see anyone given their inputs too. To be clear, the subsystem is nice and I working with daily. As I said this is something that I dealing now and I'm telling what I think that is not correct."
139,467091,471458,uncivil,"I think you didn't read my reply carefully. I'm not saying I3C == USB, I'm just saying that the way you interact with an I3C from a SW PoV is not at all the same as you would do for an I2C device. Do you deny that? Looks like there's a misunderstanding here. The question is not whether I3C will replace I2C or USB, of course it's meant to overcome the limitations of I2C. I'm just pointing out that, if we have to expose I3C devices, we should look at what other discoverable buses do (PCI, USB, ...), not what I2C does. 
There's a difference between a secondary master that waits for its time to become the currrent master, and a secondary master that provides I3C device features when it's acting as a slave (sensor, GPIO controller, ...). So far we focused on supporting the former. If there's a need for the latter, then we should start thinking about the slave framework...
Can you detail a bit more what you have in mind? I don't think we can do like I2C, simply because we need to expose a valid DCR  manuf-ID/PID so that other masters can bind the device to the appropriate driver on their side. Plus, if we're about to expose generic profiles, we likely don't want each I3C slave controller driver to do that on its own.
If we mix everything in the same subdir, I'd like to have an easy way to quickly identify those that are slave controllers and those that are master controllers. For the dual-role thing, maybe we can consider them as master (ones with advances slave features).
Would you be okay with drivers/i3c/controllers/{designware,dw}/..., so that you can have all designware drivers (for both slave and master blocks) in the same dir? For those that are placed directly under drivers/i3c/controllers/... (because they only have one .c file), I'd like to keep a standard prefix. And again, I'm questioning the necessity of per-IP directories at the root level. I'm not against per-IP directories, as long as they are classified like other HW blocks. No it's not vain, it's how we do discuss things in the community. I'm not saying I'm always right, but I need to understand the problems you're trying to solve to take a decision, and I don't think you initially gave all the details I needed to understand your PoV. That's a bit clearer now, even if I still disagree on a few aspects. They will come. Come on! All I've seen so far are complaints on tiny details, it definitely doesn't prevent you from adding new features.",471421,uncivil,"he major feature close to USB is this one and it can be found in others  protocols (standardization process).  Just to close this topic I3C vs USB, IMO it's wrong to pass the message  that the I3C is closer to USB than I2C even more because I3C support the  I2C on the fly.     Sorry, with the proliferation of sensors I cannot see a multi master  sensor network based on USB. Yes, we already talked about secondary master support.     I would bet to do something like in i2c, we don't need the same level of  complexity found in USB.  I agree with the controller folder but not with prefix. Please check  what is already in the kernel. In this case and taking what is already in the kernel it will be  drivers/i3c/{master, slave, dwc, other with the same architecture as dwc}.     I miss to mention PCI but since the beginning refer the slave and the  common part.  Splitting the driver is something that soon or later I will have to do.  If you prefer later I'm ok with that.   I think this discussion is starting to be counterproductive with arguing  of both parts. Unfortunately I don't see anyone given their inputs too.  To be clear, the subsystem is nice and I working with daily. As I said  this is something that I dealing now and I'm telling what I think that  is not correct.","he major feature close to USB is this one and it can be found in others  protocols (standardization process).  Just to close this topic I3C vs USB, IMO it's wrong to pass the message  that the I3C is closer to USB than I2C even more because I3C support the  I2C on the fly.     Sorry, with the proliferation of sensors I cannot see a multi master  sensor network based on USB. Yes, we already talked about secondary master support.     I would bet to do something like in i2c, we don't need the same level of  complexity found in USB.  I agree with the controller folder but not with prefix. Please check  what is already in the kernel. In this case and taking what is already in the kernel it will be  drivers/i3c/{master, slave, dwc, other with the same architecture as dwc}.     I miss to mention PCI but since the beginning refer the slave and the  common part.  Splitting the driver is something that soon or later I will have to do.  If you prefer later I'm ok with that.   I think this discussion is starting to be counterproductive with arguing  of both parts. Unfortunately I don't see anyone given their inputs too.  To be clear, the subsystem is nice and I working with daily. As I said  this is something that I dealing now and I'm telling what I think that  is not correct. I think you didn't read my reply carefully. I'm not saying I3C == USB, I'm just saying that the way you interact with an I3C from a SW PoV is not at all the same as you would do for an I2C device. Do you deny that? Looks like there's a misunderstanding here. The question is not whether I3C will replace I2C or USB, of course it's meant to overcome the limitations of I2C. I'm just pointing out that, if we have to expose I3C devices, we should look at what other discoverable buses do (PCI, USB, ...), not what I2C does. 
There's a difference between a secondary master that waits for its time to become the currrent master, and a secondary master that provides I3C device features when it's acting as a slave (sensor, GPIO controller, ...). So far we focused on supporting the former. If there's a need for the latter, then we should start thinking about the slave framework...
Can you detail a bit more what you have in mind? I don't think we can do like I2C, simply because we need to expose a valid DCR  manuf-ID/PID so that other masters can bind the device to the appropriate driver on their side. Plus, if we're about to expose generic profiles, we likely don't want each I3C slave controller driver to do that on its own.
If we mix everything in the same subdir, I'd like to have an easy way to quickly identify those that are slave controllers and those that are master controllers. For the dual-role thing, maybe we can consider them as master (ones with advances slave features).
Would you be okay with drivers/i3c/controllers/{designware,dw}/..., so that you can have all designware drivers (for both slave and master blocks) in the same dir? For those that are placed directly under drivers/i3c/controllers/... (because they only have one .c file), I'd like to keep a standard prefix. And again, I'm questioning the necessity of per-IP directories at the root level. I'm not against per-IP directories, as long as they are classified like other HW blocks. No it's not vain, it's how we do discuss things in the community. I'm not saying I'm always right, but I need to understand the problems you're trying to solve to take a decision, and I don't think you initially gave all the details I needed to understand your PoV. That's a bit clearer now, even if I still disagree on a few aspects. They will come. Come on! All I've seen so far are complaints on tiny details, it definitely doesn't prevent you from adding new features."
140,467091,479249,uncivil,"If you want. Actually that's the most interesting part for me: discussing how we want to support I3C slave controllers or mixed master/slave controllers. All the driver split we're talking about here is just bikeshedding. Ok. I don't see why. If the driver is simple enough to fit in one file, there's no reason to create a new subdir. You think your DW IP is so complex and configurable that it requires several source files, fine, but please don't force others to do the same. Yes. You mean, inside a sub-folder? It depends what you do with those source files. If they are to be exposed directly as modules, then they should be prefixed. On the other hand, if you create a single module out of several source files, source files don't need to be prefixed, as long as the resulting module as a proper prefix.
I'm not saying the discussion is useless, just that it's happening way too early compared to the other things we should work on. If you were adding support for slaves, and were doing this split as part of this patch series explaining that part of the code between slave and master can be shared, then we wouldn't have this debate. But right now, you're telling me that we need to split the DW driver to prepare for features that have not even been discussed/proposed. That's what I'm complaining about.",478983,technical,"Sorry for the delayed response. I think this should be discuss in another thread. Do you agree? Yes, that was what I trying to tell you. For me this might be the best  option.  I would like to avoid having dual role i3c driver in a master folder. I don't disagree, and for those that have more than one file they should  be in a folder, right?  What prefix do you have in mind for those files inside a folder? No, it's not. But as you can see to slipt the driver in parts this  subject has some relevance.","Sorry for the delayed response. I think this should be discuss in another thread. Do you agree? Yes, that was what I trying to tell you. For me this might be the best  option.  I would like to avoid having dual role i3c driver in a master folder. I don't disagree, and for those that have more than one file they should  be in a folder, right?  What prefix do you have in mind for those files inside a folder? No, it's not. But as you can see to slipt the driver in parts this  subject has some relevance. If you want. Actually that's the most interesting part for me: discussing how we want to support I3C slave controllers or mixed master/slave controllers. All the driver split we're talking about here is just bikeshedding. Ok. I don't see why. If the driver is simple enough to fit in one file, there's no reason to create a new subdir. You think your DW IP is so complex and configurable that it requires several source files, fine, but please don't force others to do the same. Yes. You mean, inside a sub-folder? It depends what you do with those source files. If they are to be exposed directly as modules, then they should be prefixed. On the other hand, if you create a single module out of several source files, source files don't need to be prefixed, as long as the resulting module as a proper prefix.
I'm not saying the discussion is useless, just that it's happening way too early compared to the other things we should work on. If you were adding support for slaves, and were doing this split as part of this patch series explaining that part of the code between slave and master can be shared, then we wouldn't have this debate. But right now, you're telling me that we need to split the DW driver to prepare for features that have not even been discussed/proposed. That's what I'm complaining about."
141,476403,477115,uncivil,So I strongly disagree with this. Anybody that has trouble with 0/1 vs false/true needs to stay the heck away from C. I would suggest we delete that stupid coccinelle scripts that generates these pointless warns.,476592,technical,Adding the current maintainers on CC.  ,Adding the current maintainers on CC.   So I strongly disagree with this. Anybody that has trouble with 0/1 vs false/true needs to stay the heck away from C. I would suggest we delete that stupid coccinelle scripts that generates these pointless warns.
142,476403,477136,uncivil,Not to mention that WARN is gramatically incorrect. We're not assigning 'bool' to 0/1 but the other way around. What crap..,477115,uncivil,So I strongly disagree with this. Anybody that has trouble with 0/1 vs false/true needs to stay the heck away from C.  I would suggest we delete that stupid coccinelle scripts that generates these pointless warns.,So I strongly disagree with this. Anybody that has trouble with 0/1 vs false/true needs to stay the heck away from C.  I would suggest we delete that stupid coccinelle scripts that generates these pointless warns. Not to mention that WARN is gramatically incorrect. We're not assigning 'bool' to 0/1 but the other way around. What crap..
143,476403,478120,uncivil,"Then those tools are broken per the C spec. The C language spec, specifies _Bool as an integer type wide enough to at least store 0 and 1. IOW, 0 and 1 are perfectly valid valus to assign to a _Bool. And fundamentally that has to be so. That's how computers work. 0 is false, 1 is true. The kernel is not the place to try and abstract such stuff, C is our portable assembler. We muck with hardware, we'd better know how the heck it works.",477173,technical,"Personally, I would prefer that assignments involving boolean variables use true or false.  It seems more readable.  Potentially better for tools as well.  But if the community really prefers 0 and 1, then the test can be deleted.","Personally, I would prefer that assignments involving boolean variables use true or false.  It seems more readable.  Potentially better for tools as well.  But if the community really prefers 0 and 1, then the test can be deleted. Then those tools are broken per the C spec. The C language spec, specifies _Bool as an integer type wide enough to at least store 0 and 1. IOW, 0 and 1 are perfectly valid valus to assign to a _Bool. And fundamentally that has to be so. That's how computers work. 0 is false, 1 is true. The kernel is not the place to try and abstract such stuff, C is our portable assembler. We muck with hardware, we'd better know how the heck it works."
144,498594,573986,uncivil,What's advertisement there? Huch? Care to tell what's a lie instead of making bold statements?,573878,technical,"Ping? Jonathan, can you pick this up?  ","Ping? Jonathan, can you pick this up?   What's advertisement there? Huch? Care to tell what's a lie instead of making bold statements?"
145,498594,573990,uncivil,"""No problem here, no performance issues, nothing to be seen unless you
are running VM."" Take a care to look at the patch I submitted? Lie: A system with an up to date kernel is protected against attacks from malicious user space applications. 3GB system running 32bit kernel is not protected. Same is true for really big 64bit systems. If I do what dmesg suggests, this becomes untrue: The Linux kernel contains a mitigation for this attack vector, PTE inversion, which is permanently enabled and has no performance impact. Limiting memory to 2GB _is_ going to have severe perfomance impact.",573986,uncivil,What's advertisement there?   Huch? Care to tell what's a lie instead of making bold statements?,"What's advertisement there?   Huch? Care to tell what's a lie instead of making bold statements? ""No problem here, no performance issues, nothing to be seen unless you
are running VM."" Take a care to look at the patch I submitted? Lie: A system with an up to date kernel is protected against attacks from malicious user space applications. 3GB system running 32bit kernel is not protected. Same is true for really big 64bit systems. If I do what dmesg suggests, this becomes untrue: The Linux kernel contains a mitigation for this attack vector, PTE inversion, which is permanently enabled and has no performance impact. Limiting memory to 2GB _is_ going to have severe perfomance impact."
146,498594,574206,civil,"I would really like to get an ack from the people who have been deep into this first.  If you can get that, and preferably resubmit with a less condescending changelog, I can pick it up.",573990,uncivil,"No problem here, no performance issues, nothing to be seen unless you are running VM.""   Take a care to look at the patch I submitted?  Lie: A system with an up to date kernel is protected against attacks from malicious user space applications.  3GB system running 32bit kernel is not protected. Same is true for for really big 64bit systems.  If I do what he suggests, this becomes untrue: The Linux kernel contains a mitigation for this attack vector, PTE inversion, which is permanently enabled and has no performance # impact.  Limiting memory to 2GB _is_ going to have severe perfomance impact. ","No problem here, no performance issues, nothing to be seen unless you are running VM.""   Take a care to look at the patch I submitted?  Lie: A system with an up to date kernel is protected against attacks from malicious user space applications.  3GB system running 32bit kernel is not protected. Same is true for for really big 64bit systems.  If I do what he suggests, this becomes untrue: The Linux kernel contains a mitigation for this attack vector, PTE inversion, which is permanently enabled and has no performance # impact.  Limiting memory to 2GB _is_ going to have severe perfomance impact.  I would really like to get an ack from the people who have been deep into this first.  If you can get that, and preferably resubmit with a less condescending changelog, I can pick it up."
147,498594,574882,uncivil,"I agree that this statement is incorrect.

Calling this a lie is a completly unjustified personal attack on those who
spent quite a lot of time on writing up documentation in the first
place. It's suggesting that this document was written with malicious intent
and the purpose of deceiving someone. Care to explain why you are assuming
this to be the case?


Sure. That still does not justify the ""changelog"" you provided.


It's interesting that quite some people were actually happy about that
document. Sorry, that we weren't able to live up to your high standards.


What is the advertisement part again?


It's a document targeted at system administrators and it definitely should
not be burried somewhere in Documentation/x86. As there are more documents
being worked on for the other issues, I have a patch ready which moves that
stuff into a separate hardware vulnerabilites folder in the admin-guide.

FWIW, to the best of my knowledge the documentation about writing
changelogs is neither incorrect nor is it optional to adhere to it.


The 'Affected processors' section right below this is very clear about this being an Intel only issue (for now). So what exactly is the point of this change?
On x86-32? That's incorrect, because there are a lot of x86-32 systems which are not affected. Also it has nothing to do with the bit-width of the hardware. A 32bit kernel booted on a 64bit capable CPU has the same issue. For further correctness, this needs to mention that !PAE enabled kernels cannot do PTE inversion at all. The 2G limitation is not a general limitation. The limitation depends on the number of physical address bits supported by the cache (not the number of physical addresss bits exposed as pins) and is definitely not hardcoded to 2G. Just because your machine emits the 2G number does not make it universally correct. On a system with 36bit physical address space the limit is 32G and on some CPUs that's actually wrong as well, see: override_cache_bits. Quoting yourself, Where is the explanation for the 'really big 64bit systems' issue for correctness sake?",574206,civil,"I would really like to get an ack from the people who have been deep into this first.  If you can get that, and preferably resubmit with a less condescending changelog, I can pick it up.","I would really like to get an ack from the people who have been deep into this first.  If you can get that, and preferably resubmit with a less condescending changelog, I can pick it up. I agree that this statement is incorrect.

Calling this a lie is a completly unjustified personal attack on those who
spent quite a lot of time on writing up documentation in the first
place. It's suggesting that this document was written with malicious intent
and the purpose of deceiving someone. Care to explain why you are assuming
this to be the case?


Sure. That still does not justify the ""changelog"" you provided.


It's interesting that quite some people were actually happy about that
document. Sorry, that we weren't able to live up to your high standards.


What is the advertisement part again?


It's a document targeted at system administrators and it definitely should
not be burried somewhere in Documentation/x86. As there are more documents
being worked on for the other issues, I have a patch ready which moves that
stuff into a separate hardware vulnerabilites folder in the admin-guide.

FWIW, to the best of my knowledge the documentation about writing
changelogs is neither incorrect nor is it optional to adhere to it.


The 'Affected processors' section right below this is very clear about this being an Intel only issue (for now). So what exactly is the point of this change?
On x86-32? That's incorrect, because there are a lot of x86-32 systems which are not affected. Also it has nothing to do with the bit-width of the hardware. A 32bit kernel booted on a 64bit capable CPU has the same issue. For further correctness, this needs to mention that !PAE enabled kernels cannot do PTE inversion at all. The 2G limitation is not a general limitation. The limitation depends on the number of physical address bits supported by the cache (not the number of physical addresss bits exposed as pins) and is definitely not hardcoded to 2G. Just because your machine emits the 2G number does not make it universally correct. On a system with 36bit physical address space the limit is 32G and on some CPUs that's actually wrong as well, see: override_cache_bits. Quoting yourself, Where is the explanation for the 'really big 64bit systems' issue for correctness sake?"
148,498594,576302,uncivil,"So how should it be called? I initally used less strong words, only to get ""Care to tell what's a lie instead of making bold statements?"" back. Also look at the timing of the thread. Ok, now can we have that document updated to meet the standards? Making it very clear from the begining this is x86-only issue. Yes, you can kind-of figure it out from the next section... except for Intel StrongArm. Next sentence speaks about ""present bit"" of ""page table entry"". That may be confusing for people familiar with other architectures, which may not have such bit. We should mention this is x86 before using x86-specific terminology. Ok. I don't know the detailed limits for each system; what about this?",574882,uncivil,"I agree that this statement is incorrect.  Calling this a lie is a completly unjustified personal attack on those who spent quite a lot of time on writing up documentation in the first place. It's suggesting that this document was written with malicious intent and the purpose of deceiving someone. Care to explain why you are assuming this to be the case? Sure. That still does not justify the changelog"" you provided.   It's interesting that quite some people were actually happy about that document. Sorry, that we weren't able to live up to your high standards.   What is the advertisement part again?   It's a document targeted at system administrators and it definitely should not be burried somewhere in Documentation/x86. As there are more documents being worked on for the other issues, I have a patch ready which moves that stuff into a separate hardware vulnerabilites folder in the admin-guide.  FWIW, to the best of my knowledge the documentation about writing changelogs is neither incorrect nor is it optional to adhere to it.   The 'Affected processors' section right below this is very clear about this being an Intel only issue (for now). So what exactly is the point of this change?   On x86-32? That's incorrect, because there are a lot of x86-32 systems which are not affected. Also it has nothing to do with the bit-width of the hardware. A 32bit kernel booted on a 64bit capable CPU has the same issue. For further correctness, this needs to mention that !PAE enabled kernels cannot do PTE inversion at all.   The 2G limitation is not a general limitation. The limitation depends on the number of physical address bits supported by the cache (not the number of physical addresss bits exposed as pins) and is definitely not hardcoded to 2G. Just because your machine emits the 2G number does not make it universally correct. On a system with 36bit physical address space the limit is 32G and on some CPUs that's actually wrong as well, see: override_cache_bits().  Quoting yourself:   Where is the explanation for the 'really big 64bit systems' issue for correctness sake?""","I agree that this statement is incorrect.  Calling this a lie is a completly unjustified personal attack on those who spent quite a lot of time on writing up documentation in the first place. It's suggesting that this document was written with malicious intent and the purpose of deceiving someone. Care to explain why you are assuming this to be the case? Sure. That still does not justify the changelog"" you provided.   It's interesting that quite some people were actually happy about that document. Sorry, that we weren't able to live up to your high standards.   What is the advertisement part again?   It's a document targeted at system administrators and it definitely should not be burried somewhere in Documentation/x86. As there are more documents being worked on for the other issues, I have a patch ready which moves that stuff into a separate hardware vulnerabilites folder in the admin-guide.  FWIW, to the best of my knowledge the documentation about writing changelogs is neither incorrect nor is it optional to adhere to it.   The 'Affected processors' section right below this is very clear about this being an Intel only issue (for now). So what exactly is the point of this change?   On x86-32? That's incorrect, because there are a lot of x86-32 systems which are not affected. Also it has nothing to do with the bit-width of the hardware. A 32bit kernel booted on a 64bit capable CPU has the same issue. For further correctness, this needs to mention that !PAE enabled kernels cannot do PTE inversion at all.   The 2G limitation is not a general limitation. The limitation depends on the number of physical address bits supported by the cache (not the number of physical addresss bits exposed as pins) and is definitely not hardcoded to 2G. Just because your machine emits the 2G number does not make it universally correct. On a system with 36bit physical address space the limit is 32G and on some CPUs that's actually wrong as well, see: override_cache_bits().  Quoting yourself:   Where is the explanation for the 'really big 64bit systems' issue for correctness sake?"" So how should it be called? I initally used less strong words, only to get ""Care to tell what's a lie instead of making bold statements?"" back. Also look at the timing of the thread. Ok, now can we have that document updated to meet the standards? Making it very clear from the begining this is x86-only issue. Yes, you can kind-of figure it out from the next section... except for Intel StrongArm. Next sentence speaks about ""present bit"" of ""page table entry"". That may be confusing for people familiar with other architectures, which may not have such bit. We should mention this is x86 before using x86-specific terminology. Ok. I don't know the detailed limits for each system; what about this?"
149,498594,590551,uncivil,"You called it a lie from the very beginning or what do you think made me
tell you that? Here is what you said. Nice try. What is 'the standards'? Your's or is there a general agreement? It's pretty clear, but yes admittedly we forgot to mention that Intel StrongARM is not affected. That's truly important because its widely deployed in the cloud space and elsewhere. X86 terminology? Care to check how pte_present() is implemented across the architectures? Most of them use the PRESENT bit naming convention, just a few use VALID. That's truly confusing and truly x86 specific. It's not about detailed limits for particular systems. It's about the way the limit is determined on certain class of systems. And that can be deduced from the code. If you want to provide more accurate documentation then you better come up with something which is helpful instead of completely useless blurb like the below. How is the admin going to figure that out? What kind of systems might be affected by this? No. The mitigation is available when the kernel provides it. Numbers are irrelevant because that documentation has to be applicable for stable kernels as well. And what is a recent -stable kernel? Also the PAE part needs to go to a completely different section.",576302,uncivil,"So how should it be called? I initally used less strong words, only to get Care to tell what's a lie instead of making bold statements?"" back. Also look at the timing of the thread.   Ok, now can we have that document updated to meet the standards?   Making it very clear from the begining this is x86-only issue. Yes, you can kind-of figure it out from the next section... except for Intel StrongArm.  Next sentence speaks about ""present bit"" of ""page table entry"". That may be confusing for people familiar with other architectures, which may not have such bit. We should mention this is x86 before using x86-specific terminology.   Ok.   I don't know the detailed limits for each system, what about this?  Terminal Fault  1 Terminal Fault is a hardware vulnerability which allows unprivileged -speculative access to data which is available in the Level 1 Data Cache -when the page table entry controlling the virtual address, which is used -for the access, has the Present bit cleared or other reserved bits set. L1 Terminal Fault is a hardware vulnerability on most recent Intel x86 CPUs which allows unprivileged speculative access to data which is available in the Level 1 Data Cache when the page table entry controlling the virtual address, which is used for the access, has the Present bit cleared or other reserved bits set.    Affected processors  Attack scenarios     deterministic and more practical.       The Linux kernel contains a mitigation for this attack vector, PTE -   inversion, which is permanently enabled and has no performance -   impact. The kernel ensures that the address bits of PTEs, which are not -   marked present, never point to cacheable physical memory space. - -   A system with an up to date kernel is protected against attacks from -   malicious user space applications.    inversion, which has no measurable performance impact in most    configurations. The kernel ensures that the address bits of PTEs,    which are not marked present, never point to cacheable physical    memory space. For mitigation to be effective, physical memory needs    to be limited in some configurations.     Mitigation is present in kernels v4.19 and newer, and in    recent -stable kernels. PAE needs to be enabled for mitigation to    work.    2. Malicious guest in a virtual machine  ]","So how should it be called? I initally used less strong words, only to get Care to tell what's a lie instead of making bold statements?"" back. Also look at the timing of the thread.   Ok, now can we have that document updated to meet the standards?   Making it very clear from the begining this is x86-only issue. Yes, you can kind-of figure it out from the next section... except for Intel StrongArm.  Next sentence speaks about ""present bit"" of ""page table entry"". That may be confusing for people familiar with other architectures, which may not have such bit. We should mention this is x86 before using x86-specific terminology.   Ok.   I don't know the detailed limits for each system, what about this?  Terminal Fault  1 Terminal Fault is a hardware vulnerability which allows unprivileged -speculative access to data which is available in the Level 1 Data Cache -when the page table entry controlling the virtual address, which is used -for the access, has the Present bit cleared or other reserved bits set. L1 Terminal Fault is a hardware vulnerability on most recent Intel x86 CPUs which allows unprivileged speculative access to data which is available in the Level 1 Data Cache when the page table entry controlling the virtual address, which is used for the access, has the Present bit cleared or other reserved bits set.    Affected processors  Attack scenarios     deterministic and more practical.       The Linux kernel contains a mitigation for this attack vector, PTE -   inversion, which is permanently enabled and has no performance -   impact. The kernel ensures that the address bits of PTEs, which are not -   marked present, never point to cacheable physical memory space. - -   A system with an up to date kernel is protected against attacks from -   malicious user space applications.    inversion, which has no measurable performance impact in most    configurations. The kernel ensures that the address bits of PTEs,    which are not marked present, never point to cacheable physical    memory space. For mitigation to be effective, physical memory needs    to be limited in some configurations.     Mitigation is present in kernels v4.19 and newer, and in    recent -stable kernels. PAE needs to be enabled for mitigation to    work.    2. Malicious guest in a virtual machine  ] You called it a lie from the very beginning or what do you think made me
tell you that? Here is what you said. Nice try. What is 'the standards'? Your's or is there a general agreement? It's pretty clear, but yes admittedly we forgot to mention that Intel StrongARM is not affected. That's truly important because its widely deployed in the cloud space and elsewhere. X86 terminology? Care to check how pte_present() is implemented across the architectures? Most of them use the PRESENT bit naming convention, just a few use VALID. That's truly confusing and truly x86 specific. It's not about detailed limits for particular systems. It's about the way the limit is determined on certain class of systems. And that can be deduced from the code. If you want to provide more accurate documentation then you better come up with something which is helpful instead of completely useless blurb like the below. How is the admin going to figure that out? What kind of systems might be affected by this? No. The mitigation is available when the kernel provides it. Numbers are irrelevant because that documentation has to be applicable for stable kernels as well. And what is a recent -stable kernel? Also the PAE part needs to go to a completely different section."
150,498594,770007,uncivil,"Hi! Actually, I still call it a lie. Document clearly says that bug is fixed in non-virtualized cases, when in fact it depends on PAE and limited memory. At this point I want you to fix it yourself. Lying about security bugs being fixed when they are not is not cool. I tried to be helpful and submit a patch, but I don't feel like you are cooperating on getting the patch applied.",590551,uncivil,"You called it a lie from the very beginning or what do you think made me tell you that? Here is what you said:   Nice try.   What is 'the standards'? Your's or is there a general agreement?   It's pretty clear, but yes admittedly we forgot to mention that Intel StrongARM is not affected. That's truly important because its widely deployed in the cloud space and elsewhere.   X86 terminology? Care to check how pte_present() is implemented across the architectures? Most of them use the PRESENT bit naming convention, just a few use VALID. That's truly confusing and truly x86 specific.   It's not about detailed limits for particular systems. It's about the way the limit is determined on certain class of systems. And that can be deduced from the code.  If you want to provide more accurate documentation then you better come up with something which is helpful instead of completely useless blurb like the below:   How is the admin going to figure that out? What kind of systems might be affected by this?   No. The mitigation is available when the kernel provides it. Numbers are irrelevant because that documentation has to be applicable for stable kernels as well. And what is a recent -stable kernel?  Also the PAE part needs to go to a completely different section.","You called it a lie from the very beginning or what do you think made me tell you that? Here is what you said:   Nice try.   What is 'the standards'? Your's or is there a general agreement?   It's pretty clear, but yes admittedly we forgot to mention that Intel StrongARM is not affected. That's truly important because its widely deployed in the cloud space and elsewhere.   X86 terminology? Care to check how pte_present() is implemented across the architectures? Most of them use the PRESENT bit naming convention, just a few use VALID. That's truly confusing and truly x86 specific.   It's not about detailed limits for particular systems. It's about the way the limit is determined on certain class of systems. And that can be deduced from the code.  If you want to provide more accurate documentation then you better come up with something which is helpful instead of completely useless blurb like the below:   How is the admin going to figure that out? What kind of systems might be affected by this?   No. The mitigation is available when the kernel provides it. Numbers are irrelevant because that documentation has to be applicable for stable kernels as well. And what is a recent -stable kernel?  Also the PAE part needs to go to a completely different section. Hi! Actually, I still call it a lie. Document clearly says that bug is fixed in non-virtualized cases, when in fact it depends on PAE and limited memory. At this point I want you to fix it yourself. Lying about security bugs being fixed when they are not is not cool. I tried to be helpful and submit a patch, but I don't feel like you are cooperating on getting the patch applied."
151,511533,512783,uncivil,"Again, no.",511533,technical,Remove unusual_cypress.h which is included more than once. ,"Remove unusual_cypress.h which is included more than once.  Again, no."
152,521470,533953,civil,"Is this patchset still an RFC or you really want to see this merged ASAP? If I am not mistaken there is still some work in progress trying to push all the SCP stuff? personally I have some concerns. Looks like the cros_* family is increasing quickly lately and I am wondering if we are really doing well all this. To be honest, I'd like to take a deeper look before merge this, btw I thought there was no hurry because of the RFC and I guess there are still some scp things that are missing. I might be wrong, and if that's not the case I can take a look deeper and the end of the week.",533830,technical,"Just to clarify to the new Cc'ed list, I'm waiting on one of the Chromium guys to review before I put my mucky paws over it. ","Just to clarify to the new Cc'ed list, I'm waiting on one of the Chromium guys to review before I put my mucky paws over it.  Is this patchset still an RFC or you really want to see this merged ASAP? If I am not mistaken there is still some work in progress trying to push all the SCP stuff? personally I have some concerns. Looks like the cros_* family is increasing quickly lately and I am wondering if we are really doing well all this. To be honest, I'd like to take a deeper look before merge this, btw I thought there was no hurry because of the RFC and I guess there are still some scp things that are missing. I might be wrong, and if that's not the case I can take a look deeper and the end of the week."
153,522705,522783,uncivil,"I would drop this patch for being too ugly and if nothing else, for lack of users (epoll will no longer need dlock).",522702,technical,"To enable the use of dlock-list in an interrupt handler, a new irqsafe mode can now be specified at dlock-list allocation time as an additional argument to alloc_dlock_list_heads(). With that mode specified, the spin_lock_irqsave/spin_unlock_irqrestore pair will be used instead of the regular lock and unlock calls. There is a slight chance that the list may become empty just  	 * before the lock is acquired. So an additional check is --","To enable the use of dlock-list in an interrupt handler, a new irqsafe mode can now be specified at dlock-list allocation time as an additional argument to alloc_dlock_list_heads(). With that mode specified, the spin_lock_irqsave/spin_unlock_irqrestore pair will be used instead of the regular lock and unlock calls. There is a slight chance that the list may become empty just  	 * before the lock is acquired. So an additional check is -- I would drop this patch for being too ugly and if nothing else, for lack of users (epoll will no longer need dlock)."
154,539613,546296,civil,"You are missing a cover letter from this patch set. Please have it in v2. Also use tag ""selftests/tpm2"" instead of having two tags in the short summaries. Now they look a bit weird. Remove.",539614,technical,"Three new tests added: 1. Send get random cmd, read header in 1st read, read the rest in second    read - expect success 2. Send get random cmd, read only part of the response, send another    get random command, read the response - expect success 3. Send get random cmd followed by another get random cmd, without    reading the first response - expect the second cmd to fail with -EBUSY ","Three new tests added: 1. Send get random cmd, read header in 1st read, read the rest in second    read - expect success 2. Send get random cmd, read only part of the response, send another    get random command, read the response - expect success 3. Send get random cmd followed by another get random cmd, without    reading the first response - expect the second cmd to fail with -EBUSY  You are missing a cover letter from this patch set. Please have it in v2. Also use tag ""selftests/tpm2"" instead of having two tags in the short summaries. Now they look a bit weird. Remove."
155,539613,546377,uncivil,"Since when is the cover letter mandatory? I understand that is helps for a complicated patch set to explain the problem and solution in the cover letter, but for this simple test case addition what's the point? And there is nothing forcing a cover letter in the documentation. Also double tags seams to be quite common for selftest. See git logtools.",546296,civil,You are missing a cover letter from this patch set. Please have it in v2. Also use tag instead of having two tags in the short summaries. Now they look a bit weird.  ,"You are missing a cover letter from this patch set. Please have it in v2. Also use tag instead of having two tags in the short summaries. Now they look a bit weird.   Since when is the cover letter mandatory? I understand that is helps for a complicated patch set to explain the problem and solution in the cover letter, but for this simple test case addition what's the point? And there is nothing forcing a cover letter in the documentation. Also double tags seams to be quite common for selftest. See git logtools."
156,541450,541515,uncivil,"I'm not sure that forcing a library on users is a good reason to break UAPI. The patch is going into the latest, but can also be backported on future stables. I don't think ""not fixing it because it's not fixed yet"" is a good reason to keep things the way they are. But maybe that's just me. Given that the structure has already been extended several times, there is pretty much nothing to keep this from happening again and again.",541509,technical,"This should be safe to copy an arbitrary amount, the only restriction is that optlen can't exceed the size of the buffer receiving the data in the kernel.  From that standpoint your patch is safe.  However,  that exposes the problem of checking any tail data on the userspace buffer.  That is to say, if you want to ensure that any extra data that gets sent from userspace isn't 'set', you would have to copy that extra data in consumable chunks and check them individaully, and that screams DOS to me (i.e. imagine a user passing in a 4GB buffer, and having to wait for the kernel to copy each X sized chunk, looking for non-zero values).","This should be safe to copy an arbitrary amount, the only restriction is that optlen can't exceed the size of the buffer receiving the data in the kernel.  From that standpoint your patch is safe.  However,  that exposes the problem of checking any tail data on the userspace buffer.  That is to say, if you want to ensure that any extra data that gets sent from userspace isn't 'set', you would have to copy that extra data in consumable chunks and check them individaully, and that screams DOS to me (i.e. imagine a user passing in a 4GB buffer, and having to wait for the kernel to copy each X sized chunk, looking for non-zero values). I'm not sure that forcing a library on users is a good reason to break UAPI. The patch is going into the latest, but can also be backported on future stables. I don't think ""not fixing it because it's not fixed yet"" is a good reason to keep things the way they are. But maybe that's just me. Given that the structure has already been extended several times, there is pretty much nothing to keep this from happening again and again."
157,541450,541516,uncivil,"There probably is a decent compromise to find between ""not accepting a single additional byte"" and accepting several GB. For example how likely is it that the growth of this structure make it go over a page? I would hope not at all. By choosing a large but decent high limit, I think we can find a future-compatible compromise that doesn't rely on a preliminary getsockopt() just for structure trucation decision...",541515,uncivil,"I'm not sure that forcing a library on users is a good reason to break UAPI.   The patch is going into the latest, but can also be backported on future stables. I don't think not fixing it because it's not fixed yet"" is a good reason to keep things the way they are. But maybe that's just me. Given that the structure has already been extended several times, there is pretty much nothing to keep this from happening again and again.   --""","I'm not sure that forcing a library on users is a good reason to break UAPI.   The patch is going into the latest, but can also be backported on future stables. I don't think not fixing it because it's not fixed yet"" is a good reason to keep things the way they are. But maybe that's just me. Given that the structure has already been extended several times, there is pretty much nothing to keep this from happening again and again.   --"" There probably is a decent compromise to find between ""not accepting a single additional byte"" and accepting several GB. For example how likely is it that the growth of this structure make it go over a page? I would hope not at all. By choosing a large but decent high limit, I think we can find a future-compatible compromise that doesn't rely on a preliminary getsockopt() just for structure trucation decision..."
158,541450,542126,uncivil,"Thats a misleading statement.  We've never supported running newer applications on older kernels, and no one is forcing anyone to use the lksctp-tools library, I was only suggesting that, if we were to support this compatibility, that might be a place to offer it. Its also worth noting that we have precident for this.  If you look at the git log, this particular structure has been extended about 6 times in the life of sctp.
Also misleading, as it assumes that we're not intentionally doing this.  I get wanting to support running applications built for newer kernels on older kernels, but thats just not something that we do, and to say thats broken is misleading.  Older applications are required to run on newer kernels, but not vice versa, which is what you are asking for. 
And yes, this patch can be backported to older stable kernels, but by that same token, so can the patches that extend the struct, which would also fix the problem, while supporting the newer features, which seems to me to be the better solution for applications which are looking for that support.",541519,technical,"And I was just reminded about huge pages. But still, my point of finding a compromise still stands.","And I was just reminded about huge pages. But still, my point of finding a compromise still stands. Thats a misleading statement.  We've never supported running newer applications on older kernels, and no one is forcing anyone to use the lksctp-tools library, I was only suggesting that, if we were to support this compatibility, that might be a place to offer it. Its also worth noting that we have precident for this.  If you look at the git log, this particular structure has been extended about 6 times in the life of sctp.
Also misleading, as it assumes that we're not intentionally doing this.  I get wanting to support running applications built for newer kernels on older kernels, but thats just not something that we do, and to say thats broken is misleading.  Older applications are required to run on newer kernels, but not vice versa, which is what you are asking for. 
And yes, this patch can be backported to older stable kernels, but by that same token, so can the patches that extend the struct, which would also fix the problem, while supporting the newer features, which seems to me to be the better solution for applications which are looking for that support."
159,541450,543076,uncivil,"I disagree with this, at least as a unilateral statement.  I would assert that an old program, within the constraints of the issue being discussed here, will run perfectly well, when built and run against a new kernel.
At issue is the size of the structure sctp_event_subscribe, and the fact that in several instances over the last few years, its been extended to be larger and encompass more events to subscribe to. Nominally an application will use this structure (roughly) as follows. Assume this code will be built and run against kernel versions A and B, in which:
A) has a struct with a size of 9 bytes
B) has a struct with a size of 10 bytes (due to the added
field)
That gives us 4 cases to handle
1) Application build against kernel A and run on kernel A.  This works fine, the sizes of the struct in question will always match
2) Application is built against kernel A and run on kernel B.  In this case, everything will work because the application passes a buffer of size 9, and the kernel accepts it, because it allows for buffers to be shorter than the current struct sctp_event_subscribe size. The kernel simply operates on the options available in the buffer.  The application is none the wiser, because it has no knoweldge of the new option, nor should it because it was built against kernel A, that never offered that option
3) Application is built against kernel B and run on kernel B.  This works fine for the same reason as (1).
4) Application is built against kernel B and run on kernel A.  This will break because the application is passing a buffer that is larger than what the kernel expects, and rightly so.   The application is passing in a buffer that is incompatible with what the running kernel expects.

We could look into ways in which to detect the cases in which this might be 'ok', but I don't see why we should bother, because at some point its still an error to pass in an incompatible buffer.  In my mind this is no different than trying to run a program that allocates hugepages on a kernel that doesn't support hugepages (just to make up an example).  Applications built against newer kernel can't expect all the features/semantics/etc to be identical to older kernels.

It shouldn't.  Assuming you have a program built against headers from kernel B (above), if you set a field in the structure that only exists in kernel B, and try to run it on kernel A, you will get an EINVAL return, which is correct behavior because you are attempting to deliver information to the kernel that kernel A (the running kernel) doesn't know about.  Thats correct behavior. I won't disagree about the niceness of versioning, but that ship has sailed.To be clear,  this is situation (1) above, and yeah, running on the kernel you built your application against should always work from a compatibility standpoint. Yes, but this is alawys the case for structures that change.  If you have an application built against kernel (B), and uses structure fields that only exist in that version of the kernel (and not earlier) will fail to compile when built against kernel (A) headers, and thats expected.  This happens with any kernel api that exists in a newer kernel but not an older kernel. Any time you make a system call to the kernel, you have to be prepared to handle the resulting error condition, thats not unexpected.  To assume that a system call will always work is bad programming practice.",542900,technical,More confusing than I intended...  With the current kernel and headers a 'new program' (one that needs the new options) will fail to run on an old kernel - which is good. However a recompilation of an 'old program' (that doesn't use the new options) will also fail to run on an old kernel - which is bad.  Changing the kernel to ignore extra events flags breaks the 'new' program.  Versioning the structure now (even though it should have been done earlier) won't change the behaviour of existing binaries.  However a recompilation of an 'old' program would use the 'old' structure and work on old kernels. Attempts to recompile a 'new' program will fail - until the structure name (or some #define to enable the extra fields) is changed.  Breaking compilations is much better than unexpected run-time behaviour. ,"More confusing than I intended...  With the current kernel and headers a 'new program' (one that needs the new options) will fail to run on an old kernel - which is good. However a recompilation of an 'old program' (that doesn't use the new options) will also fail to run on an old kernel - which is bad.  Changing the kernel to ignore extra events flags breaks the 'new' program.  Versioning the structure now (even though it should have been done earlier) won't change the behaviour of existing binaries.  However a recompilation of an 'old' program would use the 'old' structure and work on old kernels. Attempts to recompile a 'new' program will fail - until the structure name (or some #define to enable the extra fields) is changed.  Breaking compilations is much better than unexpected run-time behaviour.  I disagree with this, at least as a unilateral statement.  I would assert that an old program, within the constraints of the issue being discussed here, will run perfectly well, when built and run against a new kernel.
At issue is the size of the structure sctp_event_subscribe, and the fact that in several instances over the last few years, its been extended to be larger and encompass more events to subscribe to. Nominally an application will use this structure (roughly) as follows. Assume this code will be built and run against kernel versions A and B, in which:
A) has a struct with a size of 9 bytes
B) has a struct with a size of 10 bytes (due to the added
field)
That gives us 4 cases to handle
1) Application build against kernel A and run on kernel A.  This works fine, the sizes of the struct in question will always match
2) Application is built against kernel A and run on kernel B.  In this case, everything will work because the application passes a buffer of size 9, and the kernel accepts it, because it allows for buffers to be shorter than the current struct sctp_event_subscribe size. The kernel simply operates on the options available in the buffer.  The application is none the wiser, because it has no knoweldge of the new option, nor should it because it was built against kernel A, that never offered that option
3) Application is built against kernel B and run on kernel B.  This works fine for the same reason as (1).
4) Application is built against kernel B and run on kernel A.  This will break because the application is passing a buffer that is larger than what the kernel expects, and rightly so.   The application is passing in a buffer that is incompatible with what the running kernel expects.

We could look into ways in which to detect the cases in which this might be 'ok', but I don't see why we should bother, because at some point its still an error to pass in an incompatible buffer.  In my mind this is no different than trying to run a program that allocates hugepages on a kernel that doesn't support hugepages (just to make up an example).  Applications built against newer kernel can't expect all the features/semantics/etc to be identical to older kernels.

It shouldn't.  Assuming you have a program built against headers from kernel B (above), if you set a field in the structure that only exists in kernel B, and try to run it on kernel A, you will get an EINVAL return, which is correct behavior because you are attempting to deliver information to the kernel that kernel A (the running kernel) doesn't know about.  Thats correct behavior. I won't disagree about the niceness of versioning, but that ship has sailed.To be clear,  this is situation (1) above, and yeah, running on the kernel you built your application against should always work from a compatibility standpoint. Yes, but this is alawys the case for structures that change.  If you have an application built against kernel (B), and uses structure fields that only exist in that version of the kernel (and not earlier) will fail to compile when built against kernel (A) headers, and thats expected.  This happens with any kernel api that exists in a newer kernel but not an older kernel. Any time you make a system call to the kernel, you have to be prepared to handle the resulting error condition, thats not unexpected.  To assume that a system call will always work is bad programming practice."
160,541450,544269,uncivil,"What a complete mess we have here. Use new socket option numbers next time, do not change the size and/or layout of existing socket options. This whole thread, if you read it, is basically ""if we compatability this way, that breaks, and if we do compatability this other way oh shit this other thing doesn't work."" I think we really need to specifically check for the difference sizes that existed one by one, clear out the part not given by the user, and backport this as far back as possible in a way that in the older kernels we see if the user is actually trying to use the new features and if so error out. Which, btw, is terrible behavior.  Newly compiled apps should work on older kernels if they don't try to use the new features, and if they can the ones that want to try to use the new features should be able to fall back when that feature isn't available in a non-ambiguous and precisely defined way. The fact that the use of the new feature is hidden in the new structure elements is really rotten. This patch, at best, needs some work and definitely a longer and more detailed commit message.",543076,uncivil,"I disagree with this, at least as a unilateral statement.  I would assert that an old program, within the constraints of the issue being discussed here, will run perfectly well, when built and run against a new kernel.  At issue is the size of the structure sctp_event_subscribe, and the fact that in several instances over the last few years, its been extended to be larger and encompass more events to subscribe to.  Nominally an application will use this structure (roughly) as follows:  Assume this code will be built and run against kernel versions A and B, in which: A) has a struct sctp_event_subscribe with a size of 9 bytes B) has a struct sctp_event_subscribe with a size of 10 bytes (due to the added field sctp_sender_dry_event)  That gives us 4 cases to handle  1) Application build against kernel A and run on kernel A.  This works fine, the sizes of the struct in question will always match  2) Application is built against kernel A and run on kernel B.  In this case, everything will work because the application passes a buffer of size 9, and the kernel accepts it, because it allows for buffers to be shorter than the current struct sctp_event_subscribe size. The kernel simply operates on the options available in the buffer.  The application is none the wiser, because it has no knoweldge of the new option, nor should it because it was built against kernel A, that never offered that option  3) Application is built against kernel B and run on kernel B.  This works fine for the same reason as (1).  4) Application is built against kernel B and run on kernel A.  This will break because the application is passing a buffer that is larger than what the kernel expects, and rightly so.   The application is passing in a buffer that is incompatible with what the running kernel expects.  We could look into ways in which to detect the cases in which this might be 'ok', but I don't see why we should bother, because at some point its still an error to pass in an incompatible buffer.  In my mind this is no different than trying to run a program that allocates hugepages on a kernel that doesn't support hugepages (just to make up an example).  Applications built against newer kernel can't expect all the features/semantics/etc to be identical to older kernels.  It shouldn't.  Assuming you have a program built against headers from kernel B (above), if you set a field in the structure that only exists in kernel B, and try to run it on kernel A, you will get an EINVAL return, which is correct behavior because you are attempting to deliver information to the kernel that kernel A (the running kernel) doesn't know about.  Thats correct behavior.  I won't disagree about the niceness of versioning, but that ship has sailed.  To be clear,  this is situation (1) above, and yeah, running on the kernel you built your application against should always work from a compatibility standpoint.   Yes, but this is alawys the case for structures that change.  If you have an application built against kernel (B), and uses structure fields that only exist in that version of the kernel (and not earlier) will fail to compile when built against kernel (A) headers, and thats expected.  This happens with any kernel api that exists in a newer kernel but not an older kernel.  Any time you make a system call to the kernel, you have to be prepared to handle the resulting error condition, thats not unexpected.  To assume that a system call will always work is bad programming practice.  ","I disagree with this, at least as a unilateral statement.  I would assert that an old program, within the constraints of the issue being discussed here, will run perfectly well, when built and run against a new kernel.  At issue is the size of the structure sctp_event_subscribe, and the fact that in several instances over the last few years, its been extended to be larger and encompass more events to subscribe to.  Nominally an application will use this structure (roughly) as follows:  Assume this code will be built and run against kernel versions A and B, in which: A) has a struct sctp_event_subscribe with a size of 9 bytes B) has a struct sctp_event_subscribe with a size of 10 bytes (due to the added field sctp_sender_dry_event)  That gives us 4 cases to handle  1) Application build against kernel A and run on kernel A.  This works fine, the sizes of the struct in question will always match  2) Application is built against kernel A and run on kernel B.  In this case, everything will work because the application passes a buffer of size 9, and the kernel accepts it, because it allows for buffers to be shorter than the current struct sctp_event_subscribe size. The kernel simply operates on the options available in the buffer.  The application is none the wiser, because it has no knoweldge of the new option, nor should it because it was built against kernel A, that never offered that option  3) Application is built against kernel B and run on kernel B.  This works fine for the same reason as (1).  4) Application is built against kernel B and run on kernel A.  This will break because the application is passing a buffer that is larger than what the kernel expects, and rightly so.   The application is passing in a buffer that is incompatible with what the running kernel expects.  We could look into ways in which to detect the cases in which this might be 'ok', but I don't see why we should bother, because at some point its still an error to pass in an incompatible buffer.  In my mind this is no different than trying to run a program that allocates hugepages on a kernel that doesn't support hugepages (just to make up an example).  Applications built against newer kernel can't expect all the features/semantics/etc to be identical to older kernels.  It shouldn't.  Assuming you have a program built against headers from kernel B (above), if you set a field in the structure that only exists in kernel B, and try to run it on kernel A, you will get an EINVAL return, which is correct behavior because you are attempting to deliver information to the kernel that kernel A (the running kernel) doesn't know about.  Thats correct behavior.  I won't disagree about the niceness of versioning, but that ship has sailed.  To be clear,  this is situation (1) above, and yeah, running on the kernel you built your application against should always work from a compatibility standpoint.   Yes, but this is alawys the case for structures that change.  If you have an application built against kernel (B), and uses structure fields that only exist in that version of the kernel (and not earlier) will fail to compile when built against kernel (A) headers, and thats expected.  This happens with any kernel api that exists in a newer kernel but not an older kernel.  Any time you make a system call to the kernel, you have to be prepared to handle the resulting error condition, thats not unexpected.  To assume that a system call will always work is bad programming practice.   What a complete mess we have here. Use new socket option numbers next time, do not change the size and/or layout of existing socket options. This whole thread, if you read it, is basically ""if we compatability this way, that breaks, and if we do compatability this other way oh shit this other thing doesn't work."" I think we really need to specifically check for the difference sizes that existed one by one, clear out the part not given by the user, and backport this as far back as possible in a way that in the older kernels we see if the user is actually trying to use the new features and if so error out. Which, btw, is terrible behavior.  Newly compiled apps should work on older kernels if they don't try to use the new features, and if they can the ones that want to try to use the new features should be able to fall back when that feature isn't available in a non-ambiguous and precisely defined way. The fact that the use of the new feature is hidden in the new structure elements is really rotten. This patch, at best, needs some work and definitely a longer and more detailed commit message."
161,546823,547238,uncivil,Looking more flexible does not make it more correct.,547224,technical,Basic C programming course:   The prototype must be available before the declaration of the global  function.  Oh well....,Basic C programming course:   The prototype must be available before the declaration of the global  function.  Oh well.... Looking more flexible does not make it more correct.
162,571417,571702,civil,This is phenomenal. Thank you so much for digging into this. I'm hoping this will greatly reduce the risk of future leakage.,571698,technical,Agreed... it seems fishy at least.  ,Agreed... it seems fishy at least.   This is phenomenal. Thank you so much for digging into this. I'm hoping this will greatly reduce the risk of future leakage.
163,72369,470502,uncivil,"I mean the level of a resource in IOMEM tree (the one that's printed from here). 1-st level means its parent is root and so on. If it's not a problem anymore IIUC, can we revert the change as it still breaks for the reasons I described above? Nothing prevents - true, but that's plainly wrong from OS point of view to grab physical ranges for something without knowing what's actually behind on that platform. I think we shouldn't consider this as a valid thing to do and don't try to workaround initially incorrect code.",470438,technical,"What do you mean by 1st and 2nd level?     Pretty much. (3) is true in the sense that memory is first allocated from hostmem_resource (which is non-dom0 RAM).    Not anymore, as far as that particular commit is concerned, but that's because of this, (x86/PCI: Move and shrink AMD 64-bit window to avoid conflict"") which was introduced after balloon patch. IIRC there were some issues with unrelated to balloon.    The concern is that in principle nothing prevents someone else to do exact same thing this commit did, which is grab something from right above end of RAM as the kernel sees it. And that can be done at any point.   ","What do you mean by 1st and 2nd level?     Pretty much. (3) is true in the sense that memory is first allocated from hostmem_resource (which is non-dom0 RAM).    Not anymore, as far as that particular commit is concerned, but that's because of this, (x86/PCI: Move and shrink AMD 64-bit window to avoid conflict"") which was introduced after balloon patch. IIRC there were some issues with unrelated to balloon.    The concern is that in principle nothing prevents someone else to do exact same thing this commit did, which is grab something from right above end of RAM as the kernel sees it. And that can be done at any point.    I mean the level of a resource in IOMEM tree (the one that's printed from here). 1-st level means its parent is root and so on. If it's not a problem anymore IIUC, can we revert the change as it still breaks for the reasons I described above? Nothing prevents - true, but that's plainly wrong from OS point of view to grab physical ranges for something without knowing what's actually behind on that platform. I think we shouldn't consider this as a valid thing to do and don't try to workaround initially incorrect code."
