comment_code,concat_body
technical,"I'm surprised by the pushback this has received, with people going as far as stop using a tool they enjoyed for years **for free.** I don't even look at the output of npm i unless it fails.  The only reason why this should not be here was the one described by arcanis. Other than that, it should just be smaller and more clear about its origin.  I don't even look at the output of npm i unless it fails. then what's the point of this?  ctrl+f readme"
technical,"My terminal is the one last stronghold, the one last haven of peace that doesn't endlessly serve me ads from corporate overlords all day long. I vehemently oppose this idea as I believe it is fundamentally opposed to the open source ethos we've built up over decades.  Yes, it's important to help out open source contributors and project owners - but ads are not the solution. -1 for that kind of non-permissive licensing, it does not even seem to fit the definition of ""open source"" ref:  +1 for offer of services from the primary author, which is sometimes needed to help with open-source sustainability   -1 for promotion of third-party products or services (noisy, distracting, etc.)"
technical,"I believe a very promising model of funding that I truly believe feross would find agreeable would be to encrypt all documents on the computers of developers who install this eslint config either as a dependency or explicitly, and then demand a payment of 1000 in bitcoin to recover the files. It has great customer turnover. *cough left-pad *cough"
technical,"On August 28, 2019, I released standard 14.1.0 which removed the funding dependency. See changelog here # The experiment is over read the recap here!  Hey folks!   I've already spoken to a few of you about this idea, but I'd like to share it with the broader community now that standard 14 has shipped.  **Background:** I think that the current model of sustaining open source is not working and we need more experimentation. This is one such experiment.  **What's the experiment?** Whenever standard 14 is installed, we'll display a message from a company that supports open source. The sponsorship pays directly for maintainer time. That is, writing new features, fixing bugs, answering user questions, and improving documentation.  **What's the goal?** My goal with this experiment is to make standard healthier. If we learn that the experiment works, perhaps we can help make all open source healthier, too. For complex reasons, companies are generally hesitant or unwilling to fund OSS directly. When it does happen, it's never enough and it never reaches packages which are transitive dependencies (i.e. packages that no one installs explicitly and therefore no one knows exists). Essentially, we have a public good which is consumed by huge numbers of users, but which almost no one pays for. Fortunately, there exists a funding model that usually works for public goods like this “ ads. The goal of this experiment is to answer the question: Can we use ethical ads “ ads that don't track users or collect data “ to fund open source software?  **What does the code do?** The code for this experiment is available here: feross/funding. Essentially, it calls console.log() on some text. There is no tracking, data collecting, or unexpected behavior. You can look at the code to verify “ indeed, this is the beauty of open source!  **What will the funds be used for?** The funds raised so far (2,000) have paid for Feross's time to release Standard 14 which has taken around five days. If we are able to raise additional funds, the next thing I'd like to focus on is out-of-the-box TypeScript support in StandardJS (one of the most common feature requests!) and modernizing the various text editor plugins (many of which are currently unmaintained). If others in the community are interested in taking the lead on any of these issues, I'd like to direct some funds to you.  Feedback welcome!  ## EDIT: This thread is now locked :lock:  For why + next steps, scroll to the end   ## EDIT: The experiment is over “ Feross posted a recap on his blog"
technical,"I don't install npm packages for entertainment. I'm working. Deliberately inserting ads into my workflow in the hope that I'll stop working and go screw around on the web and maybe buy something is absolutely predatory.  feross should be ashamed of himself. ## Next steps  Hey all, it's great to see people care as much about the future of Open Source as we do. There have been a bunch of great questions and suggestions raised here, but this is turning into a mega-thread with a lot going on in it.  I've taken some of the themes I've seen emerging and started some threads for more focused conversations: - What's wrong with Open Source right now - What do we want Open Source to look like in 10 years - Process for Evaluating this version (of funding) - Other ideas we could experiment with - I'm angry and I want to yell some"
technical,"I am whole-heartedly behind this experiment. It's not the perfect end solution, but that's not the point - it's about moving the conversation about how to build healthy relationships between our commons and companies forward.  Thanks for taking leadership on this feross . Admittedly, I haven't contributed an immense amount to open source code, so I realize I don't have the same perspective on this issue. That being said, I sincerely hope that this solution does not become standard.  I agree wholeheartedly that this is a problem that needs a solution, and I am glad that the conversation is being pushed forward by this experiment. Advertisements in my terminal is not the solution.  For me, yarn or ""standard"": ""13.0.0"" is going to have to be my ""standard"" until this is removed."
technical,"And that's the end of standard folks. Who wants to fork and make an ad free version? Advertising is a terrible funding model.  Even though advertisers (or ""sponsors"", which amounts to the same thing) will often try to pretend otherwise, running advertising means that you are dependent on not doing anything they dislike, at the risk of suddenly having your funding yanked - after all, their name is associated with it. It produces a chilling effect.  I don't think it's a good idea *at all* to introduce that sort of problem to the open-source ecosystem any more than it already has, if anything, there should be *less* advertising."
technical,Something I wanted to put here from a comment feross wrote in the main funding repo:  It makes the whole thing seem a *lot* more reasonable. Agree with. I want to add that there are a lot of misconceptions here about what it means to maintain a (hugely popular) OSS project and the effort involved. I just became a sponsor just for feross having to read this thread.
technical,"Oh-My-Zsh does that (with .zshrc though :D). I don't see the issue because I get a free service from them. AlexanderJohnston me as well. I just let linode know why I'm cancelling, and let logrocket know they,re no longer being considered."
technical,"as I said before if you need money dont do OSS, You can convert standardjs 1 per download and then you can see how much its really worth. Since as you know nobody will ever pay 1 for an eslint config which thinks semi column isn't necessary in JS.  This library is no where near webpack or babel.  To see what you really worth do 1 per download. All the rationalizations aside, let's be honest and call this what it is: an attempt to redefine terms.  Writing software for no compensation is what you signed up for when you put OSS out there.  And now you realize you don't like that.  You made a mistake, own up to it."
technical,"This is absolutely disgusting behavior, you should be ashamed of yourself. And that's the end of standard folks. Who wants to fork and make an ad free version?"
technical,"This is absolutely disgusting behavior, you should be ashamed of yourself. as I said before if you need money dont do OSS, You can convert standardjs 1 per download and then you can see how much its really worth. Since as you know nobody will ever pay 1 for an eslint config which thinks semi column isn't necessary in JS.  This library is no where near webpack or babel.  To see what you really worth do 1 per download."
technical,"This is absolutely disgusting behavior, you should be ashamed of yourself. As maintainer of Yarn I'm strongly against this pattern, although not for the reasons you might think. **Post-install scripts deoptimize packages and break workflows.**  When a package has postinstall scripts, we (package managers) cannot assume anymore that it's safe to share its directory between projects. Even worse, we must extract it on the disk (in case there's a build step) even if it's actually completely unnecessary (because you only print things). We've been exploring a lot of optimisations in this space lately (Yarn 2 will keep your packages within their archives by default), and this kind of pattern will prevent your users from exploiting them to full benefits.  *(Note that I'm not saying the postinstall scripts are deprecated - just that they should be reserved to the use case they were designed for, which is building a package against a local environment)*  Fwiw, as is mentioned somewhere in this thread, Yarn already doesn't print the build logs unless they make the installs crash, so this post-install script wouldn't have any visible effect for our users. Still, I value the health of the ecosystem a lot, both from the point of view of maintainers and users, and I would be happy to discuss how we could satisfy this use case in a more integrated and less intrusive way (for example by adding a specific field to the package.json). I've actually opened an issue against the OpenCollective repo to discuss that, but it got no traction until now."
technical,"This is absolutely disgusting behavior, you should be ashamed of yourself. Avamander mind if I get to put some echos into your .bashrc? They're benign, right?"
technical,"I agree with a bunch of folks here that the goal here is great, but I already have enough trouble getting devs to pay attention to warnings like ""unmet peer dependencies"" when an npm install is finished. Having screens of ads go by makes it impossible to see them. ('core-js' comes to mind begging for money AND a job for the lead dev)  [Update: spelling] Companies should sponsor OSS projects because they use them. Not to get ads in front of the project's users.  Also, this kind of thing is easily blocked so it's unlikely to be a good revenue stream anyway."
technical,Silence opposition. That usually works.  Sadly.  Are you sick of reasonable regimes? Move to the DEMOCRATIC PEOPLES REPUBLIC OF NORTH KOREA. Call now for details. Could not be more on topic.
technical,"If it will change the name of this project to something sensible, I will greet them with open arms Dang it. Just enabled it the other week, already going to have to raise another PR to remove from the project now :disappointed:"
technical,"which is ironic considering yarn was built by one of the most dystopian, ad-pushing, privacy invading companies on the planet.  this may surprise you, but not  everything  has to be completely and utterly infested with advertising... or maybe it does, I dunno. Linux is free, can someone open a PR to stuff the kernel with adverts too? There's nothing I love more when trying to debug a driver issue than having to drink a verification can for 5 minutes. TL,DR: ""iTs fREe"" isn't a good reason to stuff it with advertising Dude, if you can't maintain the lib just pass it to someone else or seek for sponsors on patreon /GitHub"
technical,"I didn't mean to suggest that anyone disagreeing was acting as part of a mob. It was meant to apply specifically to those people treating a github issue like it's a place for trolling, nasty comments, etc. My apologies if it came off that way. Either way, the name of the package has nothing to do with the discussion at hand."
technical,"I didn't mean to suggest that anyone disagreeing was acting as part of a mob. It was meant to apply specifically to those people treating a github issue like it's a place for trolling, nasty comments, etc. My apologies if it came off that way. ENLARGE YOUR DISK DRIVE WITH THIS ONE SIMPLE TRICK DISCOVERED BY A LOCAL USER -- NPM DEVELOPERS HATE HIM!"
technical,"I'm all for OSS and companies sponsoring projects, I maintain cdnjs.com and we have multiple companies sponsoring what we do there, but showing an ad after every installation of a package isn't the way to do it. Consider keeping sponsors to your readme (which you already do, not sure why you're adding this new way to promote sponsors...) and websites. evandentremont ""Stomping"" on someone providing you a free service is definitely not going to net you the results you expect.   I think this is a good case right now to demonstrate what happens if you don't support what you use. If you don't like these practices you have two very very simple options: a) Maintain your own code with your own money and time b) Pay for someone to do that for you or don't come here insulting the developers who do this out of their goodwill  Or settle with the option c) that you can see right now and are unreasonably mad at."
technical,"This isn't about having our cake and eating it too. Selling ad-space is not innovative. And it's particularly unhelpful in my logs.  I don't fully agree with ckipp01 on the sponsorship driven OSS. It is a risk, but it already exists outside of selling ad-space in logs.  For me, the issue is more that I don't want stuff that doesn't help me in my logs. I wholeheartedly agree with putting your ""supported by company X"" in the readme. That helps me understand, it does resonate with me when I see certain companies donating money to OSS.  By the way, just stating that ""if you don't like it, come up with a better solution"" is a cop-out. There  is  value in feedback,  especially  when you don't agree.  EDIT: PS: I too want to live in a perfect world where every developer can live, pay rent and only work on projects they like. That perfect world for me does not include ads in my terminal.  EDIT2: PPS: Support of my peers for me is a  big  reason to work on stuff. I know others that earn enough in their day-job that they enthusiastically spend time on OSS in their free time as a hobby and get value out of verbal support from their peers. That support is more often than not shown in a verbal/written way. Fact of the matter is, OSS maintainers need money  today . Better solutions may come along, putting up with ads in the mean time is a small price to pay. While I don't particularly like seeing ads in this space, I understand its necessity and fully support it.  I do agree that it could be more clear why the ads there, and specifically that it's making the tools you love maintainable."
technical,"I now know how long it took for people to understand that the economics of open source are fundamentally broken. Is open source economicalyl sustainable?  please discuss.  this has to be addressed while we can still feed our families. feross, you describe this as an experiment but you have not outlined evaluation criteria nor success/failure criteria of the experiment. I think you could deter some of this negative feedback that you are receiving by explicitly stating:  a) a start and end date for the experiment, b) an evaluation criteria, and c) how that evaluation criteria determines success or failure of the experiment.  Notably, I think your evaluation criteria needs to factor in the annoyance factor for the end-user of the ad placement."
technical,"If you do not wish to contribute to an open source project for free, please do not feel obligated to do so. FOSS is not detrimental to the industry. It is a dangerous precedent.  Advertising by FOSS projects has been done before. The dangerous thing here is only your entitlement and unreasonable expectations what people should do for free for you."
technical,"FOSS is not detrimental to the industry. It is a dangerous precedent.  Advertising by FOSS projects has been done before. The dangerous thing here is only your entitlement and unreasonable expectations what people should do for free for you. FOSS is not, advertising in tooling is absolutely detrimental. Please don't conflate the two.   Expecting my terminal to remain ad-free is not entitled. Its entitled to think it's reasonable to jam ads down peoples throats.  To be clear, I don't have unreasonable expectations of what people should do for free.  But this isn't free. Theres a catch. That catch is the problem. Edit: typo."
technical,"Must be trolling, because there's no way you could be so oblivious that you would think this would fly.  And...Linode?  I hope they didn't know they would be spamming developers' terminals.  Shame. Friendly request for the crowds that are invariably going to roll into this thread now: please keep your comments civil, and don't start pile-ons.  Even if you disagree with this change (as do I, strongly), spamming memes and angry comments - as so often happens in threads that gain notoriety - is just going to get the thread locked. Let's keep things from spiralling down."
technical,"Dang it. Just enabled it the other week, already going to have to raise another PR to remove from the project now :disappointed: Given that this package is essentially a config file and thin wrapper script for ESLint, I am curious how much of this revenue will be shared with the upstream ESLint developers."
technical,"First off, I love standard and use it in all my JS projects. Thank you so much for your hard work in making a fantastic, easy-to-use, and solid linter.  I was really taken off guard seeing this in my terminal today. It's the last place I expect to see an ad, and really the last place I want to. While I get the idea behind it, and I agree that we must think of innovative ways to support open-source, relying on displaying an ad in a place that typically doesn't have them opens up the doors to new behavior of ad spamming when I simply want to install a dependency.  I'm not saying this is the case here, but let's say that this really frustrates a percentage of developers and they decide to no longer use the project/tool/library, but the project does see an uptick in sponsorship. More than likely they'd see that as a win, and this could slowly shift the focus from a community-driven project to a sponsorship-driven project (and not as in individual sponsors, but company sponsorship). Again, I'm not saying this is the case here, but you can sort of see how that could happen if this becomes a normal practice in how we try to support open-source projects.  I'd much rather see an innovative solution that tries to encourage individual community members to give more to projects they want to support rather than relying on sponsorship ads.  Finally, I also agree with mhogerheijde, when I first saw this I was confused, and my initial thought was to immediately find the package and to remove it since it wasn't clear at all where this was coming from or why an ad was in my terminal.  EDIT: spelling Great point that it could be more clear why the ads there, and specifically that it's making the tools you love maintainable.  I think if you're not into it, then the obligation is in you to put some work in to come up with more aligned solutions.  Enthusiastic verbal support does not help maintainers thrive.  At the moment many of them are generously giving and experience a few breadcrumbs of thanks in return.  I want to live in a world where maintainers have enough to pay rent, eat food, have health care, save money, dream about big future projects."
technical,"First off, I love standard and use it in all my JS projects. Thank you so much for your hard work in making a fantastic, easy-to-use, and solid linter.  I was really taken off guard seeing this in my terminal today. It's the last place I expect to see an ad, and really the last place I want to. While I get the idea behind it, and I agree that we must think of innovative ways to support open-source, relying on displaying an ad in a place that typically doesn't have them opens up the doors to new behavior of ad spamming when I simply want to install a dependency.  I'm not saying this is the case here, but let's say that this really frustrates a percentage of developers and they decide to no longer use the project/tool/library, but the project does see an uptick in sponsorship. More than likely they'd see that as a win, and this could slowly shift the focus from a community-driven project to a sponsorship-driven project (and not as in individual sponsors, but company sponsorship). Again, I'm not saying this is the case here, but you can sort of see how that could happen if this becomes a normal practice in how we try to support open-source projects.  I'd much rather see an innovative solution that tries to encourage individual community members to give more to projects they want to support rather than relying on sponsorship ads.  Finally, I also agree with mhogerheijde, when I first saw this I was confused, and my initial thought was to immediately find the package and to remove it since it wasn't clear at all where this was coming from or why an ad was in my terminal.  EDIT: spelling Has you heard about this great new company LogRocket? They allow you to get your logs as high as they need to be.  Think smarter. Think Logrocket.  * Please don't complain as you haven't paid for githubs server time."
technical,"I'm sorry, what ""rule"" did the devs break by this? How is it ""impulsive"" to monetize work they've done? If anything you're the one being non-empathetic by not thinking why the devs might have done so if you get this mad at them. It's even more non-empathetic if you previously thought this was never going to happen. Have you tried Mr Clean Magic eraser? Wipe down messes with ease. Nice cherry pick. And you're standing up for this harm  Nike. Just do it."
technical,"How about you stop picking fights in this thread? This isn't doing anyone a favour. Heh, I'm not picking fights, only saying where people are being stupid. Cussing at devs who wrote code for you is stupid."
technical,"That reminds me of the article. Basically, our hated ads-driven business model of internet (google, facebook, ...) was forced to original publishers by people.... (posting it as a slight OT because when people dont know history, they tend to make same mistakes and desicisons) Hi, I would favor advertisements for services that you would be willing to offer, either personally or from a company that you own and run."
technical,"Could not be more on topic. Hmmm... I'm kind of keen to see where this goes. I'm not sure how I feel about it yet. I'm not necessarily happy with it, but at the same time, OSS has really struggled to find a way to fund itself. Relying heavily on donations which rarely materialise and most often, the financial generosity of the contributors. Companies never pay for it, so the contributors have two jobs. One day job and a night job creating and maintaining OSS. The latter is as hard as the former, but OSS authors are not paid for it. Often this means it becomes abandonware, which is something companies themselves worry about. Backing the wrong horse can be an expensive mistake. Especially if they're subject to SLA's they must control.  Also, as a side issue, a lot of GH abandonware is due to devs trying to find a job and using it for learning or portfolio (which is cool. Many good things have come out of it). But once they have found work, contributions disappear as fast as their spare time.  Patreon doesn't seem to be effective in generating enough revenue and isn't seen as the right platform. Plus, consultancy models with free software, needs devs to be (or to hire) sales specialist or local consultants. It's a pain and is costly.  Initial thoughts:  - it's competing in a space with other ad platforms and provided no insight to companies. Unlike the others. As much as I like that, it's a fact that marketers consider targeting crucial.  - irritant? I've not found it. Yet.  - if it becomes very successful, but only sends a wall of ads, it'll frustrate and folk may move away from Standard  - it's not clear what they intend to do if/when it gets lots of ads or if companies don't get anything out of ads.  - these ads are a way to give companies something for contributions. The question is whether and what size company MUST pay?  Keeping an open mind though"
technical,"I would rather this project not exist than exist with this kind of ads.  Its fucking entitled to complain about someon,s fucking choice of words without considering the fucking reason someone might be fucking upset. Hopefully this issue will encourage more developers to think more carefully about the dependency chains that they create when they include packages from third-party authors. You never know what's gonna happen in the future, especially if you are depending on a hundred plus packages."
technical,they both do now. I asked both to disown this shit. How about you stop picking fights in this thread? This isn't doing anyone a favour.
technical,"This is a very bad idea. It violates the user's right to privacy and choice.  I even go farther and say that it is abuse by upstream developers (does not matter WHO these developers are, feross could be replaced by any other developer who wants to abuse users).  If the goal is to seek financing, it would be better to request donations etc...  Note that some users may be ok with ads, but it is NOT ok to NOT ask users whether they are fine with that. Because many are NOT fine with targeted ads. How can you be so smart and stupid at the same time. This idea is horrible."
technical,"There's a sentence that I never thought I would ever see in my life. How the hell would that work in the NPM ecosystem? This package for example seems to be heavily relying on ESLint. Would the developer be able to guarantee an SLA on bug and security fixes that goes down to ESLint? How about to file-entry-cache, which is a requirement for ESLint? Or to flat-cache, which is a dependency for file-entry-cache? And to flatted, which flat-cache requires?  What about profit sharing? Installing this one package will generate a lot of downloads for its dependencies. Are they guaranteed a share of the pie? Or do you need to pay for every dependency on its own? Do the dependency authors also need to sign up to guarantee SLA? Does this also apply to the dependencies? Because sure, the source code for the dependencies are visible, but are you, as a customer, expected to generate the dependency tree and inspect the source code for every part of the package.  Or is the idea that if you sign onto an NPM SLA contract-thingy, you are not allowed to ship any kind of dependencies at all? I mean, that sounds great since the NPM dependency hell is a nightmare, but I'm gonna be shocked if your average developer will sign onto that. Will standard drop its dependence on ESLint and the work of its authors? I just don't know."
technical,"As maintainer of Yarn I'm strongly against this pattern, although not for the reasons you might think. **Post-install scripts deoptimize packages and break workflows.**  When a package has postinstall scripts, we (package managers) cannot assume anymore that it's safe to share its directory between projects. Even worse, we must extract it on the disk (in case there's a build step) even if it's actually completely unnecessary (because you only print things). We've been exploring a lot of optimisations in this space lately (Yarn 2 will keep your packages within their archives by default), and this kind of pattern will prevent your users from exploiting them to full benefits.  *(Note that I'm not saying the postinstall scripts are deprecated - just that they should be reserved to the use case they were designed for, which is building a package against a local environment)*  Fwiw, as is mentioned somewhere in this thread, Yarn already doesn't print the build logs unless they make the installs crash, so this post-install script wouldn't have any visible effect for our users. Still, I value the health of the ecosystem a lot, both from the point of view of maintainers and users, and I would be happy to discuss how we could satisfy this use case in a more integrated and less intrusive way (for example by adding a specific field to the package.json). I've actually opened an issue against the OpenCollective repo to discuss that, but it got no traction until now. I agree with a bunch of folks here that the goal here is great, but I already have enough trouble getting devs to pay attention to warnings like ""unmet peer dependencies"" when an npm install is finished. Having screens of ads go by makes it impossible to see them. ('core-js' comes to mind begging for money AND a job for the lead dev)  [Update: spelling]"
technical,"While I'm totally OK with this on development machines, I think this is strange behavior for staging/production/etc.  Perhaps thos should have a NODE ENV check?  Then again, I don't imagine standard is going to be installed outside of NODE ENV={test, development} anyway... so it's probably OK! I am whole-heartedly behind this experiment. It's not the perfect end solution, but that's not the point - it's about moving the conversation about how to build healthy relationships between our commons and companies forward.  Thanks for taking leadership on this feross ."
technical,"## Next steps  Hey all, it's great to see people care as much about the future of Open Source as we do. There have been a bunch of great questions and suggestions raised here, but this is turning into a mega-thread with a lot going on in it.  I've taken some of the themes I've seen emerging and started some threads for more focused conversations: - What's wrong with Open Source right now - What do we want Open Source to look like in 10 years - Process for Evaluating this version (of funding) - Other ideas we could experiment with - I'm angry and I want to yell some I appreciate the thoughtful discussion and feedback. I just ended the experiment. I shared some thoughts about how the experiment went from my perspective on my blog:"
technical,"Hopefully this issue will encourage more developers to think more carefully about the dependency chains that they create when they include packages from third-party authors. You never know what's gonna happen in the future, especially if you are depending on a hundred plus packages. I believe a very promising model of funding that I truly believe feross would find agreeable would be to encrypt all documents on the computers of developers who install this eslint config either as a dependency or explicitly, and then demand a payment of 1000 in bitcoin to recover the files. It has great customer turnover."
technical,"Hopefully this issue will encourage more developers to think more carefully about the dependency chains that they create when they include packages from third-party authors. You never know what's gonna happen in the future, especially if you are depending on a hundred plus packages. I can understand open source projects need more money for maintenance, but through ads on terminals... I mean, we're seeing ads almost great part of the day, but seeing it also on our terminals. I strongly disagree.  We have to look for better ideas, try to design better business models that make these projects profitable and not too intrusive."
technical,"In fairness, this appears to not actually be very targeted - it's just a postinstall script in the funding package. I do love OSS and all the amazing open source projects available  and I do very much understand that funding is required now for many projects and services such as Open Collective and/or GitHub Sponsors either don't work for their needs or don't meet their funding needs. However, my fear is that this experiment will result in many more OSS projects picking it up and lead to a future where CI build logs are full of banner ads and then potentially lead to other worse advertising means. The user u/crabbytag put it best for me and my concerns for what this could start in the future.  The maintainer is getting 2000 for these banners because no one else is displaying ads there. Once other library authors notice this opportunity, they'll start adding ads too. Then the average payout comes down. But since we've already accepted ads here, some authors will include more annoying ads for slightly more money. For example, 2x the payout if the developer is required to take some action ('press enter to unpause the build) and 3x if the action is more annoying ('type out ""Linode rocks"" to unpause the build).  That's just me of course but I feel like this is not the correct direction to go towards in helping to fund open source projects and we need to continue looking for another alternative."
technical,"The existence of this issue infers the obvious must be stated. I just recieved this response.  Hello, We definitely understand your objection to an advertisement of this nature. This ad was not paid for or solicited by Linode. There is an open issue/thread regarding this advertisement on the package's Github repository.  We appreciate you voicing your concerns about this ad, and I've passed along your feedback to our team who will be investigating this matter. If you have any other questions or concerns please let us know.  I expect this was merely a misunderstanding on the part of this support agent. He probably wasn't aware of the facts.  They have public tweets where they say they've ""reconsidered"": Linode has been very supportive at exploring new ways to support open source. It's a bummer that this didn't work out, but I understand and don't hold it against them."
technical,ENLARGE YOUR DISK DRIVE WITH THIS ONE SIMPLE TRICK DISCOVERED BY A LOCAL USER -- NPM DEVELOPERS HATE HIM! I love how ThisIsADogHello's comment was auto-flagged as spam. Ironic.
technical,"I'd wait a bit before throwing Linode or LogRocket under the bus. It's quite possible that they didn't have any knowledge of this miserable idea. I mean, yes? Literally sometimes. There are valid reasons for that.  I shouldn't have to scroll past things that shouldn't be there."
technical," I noticed the (fairly large bright bold) banner. It reminds me of the OpenCollective-style banners used by webpack / corejs. I think it's OK... I do worry that npm install will just become a long trail of banner ads though eventually and it won't scale. Because if  every  npm package adds ads, the noticeability of each ad will diminish. (Interestingly, the most valuable ""realestate"" will be packages whose banner is displayed  last , so if it becomes a literal ""race-to-the-bottom"" people might add sleep statements to their post-install scripts so they are displayed nearest the bottom. What a dystopian installation experience!)  Fun fact: yarn does not display the output of post-install scripts. One might say yarn has built-in ad-blocking."
technical,Selling out developer tooling. Now that one asshole has done it more will follow unless this is stomped out immediately. I now know how long it took for people to understand that the economics of open source are fundamentally broken. Is open source economicalyl sustainable?  please discuss.  this has to be addressed while we can still feed our families.
technical,"If you read the whole thing it's with a special dev flag. I read through the thread again, and it seems like the OP was trying it out by installing it --save-dev, but the funding package's behavior should be the same regardless of how it's installed."
technical,"NPM does. Haven't found an enforcement mechanism. I recommend using Prettier with the following settings to mimic Standard's style guide:  In my experience, it also has better tooling integrations than Standard, and much more powerful formatting (it parses and reformats the entire AST consistently, while Standard can only use ESLint's very limited formatting engine which is meant primarily for linting rather than formatting)."
technical,"I recommend using Prettier with the following settings to mimic Standard's style guide:  In my experience, it also has better tooling integrations than Standard, and much more powerful formatting (it parses and reformats the entire AST consistently, while Standard can only use ESLint's very limited formatting engine which is meant primarily for linting rather than formatting). I think I'm actually OK with this, assuming the following conditions:  1. Adverts only appear in ""dev mode"" 2. They can be disabled 3. They aren't too frequent 4. They are & remain relevant to the developer  Basically use this as an example :point right:"
technical,"I recommend using Prettier with the following settings to mimic Standard's style guide:  In my experience, it also has better tooling integrations than Standard, and much more powerful formatting (it parses and reformats the entire AST consistently, while Standard can only use ESLint's very limited formatting engine which is meant primarily for linting rather than formatting). I will actually pay money for that ad."
technical,"It's only directly caused by you, the user, not contributing enough. It's absolutely entitled to cuss out developers that have let you know they want to be supported. This might be yet the best place where to use the surprised Pikachu meme. I would rather this project not exist than exist with this kind of ads.  Its fucking entitled to complain about someon,s fucking choice of words without considering the fucking reason someone might be fucking upset."
technical,Have you tried Mr Clean Magic eraser? Wipe down messes with ease. Nice cherry pick. And you're standing up for this harm  Nike. Just do it. I'd like you to be the first to know Macys(R) is having a store wide sale THIS WEEK ONLY  Type 'I want to save' to continue.
technical,Point being? Are you using a 300 baud terminal that it actually took time to scroll past the ad you got in exchange for getting to use the library that took hours and hours to write? I'd wait a bit before throwing Linode or LogRocket under the bus. It's quite possible that they didn't have any knowledge of this miserable idea.
technical,*cough left-pad *cough I'll be e-mailing all of your advertisers to let them know that this will negatively affect my choice to use their services.
technical,"Companies should sponsor OSS projects because they use them. Not to get ads in front of the project's users.  Also, this kind of thing is easily blocked so it's unlikely to be a good revenue stream anyway. I'm all for (better) compensation of OSS developers but I will go lengths to avoid ads, just any ads in general. Ads are a toxic pest in modern days, wherever you go you're slammed with them. The console is not intended for this either.  If this means I have to change a dependency or library to not have ads, I will do it, not because you don't deserve money, but because you're intruding an area you're not supposed to intrude by design.   Edited a word."
technical,"feross, you describe this as an experiment but you have not outlined evaluation criteria nor success/failure criteria of the experiment. I think you could deter some of this negative feedback that you are receiving by explicitly stating:  a) a start and end date for the experiment, b) an evaluation criteria, and c) how that evaluation criteria determines success or failure of the experiment.  Notably, I think your evaluation criteria needs to factor in the annoyance factor for the end-user of the ad placement. I'm all for OSS and companies sponsoring projects, I maintain cdnjs.com and we have multiple companies sponsoring what we do there, but showing an ad after every installation of a package isn't the way to do it. Consider keeping sponsors to your readme (which you already do, not sure why you're adding this new way to promote sponsors...) and websites."
technical,"Avamander mind if I get to put some echos into your .bashrc? They're benign, right? I'm clearly not the only person with a problem here. Nice try deflecting.  You got triggered over a 'cuss' for fuck's sake. Kettle meet pot?"
technical,"npm, yarn, NuGet, etc. all need a pay-to-play model.  Curated feeds where the devs sign contracts guaranteeing an SLA on bug and security fixes would be the driver for getting you packages through the pay-per-download system.  Each download to a new machine become a sale to that package author.  The repos escrow the money and the dev withdraw in local or crypto currency.  when devs release a new  major  version, downloaders have to pay to upgrade to it from previously purchased versions.  Each purchase would be a 1 dev on 1 machine license.  number of projects used with would be irrelevant.  Build servers would require a license, same as developers.  Because open source is what it is, refunds would not be allowed because the code is right there to preview before deciding to buy.  This is fair.  It pays people at the earliest point possible.  It gives customers 100% access to the code.  What has to change is licensing. * code is freely available for non commercial use * commercial use requires development from the signed packages of the curated repositories, not git pulls.  SLA on bug and security fixes do not apply to modified codebases. * Bugs found subject to the SLA must be remediated with n days of the bug being verified. * Security bugs found must be remediated within n days of acknowledged by the repo.  Should the developer be unable to fix the bug, then responsibility will fall to the repository to provide the fix to the code. * Excessive absence or inability of the maintainer to honor their obligations will result in the removal of all the their packages from the curated repository.  This will be a major shock to the way it's don e now.  5 line npm packages will and should go away.  Software development will not lose any of the collaborative features of open source and will gain accountability and credibility.  The best part is that advertisers are completely cut out of this loop and their influence cannot take over development culture. I'm sorry, what ""rule"" did the devs break by this? How is it ""impulsive"" to monetize work they've done? If anything you're the one being non-empathetic by not thinking why the devs might have done so if you get this mad at them. It's even more non-empathetic if you previously thought this was never going to happen."
technical,"immediate uninstall if any of my OSS advertises in my terminal I'm surprised by the pushback this has received, with people going as far as stop using a tool they enjoyed for years **for free.** I don't even look at the output of npm i unless it fails.  The only reason why this should not be here was the one described by arcanis. Other than that, it should just be smaller and more clear about its origin."
technical,"Admittedly, I haven't contributed an immense amount to open source code, so I realize I don't have the same perspective on this issue. That being said, I sincerely hope that this solution does not become standard.  I agree wholeheartedly that this is a problem that needs a solution, and I am glad that the conversation is being pushed forward by this experiment. Advertisements in my terminal is not the solution.  For me, yarn or ""standard"": ""13.0.0"" is going to have to be my ""standard"" until this is removed. I'm with morgansliman, of any place I don't want to see ads, my terminal is on the top of that list.  I would a low a ""buy me a coffee""-link, but that's about it.  On another note: For me, it was totally not clear that this ad was for a company that donates to standard. So it took me a while to figure out where this ad came from. So I'm not sure what the ad should convey, but I don't think it is the feeling ""what is this and how do I get rid of it?""."
technical,"I'm all for (better) compensation of OSS developers but I will go lengths to avoid ads, just any ads in general. Ads are a toxic pest in modern days, wherever you go you're slammed with them. The console is not intended for this either.  If this means I have to change a dependency or library to not have ads, I will do it, not because you don't deserve money, but because you're intruding an area you're not supposed to intrude by design.   Edited a word. If a company is advertising in my terminal, even if they're technically sponsoring a project as means to do so, I will do everything I can to avoid that company. Whether or not they know it, I'm pretty sure this is the exact opposite of the sort of publicity these sponsors would want among developers."
technical,"This does 2 things, 1 is objectively bad and one is subjective.  1. Encourages users to run with --silent as an adblocker - Bad because it means users won't see critical errors, security warnings etc - If we can agree it's bad to encourage users to ignore key warnings, we inherently agree that polluting the output of install is bad  for the exact same reason . 2. It's the most annoying thing you could possibly do as a package maintainer other than remove your package.  Regardless, NPM will undoubtedly decide to block any method of serving advertisements through the terminal. It doesn't matter what Standard chooses to do. If it will change the name of this project to something sensible, I will greet them with open arms"
technical,"The short answer is that WinUI relies on the underlying OS, even in it's decoupled state.  WinUI is the UI framework, and can be updated faster than the OS.  As for your views about Windows 10, you are entitled to believe what you like, but Windows 7 is coming to an end, and if you do not wish to remain a Windows user, there is macOS and various forms of Linux, for you to choose from.  I however suspect your comments are a little disingenuous, but I hope I have provided an accurate answer to your query. If we start turning NPM packages into malware, people would also be a lot more careful about the crap they select. I don't think it's a good idea though."
technical,"I can understand open source projects need more money for maintenance, but through ads on terminals... I mean, we're seeing ads almost great part of the day, but seeing it also on our terminals. I strongly disagree.  We have to look for better ideas, try to design better business models that make these projects profitable and not too intrusive. If you do not wish to contribute to an open source project for free, please do not feel obligated to do so."
technical,"Not when that feedback is 100% predicable because it's nothing but our inner two year old being triggered to whine when everything doesn't go 100% his or her way.  We humans are opportunists.  Any time a cost to us is >0 we will not like that cost and we'll complain about it.  In discussing a problem whose solution is necessarily going to impart a cost it doesn't need to be stated that those who need to shoulder said cost are going to find it less preferable than having it at no cost.  This inner alt-right perspective is expected.   Anyway, the solution to monetizing an artist's reproducible work has already been solved.  It's called copyright.  We the People have expressly removed our liberty to freely copy an artist's work and granted that right solely to the creator in order to give the artist the ability to financially support his artistic output.   To place a copyleft licence on the work and then complain that you can't monetize it because people are copying it for free is an... interesting complaint. If you read the whole thing it's with a special dev flag."
technical,"PolymorphBlue indeed. Sometimes I also get comments like that:  If you don't do this I'll uninstall it Alright  I don't know what's the expected response. If you want more money just make the product paid. Or just shut it down, but ads in open-source project Jesus, that's just disgusting."
technical,"I do love OSS and all the amazing open source projects available  and I do very much understand that funding is required now for many projects and services such as Open Collective and/or GitHub Sponsors either don't work for their needs or don't meet their funding needs. However, my fear is that this experiment will result in many more OSS projects picking it up and lead to a future where CI build logs are full of banner ads and then potentially lead to other worse advertising means. The user u/crabbytag put it best for me and my concerns for what this could start in the future.  The maintainer is getting 2000 for these banners because no one else is displaying ads there. Once other library authors notice this opportunity, they'll start adding ads too. Then the average payout comes down. But since we've already accepted ads here, some authors will include more annoying ads for slightly more money. For example, 2x the payout if the developer is required to take some action ('press enter to unpause the build) and 3x if the action is more annoying ('type out ""Linode rocks"" to unpause the build).  That's just me of course but I feel like this is not the correct direction to go towards in helping to fund open source projects and we need to continue looking for another alternative. immediate uninstall if any of my OSS advertises in my terminal"
technical,"yarn remove standard In a very realistic sense, does it matter if someone using it for free (and highly more than likely contributing nothing back to it) uninstalls it?"
technical,"How can you be so smart and stupid at the same time. This idea is horrible. In fairness, this appears to not actually be very targeted - it's just a postinstall script in the funding package."
technical,"Try using that eraser on your comments. No, I'm standing up for fellow FOSS developers getting compensated for the work they do if they wish to do so and against the weird pattern of thought that random dude from random place is obligated to not to do anything you don't expect. Input not recognized. Type 'I want to save' to continue debating standard"
technical,"Nah, I called you entitled and you used ""fuck"" as every second word, safe to say you're the one triggered.  I'm sorry maybe you have a condition I don't know of yet, did the ASCII letters hurt your eyes? is your time free?  Have a 'free' beer but you have to extol the virtues of Linode(tm) first."
technical,"Nah, I called you entitled and you used ""fuck"" as every second word, safe to say you're the one triggered.  I'm sorry maybe you have a condition I don't know of yet, did the ASCII letters hurt your eyes? It is still a misleading name, no matter how it is spinned by the developers."
technical,"How the hell would that work in the NPM ecosystem? This package for example seems to be heavily relying on ESLint. Would the developer be able to guarantee an SLA on bug and security fixes that goes down to ESLint? How about to file-entry-cache, which is a requirement for ESLint? Or to flat-cache, which is a dependency for file-entry-cache? And to flatted, which flat-cache requires?  What about profit sharing? Installing this one package will generate a lot of downloads for its dependencies. Are they guaranteed a share of the pie? Or do you need to pay for every dependency on its own? Do the dependency authors also need to sign up to guarantee SLA? Does this also apply to the dependencies? Because sure, the source code for the dependencies are visible, but are you, as a customer, expected to generate the dependency tree and inspect the source code for every part of the package.  Or is the idea that if you sign onto an NPM SLA contract-thingy, you are not allowed to ship any kind of dependencies at all? I mean, that sounds great since the NPM dependency hell is a nightmare, but I'm gonna be shocked if your average developer will sign onto that. Will standard drop its dependence on ESLint and the work of its authors? I just don't know. it only takes a second. You're not respecting my right to monetize my comments. Please type 'I want to save' to continue"
technical,"How the hell would that work in the NPM ecosystem? This package for example seems to be heavily relying on ESLint. Would the developer be able to guarantee an SLA on bug and security fixes that goes down to ESLint? How about to file-entry-cache, which is a requirement for ESLint? Or to flat-cache, which is a dependency for file-entry-cache? And to flatted, which flat-cache requires?  What about profit sharing? Installing this one package will generate a lot of downloads for its dependencies. Are they guaranteed a share of the pie? Or do you need to pay for every dependency on its own? Do the dependency authors also need to sign up to guarantee SLA? Does this also apply to the dependencies? Because sure, the source code for the dependencies are visible, but are you, as a customer, expected to generate the dependency tree and inspect the source code for every part of the package.  Or is the idea that if you sign onto an NPM SLA contract-thingy, you are not allowed to ship any kind of dependencies at all? I mean, that sounds great since the NPM dependency hell is a nightmare, but I'm gonna be shocked if your average developer will sign onto that. Will standard drop its dependence on ESLint and the work of its authors? I just don't know. It seems like a lot of your frustration comes from this part: So how many projects does standard rely on?  So, uh, how much of the 2000 you got is going to those 125 other contributors? For those interested, here's an npm ls on a brand new project with only this package installed"
technical,"This is an unbelievably poor decision and I will spend the time to remove your package from anything I write, or just fork it off and snip the advertising code.  The terminal and the build log is not where ads belong. Put it in a ReadMe file. It's only directly caused by you, the user, not contributing enough. It's absolutely entitled to cuss out developers that have let you know they want to be supported. This might be yet the best place where to use the surprised Pikachu meme."
technical,"Advertising is a terrible funding model.  Even though advertisers (or ""sponsors"", which amounts to the same thing) will often try to pretend otherwise, running advertising means that you are dependent on not doing anything they dislike, at the risk of suddenly having your funding yanked - after all, their name is associated with it. It produces a chilling effect.  I don't think it's a good idea *at all* to introduce that sort of problem to the open-source ecosystem any more than it already has, if anything, there should be *less* advertising. Must be trolling, because there's no way you could be so oblivious that you would think this would fly.  And...Linode?  I hope they didn't know they would be spamming developers' terminals.  Shame."
technical,"This comment was marked as off-topic. My only relevant thought - if this is the new status quo, well I sincerely look forward to the release of some form of ad-blocking add-on for npm."
technical,"If a company is advertising in my terminal, even if they're technically sponsoring a project as means to do so, I will do everything I can to avoid that company. Whether or not they know it, I'm pretty sure this is the exact opposite of the sort of publicity these sponsors would want among developers. My terminal is the one last stronghold, the one last haven of peace that doesn't endlessly serve me ads from corporate overlords all day long. I vehemently oppose this idea as I believe it is fundamentally opposed to the open source ethos we've built up over decades.  Yes, it's important to help out open source contributors and project owners - but ads are not the solution."
technical,"AlexanderJohnston me as well. I just let linode know why I'm cancelling, and let logrocket know they,re no longer being considered. Nah, I called you entitled and you used ""fuck"" as every second word, safe to say you're the one triggered.  I'm sorry maybe you have a condition I don't know of yet, did the ASCII letters hurt your eyes?"
technical,"Hi, I would favor advertisements for services that you would be willing to offer, either personally or from a company that you own and run. Nobody is denying that money needs to flow to OSS maintainers. I don't mind putting up with ads  anywhere else  than in the logs of my build. There is plenty of space in things like README's and landing pages for links to sponsors and patreon(-like) solutions. Logs for me are a way of doing my job. I will (try to) weed out anything that negatively impacts my ability to do work.  Also, consider the effect that letting people know that you've got sponsoring can diminish the amount of support you get from your peers, as Matthias Wandel points out in one of his videos: interesting read!  you mean you prefer ads for services from feross over 3rd party ads? I agree, I probably wouldn't make this much a fuss if the advertisement was a single line with a link to buy them a cup of coffee or a side-project."
technical,Prettier is better and doesn't push a non-standard format while calling it standard. Anyone complaining can just use that. Problem solved. not quite. Pandora's box is already opened. Just using something else isn't enough.
technical,"Ask for PayPal/patreon donations in a single line, sure, but what the hell is this? A whole box in your terminal dedicated to promoting other companies? No thanks.  NPM should ban this type of promotion. If this becomes a trend, you're likely going to get all types of promotion for donation channels banned from package managers like this.  YOU choose to spend your time on this project. Stop taking money for something that should be free and open. NPM does. Haven't found an enforcement mechanism."
technical,"This message is brought to you by linode. Scale your way with our patented nodes.   A sociopath is a term used to describe someone who has antisocial personality disorder (ASPD). People with ASPD can't understand others' feelings. They'll often break rules or make impulsive decisions without feeling guilty for the harm they cause.   Drink Coca-Cola. Refreshing. npm, yarn, NuGet, etc. all need a pay-to-play model.  Curated feeds where the devs sign contracts guaranteeing an SLA on bug and security fixes would be the driver for getting you packages through the pay-per-download system.  Each download to a new machine become a sale to that package author.  The repos escrow the money and the dev withdraw in local or crypto currency.  when devs release a new  major  version, downloaders have to pay to upgrade to it from previously purchased versions.  Each purchase would be a 1 dev on 1 machine license.  number of projects used with would be irrelevant.  Build servers would require a license, same as developers.  Because open source is what it is, refunds would not be allowed because the code is right there to preview before deciding to buy.  This is fair.  It pays people at the earliest point possible.  It gives customers 100% access to the code.  What has to change is licensing. * code is freely available for non commercial use * commercial use requires development from the signed packages of the curated repositories, not git pulls.  SLA on bug and security fixes do not apply to modified codebases. * Bugs found subject to the SLA must be remediated with n days of the bug being verified. * Security bugs found must be remediated within n days of acknowledged by the repo.  Should the developer be unable to fix the bug, then responsibility will fall to the repository to provide the fix to the code. * Excessive absence or inability of the maintainer to honor their obligations will result in the removal of all the their packages from the curated repository.  This will be a major shock to the way it's don e now.  5 line npm packages will and should go away.  Software development will not lose any of the collaborative features of open source and will gain accountability and credibility.  The best part is that advertisers are completely cut out of this loop and their influence cannot take over development culture."
technical,"Has you heard about this great new company LogRocket? They allow you to get your logs as high as they need to be.  Think smarter. Think Logrocket.  * Please don't complain as you haven't paid for githubs server time. O boy, if ""putting ads in a free service"" is a questionable practice for you then I suggest you go look around online :) And FYI, not a shill, there's just bigger issues here than console.log() advertising. I didn't see this outrage when another dev asked for a job like this. It's interesting."
technical,I'm clearly not the only person with a problem here. Nice try deflecting.  You got triggered over a 'cuss' for fuck's sake. Kettle meet pot? Oh-My-Zsh does that (with .zshrc though :D). I don't see the issue because I get a free service from them.
technical,"The experiment is successful: you learned something.  I hope it is that this: 1. annoys people, 2. won't scale (if many packages do it, few will see yours, lowering what you can charge), 3. is easily blockable/forkable (lowering what you can charge).  The last two reasons should make it clear why  this is not a sustainable revenue stream  for you, regardless of how we all actually feel about it.  The economics of it won't work. Okay, so imagine something like the node UUID package starts doing this. Now whenever you install the AWS SDK (and a hundred other packages) you see ads for some random company that sponsors node-uuid. Can you imagine?  AWS would just remove the dependency. No large company wants to propagate another companies ads (that they have no control over the quality or nature of), for free. This would have a massive chilling effect on OSS"
technical,"I appreciate the thoughtful discussion and feedback. I just ended the experiment. I shared some thoughts about how the experiment went from my perspective on my blog: On August 28, 2019, I released standard 14.1.0 which removed the funding dependency. See changelog here"
technical,"I encourage everyone to report this package to npm as its breaking their rules and harmful for npm ecosystem and its atacking users computer.  Whats the difference between a virus which opens a website and this package? Since npm does not have a ad policy This package has to be removed from npm  Please report One of the major issues with NPM that this funding package has shed light on, is the inability to wholesale block an NPM package from being installed, even though nested dependencies."
technical,"Agree with. I want to add that there are a lot of misconceptions here about what it means to maintain a (hugely popular) OSS project and the effort involved. I just became a sponsor just for feross having to read this thread. People disagreeing with you aren't a mob, they're real people that just so happen to have a different opinion than you on the matter. There may be one or two people acting inappropriately, as I have seen myself on both sides, but for the majority people are just sharing their views.  I don't think anyone, either for or against this experiment, should not have their opinion heard. That very much too goes for people who support the proposal and I hope everyone can share their feedback."
technical,"The first step to the Tragedy of the Commons has thus started. Every other popular package will copy this bright idea, npm and yarn will realize that spamming dozens of pages of sponsorship or donation request banners is a bad user experience, and eventually *block* all install script output from the CLI.  You at least got in on the ground floor before it was ruined for everyone. Point being? Are you using a 300 baud terminal that it actually took time to scroll past the ad you got in exchange for getting to use the library that took hours and hours to write?"
technical,"In a very realistic sense, does it matter if someone using it for free (and highly more than likely contributing nothing back to it) uninstalls it? PolymorphBlue indeed. Sometimes I also get comments like that:  If you don't do this I'll uninstall it Alright  I don't know what's the expected response."
technical,we could use the blockchain (tm)  Somewhat unironically actually this is one of the few cases where ledger holds true. Prettier is better and doesn't push a non-standard format while calling it standard. Anyone complaining can just use that. Problem solved.
technical,"we could use the blockchain (tm)  Somewhat unironically actually this is one of the few cases where ledger holds true. Pretty unfair dig at Google, who not only bankroll a lot of open source projects (Android, Chromium, Kubernetes, etc.), but also have programs like Google Summer of Code and Google Code-in, and donate to organisations like MetaBrainz, which hosts MusicBrainz and other services."
technical,"No, not all humans have this reaction.  The reaction you are projecting on all humans infers that all humans are greedy, inconsiderate thieves. Probably hard to say since that'd require all of them to have their financials disclosed. I had a random look at some projects to see what they disclose and libjpeg-turbo sponsors page has Google's logo on it and says that it requires at least a donation of at least 1000 to get their logo there. And some of the projects there are actually Google projects, so the people working on those were probably paid by Google."
technical,"Heh, I'm not picking fights, only saying where people are being stupid. Cussing at devs who wrote code for you is stupid. Putting ads in install scripts is sociopathic. The word 'fuck' doesn't make me stupid and it's a logical fallacy to hold it against me."
technical,"FOSS is not, advertising in tooling is absolutely detrimental. Please don't conflate the two.   Expecting my terminal to remain ad-free is not entitled. Its entitled to think it's reasonable to jam ads down peoples throats.  To be clear, I don't have unreasonable expectations of what people should do for free.  But this isn't free. Theres a catch. That catch is the problem. Edit: typo. Running ads in a terminal makes no sense at all. Good way to get people to stop using the software.  I understand ad revenue is important but, bluntly: if you want me to respect your choices as a maintainer, stay the hell out of my computer. Stick to established advertisement channels such as the Web itself, where advertising makes sense because everything rendered originates from an external provider.  Don't make me choose between your library, and an undisturbed workflow within possibly the most important program I run on my local system, because you will lose that battle."
technical,The money you paid for this service has been refunded. Kthxbye! Selling out developer tooling. Now that one asshole has done it more will follow unless this is stomped out immediately.
technical,The money you paid for this service has been refunded. Kthxbye! Silence opposition. That usually works.  Sadly.  Are you sick of reasonable regimes? Move to the DEMOCRATIC PEOPLES REPUBLIC OF NORTH KOREA. Call now for details.
technical,"Probably hard to say since that'd require all of them to have their financials disclosed. I had a random look at some projects to see what they disclose and libjpeg-turbo sponsors page has Google's logo on it and says that it requires at least a donation of at least 1000 to get their logo there. And some of the projects there are actually Google projects, so the people working on those were probably paid by Google. Since funding is one of the dependencies, it's in the list."
technical,"Probably hard to say since that'd require all of them to have their financials disclosed. I had a random look at some projects to see what they disclose and libjpeg-turbo sponsors page has Google's logo on it and says that it requires at least a donation of at least 1000 to get their logo there. And some of the projects there are actually Google projects, so the people working on those were probably paid by Google. So did they release an npm-ad-blocker yet? If not it should surely be out within the next 5-10 mins.  Then we can get to work on the PR that blocks the ad blocker."
technical,"I think I'm actually OK with this, assuming the following conditions:  1. Adverts only appear in ""dev mode"" 2. They can be disabled 3. They aren't too frequent 4. They are & remain relevant to the developer  Basically use this as an example :point right: So I did a bit of math. 2,000 over 5 days? Let's assume you worked for 24 hours a day, for 5 days straight. That's 17 an hour. Almost 10 more than the federal minimum wage. And what exactly changed between v13.1.0 and v14.0.0? Well GitHub thankfully lets us see! Not a lot! It seems to just be a straight conversion from ES5 to ES6, alongside some markdown changes and general admin stuff every project has to do, and barely any are paid for.  Beyond this, how big is standard anyway? About 10-11kb of actual code across 6 files and 2 repos, and 450 lines, excluding tests. The rest is pretty much purely ESLint. This repo especially is approximately 600 **bytes** of actual code. If you do some more math, that 2000 equates to approx 5 per line of real code.  Look, I get it, I really do. It's not always fun to see your work used with no kickback for yourself, but this is (as everyone else has said) what you sign up for when you make open source software, and considering how honestly tiny this package is for what it is, and how much it depends heavily on other people's work, i.e. ESLint, it just comes off as really out of touch.  Edit: forgot the real kicker"
technical,"I'm sorry that this is the kind of response you're receiving. Please know that you have the silent support of many who aren't willing to engage with this kind of mob. A message in an install log is a minor annoyance at worst, not the big deal many are making it out to be.  It is sad that so many with such strong opinions have no contributions in the last year outside of this issue. If you aren't willing to contribute your code or your ideas to open source and aren't willing to contribute to projects monetarily, you should consider tempering your views. Something I wanted to put here from a comment feross wrote in the main funding repo:  It makes the whole thing seem a *lot* more reasonable."
technical,"There are number of other approaches for OSS funding.  They can be read at wikipedia here but I'll list some of them: - **Dual-licensing** - **Selling professional services** (You already have this one) - **Partnership with funding organizations** - **Pre-order/crowdfunding/reverse-bounty model** - **Selling of optional proprietary extensions** - **Selling of required proprietary parts of a software product**  Above mentioned article also states **Advertising-supported software** as one of the approaches, but as user mhogerheijde wrote ""The terminal is the last place I want ads"".  Maybe create some business model for **WebTorrent** instead of giving it away for free (for example, Sublime Text has message appear after every  n  save of files. You could put your advertising message there perhaps after every n downloads :thinking: )  **Conclusion**:  I'm totally with you to get paid by maintaining all these open source packages ( 100+ packages on npm which are downloaded 100+ million times per month  ), but not through my terminal :nerd face: Stomping on something that is detrimental to an entire industry and a net negative.  It's not free. It's less than free. It is a liability.  It is a dangerous precedent."
technical,"So I did a bit of math. 2,000 over 5 days? Let's assume you worked for 24 hours a day, for 5 days straight. That's 17 an hour. Almost 10 more than the federal minimum wage. And what exactly changed between v13.1.0 and v14.0.0? Well GitHub thankfully lets us see! Not a lot! It seems to just be a straight conversion from ES5 to ES6, alongside some markdown changes and general admin stuff every project has to do, and barely any are paid for.  Beyond this, how big is standard anyway? About 10-11kb of actual code across 6 files and 2 repos, and 450 lines, excluding tests. The rest is pretty much purely ESLint. This repo especially is approximately 600 **bytes** of actual code. If you do some more math, that 2000 equates to approx 5 per line of real code.  Look, I get it, I really do. It's not always fun to see your work used with no kickback for yourself, but this is (as everyone else has said) what you sign up for when you make open source software, and considering how honestly tiny this package is for what it is, and how much it depends heavily on other people's work, i.e. ESLint, it just comes off as really out of touch.  Edit: forgot the real kicker Such a thing exists. It's called Tidelift and standard is already a member. It costs at least 1500/month."
technical,"My two cents:  It's your package and I don't really care what you do in post install scripts since afterall it's open source code and I'm not entitled to anything (thanks for creating standardjs). But I will say that it pollutes install logs and I hope that there is a more reasonable way of asking for funding since this practice will encourage many other projects to do the same and it would be unfortunate for our terminal output logs to look like nascar advertisements after everyone starts doing the same thing, which starts looking like the tragedy of the commons. sustained   And ideally ESLint itself, as this is basically configuration and a wrapper for that."
technical,"Fact of the matter is, OSS maintainers need money  today . Better solutions may come along, putting up with ads in the mean time is a small price to pay. While I don't particularly like seeing ads in this space, I understand its necessity and fully support it.  I do agree that it could be more clear why the ads there, and specifically that it's making the tools you love maintainable. That reminds me of the article. Basically, our hated ads-driven business model of internet (google, facebook, ...) was forced to original publishers by people.... (posting it as a slight OT because when people dont know history, they tend to make same mistakes and desicisons)"
technical,"Will you be sharing revenue with  * boxen * chalk * ci-info * term-size * word-wrap  As these are all dependencies of funding and it couldn't exist without their free labor. That's actually an interesting point. How does standard fit with NPM's terms of service? Because for example, Acceptable Content policy prohibits adware. It could be argued that this project now counts as adware. Has the advertising used in this project been cleared with NPM?  There's also this.  As far as I know, there are a lot of different laws regarding advertising around the world. Have the legal implications of this kind of advertising been considered?"
technical,"Not letting your inner two year old rule you does not mean he is not present.  His complaints are predictable, and those who are voicing complaints from him are not needed. The existence of this issue infers the obvious must be stated."
technical,"If you want more money just make the product paid. Or just shut it down, but ads in open-source project Jesus, that's just disgusting. The experiment is successful: you learned something.  I hope it is that this: 1. annoys people, 2. won't scale (if many packages do it, few will see yours, lowering what you can charge), 3. is easily blockable/forkable (lowering what you can charge).  The last two reasons should make it clear why  this is not a sustainable revenue stream  for you, regardless of how we all actually feel about it.  The economics of it won't work."
technical,"is your time free?  Have a 'free' beer but you have to extol the virtues of Linode(tm) first. The first step to the Tragedy of the Commons has thus started. Every other popular package will copy this bright idea, npm and yarn will realize that spamming dozens of pages of sponsorship or donation request banners is a bad user experience, and eventually *block* all install script output from the CLI.  You at least got in on the ground floor before it was ruined for everyone."
technical,"That's actually an interesting point. How does standard fit with NPM's terms of service? Because for example, Acceptable Content policy prohibits adware. It could be argued that this project now counts as adware. Has the advertising used in this project been cleared with NPM?  There's also this.  As far as I know, there are a lot of different laws regarding advertising around the world. Have the legal implications of this kind of advertising been considered? The funding package appears to only be changed if you pay the maimtainers.  That seems problematic in a FOSS context.  It's clear a majority would support this PR"
technical,Well here's a shining example of everything wrong with the software industry.  I will make it a point to boycott any companies advertising in a terminal. In any way. Period. And encourage others to do so.  Fucking sellout. The money you paid for this service has been refunded. Kthxbye!
technical,"-1 for that kind of non-permissive licensing, it does not even seem to fit the definition of ""open source"" ref:  +1 for offer of services from the primary author, which is sometimes needed to help with open-source sustainability   -1 for promotion of third-party products or services (noisy, distracting, etc.) The package maintainers are welcome to do as they see fit. However, I would like a way to opt-out entirely from this package ever being install into my system. As far as I can tell, that is not possible with NPM."
technical,"I mean, yes? Literally sometimes. There are valid reasons for that.  I shouldn't have to scroll past things that shouldn't be there. The sponsors used in Caddy's incredibly misguided response header spam had no idea either (their permission wasn't sought), and ceased their involvement with the project after it was brought to their attention - not by myself, I should add."
technical,"evandentremont ""Stomping"" on someone providing you a free service is definitely not going to net you the results you expect.   I think this is a good case right now to demonstrate what happens if you don't support what you use. If you don't like these practices you have two very very simple options: a) Maintain your own code with your own money and time b) Pay for someone to do that for you or don't come here insulting the developers who do this out of their goodwill  Or settle with the option c) that you can see right now and are unreasonably mad at. There are number of other approaches for OSS funding.  They can be read at wikipedia here but I'll list some of them: - **Dual-licensing** - **Selling professional services** (You already have this one) - **Partnership with funding organizations** - **Pre-order/crowdfunding/reverse-bounty model** - **Selling of optional proprietary extensions** - **Selling of required proprietary parts of a software product**  Above mentioned article also states **Advertising-supported software** as one of the approaches, but as user mhogerheijde wrote ""The terminal is the last place I want ads"".  Maybe create some business model for **WebTorrent** instead of giving it away for free (for example, Sublime Text has message appear after every  n  save of files. You could put your advertising message there perhaps after every n downloads :thinking: )  **Conclusion**:  I'm totally with you to get paid by maintaining all these open source packages ( 100+ packages on npm which are downloaded 100+ million times per month  ), but not through my terminal :nerd face:"
technical,Input not recognized. Type 'I want to save' to continue debating standard There's a sentence that I never thought I would ever see in my life.
technical,"The sponsors used in Caddy's incredibly misguided response header spam had no idea either (their permission wasn't sought), and ceased their involvement with the project after it was brought to their attention - not by myself, I should add. they both do now. I asked both to disown this shit."
technical,I love how ThisIsADogHello's comment was auto-flagged as spam. Ironic. This comment was marked as off-topic.
technical,"The package maintainers are welcome to do as they see fit. However, I would like a way to opt-out entirely from this package ever being install into my system. As far as I can tell, that is not possible with NPM. This does 2 things, 1 is objectively bad and one is subjective.  1. Encourages users to run with --silent as an adblocker - Bad because it means users won't see critical errors, security warnings etc - If we can agree it's bad to encourage users to ignore key warnings, we inherently agree that polluting the output of install is bad  for the exact same reason . 2. It's the most annoying thing you could possibly do as a package maintainer other than remove your package.  Regardless, NPM will undoubtedly decide to block any method of serving advertisements through the terminal. It doesn't matter what Standard chooses to do."
technical,"We are going to need adblockers for npm install output now...  Now I'm actually wondering how that might work... maybe just blocking specific packages' postinstall scripts would do it. For now. It's not like there's some easy way to detect sponsored messages. This is a very bad idea. It violates the user's right to privacy and choice.  I even go farther and say that it is abuse by upstream developers (does not matter WHO these developers are, feross could be replaced by any other developer who wants to abuse users).  If the goal is to seek financing, it would be better to request donations etc...  Note that some users may be ok with ads, but it is NOT ok to NOT ask users whether they are fine with that. Because many are NOT fine with targeted ads."
technical,"we don't use terminal for entertainment or surfing web. We use it for WORK. Its like showing adds to a pilot in a mid of a flight. There should be no adds in terminal. Period. This is absolutely disgusting behavior, you should be ashamed of yourself."
technical,"Running ads in a terminal makes no sense at all. Good way to get people to stop using the software.  I understand ad revenue is important but, bluntly: if you want me to respect your choices as a maintainer, stay the hell out of my computer. Stick to established advertisement channels such as the Web itself, where advertising makes sense because everything rendered originates from an external provider.  Don't make me choose between your library, and an undisturbed workflow within possibly the most important program I run on my local system, because you will lose that battle. This is an unbelievably poor decision and I will spend the time to remove your package from anything I write, or just fork it off and snip the advertising code.  The terminal and the build log is not where ads belong. Put it in a ReadMe file."
technical,"Great point that it could be more clear why the ads there, and specifically that it's making the tools you love maintainable.  I think if you're not into it, then the obligation is in you to put some work in to come up with more aligned solutions.  Enthusiastic verbal support does not help maintainers thrive.  At the moment many of them are generously giving and experience a few breadcrumbs of thanks in return.  I want to live in a world where maintainers have enough to pay rent, eat food, have health care, save money, dream about big future projects. This isn't about having our cake and eating it too. Selling ad-space is not innovative. And it's particularly unhelpful in my logs.  I don't fully agree with ckipp01 on the sponsorship driven OSS. It is a risk, but it already exists outside of selling ad-space in logs.  For me, the issue is more that I don't want stuff that doesn't help me in my logs. I wholeheartedly agree with putting your ""supported by company X"" in the readme. That helps me understand, it does resonate with me when I see certain companies donating money to OSS.  By the way, just stating that ""if you don't like it, come up with a better solution"" is a cop-out. There  is  value in feedback,  especially  when you don't agree.  EDIT: PS: I too want to live in a perfect world where every developer can live, pay rent and only work on projects they like. That perfect world for me does not include ads in my terminal.  EDIT2: PPS: Support of my peers for me is a  big  reason to work on stuff. I know others that earn enough in their day-job that they enthusiastically spend time on OSS in their free time as a hobby and get value out of verbal support from their peers. That support is more often than not shown in a verbal/written way."
technical,"O boy, if ""putting ads in a free service"" is a questionable practice for you then I suggest you go look around online :) And FYI, not a shill, there's just bigger issues here than console.log() advertising. I didn't see this outrage when another dev asked for a job like this. It's interesting. This message is brought to you by linode. Scale your way with our patented nodes.   A sociopath is a term used to describe someone who has antisocial personality disorder (ASPD). People with ASPD can't understand others' feelings. They'll often break rules or make impulsive decisions without feeling guilty for the harm they cause.   Drink Coca-Cola. Refreshing."
technical,"I'd like you to be the first to know Macys(R) is having a store wide sale THIS WEEK ONLY  Type 'I want to save' to continue. Try using that eraser on your comments. No, I'm standing up for fellow FOSS developers getting compensated for the work they do if they wish to do so and against the weird pattern of thought that random dude from random place is obligated to not to do anything you don't expect."
technical,"Given that this package is essentially a config file and thin wrapper script for ESLint, I am curious how much of this revenue will be shared with the upstream ESLint developers. We are going to need adblockers for npm install output now...  Now I'm actually wondering how that might work... maybe just blocking specific packages' postinstall scripts would do it. For now. It's not like there's some easy way to detect sponsored messages."
technical,it only takes a second. You're not respecting my right to monetize my comments. Please type 'I want to save' to continue we could use the blockchain (tm)  Somewhat unironically actually this is one of the few cases where ledger holds true.
technical, I don't even look at the output of npm i unless it fails. then what's the point of this?  ctrl+f readme we don't use terminal for entertainment or surfing web. We use it for WORK. Its like showing adds to a pilot in a mid of a flight. There should be no adds in terminal. Period.
technical,not quite. Pandora's box is already opened. Just using something else isn't enough. we have not recieved your input. Sorry. This debate is only avaliable to those who contribute their time to unobtrusive Macys(R) Advertisements.  Please type 'I want to save'
technical,"Okay, so imagine something like the node UUID package starts doing this. Now whenever you install the AWS SDK (and a hundred other packages) you see ads for some random company that sponsors node-uuid. Can you imagine?  AWS would just remove the dependency. No large company wants to propagate another companies ads (that they have no control over the quality or nature of), for free. This would have a massive chilling effect on OSS Well here's a shining example of everything wrong with the software industry.  I will make it a point to boycott any companies advertising in a terminal. In any way. Period. And encourage others to do so.  Fucking sellout."
technical,Do you mind refraining from ad-hominem attacks? Calling people triggered whining two-year olds doesn't contribute to this discussion constructively. what compensation exactly are open source authors entitled to?
technical,"Hmmm... I'm kind of keen to see where this goes. I'm not sure how I feel about it yet. I'm not necessarily happy with it, but at the same time, OSS has really struggled to find a way to fund itself. Relying heavily on donations which rarely materialise and most often, the financial generosity of the contributors. Companies never pay for it, so the contributors have two jobs. One day job and a night job creating and maintaining OSS. The latter is as hard as the former, but OSS authors are not paid for it. Often this means it becomes abandonware, which is something companies themselves worry about. Backing the wrong horse can be an expensive mistake. Especially if they're subject to SLA's they must control.  Also, as a side issue, a lot of GH abandonware is due to devs trying to find a job and using it for learning or portfolio (which is cool. Many good things have come out of it). But once they have found work, contributions disappear as fast as their spare time.  Patreon doesn't seem to be effective in generating enough revenue and isn't seen as the right platform. Plus, consultancy models with free software, needs devs to be (or to hire) sales specialist or local consultants. It's a pain and is costly.  Initial thoughts:  - it's competing in a space with other ad platforms and provided no insight to companies. Unlike the others. As much as I like that, it's a fact that marketers consider targeting crucial.  - irritant? I've not found it. Yet.  - if it becomes very successful, but only sends a wall of ads, it'll frustrate and folk may move away from Standard  - it's not clear what they intend to do if/when it gets lots of ads or if companies don't get anything out of ads.  - these ads are a way to give companies something for contributions. The question is whether and what size company MUST pay?  Keeping an open mind though What if the package owner does not want to sign onto an SLA contract? Like, say that the author of flat-cache does not want to be obligated to provide updates within 72 hours of reporting. Does that mean that package owner is not paid for his work when it's included? Or that it cannot be included at all. If you install a package, but only 20% of its dependency tree is under the SLA, can you really have confidence in the SLA you are paying for?  Also, this sounds real expensive in the NPM world. If we are talking about a 5 flat fee for each license per developer, standard costs 199 x 5 per developer. That is to say 995 per developer and CI server. I think 995 per developer and CI server is quite a lot for a linter. A lot of people don't even pay that much for nginx, which I'd argue is a pretty important project in the world.  And if you let authors decide what the payment is then it's really whatever. I think it might also incentivise developers to break down their packages into smaller packages and license them out separately as part of the dependency tree."
technical,"Friendly request for the crowds that are invariably going to roll into this thread now: please keep your comments civil, and don't start pile-ons.  Even if you disagree with this change (as do I, strongly), spamming memes and angry comments - as so often happens in threads that gain notoriety - is just going to get the thread locked. Let's keep things from spiralling down. which is ironic considering yarn was built by one of the most dystopian, ad-pushing, privacy invading companies on the planet.  this may surprise you, but not  everything  has to be completely and utterly infested with advertising... or maybe it does, I dunno. Linux is free, can someone open a PR to stuff the kernel with adverts too? There's nothing I love more when trying to debug a driver issue than having to drink a verification can for 5 minutes. TL,DR: ""iTs fREe"" isn't a good reason to stuff it with advertising"
technical,"I noticed the (fairly large bright bold) banner. It reminds me of the OpenCollective-style banners used by webpack / corejs. I think it's OK... I do worry that npm install will just become a long trail of banner ads though eventually and it won't scale. Because if  every  npm package adds ads, the noticeability of each ad will diminish. (Interestingly, the most valuable ""realestate"" will be packages whose banner is displayed  last , so if it becomes a literal ""race-to-the-bottom"" people might add sleep statements to their post-install scripts so they are displayed nearest the bottom. What a dystopian installation experience!)  Fun fact: yarn does not display the output of post-install scripts. One might say yarn has built-in ad-blocking. While I'm totally OK with this on development machines, I think this is strange behavior for staging/production/etc.  Perhaps thos should have a NODE ENV check?  Then again, I don't imagine standard is going to be installed outside of NODE ENV={test, development} anyway... so it's probably OK!"
technical,"Pretty unfair dig at Google, who not only bankroll a lot of open source projects (Android, Chromium, Kubernetes, etc.), but also have programs like Google Summer of Code and Google Code-in, and donate to organisations like MetaBrainz, which hosts MusicBrainz and other services. Why does a style guide need thousands of dollars to be maintained? Why does this author get paid but not all the upstream authors (ESLint, etc)? Why not just put a big ad in the readme like everyone else? If you need to be constantly paid thousands of dollars to maintain this library, maybe open-source isn't the right choice, and you should start selling it? Has the author asked NPM if this is allowed?  For readers using this library, if this annoys you I would remind you that all you need to replace it is ESLint. Also look into prettier for formatting. Both take only 2mins to setup, and are ad-free.  OSS should be a safe haven from stuff like this."
technical,"My only relevant thought - if this is the new status quo, well I sincerely look forward to the release of some form of ad-blocking add-on for npm. Why the fuck is everything 'Off topic'  It's very much not.  This message brought to you by Pepsi"
technical,"One of the major issues with NPM that this funding package has shed light on, is the inability to wholesale block an NPM package from being installed, even though nested dependencies. Will you be sharing revenue with  * boxen * chalk * ci-info * term-size * word-wrap  As these are all dependencies of funding and it couldn't exist without their free labor."
technical,You already have a sponsor button **and** get your work funded by other companies. No need to be a greedy bastard. yarn remove standard
technical,"So did they release an npm-ad-blocker yet? If not it should surely be out within the next 5-10 mins.  Then we can get to work on the PR that blocks the ad blocker. Yeah advertising as dependencies is kiNd of terrifying,  I'd rather my car didn't run unsigned ads."
technical,"Dude, if you can't maintain the lib just pass it to someone else or seek for sponsors on patreon /GitHub You already have a sponsor button **and** get your work funded by other companies. No need to be a greedy bastard."
technical,"If anyone wants this, fork lerna and do it yourself. This conversation is over. - Provide a general summary of the issue in the Title above -- Many libraries in the Angular ecosystem publish to NPM from a subdirectory that was cooked during the build process. The process is usually as followed: - build the library in a subdirectory (i.e dist directory) - copy package.json, license and readme to the dist directory - do some package.json cleanups in the dist directory (delete devDepedencies, move dependencies to peerDependencies, remove scripts...) - npm publish from the dist directory.  I understand that it is not a common technique for node.js package developers but i think it is common for web package developers and my libraries uses the same technique..  While embedding lerna into our shared packages monorepo - kaltura-ng I read a lot of issues in lerna and googled about this topic. I read carefully the conversation of issue #91 and even used the same subject with my issue. Unless I missed new issues that address this feature it was marked as 'wontfix' with a recommendation to use the 'package.json:files' array instead.  The reasons for using this approach instead of package.json:files array are: - Many known libraries in the Angular ecosystem does it (angular, ReactiveX) so they must have a reason. - There is a lot of hoo-ha/complexity with the way node.js resolve modules  for the **web projects** since during the bundling process you **must** refer to the same instance of the library. Unless the bundler (typescript, webpack etc...) provide a hack/workaround/solution to force the library to use its' own node modules, it will not work. Publishing from sub-directory works just because the dist/node modules not exists. - During development if the symlink is done against the root, when you import nested class which was not exported in the main index, you will need to refer to the dist as part of the path import { something } from 'my-package/dist/something. but once you publish from dist folder directly, you should somehow fix the pass by removing the dist during the tranpiling which is not a valid option. - The libraries being used as dependencies during development should be assigned as peerDependencies at runtime because you want the application to provide them.  so to recap, we cannot just publish the package with a dist folder, we need to publish from dist directly  There are some caveats that I could think about with my suggested approach: 1. The dist folder must exists with a package.json inplace before the lerna bootstrap process symlink the folders. 2. The build process should not delete the 'dist' folder, instead it should just clear its content otherwise the symlink of dependent libraries will be broken.  IMO those two caveats are manageable as: 1. we can use a preinstall script to create the folder and a simple package.json file (with at least 'id','name'). 2. the build scripts should clear the folder content instead of rm -rf the folder itself.  ## Expected Behavior - If you're describing a bug, tell us what should happen -- - If you're suggesting a change/improvement, tell us how it should work -- when bootstrapping/publishing a package the package.json is being queried for the following config: if this config exists, it will symlink to that folder during bootstrap command and will publish from that folder during  publish command  using the 'config' attribute allow using the same configuration both in lerna and in other node scripts.  I already modified the 'bootstrap' command in a fork esakal/lerna. I didn't create a PR yet because I'm missing the 'publish' command. I will be happy to continue my work if you are going to consider this feature.  You can see it in action in our repo"
technical,"Same issue here I ran into this problem, when developing its fine however I want to publish only contents under a sub directory. would love to get this feature, I am even ready to do a PR.  evocateur any thoughts? As far as I know this is the only way to accomplish ""flat"" package structure (without the /dist or /lib section in the import path) unless skipping src/dist directories entirely, which isn't practicable if your source requires transpilation.  Let's say I want to import only the tinyUtil module from the utils namespace inside my big-obese-package, what I would like it to look like is: javascript If the source is ts/jsx/esnext etc, the recommended way of distributing the package is to transpile it to a dist directory (""npmDistDirectory"" in esakal's proposal), include package.json and publish. This doesn't seem to be possible with lerna at this moment, which is a pity.  Instead I will need to transpile it to ./dist, reference it from the files prop in package.json and then accept the following import: javascript After spending precious time on your API design, naming convention etc, this is a bitter tradeoff."
technical,"This will be a major limitation for TypeScript and Angular developers, and in fact some people using lerna with typescript had to do their own publish, or patch the existing implementation.  I myself want to publish from a subdirectory which does contain a package.json for several reasons,  1) I don't want to transpile my TS in the src folder because I'll end up having a messy file system. .ts, .metadata.json, .js, .d.ts all next to each other, and than to clean after build will be a total mess. 2) I want to following Google's Angular Package. 3) Seperation of concerns, why should I have my dist files within the same folder of the src?  My folder structure  So in reality i can publish from a subfolder as I do have a package.json Author of several typescript modules here who has hit the exact same issue, all my modules publish from a dist folder. Wish I'd known this before starting to use lerna! Would love this feature to be implemented, but will probably have to switch to something else instead now."
technical,"This will be a major limitation for TypeScript and Angular developers, and in fact some people using lerna with typescript had to do their own publish, or patch the existing implementation.  I myself want to publish from a subdirectory which does contain a package.json for several reasons,  1) I don't want to transpile my TS in the src folder because I'll end up having a messy file system. .ts, .metadata.json, .js, .d.ts all next to each other, and than to clean after build will be a total mess. 2) I want to following Google's Angular Package. 3) Seperation of concerns, why should I have my dist files within the same folder of the src?  My folder structure  So in reality i can publish from a subfolder as I do have a package.json evocateur btw, I do not see this related to Typescript only but any package that requires a build process and normally they will put the final package inside specific folder.  Also, the command already allow you to specify a folder so it should be the matter of adding the config and use for run the npm publish with the configured path "
technical,"This will be a major limitation for TypeScript and Angular developers, and in fact some people using lerna with typescript had to do their own publish, or patch the existing implementation.  I myself want to publish from a subdirectory which does contain a package.json for several reasons,  1) I don't want to transpile my TS in the src folder because I'll end up having a messy file system. .ts, .metadata.json, .js, .d.ts all next to each other, and than to clean after build will be a total mess. 2) I want to following Google's Angular Package. 3) Seperation of concerns, why should I have my dist files within the same folder of the src?  My folder structure  So in reality i can publish from a subfolder as I do have a package.json evocateur: I will not go into argument whether x is better than y - most often I am wrong. But if we ignore my poor choice of lorem-ipsum name (we can call it something else, such as ""tiny-lodash""), I can't really see why lerna should hinder the author from using a nested structure within a package and at the same time provide ""semantic imports"".  I came here to evaluate lerna as  a tool for managing JavaScript projects with multiple packages , not as a tool to limit me on how to structure the internals of my packages (whether good or bad)."
technical,"This will be a major limitation for TypeScript and Angular developers, and in fact some people using lerna with typescript had to do their own publish, or patch the existing implementation.  I myself want to publish from a subdirectory which does contain a package.json for several reasons,  1) I don't want to transpile my TS in the src folder because I'll end up having a messy file system. .ts, .metadata.json, .js, .d.ts all next to each other, and than to clean after build will be a total mess. 2) I want to following Google's Angular Package. 3) Seperation of concerns, why should I have my dist files within the same folder of the src?  My folder structure  So in reality i can publish from a subfolder as I do have a package.json For those that are still trying to get ""flat-pack"" imports working: import Bar from 'foo/Bar'  I was able to solve it by leveraging the preversion, version, and postversion scripts that were added   For me I got it working by: - Disabling NPM publishing during the lerna publish command - lerna publish --skip-npm - Performing my linting and build during the preversion NPM script (or before) which would be built into a dist folder - Copy necessary files into the dist folder during the postversion NPM script and then call npm publish dist. This is handled in a gulp file but I've simplified below. - The package.json version has been bumped prior to the postversion script being called - postversion is only called if you do not include the --skip-git command to lerna publish - ""postversion"": ""cp package.json dist && npm publish dist""  I still think a config option for npmDistDirectory could be beneficial since it would allow a consumer to leverage the prepublish lifecycle-hook which seems a little more intuitive."
technical,"What do you mean by that? What is the npm ecosystem? Having one function package so you will end up with 100 dependencies when they can live all together in the same package? Before Tree-shaking exists, probably makes sense because you didn't want to export the whole thing but now the tooling is advance enough. Hi :wave: I think there might be a few different conversations going on at once here.  Expanding on what noherczeg mentioned, this is possible with the main and/or files options within package.json Let say your files were all under a dist folder. A consumer could retrieve the files in the following way:  There is no  technical  difference between the code above and which is what I was looking for. The only reason I was looking for this was to have a ""cleaner"" import structure."
technical," Hi,  I have a same problem in a project developed with a lot of Angular 4 libraries. If dist folder cant' be symlinked directly, the services don't be injected correctly.  I dislike this way proposed by angular (define a package in dist). But if customization of package dir can't be defined, Lerna can't be used with an angular project."
technical, I recently tried using Lerna for the first time and immediately ran into this issue. Any heavy Typescript development is going to get hamstrung by the omission of this NPM feature.
technical,"evocateur: I will not go into argument whether x is better than y - most often I am wrong. But if we ignore my poor choice of lorem-ipsum name (we can call it something else, such as ""tiny-lodash""), I can't really see why lerna should hinder the author from using a nested structure within a package and at the same time provide ""semantic imports"".  I came here to evaluate lerna as  a tool for managing JavaScript projects with multiple packages , not as a tool to limit me on how to structure the internals of my packages (whether good or bad). In my experience, as well as observation of community packages over many years, coupling consumption of a given export to the literal directory structure of a tarball is an extremely hostile anti-pattern. Especially nowadays with ES module exports and whatnot combined with tree-shaking module bundlers, there's really no fundamental  necessity  for false-basedir publishing.  Lerna is designed around the way npm works. Packages are published from the same directory as the package.json, and construct their tarball from metadata contained therein. Publishing from a different directory with a modified dependency tree is not idiomatic npm, and lerna will not support it."
technical,"Hi :wave: I think there might be a few different conversations going on at once here.  Expanding on what noherczeg mentioned, this is possible with the main and/or files options within package.json Let say your files were all under a dist folder. A consumer could retrieve the files in the following way:  There is no  technical  difference between the code above and which is what I was looking for. The only reason I was looking for this was to have a ""cleaner"" import structure. right now what I am trying to do is removing dist from the path indeed."
technical,"I recently tried using Lerna for the first time and immediately ran into this issue. Any heavy Typescript development is going to get hamstrung by the omission of this NPM feature. Same issue here I ran into this problem, when developing its fine however I want to publish only contents under a sub directory. would love to get this feature, I am even ready to do a PR.  evocateur any thoughts?"
technical,"For those that are still trying to get ""flat-pack"" imports working: import Bar from 'foo/Bar'  I was able to solve it by leveraging the preversion, version, and postversion scripts that were added   For me I got it working by: - Disabling NPM publishing during the lerna publish command - lerna publish --skip-npm - Performing my linting and build during the preversion NPM script (or before) which would be built into a dist folder - Copy necessary files into the dist folder during the postversion NPM script and then call npm publish dist. This is handled in a gulp file but I've simplified below. - The package.json version has been bumped prior to the postversion script being called - postversion is only called if you do not include the --skip-git command to lerna publish - ""postversion"": ""cp package.json dist && npm publish dist""  I still think a config option for npmDistDirectory could be beneficial since it would allow a consumer to leverage the prepublish lifecycle-hook which seems a little more intuitive. that's why angular repos not using lerna"
technical,"In my experience, as well as observation of community packages over many years, coupling consumption of a given export to the literal directory structure of a tarball is an extremely hostile anti-pattern. Especially nowadays with ES module exports and whatnot combined with tree-shaking module bundlers, there's really no fundamental  necessity  for false-basedir publishing.  Lerna is designed around the way npm works. Packages are published from the same directory as the package.json, and construct their tarball from metadata contained therein. Publishing from a different directory with a modified dependency tree is not idiomatic npm, and lerna will not support it. This will be a major limitation for TypeScript and Angular developers, and in fact some people using lerna with typescript had to do their own publish, or patch the existing implementation.  I myself want to publish from a subdirectory which does contain a package.json for several reasons,  1) I don't want to transpile my TS in the src folder because I'll end up having a messy file system. .ts, .metadata.json, .js, .d.ts all next to each other, and than to clean after build will be a total mess. 2) I want to following Google's Angular Package. 3) Seperation of concerns, why should I have my dist files within the same folder of the src?  My folder structure  So in reality i can publish from a subfolder as I do have a package.json"
technical,"Based on what you can say that? So, based on what you can assume that you are right saying that the package is too big? What will be your argument that I have to pay attention to re-exporting on the main file? Well, I wouldn't re-export anything if that is your case, the whole point of having files that behave as a module scope in NodeJS. Could you show me how to resolve this issue please? Also, I am trying to prevent to add libnam/lib/factory2 so it is a flat package. What do you mean by that? What is the npm ecosystem? Having one function package so you will end up with 100 dependencies when they can live all together in the same package? Before Tree-shaking exists, probably makes sense because you didn't want to export the whole thing but now the tooling is advance enough."
technical,"right now what I am trying to do is removing dist from the path indeed. Yes, I am aware of that but that but when you use a folder it will just copy the file inside the folder name so files: [""lib""] it will put the files inside lib Ideologically, yes you are right, there is not differences (maybe, who knows, this is based on the situation).  Technically,  There is a different indeed, in fact, I could have package/FactoryOne and package/lib/FactoryOne in the same project and have for whatever reason differences between each other.  But I am seeking the same you are seeking, have a clear importing structure."
technical,"Your packages are too big if that is the case. Embrace the patterns of the npm ecosystem. You can tell npm what folders/files to put inside a package via the files param. I never tried it for sub-directories, but if I got what you are aiming at this could be enough to solve the mentioned problem right?"
technical,"I have opened a tentative solution here to restrict it to a smaller subset of warnings and would appreciate any feedback. - Verify first that your issue is not already reported on GitHub - Also test if the latest release and devel branch are affected too - Complete *all* sections as described, this form is processed automatically  ##### SUMMARY - Explain the problem briefly below. Deprecation warning and regular (non-deprecation) warnings when using boolean vars.  ##### ISSUE TYPE - Bug Report  ##### COMPONENT NAME - Write the short name of the module, plugin, task or feature below, use your best guess if unsure -- core  ##### ANSIBLE VERSION - Paste verbatim output from ""ansible --version"" between quotes -- paste below ##### CONFIGURATION - Paste verbatim output from ""ansible-config dump --only-changed"" between quotes. ##### OS / ENVIRONMENT - Provide all relevant information below, e.g. target OS versions, network device firmware, etc. CentOS/RHEL 7, Fedora 29  ##### STEPS TO REPRODUCE - Describe exactly how to reproduce the problem, using a minimal test-case --  - Paste example playbooks or commands between quotes below. HINT: You can paste gist.github.com links for larger files  ##### EXPECTED RESULTS - Describe what you expected to happen when running the steps above.  ##### ACTUAL RESULTS - Describe what actually happened. If possible run with extra verbosity"
technical,"Opting into the new/future behavior  (i.e. setting conditional bare variables to false in ansible.cfg) is only a resonable solution for an end user.  For those that write and maintain roles consumed by others, these spurious warnings create a horrible choice: they will either need to continuously tell all their users to ignore the copious warnings/reconfigure their ansible.cfg file, or, alternatively, make silly changes to the role to make the spurious warnings go away.    Most role maintainers will simply give in, and add a "" | bool"" filter, even though using the | bool filter on a boolean is just silly.   This is really a sad situation, and I'm really surprised to see Ansible miss so badly.  Ansible's own examples cause this spurious warning, yet there have been no changes to the examples! After a discussion in our recent IRC meeting we would entertain a PR that tries to further clarify the warning, or restrict it to a smaller subset of conditions.  Accepting the PR would be dependent on the implications of the change and specific implementation, ensuring that we do not cause performance regressions or cause other problems."
technical,"Does it really make sense to show the warning even if the variable in question is undoubtedly a boolean? There is no difference in this case (as far as I understand this). It doesn't make sense to pipe a boolean variable through the |bool filter. Also, I think the warning itself is misleading. It says evaluating x as a bare variable, this behaviour will go away. This sounds like bare variables won't be supported at all in the future, but in reality they will just be interpreted differently in cases where the variable is of type String. Completely agree with him -- shouldn't this warning be only issued when a bare **string** variable is used in a conditional?  That would be deserving of a warning. However, this deprecation warning also seems to be issued when a bare **boolean** variable is being used in a conditional.  Let's look at an example included in ansible's own documentation. Why is this triggering the deprecation warning?   This warning will cause a tremendous amount of work for the ansible community... that is, tons of playbooks will need to be rewritten that have conditionals checking bare booleans.  (for example, adding the | bool filter to make the warning go away)  Wouldn't it make sense to only have the deprecation warning pop out when it's a naked string variable is being evaluated, and not have the warning for a naked boolean variable?"
technical,"It is opt-in, and if you opt-in, the warning will disappear.  Once 2.12 lands, the default will swap to False, the current default is True Does it really make sense to show the warning even if the variable in question is undoubtedly a boolean? There is no difference in this case (as far as I understand this). It doesn't make sense to pipe a boolean variable through the |bool filter. Also, I think the warning itself is misleading. It says evaluating x as a bare variable, this behaviour will go away. This sounds like bare variables won't be supported at all in the future, but in reality they will just be interpreted differently in cases where the variable is of type String."
technical,"This is a purposeful warning for this situation. The behavior that it is warning about, relates to how a single bare variable is handled in conditionals. We have a piece of logic, that can perform unexpected actions that we are deprecating in 2.12.  That logic would enable the following bad behavior. This task actually gets skipped, instead of run.  A string should be a truthy value, but is unwound in a way that makes it falsy. Instead of treating that as this, it instead get's treated as this. I assume my reproducer playbook will work identically as it does today (2.7) once the deprecation message is gone.  Can an option to be added to squash only this warning?  There is nothing wrong w/ the playbook I described above. The second warning in the ""complex"" task for certain seems like a recurrence of a bug that happened a few releases ago.  Edit: it would be most useful to only show the warning for cases where a string is converted implicitly to a bool."
technical,"After a discussion in our recent IRC meeting we would entertain a PR that tries to further clarify the warning, or restrict it to a smaller subset of conditions.  Accepting the PR would be dependent on the implications of the change and specific implementation, ensuring that we do not cause performance regressions or cause other problems. I have opened a tentative solution here to restrict it to a smaller subset of warnings and would appreciate any feedback."
technical,"So this feature is opt-in for now?  And if I opt-in, it will squash the warning? It is opt-in, and if you opt-in, the warning will disappear.  Once 2.12 lands, the default will swap to False, the current default is True"
technical,"Completely agree with him -- shouldn't this warning be only issued when a bare **string** variable is used in a conditional?  That would be deserving of a warning. However, this deprecation warning also seems to be issued when a bare **boolean** variable is being used in a conditional.  Let's look at an example included in ansible's own documentation. Why is this triggering the deprecation warning?   This warning will cause a tremendous amount of work for the ansible community... that is, tons of playbooks will need to be rewritten that have conditionals checking bare booleans.  (for example, adding the | bool filter to make the warning go away)  Wouldn't it make sense to only have the deprecation warning pop out when it's a naked string variable is being evaluated, and not have the warning for a naked boolean variable? It should also be mentioned that ansible-lint rule E602 explicitly encourages the use of bare-variable conditionals that trigger deprecation warnings now"
technical,"I assume my reproducer playbook will work identically as it does today (2.7) once the deprecation message is gone.  Can an option to be added to squash only this warning?  There is nothing wrong w/ the playbook I described above. The second warning in the ""complex"" task for certain seems like a recurrence of a bug that happened a few releases ago.  Edit: it would be most useful to only show the warning for cases where a string is converted implicitly to a bool. It should work the same.  You can try to silence it by setting CONDITIONAL BARE VARS to False. There are issues aside from bool conversion as well.  In the case of thing: ""foo"", it would be changed to this which would give an undefined var error, if foo did not exist, or if it existed, it could also give an unexpected outcome."
technical,"It should also be mentioned that ansible-lint rule E602 explicitly encourages the use of bare-variable conditionals that trigger deprecation warnings now Nothing is really being deprecated for those using true booleans, only a default changed.  I agree that it should not be triggered in this case. You can squash these warnings by opting in to the new behavior, which is what everyone wanted anyway from the start."
technical,"Nothing is really being deprecated for those using true booleans, only a default changed.  I agree that it should not be triggered in this case. You can squash these warnings by opting in to the new behavior, which is what everyone wanted anyway from the start. Opting into the new/future behavior  (i.e. setting conditional bare variables to false in ansible.cfg) is only a resonable solution for an end user.  For those that write and maintain roles consumed by others, these spurious warnings create a horrible choice: they will either need to continuously tell all their users to ignore the copious warnings/reconfigure their ansible.cfg file, or, alternatively, make silly changes to the role to make the spurious warnings go away.    Most role maintainers will simply give in, and add a "" | bool"" filter, even though using the | bool filter on a boolean is just silly.   This is really a sad situation, and I'm really surprised to see Ansible miss so badly.  Ansible's own examples cause this spurious warning, yet there have been no changes to the examples!"
technical," same here. (shown with silencing workaround, without this '|bool' each task in block shows that warning)   [DEPRECATION WARNING]: evaluating swap lv as a bare variable, this behaviour will go away and you might need to add |bool to the expression in the future. Also see CONDITIONAL BARE VARS configuration toggle.. This feature will be removed in version 2.12. Deprecation warnings can be disabled by setting deprecation warnings=False in ansible.cfg."
technical,"It should work the same.  You can try to silence it by setting CONDITIONAL BARE VARS to False. There are issues aside from bool conversion as well.  In the case of thing: ""foo"", it would be changed to this which would give an undefined var error, if foo did not exist, or if it existed, it could also give an unexpected outcome. So this feature is opt-in for now?  And if I opt-in, it will squash the warning?"
technical,"same here. (shown with silencing workaround, without this '|bool' each task in block shows that warning)   [DEPRECATION WARNING]: evaluating swap lv as a bare variable, this behaviour will go away and you might need to add |bool to the expression in the future. Also see CONDITIONAL BARE VARS configuration toggle.. This feature will be removed in version 2.12. Deprecation warnings can be disabled by setting deprecation warnings=False in ansible.cfg. This is a purposeful warning for this situation. The behavior that it is warning about, relates to how a single bare variable is handled in conditionals. We have a piece of logic, that can perform unexpected actions that we are deprecating in 2.12.  That logic would enable the following bad behavior. This task actually gets skipped, instead of run.  A string should be a truthy value, but is unwound in a way that makes it falsy. Instead of treating that as this, it instead get's treated as this."
technical,Putting this on the tsc agenda for discussion -1 on renaming the branch
technical,I am -1 on renaming/changing the branch. -1 on this.
technical,"not sure how I missed that it had already been done, thanks. **Is your feature request related to a problem? Please describe.** Lot of software developers are discussing on twitter to rename default branches for their projects from ""master"" to ""main"" or equivalent. The primary reason being master-slave an oppressive metaphor.  **Describe the solution you'd like** Node.js core follows the trend to change the industry standard, and renames default branch from ""master"" to ""main"" or similar  **Describe alternatives you've considered** Sticking with existing master branch name for the default  EDIT: Updated ""renaming master to main"" to ""renaming default branch name from 'master' to 'main'"""
technical,"Bear in mind that URLs linking to master will not redirect to the new default branch name (and for repos where it matters, which isn't this one, github-pages only works on the default branch when it's named master). It may be worth waiting for Github to fix these discrepancies before making the switch. (to be clear, i'm in favor of making the change, and ""main"" seems as good as anything else, but the disruption caused by Github's incomplete support for a non-master default branch are significant) +1 to main but I do want to see what lead GitHub takes in making this easier"
technical,When through an updated the internationalization ones that I'd opened issues on last time. Created issues to FYI/ask for concerns to rename master-main in the remaining internationalization related ones. Also opened issue in these.
technical,"I'd like to echo the sentiment from addaleax that changing the branch name need not be mutually exclusive from other changes which our project likely needs to do to be more welcoming / inclusive. We should always be examining our process and thinking about what we can do to improve and make small iterative changes towards where we want to be. The term master for the main tracking branch is something that has always bothered me. To push back against any narrative that might claim that changing the name of the branch is a fad I'd like to point to this which is one of my more popular modules. I have been using the name current rather than master for over a year. The only reason why I had not brought up making this change at the project was because I didn't think it would have the momentum to be successful, not because I didn't believe that this change should be made.  do you have specific technical failures you are concerned will happen? Would an in depth transfer that speaks to all of those changes put your mind at ease? If there was a way to ensure that anyone attempting to push or work with master in the future would be gracefully redirected by GitHub help to ease those concerns (this is a feature I'm poking at internally to see if it can be offered). We debated extensively about naming API surfaces, why should this be any different? We accepted a default and those helping to create that default are saying ""hmm, maybe we should reconsider this"". Even if you don't find it useful to attach that context does that fact that others do, and in turn it makes them less productive at what they do, not resonate with you?  I'm having a really hard time parsing the logic here. It seems like you are saying that because making this change will be easier than other changes will imply there is no other work to be done? I don't think anyone is saying that at all. What I do see people saying is that this is a small incremental change, that will take quite a bit of work to do properly... but one that I think will be meaningful to various members of our community.  FWIW I've been thinking quite a bit about our governance model, and more specifically consensus seeking, to examine if it is the best governance model for an inclusive environment. I proposed a openjs world summit session to discuss it as well!  I truly think that making positive social change is an ongoing effort that requires constant small change in the right direction. In fact I would argue that choosing to not make smaller, more obvious, iterative changes (such as changing our default branch) would have the inverse effect of telling people that Node.js is an environment that does not care about this. Also some interesting references for this discussion: Original Discussion on git mailing list Proposed change to git to allow overriding the default branch. A contributor from bitbucket stating the github + bitbucket + git are all working on this together. A great twitter thread explaining why making this change is the right choice. A statement from a GitHub employee that main is likely to be the new default. A checklist used by the github cli to change default branch. A tool by gr2m to automate the process of changing the branch. Discussion about changing defaults in GitLab."
technical,"I would recommend we escalate this (to a vote for example) when: - We have a clear plan on how to make this change addressing the raised issues (GH links, build infrastructure etc). - We have an individual or group willing to champion those changes. Can either of the collaborators -1ing speak up regarding what in particular they are objecting to? (The process of changing it? the name main itself?) Also, this issue is locked because it has received a large number of abuse comments. Like the website change - changes with this flavor tend to get a lot of attention."
technical,"I don't feel strongly against using master branch name. I proposed it as I plan to follow it in my personal projects and work projects, and wanted to ask Node.js community. Should we add this to tsc-agenda? Is there an equivalent ask in git repo? If not, we should create one. Also, to all the people making the drive by comments: please keep discussion civil and remember Node.js has a code of conduct. If you can't be polite here you really don't have to comment. It's fine to either support or object to the proposed change (or any proposed change) as long as you are civil - **we do not tolerate abuse** towards the project and its members here.  To collaborators: kind reminder that you are allowed to ban users that make these sort of comments and to hide said comments - but please update the project (as explained there) with what actions you took so that any moderation actions taken are done in full transparency."
technical,"Needlessly censoring words does nothing to resolve the social issues surrounding them. The reasoning behind this is the same used when changing the gun emoji to a squirt gun. But in the end, such efforts only take away words and symbols we can use to easily describe concepts, such as the hierarchical and control structures we work with every day. It's destructive at worst and pointless virtue signalling at best. Anyway, -1 to this. Not worth the effort."
technical,"There are definitely some very real and difficult technical challenges to address here. My suggestion would be to make this a strategic initiative with the first step being to establish precisely how to make this change with the least amount of disruption. As a community that has pledged for inclusivity, it is natural to be sensitive and sympathetic to the current situations. However, I guess taking a step back and looking at things from a more wider perspective, I would ask these questions myself, in order:  - Is Node.js community well represented by race, gender, ...? any process change required for the membership criteria for better inclusion of under-represented groups? - Is Node.js leadership well represented by race, gender, ...? again, any process change required? - Is Node.js process and practices designed to be inclusive, and is it working well? a retrospective session with commitment to acting on the results? - Is Node.js nomenclature, words used etc. free from offensive terms? (items such as this)  Doing the naming change first - easy, but causes turbulence in the eco-system, plus gives an impression that we are good with the rest (it is so possible that we are doing good there, but we haven't inspected). Addressing the other things - difficult and time consuming, but addresses the root of the issue from systemic perspectives.  I am willing, and eager to taking part, and / or driving such initiatives, if there is a consensus on Let us do these things first, and attack this later!!!"
technical,"To let tsc make a call on this request. Other option would be to keep this issue open for a week or so, and see if it gathers more feedback or support. As far as I understand it that's not how our governance works. Any collaborator may add the tsc-agenda label for an issue so it gets TSC eyes on it - but that should only be done if consensus seeking fails. It's an escape hatch for when we need to  force a vote  which is pretty rare. At least that is my understanding of the process.  So far everyone here seems to be pretty in sync (no one is opposed but no one is particularly in favor). We're not even in disagreement ˜…"
technical,"I've sent an email to Git Community mailing list, and will update here once they come up with a decision. Bear in mind that URLs linking to master will not redirect to the new default branch name (and for repos where it matters, which isn't this one, github-pages only works on the default branch when it's named master). It may be worth waiting for Github to fix these discrepancies before making the switch. (to be clear, i'm in favor of making the change, and ""main"" seems as good as anything else, but the disruption caused by Github's incomplete support for a non-master default branch are significant)"
technical,Today I renamed the branch on 22 repositories and opened a pull requests to update workflows where necessary. did you updat? just want to make sure we have a good view on what's left.
technical,This is not a technically difficult task but it might break some things. I definitely think we should try to change it. Does this mean we would have to change the cluster API (which includes master in its vocabulary) as well in a semver-major?
technical,"So far no objections and everyone in the conversation is +0 -0 or +1. What about this? Also, I think GitHub might pick trunk instead of main. I don't object... but I don't think it's a good idea to rush this... Fwiw there is quite a lot of work for us to do in order to make sure we do this in a way that is not disruptive. To their point, we definitely need to do a large audit and preparation before moving forward. I was putting together some notes yesterday outlining steps to take and what to consider before making a change like this. I'd like to suggest that we pause discussion until Monday and I can come back with a suggestion of what we should audit and steps to follow to do this in a way that would minimize disruption"
technical,"So we can change any links referring to master to refer to HEAD and they will ""just work"" with whatever our default branch is. The branch-rename repo I pointed at has an action for mirroring master / main GitHub has released a repo with official guidance. It mentions a new redirect feature that launched today   Now: supporting early movers. Some projects on GitHub have already renamed their default branch. As a result, any links to those projects that contained the old branch name would previously have been broken. So, our first change, shipped on July 17th, updates GitHub.com to redirect links that contain a deleted branch name to the corresponding link in the repository's default branch. This change supports projects that have already moved. If you haven't moved yet, we recommend not moving right now, and waiting until later this year. We're investing in tools to make the renaming the default branch of an existing repository a seamless experience for both maintainers and contributors."
technical,"Also opened issue in these. I already did education, apparently clicking the checkbox didn't take"
technical,"I've just finished renaming CITGM. Steps included.  * Create main branch - git checkout -b main * Push main branch - git push upstream main * changing default branch in this. * Using the retarget prs utility to update all open pull-requests targeting master - I generated a temporary personal access token with all repo permissions for this * Delete master branch - git push upstream :master * Open a PR to update documentation  So far there do not seem to be issues. Some highlights  - All old URLs are automatically getting redirected. - retargeting prs appears to have worked without issue - all existing integrations with actions + codecov seem to still work Tracking issue I also updated nodejs/examples. Here's the steps - they're slightly different than the ones Myles took since I just used the web UI:  - Create main from master in web UI (dropdown on the repo's main page) - Change default branch from master to main in the repo's settings - Manually retargeted PRs (there were only two, pretty easy to just do this through the PRs in the web UI) - Delete master (can be restored if needed)"
technical,"Anyway, -1 to this. Not worth the effort. I am -1 on renaming/changing the branch."
technical,"Also, this issue is locked because it has received a large number of abuse comments. Like the website change - changes with this flavor tend to get a lot of attention. I can come back with a suggestion of what we should audit and steps to follow to do this in a way that would minimize disruption. Also think about how to roll back when things go wrong. Auditing Jenkins jobs is the kind of mind-numbing tedium that makes human error more likely than not."
technical,"I'm -0 to this change, Many open source project are considering this change but I'm very fearful of all the changes to scripts/automation. As they mentioned, we will need to do a full audit before considering this change. I'd also suggest pushing out a blog post/tweets detailing the changes several weeks before we make the switch in order to give developers a chance to migrate any tooling that they're written. I do want to see what lead GitHub takes in making this easier"
technical,"What a classic example of elitism and douchebaggery. I will stay miles away from Godot and any projects you maintain, thanks. I don't feel strongly against using master branch name. I proposed it as I plan to follow it in my personal projects and work projects, and wanted to ask Node.js community. Should we add this to tsc-agenda? Is there an equivalent ask in git repo? If not, we should create one."
technical,"I can come back with a suggestion of what we should audit and steps to follow to do this in a way that would minimize disruption. Also think about how to roll back when things go wrong. Auditing Jenkins jobs is the kind of mind-numbing tedium that makes human error more likely than not. I don't think such a change is necessary and by making such a change it would be creating a lot of technical issues. I've never heard anyone seriously say they found the branch name offensive since git's inception, much like I've never heard anyone seriously find similar terminology offensive elsewhere in computing in the many decades it's been in use. I believe attaching a single context to such technical terminology (that involves inanimate things) is not useful. Instead, everyone should be focusing their time and effort on things in the world that are *actually* and *obviously* offensive."
technical,"That is hilarious. From defacing nodejs.org with blm propaganda, to renaming master branch to avoid similarity with master-slave metaphor. What is the next requirement OpenJS will ask for? To remove the **test** directory to avoid similarity with **test**icles? Please keep technical aspects of the software free of politics and globalistic propaganda. i don't think that cluster was brought up in this thread, though it has been discussed on other occasions."
technical,"I've seen comments that indicate that blob links redirect but tree links don't (or the reverse), so it may not be a complete feature yet. Given the large number of open PRs on this repo, it seems like ""silently retargeting open PRs"" would be a pretty helpful feature to have too. I doubt CommComm folks would have any issue changing some of our repos, if you'd like to go with that."
technical,I do want to see what lead GitHub takes in making this easier I found a great guide on ways to handle the branch-rename as well as paths towards gradual migration. Will likely be experimenting with some of this stuff on a personal repo over the next two weeks.
technical,+1 to main but I do want to see what lead GitHub takes in making this easier I personally feel very strongly that we should change this. +1 to main
technical,I personally feel very strongly that we should change this. +1 to main I would be -1 untill a plan is drawn for the changes. This could cause a lot of issues with our build ci which would need to be accounted for.
technical,"-1 on renaming the branch I would recommend we escalate this (to a vote for example) when: - We have a clear plan on how to make this change addressing the raised issues (GH links, build infrastructure etc). - We have an individual or group willing to champion those changes. Can either of the collaborators -1ing speak up regarding what in particular they are objecting to? (The process of changing it? the name main itself?)"
technical,"To make it explicit, I am strongly +1 on doing this. This is not mutually exclusive with other initiatives in any way. As for the technical issues, we can either wait to see if something comes from Github, or implement tooling ourselves that would keep the current branch name synced/as an alias, for example. I'd like to echo the sentiment from addaleax that changing the branch name need not be mutually exclusive from other changes which our project likely needs to do to be more welcoming / inclusive. We should always be examining our process and thinking about what we can do to improve and make small iterative changes towards where we want to be. The term master for the main tracking branch is something that has always bothered me. To push back against any narrative that might claim that changing the name of the branch is a fad I'd like to point to this which is one of my more popular modules. I have been using the name current rather than master for over a year. The only reason why I had not brought up making this change at the project was because I didn't think it would have the momentum to be successful, not because I didn't believe that this change should be made.  do you have specific technical failures you are concerned will happen? Would an in depth transfer that speaks to all of those changes put your mind at ease? If there was a way to ensure that anyone attempting to push or work with master in the future would be gracefully redirected by GitHub help to ease those concerns (this is a feature I'm poking at internally to see if it can be offered). We debated extensively about naming API surfaces, why should this be any different? We accepted a default and those helping to create that default are saying ""hmm, maybe we should reconsider this"". Even if you don't find it useful to attach that context does that fact that others do, and in turn it makes them less productive at what they do, not resonate with you?  I'm having a really hard time parsing the logic here. It seems like you are saying that because making this change will be easier than other changes will imply there is no other work to be done? I don't think anyone is saying that at all. What I do see people saying is that this is a small incremental change, that will take quite a bit of work to do properly... but one that I think will be meaningful to various members of our community.  FWIW I've been thinking quite a bit about our governance model, and more specifically consensus seeking, to examine if it is the best governance model for an inclusive environment. I proposed a openjs world summit session to discuss it as well!  I truly think that making positive social change is an ongoing effort that requires constant small change in the right direction. In fact I would argue that choosing to not make smaller, more obvious, iterative changes (such as changing our default branch) would have the inverse effect of telling people that Node.js is an environment that does not care about this."
technical,"i don't think that cluster was brought up in this thread, though it has been discussed on other occasions. I'm -0 to the change itself. I don't think that changing it because it is ""industry standard"" is a valid argument, at least not yet. If we are changing it because we think it is a loaded/inappropriate word, then I think we should remove all occurrences to remain consistent with that decision."
technical,"Not a TSC member, but I am a +1 to this change, echoing sentiments that these do not need to be synchronous changes but can be done simultaneously as we do more intensive work. For full transparency, I've also brought up the possibility of doing this with all repos under the nodejs/community-committee. I'm -0 to this change, Many open source project are considering this change but I'm very fearful of all the changes to scripts/automation. As they mentioned, we will need to do a full audit before considering this change. I'd also suggest pushing out a blog post/tweets detailing the changes several weeks before we make the switch in order to give developers a chance to migrate any tooling that they're written."
technical,"Also, to all the people making the drive by comments: please keep discussion civil and remember Node.js has a code of conduct. If you can't be polite here you really don't have to comment. It's fine to either support or object to the proposed change (or any proposed change) as long as you are civil - **we do not tolerate abuse** towards the project and its members here.  To collaborators: kind reminder that you are allowed to ban users that make these sort of comments and to hide said comments - but please update the project (as explained there) with what actions you took so that any moderation actions taken are done in full transparency. I'm not sure where the git repo is - but I recommend trying the mailing list and asking there https:git-scm.com/community. I think ""doing whatever git does and bringing the issue to their attention"" is a viable strategy - but again, I don't feel particularly strongly about the use of ""master"" and if someone else does - sure. I'm not entirely sure why?"
technical,nodejs/community-committee or nodejs/examples are both low-risk repos that we could try. nodejs.dev is a more complex repo that could also work. I'm up for helping out with any. I've just finished renaming CITGM. Steps included.  * Create main branch - git checkout -b main * Push main branch - git push upstream main * changing default branch in this. * Using the retarget prs utility to update all open pull-requests targeting master - I generated a temporary personal access token with all repo permissions for this * Delete master branch - git push upstream :master * Open a PR to update documentation  So far there do not seem to be issues. Some highlights  - All old URLs are automatically getting redirected. - retargeting prs appears to have worked without issue - all existing integrations with actions + codecov seem to still work Tracking issue
technical,"We can run a test, but when I changed a repo on Friday the redirect was working. We could test this on another repo in the org first. I can also follow up with that team internally and see if we can get it turned on for us. I've seen comments that indicate that blob links redirect but tree links don't (or the reverse), so it may not be a complete feature yet. Given the large number of open PRs on this repo, it seems like ""silently retargeting open PRs"" would be a pretty helpful feature to have too."
technical,"As far as I understand it that's not how our governance works. Any collaborator may add the tsc-agenda label for an issue so it gets TSC eyes on it - but that should only be done if consensus seeking fails. It's an escape hatch for when we need to  force a vote  which is pretty rare. At least that is my understanding of the process.  So far everyone here seems to be pretty in sync (no one is opposed but no one is particularly in favor). We're not even in disagreement ˜… I've sent an email to Git Community mailing list, and will update here once they come up with a decision."
technical,"I doubt CommComm folks would have any issue changing some of our repos, if you'd like to go with that. if ya'll wanna pick a repo we can work from there. Maybe nodejs.dev?"
technical,"Suprised this one hasn't been archived. it should've been, good call"
technical,I found a great guide on ways to handle the branch-rename as well as paths towards gradual migration. Will likely be experimenting with some of this stuff on a personal repo over the next two weeks. Just a note with some tooling I'm seeing come up around this:  - gr2m (former CommComm member and long-term ecosystem ecosystem member) is working on a GitHub bot. He's talked a bit about it on Twitter and has a repo. - github-default-branch allows you to change the default branch for one or many repos via CLI. - retarget prs allows you to retarget PRs from one branch to another. It was built by one of the PMs on npm / maintainers of libgit2.
technical,"I found a great guide on ways to handle the branch-rename as well as paths towards gradual migration. Will likely be experimenting with some of this stuff on a personal repo over the next two weeks. List of non-archived repos, along with whether they need to be updated or not. Generated with Javascript."
technical,"that message was posted after I started writing mine so I did not see it. Sorry for the (timing) confusion. Fwiw this isn't a conceptual -1 it's a -1 until a plan is drawn for how we make the changes. I thought that not making the change quickly without discussing this or laying out how (clearly) is a given. Needlessly censoring words does nothing to resolve the social issues surrounding them. The reasoning behind this is the same used when changing the gun emoji to a squirt gun. But in the end, such efforts only take away words and symbols we can use to easily describe concepts, such as the hierarchical and control structures we work with every day. It's destructive at worst and pointless virtue signalling at best."
technical,if ya'll wanna pick a repo we can work from there. Maybe nodejs.dev? nodejs/community-committee or nodejs/examples are both low-risk repos that we could try. nodejs.dev is a more complex repo that could also work. I'm up for helping out with any.
technical,"that was an addendum. My main point is that the other items that I listed are more tangible, personal and direct to the people / group who are subjected to the theme in question, and hence present a natural order to address. I now see your submission in collab summit, and acknowledge it as part of a constant attempt towards improving our community, in the context of diversity and inclusivity! thank you!! Not a TSC member, but I am a +1 to this change, echoing sentiments that these do not need to be synchronous changes but can be done simultaneously as we do more intensive work. For full transparency, I've also brought up the possibility of doing this with all repos under the nodejs/community-committee."
technical,"I already did education, apparently clicking the checkbox didn't take not sure how I missed that it had already been done, thanks."
technical,"I would be -1 untill a plan is drawn for the changes. This could cause a lot of issues with our build ci which would need to be accounted for. Ok, it looks like there are people in favor in the org who feel strongly that we should change this. So far no objections and everyone in the conversation is +0 -0 or +1. Does anyone object to changing this (just the main branch name from master to main)? It looks like it's not particularly hard technically (+ an update to the collaborator guide and policy). We probably need to address the links as well."
technical,-1 on this. Putting this on the tsc agenda for discussion
technical,"I also updated nodejs/examples. Here's the steps - they're slightly different than the ones Myles took since I just used the web UI:  - Create main from master in web UI (dropdown on the repo's main page) - Change default branch from master to main in the repo's settings - Manually retargeted PRs (there were only two, pretty easy to just do this through the PRs in the web UI) - Delete master (can be restored if needed) see also regarding changing the default for new repos"
technical,"Ok, it looks like there are people in favor in the org who feel strongly that we should change this. So far no objections and everyone in the conversation is +0 -0 or +1. Does anyone object to changing this (just the main branch name from master to main)? It looks like it's not particularly hard technically (+ an update to the collaborator guide and policy). We probably need to address the links as well. So far no objections and everyone in the conversation is +0 -0 or +1. What about this? Also, I think GitHub might pick trunk instead of main. I don't object... but I don't think it's a good idea to rush this..."
technical,"What would really help is a way to have master automatically mirror (as in both read and write) main for a transitional and possibly long amount of time and point new users and tools towards main while gradually and at a very comfortable pace pace migrating all the existing infrastructure, docs, guides and tooling. I'm not sure if that's possible (or hard) as I'm far from an expert or tooling - but such a tool doesn't sound particularly hard to make conceptually. I think the guide Myles linked to provides one such way (via GitHub actions) that might be appropriate. Links on GitHub would also probably need to automatically redirect somehow. So we can change any links referring to master to refer to HEAD and they will ""just work"" with whatever our default branch is. The branch-rename repo I pointed at has an action for mirroring master / main"
technical,"List of non-archived repos, along with whether they need to be updated or not. Generated with Javascript. Suprised this one hasn't been archived."
technical,"Fwiw there is quite a lot of work for us to do in order to make sure we do this in a way that is not disruptive. To their point, we definitely need to do a large audit and preparation before moving forward. I was putting together some notes yesterday outlining steps to take and what to consider before making a change like this. I'd like to suggest that we pause discussion until Monday and I can come back with a suggestion of what we should audit and steps to follow to do this in a way that would minimize disruption that message was posted after I started writing mine so I did not see it. Sorry for the (timing) confusion. Fwiw this isn't a conceptual -1 it's a -1 until a plan is drawn for how we make the changes. I thought that not making the change quickly without discussing this or laying out how (clearly) is a given."
technical,"Also some interesting references for this discussion: Original Discussion on git mailing list Proposed change to git to allow overriding the default branch. A contributor from bitbucket stating the github + bitbucket + git are all working on this together. A great twitter thread explaining why making this change is the right choice. A statement from a GitHub employee that main is likely to be the new default. A checklist used by the github cli to change default branch. A tool by gr2m to automate the process of changing the branch. Discussion about changing defaults in GitLab. that was an addendum. My main point is that the other items that I listed are more tangible, personal and direct to the people / group who are subjected to the theme in question, and hence present a natural order to address. I now see your submission in collab summit, and acknowledge it as part of a constant attempt towards improving our community, in the context of diversity and inclusivity! thank you!!"
technical,"GitHub has released a repo with official guidance. It mentions a new redirect feature that launched today   Now: supporting early movers. Some projects on GitHub have already renamed their default branch. As a result, any links to those projects that contained the old branch name would previously have been broken. So, our first change, shipped on July 17th, updates GitHub.com to redirect links that contain a deleted branch name to the corresponding link in the repository's default branch. This change supports projects that have already moved. If you haven't moved yet, we recommend not moving right now, and waiting until later this year. We're investing in tools to make the renaming the default branch of an existing repository a seamless experience for both maintainers and contributors. That's good news, but also means we have to wait ""until later this year""."
technical,"As a community that has pledged for inclusivity, it is natural to be sensitive and sympathetic to the current situations. However, I guess taking a step back and looking at things from a more wider perspective, I would ask these questions myself, in order:  - Is Node.js community well represented by race, gender, ...? any process change required for the membership criteria for better inclusion of under-represented groups? - Is Node.js leadership well represented by race, gender, ...? again, any process change required? - Is Node.js process and practices designed to be inclusive, and is it working well? a retrospective session with commitment to acting on the results? - Is Node.js nomenclature, words used etc. free from offensive terms? (items such as this)  Doing the naming change first - easy, but causes turbulence in the eco-system, plus gives an impression that we are good with the rest (it is so possible that we are doing good there, but we haven't inspected). Addressing the other things - difficult and time consuming, but addresses the root of the issue from systemic perspectives.  I am willing, and eager to taking part, and / or driving such initiatives, if there is a consensus on Let us do these things first, and attack this later!!! That's literally what you're seeing here.  Are you referring to the OP? If so, that didn't exactly strike me as someone saying they personally find the term offensive. To me the issue text read more like ""hey, other people are making this change in other projects, should we do it too?"".   Keep in mind that what you might consider "" actually "" and "" obviously "" offensive will differ significantly from what others may find "" actually "" and "" obviously "" offensive.  Here ""actually"" and ""obviously"" was meant to describe things that a greater majority of people can agree are offensive."
technical,"I don't think such a change is necessary and by making such a change it would be creating a lot of technical issues. I've never heard anyone seriously say they found the branch name offensive since git's inception, much like I've never heard anyone seriously find similar terminology offensive elsewhere in computing in the many decades it's been in use. I believe attaching a single context to such technical terminology (that involves inanimate things) is not useful. Instead, everyone should be focusing their time and effort on things in the world that are *actually* and *obviously* offensive. That's literally what you're seeing here. Keep in mind that what you might consider ""*actually*"" and ""*obviously*"" offensive will differ significantly from what others may find ""*actually*"" and ""*obviously*"" offensive."
technical,"That's literally what you're seeing here. Keep in mind that what you might consider ""*actually*"" and ""*obviously*"" offensive will differ significantly from what others may find ""*actually*"" and ""*obviously*"" offensive. There are definitely some very real and difficult technical challenges to address here. My suggestion would be to make this a strategic initiative with the first step being to establish precisely how to make this change with the least amount of disruption."
technical, This is not a technically difficult task but it might break some things. I definitely think we should try to change it.
technical,"I'm not sure where the git repo is - but I recommend trying the mailing list and asking there https:git-scm.com/community. I think ""doing whatever git does and bringing the issue to their attention"" is a viable strategy - but again, I don't feel particularly strongly about the use of ""master"" and if someone else does - sure. I'm not entirely sure why? To let tsc make a call on this request. Other option would be to keep this issue open for a week or so, and see if it gathers more feedback or support."
technical,"That's literally what you're seeing here.  Are you referring to the OP? If so, that didn't exactly strike me as someone saying they personally find the term offensive. To me the issue text read more like ""hey, other people are making this change in other projects, should we do it too?"".   Keep in mind that what you might consider "" actually "" and "" obviously "" offensive will differ significantly from what others may find "" actually "" and "" obviously "" offensive.  Here ""actually"" and ""obviously"" was meant to describe things that a greater majority of people can agree are offensive. To make it explicit, I am strongly +1 on doing this. This is not mutually exclusive with other initiatives in any way. As for the technical issues, we can either wait to see if something comes from Github, or implement tooling ourselves that would keep the current branch name synced/as an alias, for example."
technical,"it should've been, good call Today I renamed the branch on 22 repositories and opened a pull requests to update workflows where necessary."
technical,Went through an opened issues in a number of internationalization repos to give people a heads up/ask if there are any concerns. Unless there are concerns by Friday I think we should be ok to go ahead and rename those.  Any help for doing that after Friday would be good :) Updated a few of the inactive/historical CommComm initiative repos and checked them off.
technical,"That's good news, but also means we have to wait ""until later this year"". We can run a test, but when I changed a repo on Friday the redirect was working. We could test this on another repo in the org first. I can also follow up with that team internally and see if we can get it turned on for us."
technical,"yes, I did. Went through an opened issues in a number of internationalization repos to give people a heads up/ask if there are any concerns. Unless there are concerns by Friday I think we should be ok to go ahead and rename those.  Any help for doing that after Friday would be good :)"
technical,"Just a note with some tooling I'm seeing come up around this:  - gr2m (former CommComm member and long-term ecosystem ecosystem member) is working on a GitHub bot. He's talked a bit about it on Twitter and has a repo. - github-default-branch allows you to change the default branch for one or many repos via CLI. - retarget prs allows you to retarget PRs from one branch to another. It was built by one of the PMs on npm / maintainers of libgit2. What would really help is a way to have master automatically mirror (as in both read and write) main for a transitional and possibly long amount of time and point new users and tools towards main while gradually and at a very comfortable pace pace migrating all the existing infrastructure, docs, guides and tooling. I'm not sure if that's possible (or hard) as I'm far from an expert or tooling - but such a tool doesn't sound particularly hard to make conceptually. I think the guide Myles linked to provides one such way (via GitHub actions) that might be appropriate. Links on GitHub would also probably need to automatically redirect somehow."
technical,Updated a few of the inactive/historical CommComm initiative repos and checked them off. When through an updated the internationalization ones that I'd opened issues on last time. Created issues to FYI/ask for concerns to rename master-main in the remaining internationalization related ones.
technical,"did you updat? just want to make sure we have a good view on what's left. yes, I did."
technical,"Feature Request: **Drop-in component for ASP Identity Administration, Portals, UI, SSO, Claims, AD-Sync, Token, Multi.Tenant Management** so that the Visual Studio templates don't require commercial licensed 3rd party components, I have been watching/waiting few yrs. patiently, however some recent changes have forced me to request -- Microsoft revisit this for their developer community, keeping in mind some of the pricing out there compares to the salaries of 10 to 15 developers in Bangladesh, Vietnam, India, Philippines etc.  Core & Basic need, yet very complex and critical.  There's a void in .NET native drop-in solution, unlike the Java, PHP Stacks etc, where there are many native drop in Identity Administration & Management Frameworks options that are leveraged across all their platforms seamlessly by developers, for e.g. the J2EE Pluggable Identity Management Framework or JSR 168 or JSR 286 or JSR-351 or JSR-375.  Why is this important? because using Java or PHP, it has allowed easy, clear, core and basic functionalities in those native stacks. As a result if you look JSR 168 or JSR 286 or JSR-351 or JSR-375 Multi-tenants, Group to roles, or to claims) vice versa is so easy vs. NET , mobile devices, portals, they all work seamlessly and cohesively with security fixes managed by either IBM or SalesForce or Oracle or Red Hat etc. This is enables developer to be productive right out of the gate.  In .Net there is void/very limited support, always requiring a combination of 3rd parties as a barrier to entry & adoption of ASP app. This is non-trivial for developers and security vulnerability that requires the attention of Microsoft Experts.  Example: We have private information sharing site non OSS for the community almost free (pay if you want), and when we started with web forms, then Simple Membership, the Identity, Identity 2 ASP MVC we had implement much of it on top of these from scratch, when we moved to .NET Core it was another effort. Besides the development there was a lot of confusion on the internal concepts and how they now meant new things. Roles, Claims, Federation, SAML then SAML 2.0 and then Open ID 2.  Describe the solution you'd like  - A drop-in is extensible solution that supports ASP Identity eco-system, Administration, UI, SSO, Token, Multi-tenant Management - A configuration section to turn on-off the various features. - Embedded into VS Templates (remove Identity Server, use native MS option etc.) - Allow-Easy AD Active Directory Integration - User Defined/Created Fields UDF from the Admin level Support. - The current options and the pricing eliminate many of the existing applications from continuing usage on the .NET stack without extensive retooling or completely abandoning the MS framework. -  SAML2/CAS per tbonham -  Allow template option Configuration.MultiTenancy.IsEnabled = true, Global DataFilter for Tenant entities Its high time MS address this core gate-keeping feature!! -Controller/API DDOS throttling support -Login Controller/API Captcha Anti Spambot support -Timeouts are easily by passed using Singal-R and there's no way to force anonymous connections to disconnect -No way to handle anonymous chats on public website. For e.g. if  Web User is Surfing/browsing for help or FAQ on product/chat  on ASP website with SignalR, we have to create a duct-taped solution for this, there is no way to identify them uniquely to create a chat stream/session unless they login."
technical,Project Reload on save with Kestrel like IIS Express does. #26091 Download compressed resources by default
technical,"Render conditional closing and opening tag. With out most of the excellent layouts are not possible to be rendered in Blazor Add all the properties of HttpMessageHandler on the client side. At present, it has lots of missing code and we are unable to use many of them"
technical,"Streaming API support to MVC. All this is great, folks. Please make sure to upvote on the related issues, rather than linking issues here. That's what we're going to look at after all."
technical, AoT compilation. AOT combined with browser task based parallelism would destroy JS.  **.NET WebAssembly support of task based parallelism. #40619**
technical,"Support for feature folders either out of the box or in a nuget. Instead of the typical messy arranging by types, arranging by features instead. This is a more logical approach to structuring any kind of application whether using ASP.NET Core, .NET Core, something totally different, anyway.  Read more about it here. This would work for MVC, Razor Pages, Rest APIs. ASP.NET Core MVC - Razor - Nested Partial and Simple Partial Child Content"
technical," Auto Upgrade option **PartialViews to ViewComponents**  Lots of legacy code is hung in the partials, we need to port that to the new ViewComponents or TagHelpers apps"
technical,"I think it'd be really useful to have built-in support for Pub/Sub, or at least something close to .NET events. Today scenarios like, having component B (child) and component A (parent), notify components C...N (not child of A), are not that easy to do. Technically, you need to come up with a hack. please file a separate issue to continue the conversation there. Automatic module separation and lazy loading."
technical,Drag & drop.  Would be nice to finish McKinnon's work Bedrock Futures: #15005
technical,"Compensating transactions are a very tricky part of micro services development. Can we come up with some sort of high level framework that will facilitate it. The lower level implementation say for example an implementation with redis or RabbitMQ can then be provided my community to augment it. better startup structure. now ""ConfigureServices"" and ""Configure"" is complicate for new asp.net developer."
technical,"RFC 7692 (WebSocket per-message compression) for SignalR. It seems this depends on runtime but it seems relevant in this context. There was an old issue referencing this, but it is now closed and I could find no equivalent open issue here, so I am recommending that people upvote on the runtime issue. blazor wasm-to-wasm interop without javascript (WASI)"
technical,"Great question. The Mono in dotnet/runtime is targeted at Android, iOS, and wasm. Built in Identity Server #27053"
technical,"thanks for the idea  but they keep closing my feature requests and locking them, so its not able to upvote!  Can you please unlock 26594 to allow voting on the feature please! Built-in admin panel template and Vue integration."
technical,Official Microsoft Design components for Blazor: #11229 It is already the most voted Feature in the Backlog with 122  upvotes Built-in way to get a child or parent component reference in Blazor
technical,Bedrock Futures: #15005 Cascade Components - Detect parent component automatically
technical, Compensating transactions are a very tricky part of micro services development. Can we come up with some sort of high level framework that will facilitate it. The lower level implementation say for example an implementation with redis or RabbitMQ can then be provided my community to augment it.
technical,"Auto Upgrade option **PartialViews to ViewComponents**  Lots of legacy code is hung in the partials, we need to port that to the new ViewComponents or TagHelpers apps Concurrent frameworks can support a wider range of architectures (this list might not be exhaustive): - NodeJs support : x86 / x64 / arm32 / arm64 / ppc64le / ppc64be / x390x - OpenJDK support : x86 / x64 / arm32 / arm64 / ppc32 / ppc64le / s390x / sparcv9 - AspNet core support : x86 / x64 / arm32 / arm64  please add support to more architectures."
technical,InputSelect support for multiple attribute. Decuple the SignInManager and UserManager  in Identity.
technical,"SASS as first-class citizen in the toolchain (recognition of variables, intellisense across all imports, and compilation) Direct support for hosting a Blazor app on a desktop, with access to local peripherals and .NET Core on the client. WASM is great but if we have the full desktop available it makes things like point-of-sale functionality to local devices that much easier"
technical,not sure when you will fix this for .Net core Drag & drop.  Would be nice to finish McKinnon's work
technical,Decuple the SignInManager and UserManager  in Identity. Dynamically creating and adding components. Angular ComponentFactoryResolver alternative.
technical,"MobileBlazorbindings in the experiment has got a good response. We hope that it can become a formal project of Microsoft as soon as possible, and can provide complete functions, so that users can focus on their own business logic, especially the hybrid applications. It seems to have a chance to surpass electron, more streamlined but more powerful. Editor & debugger improvements and Hot Reload (Edit & Continue)"
technical,please file an issue with relevant details. Enterprise UI Components for Blazor
technical,"Issue is locked, we can't to upvote. Feature Request: **Drop-in component for ASP Identity Administration, Portals, UI, SSO, Claims, AD-Sync, Token, Multi.Tenant Management** so that the Visual Studio templates don't require commercial licensed 3rd party components, I have been watching/waiting few yrs. patiently, however some recent changes have forced me to request -- Microsoft revisit this for their developer community, keeping in mind some of the pricing out there compares to the salaries of 10 to 15 developers in Bangladesh, Vietnam, India, Philippines etc.  Core & Basic need, yet very complex and critical.  There's a void in .NET native drop-in solution, unlike the Java, PHP Stacks etc, where there are many native drop in Identity Administration & Management Frameworks options that are leveraged across all their platforms seamlessly by developers, for e.g. the J2EE Pluggable Identity Management Framework or JSR 168 or JSR 286 or JSR-351 or JSR-375.  Why is this important? because using Java or PHP, it has allowed easy, clear, core and basic functionalities in those native stacks. As a result if you look JSR 168 or JSR 286 or JSR-351 or JSR-375 Multi-tenants, Group to roles, or to claims) vice versa is so easy vs. NET , mobile devices, portals, they all work seamlessly and cohesively with security fixes managed by either IBM or SalesForce or Oracle or Red Hat etc. This is enables developer to be productive right out of the gate.  In .Net there is void/very limited support, always requiring a combination of 3rd parties as a barrier to entry & adoption of ASP app. This is non-trivial for developers and security vulnerability that requires the attention of Microsoft Experts.  Example: We have private information sharing site non OSS for the community almost free (pay if you want), and when we started with web forms, then Simple Membership, the Identity, Identity 2 ASP MVC we had implement much of it on top of these from scratch, when we moved to .NET Core it was another effort. Besides the development there was a lot of confusion on the internal concepts and how they now meant new things. Roles, Claims, Federation, SAML then SAML 2.0 and then Open ID 2.  Describe the solution you'd like  - A drop-in is extensible solution that supports ASP Identity eco-system, Administration, UI, SSO, Token, Multi-tenant Management - A configuration section to turn on-off the various features. - Embedded into VS Templates (remove Identity Server, use native MS option etc.) - Allow-Easy AD Active Directory Integration - User Defined/Created Fields UDF from the Admin level Support. - The current options and the pricing eliminate many of the existing applications from continuing usage on the .NET stack without extensive retooling or completely abandoning the MS framework. -  SAML2/CAS per tbonham -  Allow template option Configuration.MultiTenancy.IsEnabled = true, Global DataFilter for Tenant entities Its high time MS address this core gate-keeping feature!!"
technical,"If you're in an environment where files with ""dll"" extensions are simply blocked, then we've provided manual steps for changing the extension. We haven't identified any additional work in this area currently. If you think we need to do more, please file an issue with your suggestion and we'll take a look. Good morning, I hope I am not mistaken but I have noticed that the **Timer ()** is slows down when you switch tabs in the browser. Make a basic example of the possible problems and this is my repository link. To detail what happened, carry out some tests and record it.  Basic and simple example, slows down 10 minutes. Example with more code, it slows down 1 minute and 50 seconds if you do not change the tab the timer does not slow down. In the description I detail what happened. I'm Percy Len from Lima-Peru Contact me Linkedin. Greetings!"
technical," Great question. The Mono in dotnet/runtime is targeted at Android, iOS, and wasm."
technical,Support older Edge (with polyfills?) Hi. Cascade state for parent child components.
technical,"Dynamically creating and adding components. Angular ComponentFactoryResolver alternative. Hi. Thanks for stopping by. We're actively working on .NET 6 planning and we would like your help with this. In the next few weeks we are going to scan through all the issues in our backlog and identify candidates for .NET 6. To make a decision whether an issue is a good candidate or not for the upcoming release, we also look into the number of upvotes  (and other reactions). To help us prioritize the issues the community feels are most important, please find the issues you want to see resolved and upvote  them. **  Please don't add descriptions of what you want to see as comments in this issue. Simply find an existing of file a new issue and upvote it**"
technical,SVG Support in Blazor HTML Autofocus supprt in Blazor WASM
technical,"In-house (Microsoft made) identity server and other security modules I can't find an issue regarding fast dev loop in general that is not mentioning blazor, I would like the whole dev experience  would be much faster"
technical," I think it'd be really useful to have built-in support for Pub/Sub, or at least something close to .NET events. Today scenarios like, having component B (child) and component A (parent), notify components C...N (not child of A), are not that easy to do. Technically, you need to come up with a hack."
technical,"This issue was closed and still there is no solution #26450. It has been also mentioned in here and here. Since .NET 5 RC1 was released, Blazor WASM stopped working with Azure. I think it'd be really useful to have built-in support for Pub/Sub, or at least something close to .NET events. Today scenarios like, having component B (child) and component A (parent), notify components C...N (not child of A), are not that easy to do. Technically, you need to come up with a hack. please file a separate issue to continue the conversation there."
technical,"you've listed multiple feature requests above. Please make sure you file separate issues for each one so we can track these better. I was considering that originally, but didn't want to mute people. If this continues, I will probably lock this conversation."
technical,MAUI (UI design) Studio for .NET 6 #26918 I'd like more app-level control over rendering
technical,Built in Identity Server #27053 IdentityServer Token Manager
technical,"Static site generator #26929 If you're in an environment where files with ""dll"" extensions are simply blocked, then we've provided manual steps for changing the extension. We haven't identified any additional work in this area currently. If you think we need to do more, please file an issue with your suggestion and we'll take a look."
technical,"Reduce DLL size/Remove dependencies. One thing that's been bugging me lately is reducing download size see #26780 It's just a little thing, but removing dependencies makes all the difference. In the future perhaps you could AOT compile DLL files into WASM files? Improve prerendering #26794  (Avoid absolute classic FOOC effect)   The concept needed here is called **SSR client side hydradation**. Examples in js SSR land explained in the first paragraph of the links below:  - Vue's hydradation - React's ReactDOM.hydrate() - Svelte's prerendering with hydradation Issue #26802 should probably be merged into the one mentioned above."
technical,"Proposal: View Component Slots is locked. Provide a built-in or otherwise framework supplied mechanism for View Components, when rendered via a tag helper, to render user-supplied child content that can be injected into the View Component's view template in pre-defined locations In-house (Microsoft made) identity server and other security modules"
technical,"better startup structure. now ""ConfigureServices"" and ""Configure"" is complicate for new asp.net developer. InputSelect support for multiple attribute."
technical,"All this is great, folks. Please make sure to upvote on the related issues, rather than linking issues here. That's what we're going to look at after all. Issue is locked, we can't to upvote."
technical,"You should seriously consider abandoning the whole call-to-action thing. Choosing what issues need solving based on community votes is why I never leave MS product feedback/bug reports anymore - if you don't have a social following or plenty of co-workers to upvote your issues they just languish and eventually get closed. My suggestion would be to select topics / general directions that the project can move in, let people pick from those, and let that guide part of the overall effort, without making specific commitments to specific issues. It's a non-binding advisory vote :)"
technical,"Multithreading in web assembly JS isolation improvement - Colocate CS, CSS and JS files"
technical,"AoT compilation. AOT combined with browser task based parallelism would destroy JS.  **.NET WebAssembly support of task based parallelism. #40619** Make it stupid simple to debug Blazor Server apps that throw circuit errors stating only to turn on detailed exceptions, but when done, don't give any better information after the fact. #26705"
technical,Responsive Menu Control for Blazor in .NET 6 #26917 MAUI (UI design) Studio for .NET 6 #26918
technical,"I can't find an issue regarding fast dev loop in general that is not mentioning blazor, I would like the whole dev experience  would be much faster Might it be a good idea to create a new call to action with comments disabled?"
technical,"#26091 Download compressed resources by default MobileBlazorbindings in the experiment has got a good response. We hope that it can become a formal project of Microsoft as soon as possible, and can provide complete functions, so that users can focus on their own business logic, especially the hybrid applications. It seems to have a chance to surpass electron, more streamlined but more powerful."
technical,"Thanks for reporting this. Can you please file a new issue to track this separately. Most of the feature requests seem to be Blazor related. While this is understandable due to it being a new technology, with respect to my fellow Blazor devs there are still those of us who don't use Blazor for various reasons that are not important. I hope that development time is not biased too much in favour of that technology to the detriment of core ASP.NET features that would help everyone due to their being core features. Here are the top features according to upvotes:  -  Add support for LetsEncrypt  (LettuceEncrypt) #4712 -  Add HTTP/3 Support #15271 -  Add Streaming API Support to MVC #11558 - Add HTTP/2: Server Push Support #4249 - Define a JSON schema for appsettings.json #2867 - Async suffix for controller action names will be trimmed by default #8998 - ProblemDetails is not returned for 404 NotFound and 500 #4953 - Update ASP.NET Core to use C# 8's nullable reference types #5680 - Add IAsyncStartup and IAsyncStartupFilter to support async startup classes and filters #5897 - ConfigureTestContainer not working with GenericHost #14907 - Bedrock Endgame #15005 - Add support for content security policy #6001 - Support the new Forwarded header (RFC 7239) #5978 - Make it easy to configure a host/scheme for absolute URLs with LinkGenerator #14192 - New WebHostFactory option to directly call a specific testing startup class when working with integration tests #26487 -  JsonPatchDocument should use System.Text.Json in ASP.NET vNext #24333 -  Separate type/parsing validation error handling from business logic validation (400 vs 422) #25732  You can see a full list of non-Blazor issues here"
technical,WebSocket performance improvement Multithreading in web assembly
technical,"Add all the properties of HttpMessageHandler on the client side. At present, it has lots of missing code and we are unable to use many of them New WebHostFactory option to directly call a specific testing startup class when working with integration tests"
technical,"Direct support for hosting a Blazor app on a desktop, with access to local peripherals and .NET Core on the client. WASM is great but if we have the full desktop available it makes things like point-of-sale functionality to local devices that much easier not sure when you will fix this for .Net core"
technical,Improve prerendering #26794  (Avoid absolute classic FOOC effect)   The concept needed here is called **SSR client side hydradation**. Examples in js SSR land explained in the first paragraph of the links below:  - Vue's hydradation - React's ReactDOM.hydrate() - Svelte's prerendering with hydradation Issue #26802 should probably be merged into the one mentioned above. Official Microsoft Design components for Blazor: #11229 It is already the most voted Feature in the Backlog with 122  upvotes
technical,"Cascade Components - Detect parent component automatically Official support for HTTP/3 (QUIC) with support on Azure services ideally ,)"
technical,project template blazor server/wasm using fast (components) as a user interface as quick start/example please add support for sftp
technical,"JS isolation improvement - Colocate CS, CSS and JS files please file an issue with relevant details."
technical,"blazor wasm-to-wasm interop without javascript (WASI) please provide a way to improve the user experience for blazor wasm load progress, progress bar and/or filename currently downloading."
technical," please upvote to the related issues and if you can't find such, create new issues for each one separately. That way we can actually track these requests. Otherwise many asks will be lost / forgotten."
technical,HTML Autofocus supprt in Blazor WASM Project Reload on save with Kestrel like IIS Express does.
technical,"please provide a way to improve the user experience for blazor wasm load progress, progress bar and/or filename currently downloading. project template blazor server/wasm using fast (components) as a user interface as quick start/example"
technical,"please add support for sftp Proposal: View Component Slots is locked. Provide a built-in or otherwise framework supplied mechanism for View Components, when rendered via a tag helper, to render user-supplied child content that can be injected into the View Component's view template in pre-defined locations"
technical,Enterprise UI Components for Blazor Publish Blazor Server or WASM to iOS/Android #27252
technical,"Official support for HTTP/3 (QUIC) with support on Azure services ideally ,) Reduce DLL size/Remove dependencies. One thing that's been bugging me lately is reducing download size see #26780 It's just a little thing, but removing dependencies makes all the difference. In the future perhaps you could AOT compile DLL files into WASM files?"
technical,Automatic module separation and lazy loading. Render conditional closing and opening tag. With out most of the excellent layouts are not possible to be rendered in Blazor
technical,"SSRS support: #362, #1528, #1764, #12666, #22304. All issues have been closed and locked, so cannot be upvoted. Currently 1005 votes on the feedback site, with no update since 2018: develop a SSRS ReportViewer for ASP.NET Core Customer Feedback for ACE Community Tooling Responsive Menu Control for Blazor in .NET 6 #26917"
technical,"Built-in admin panel template and Vue integration. RFC 7692 (WebSocket per-message compression) for SignalR. It seems this depends on runtime but it seems relevant in this context. There was an old issue referencing this, but it is now closed and I could find no equivalent open issue here, so I am recommending that people upvote on the runtime issue."
technical,"Hi. Cascade state for parent child components. SASS as first-class citizen in the toolchain (recognition of variables, intellisense across all imports, and compilation)"
technical,"Built-in way to get a child or parent component reference in Blazor SSRS support: #362, #1528, #1764, #12666, #22304. All issues have been closed and locked, so cannot be upvoted. Currently 1005 votes on the feedback site, with no update since 2018: develop a SSRS ReportViewer for ASP.NET Core Customer Feedback for ACE Community Tooling"
technical,what happened with the issue of Blazor being blocked by some types of router? Other than providing a manual workaround is anything being done to work around this? The tickets won't allow new comments. Static site generator #26929
technical,Editor & debugger improvements and Hot Reload (Edit & Continue) Streaming API support to MVC.
technical,"Use of InMemoryCache in Blazor wasm Support for feature folders either out of the box or in a nuget. Instead of the typical messy arranging by types, arranging by features instead. This is a more logical approach to structuring any kind of application whether using ASP.NET Core, .NET Core, something totally different, anyway.  Read more about it here. This would work for MVC, Razor Pages, Rest APIs."
technical,ASP.NET Core MVC - Razor - Nested Partial and Simple Partial Child Content Support older Edge (with polyfills?)
technical, SVG Support in Blazor
technical,"Good morning, I hope I am not mistaken but I have noticed that the **Timer ()** is slows down when you switch tabs in the browser. Make a basic example of the possible problems and this is my repository link. To detail what happened, carry out some tests and record it.  Basic and simple example, slows down 10 minutes. Example with more code, it slows down 1 minute and 50 seconds if you do not change the tab the timer does not slow down. In the description I detail what happened. I'm Percy Len from Lima-Peru Contact me Linkedin. Greetings! Thanks for reporting this. Can you please file a new issue to track this separately."
technical,"Concurrent frameworks can support a wider range of architectures (this list might not be exhaustive): - NodeJs support : x86 / x64 / arm32 / arm64 / ppc64le / ppc64be / x390x - OpenJDK support : x86 / x64 / arm32 / arm64 / ppc32 / ppc64le / s390x / sparcv9 - AspNet core support : x86 / x64 / arm32 / arm64  please add support to more architectures. Thanks for the feedback YohanSciubukgian. Our plan is to support most/all mainline OSes and chip combinations. We now have that. At present, the only one we're really missing is Linux x86 (32-bit). That comes up quite seldom. The additional ones that you've mentioned come up even less. The community is free to add the more niche configurations that you'd mention, but I'm doubtful that will happen. They don't, AFAICT, have any practical commercial value. Personally, I'd much rather we added support for FreeBSD (on x64) than PPC. I suspect that would benefit an order of magnitude more users (if not much more).  Also note that we're currently putting a fair bit of effort into supporting Apple Silicon and Arm64 (generally). Just supporting new chips takes a lot of investment. Context here. It is much better to focus on doing a great job for mainline chips and leave the legacy/hobbyist chips for other platforms to support. There is a company working on MIPS support for .NET Core currently. This HN post is super related."
technical," thanks for the idea  but they keep closing my feature requests and locking them, so its not able to upvote!  Can you please unlock 26594 to allow voting on the feature please!"
technical,"It's a non-binding advisory vote :) thanks for the idea but they keep closing my feature requests and locking them, so its not able to upvote! looking into the issue I believe it's not something aligned with our long-term vision. And that's the reason why blowdart has closed that issue. Given that, it seems we won't be able to prioritize that one for .NET 6 for sure."
technical,"I think it'd be really useful to have built-in support for Pub/Sub, or at least something close to .NET events. Today scenarios like, having component B (child) and component A (parent), notify components C...N (not child of A), are not that easy to do. Technically, you need to come up with a hack. This issue was closed and still there is no solution #26450. It has been also mentioned in here and here. Since .NET 5 RC1 was released, Blazor WASM stopped working with Azure."
technical,New WebHostFactory option to directly call a specific testing startup class when working with integration tests Use of InMemoryCache in Blazor wasm
technical,IdentityServer Token Manager WebSocket performance improvement
technical,I'd like more app-level control over rendering what happened with the issue of Blazor being blocked by some types of router? Other than providing a manual workaround is anything being done to work around this? The tickets won't allow new comments.
technical,"Might it be a good idea to create a new call to action with comments disabled? You should seriously consider abandoning the whole call-to-action thing. Choosing what issues need solving based on community votes is why I never leave MS product feedback/bug reports anymore - if you don't have a social following or plenty of co-workers to upvote your issues they just languish and eventually get closed. My suggestion would be to select topics / general directions that the project can move in, let people pick from those, and let that guide part of the overall effort, without making specific commitments to specific issues."
technical,"thanks for the idea but they keep closing my feature requests and locking them, so its not able to upvote! looking into the issue I believe it's not something aligned with our long-term vision. And that's the reason why blowdart has closed that issue. Given that, it seems we won't be able to prioritize that one for .NET 6 for sure. you've listed multiple feature requests above. Please make sure you file separate issues for each one so we can track these better."
technical,"I also can't focus on my work since then, please let me be productive again :santa: will be back!"
technical,"This feature request is now a candidate for our backlog. The community has 60 days to upvote the issue. If it receives 20 upvotes we will move it to our backlog. If not, we will close it. To learn more about how we handle feature requests, please see our documentation. Happy Coding! :slightly smiling face: This feature request received a sufficient number of community upvotes and we moved it to our backlog. To learn more about how we handle feature requests, please see our documentation. Happy Coding!"
technical,"I get you care about this but it's already been considered in this issue. Now that we can split editors and terminals I think it would be a good idea to have the ability to activate **editors/terminals** on mouse over.  As the owner of the terminal component I reject the idea that this should be a terminal only feature, it should apply to the editors as well at the very least. The PR revealed some usability issues with the feature and it was marked as out of scope by someone intimately familiar with the editor. Realistically this would also not get prioritized for the team to work on so it would only come in via a PR, and we've already attempted and spent a bunch of time on it reviewing, testing and fixing in this issue. It would also be a major feature in terms of work required to implement and maintain, but it would very rarely get used because it's an opt-in. The bot helps us manage issues and it messed up here by not applying the feature request label and starting the timer in May, if it did maybe it would have reached 20 upvotes in time but I also made a mistake by not closing it out as a duplicate of this issue to begin with. Do Not Delete This!   Please read our Rules of Conduct. Please search existing issues to avoid creating duplicates. --  I'm not sure if vscode already allows this or if it is already possible to focus on terminal view on mouse hover? And it would be nice to, **when you have multiple splitted terminals, you just change focus by hovering over them with the mouse.**Can i add something like a ""mouse-hover"" instead of ""key""?"
technical,"Sorry but the justification that the bot gave for closing this issue, for me it's not clear and valid (no wonder that it received 17 downvotes). On that attempt that he  tried and reverted, was focus on hover for the **EditorGroup** not for the **terminal**. Also, those references are from 2 years ago. Until now, what has been done taking into account this? Was there a change plan created for this or something? Because it looks like the community want this (not only from 2018, but also nowadays since my request received  20 upvotes). Can you pls re-check this and reopen **or** provide a more clear and valid justification? If you need any help, pls tell me. I get you care about this but it's already been considered in this issue. Now that we can split editors and terminals I think it would be a good idea to have the ability to activate **editors/terminals** on mouse over.  As the owner of the terminal component I reject the idea that this should be a terminal only feature, it should apply to the editors as well at the very least. The PR revealed some usability issues with the feature and it was marked as out of scope by someone intimately familiar with the editor. Realistically this would also not get prioritized for the team to work on so it would only come in via a PR, and we've already attempted and spent a bunch of time on it reviewing, testing and fixing in this issue. It would also be a major feature in terms of work required to implement and maintain, but it would very rarely get used because it's an opt-in. The bot helps us manage issues and it messed up here by not applying the feature request label and starting the timer in May, if it did maybe it would have reached 20 upvotes in time but I also made a mistake by not closing it out as a duplicate of this issue to begin with."
technical,"This actually duplicates this issue, which was attempted and reverted in this. We would only consider this as a workbench level feature, not just the terminal and it's closed as out of scope on that issue. Closing as duplicate. Sorry but the justification that the bot gave for closing this issue, for me it's not clear and valid (no wonder that it received 17 downvotes). On that attempt that he  tried and reverted, was focus on hover for the **EditorGroup** not for the **terminal**. Also, those references are from 2 years ago. Until now, what has been done taking into account this? Was there a change plan created for this or something? Because it looks like the community want this (not only from 2018, but also nowadays since my request received  20 upvotes). Can you pls re-check this and reopen **or** provide a more clear and valid justification? If you need any help, pls tell me."
technical,":slightly smiling face: This feature request received a sufficient number of community upvotes and we moved it to our backlog. To learn more about how we handle feature requests, please see our documentation. Happy Coding! The bot mishaved, it took 9 months to reach 20 upvotes whereas it's meant to only allow 2 months. Sorry to disappoint but I'm going to close the issue as it didn't reach that bar fast enough."
technical," This actually duplicates this issue, which was attempted and reverted in this. We would only consider this as a workbench level feature, not just the terminal and it's closed as out of scope on that issue. Closing as duplicate."
technical," This feature request is now a candidate for our backlog. The community has 60 days to upvote the issue. If it receives 20 upvotes we will move it to our backlog. If not, we will close it. To learn more about how we handle feature requests, please see our documentation. Happy Coding!"
technical,"The bot mishaved, it took 9 months to reach 20 upvotes whereas it's meant to only allow 2 months. Sorry to disappoint but I'm going to close the issue as it didn't reach that bar fast enough. Why do you think that it took 9 months? The announcement that 20 upvotes are needed was made on Nov 5, 2020 and less than three weeks later (on Nov 21, 2020) the required number of upvotes was reached. Or am I missing something?"
technical,"I don't understand why Windows Terminal Preview is favoring Xterm/Gnome compatibility over cmd.exe compatibility, especially for Ctrl+V. I think that will add frustration and will slow adoption from Windows users. But, I tried the recommended bindings to make it work. After reading the documentation I found where to put the recommended snippet (in the global section, in a ""keybindings"" array, but it is also important to notice the existing keybindings array, because if you paste a new one at then it gets overwritten by the empty array at the bottom. Yay .json!  Anyway, paste this in over the empty keybindings array at the bottom: And please consider favoring Windows compatibility, at least for Ctrl+V. Or make it easier to enable instead of requiring editing of error-prone .json files. :tada:This issue was addressed in #5217, which has now been successfully released as Windows Terminal Preview v0.11.1121.0.:tada:  Handy links"
technical,"Given that ""Ctrl+C"" is now default copy keybinding on new installations. how will standard ""Ctrl+C"" for SIGINT work?  Imagine a long-running console program with some text selected and Ctrl+C typed. What will Windows Terminal do? Copy? Interrupt the program? Both? carlos-zamora * If the user is using the new keybindings: - If text is selected: <kbdCtrl+c</kbd will copy the selected text. - If text is **NOT** selected: <kbdCtrl+c</kbd will send a interrupt, the same way <kbdCtrl+c</kbd usually behaves.  If the user doesn't like that behavior, it's pretty trivial to remove the new <kbdCtrl+c</kbd binding from their settings.json, and rely on the <kbdCtrl+shift+c</kbd binding that's in defaults.json"
technical, cmd.exe didn't support these for years and finally added them quite recently. Even more puzzling to have these missing in the new one.
technical,"cmd.exe didn't support these for years and finally added them quite recently. Even more puzzling to have these missing in the new one. Copy and Paste was added in 0.3, if you installed 0.3 (or later), they were there by default. If you upgraded to 0.3, you needed to add them manually:"
technical,"* If the user is using the new keybindings: - If text is selected: <kbdCtrl+c</kbd will copy the selected text. - If text is **NOT** selected: <kbdCtrl+c</kbd will send a interrupt, the same way <kbdCtrl+c</kbd usually behaves.  If the user doesn't like that behavior, it's pretty trivial to remove the new <kbdCtrl+c</kbd binding from their settings.json, and rely on the <kbdCtrl+shift+c</kbd binding that's in defaults.json Does Enter now copy text by default? I realized when switching from cmd.exe to the new terminal that I use that to copy text a lot. I'm not sure if there is any significant disadvantage to making that the default, and it seems that the new terminal should, where possible, ease the transition from cmd.exe."
technical,"Copy and Paste was added in 0.3, if you installed 0.3 (or later), they were there by default. If you upgraded to 0.3, you needed to add them manually: For the curious,  here is the PR where they added the default keybindings for copy and paste and they discuss the challenges of using Ctrl+C by default.  There is also #2285 that has extensive discussion about using Ctrl+C for copy vs emitting SIGINT."
technical,":tada:This issue was addressed in #5217, which has now been successfully released as Windows Terminal Preview v0.11.1121.0.:tada:  Handy links Given that ""Ctrl+C"" is now default copy keybinding on new installations. how will standard ""Ctrl+C"" for SIGINT work?  Imagine a long-running console program with some text selected and Ctrl+C typed. What will Windows Terminal do? Copy? Interrupt the program? Both? carlos-zamora"
technical,"I would've expected that the shells hosted by Terminal would interpret the keystrokes. So since CTRL+C/CTRL+V were recently added to cmd.exe, I would've expected them to be passed down the shell and work as expected.  If you are using a different shell (PowerShell, Bash, etc), then I would expect they should be passed down to that shell for it to interpret as expected.  I find it odd that Terminal is interpreting these keys instead of the shell currently in use. I would only expect Terminal to try to interpret any keys you have defined in the key-bindings. Otherwise yeah, they should just be interpreted by the shell in use, so as to provide consistency with what people expect in each shell. Hi there, while win+c is working great, win+v does not, I disabled win+v shortcut in Explorer registry section since win+v is special paste in Windows now, but terminal does not respond to win+v So my goal to mimic cmd+c and cmd+v behavior, and it's  99% ready, the only thing left  is just win+v handling with windows terminal"
technical,"this isn't the right issue for your comment, this issue is for <kbdctrl</kbd. I don't understand why Windows Terminal Preview is favoring Xterm/Gnome compatibility over cmd.exe compatibility, especially for Ctrl+V. I think that will add frustration and will slow adoption from Windows users. But, I tried the recommended bindings to make it work. After reading the documentation I found where to put the recommended snippet (in the global section, in a ""keybindings"" array, but it is also important to notice the existing keybindings array, because if you paste a new one at then it gets overwritten by the empty array at the bottom. Yay .json!  Anyway, paste this in over the empty keybindings array at the bottom: And please consider favoring Windows compatibility, at least for Ctrl+V. Or make it easier to enable instead of requiring editing of error-prone .json files."
technical,"I'm using Version: 0.5.2762.0, and was tripped up by this today.  While I agree that a mapping of ""copy"" = Ctrl+C is understood as the Windows default, it will also cause confusion with Linux based terminal applications.  My solution for now is to map: which maps to the muscle memory I've built using MacOS, however the linter in VSCode told me only ctrl|shift|alt are acceptable modifier keys.  Is there a way to allow the win key to be a modifier as well for custom key bindings? I feel pretty strongly about this -- I'm a UNIX user (also the author of the popular tcell package for building console interface apps in Golang), and understand the need to separate copy from delivering the control-c to the application.  There are challenging trade-offs here.  However, I really liked legacy conhost's solution to this -- if I have text selected, then Ctrl-C is copy just like windows.  And Ctrl-V is paste.  If I have no selected text, then CTRL-C is passed through to the application.  IMO, these should be the default settings.  They should be customizable, in case a user wants to change them.  (On macOS its CMD-C and CMD-V, and I've bound those for Windows Terminal, but what really messes me up is that in every *other* app on Windows its CTRL. )  To be honest what I really wish is that I could change the rest of Windows to use ALT-C and ALT-V (or META-C and META-V) for copy/paste, which would not collide with most other uses but keep the control bindings available for terminal windows.  I do understand why that's not practical -- given the fact that each application manages it's own key bindings.  Another possible solution is to offer some extra modifier (e.g. SHIFT-CTRL-) that would send the control sequence to the application running in the window.  Even as a UNIX user, I know for a fact that I use CTRL-C and CTRL-V occasionally in the terminal to control my apps, but far less frequently than I use copy-paste.   So I'd prefer to require keyboard-twister in the uncommon case, and be able to use copy-paste like I do everywhere else by default."
technical,"To be honest, for users used to using Terminal from Linux, Ubuntu, OSX, etc. I don't expect ctrl-V to paste. And seeing how a lot of the purpose of terminal is geared in that direction (to finally have Windows be able to start having a culture around working in the terminal instead of in GUI apps, catering towards those users seems preferable). I honestly don't know how frequently verbatim insert mode is used by folks in linux/osx.  My uneducated guess is ""very infrequently.""  On the other hand, I suspect Ctrl+V is used quite frequently by folks familiar with cmd.exe and PowerShell.  If my uneducated assumptions are accurate, then having Ctrl+V perform paste makes the experience significantly better for Windows-background users at minimal cost to Linux/OSX-background users."
technical,"We can 100% bind <kbdCtrl+C</kbd by default, now that we have support for passing through bindings that didn't trigger an action. We can **100% not** bind <kbdCtrl+V</kbd by default. I know Ctrl+C cancels the current execution, but what does Ctrl+V do? Out of curiosity."
technical,"The same way as powershell, ctrl+c should copy if anything selected and break if nothing selected. Even cmd have it - Enter to copy if there is any selection and execute if there is no selection. Though Enter never been really convenient.  There is specifically Break button to enforce break of execution in case of emergency, if there is selection. I would've expected that the shells hosted by Terminal would interpret the keystrokes. So since CTRL+C/CTRL+V were recently added to cmd.exe, I would've expected them to be passed down the shell and work as expected.  If you are using a different shell (PowerShell, Bash, etc), then I would expect they should be passed down to that shell for it to interpret as expected.  I find it odd that Terminal is interpreting these keys instead of the shell currently in use. I would only expect Terminal to try to interpret any keys you have defined in the key-bindings. Otherwise yeah, they should just be interpreted by the shell in use, so as to provide consistency with what people expect in each shell."
technical,"If it will be ""smart"" already, then it's even more puzzling why this isn't already bound by default (again, as it is in VS Code terminal, where I think people seem to be mostly happy with it) I'm curious if we have any data or telemetry to indicate what percentage of users prefer Ctrl+C and Ctrl+V to copy/paste. My intuition tells me it'll be the majority. Curious how many people turn on QuickEdit mode in cmd.exe."
technical,"I am new to Microsoft terminal (v0.6.2951.0) and am starting to warm up to the idea of having access to powershell, cmd, ubuntu/wsl and azure cloud shell baked directly into a native Windows 10 app.  BUT, I was pulling my hair out today trying to copy the contents of a Ubuntu console, and would have assumed this functionality was baked in by default to a modern Win10 app. I acknowledge the ctrl-c issues above, but why not enable the copy functionality through a right mouse click, like that implemented in the more modern cmd versions? If you open cmd using Windows terminal there is no right click options enabled, whereas using the old cmd you get  It'd be great if a future version of Windows Terminal baked this in by default across all shells - it's really useful to have I'm using Version: 0.5.2762.0, and was tripped up by this today.  While I agree that a mapping of ""copy"" = Ctrl+C is understood as the Windows default, it will also cause confusion with Linux based terminal applications.  My solution for now is to map: which maps to the muscle memory I've built using MacOS, however the linter in VSCode told me only ctrl|shift|alt are acceptable modifier keys.  Is there a way to allow the win key to be a modifier as well for custom key bindings?"
technical,"The fact people think this keybinding is obscure demonstrates ignorance. I recommend learning to use the standard keybindings before bashing them. (heh)  What ctrl+v does depends on what terminal application is running. For people who use terminals on a daily basis, this usually means ""page down"" for pagers and editors (reading a man page, emacs, vim). alt-v is usually page up. Counter to your intuition, this is  quite frequently used .  At this point, why not steal the keybindings for ctrl-a, ctrl-t, and crtl-w, since windows users expect them to be select all, new tab, and close tab? The fact is, these are also very useful keybindings in terminal applications. We should encourage people to leave their comfort zone and learn the terminal, instead of defaulting to a Frankenstein with inconsistent exceptions. I've read the other ""copy not working"" bugs and the universal suggestion seems to be ""just create your own keybinding"".  It's quite puzzling for a *Windows* terminal to not come with the most commonly used *Windows* shortcuts ever.  I'd suggest these two are added to the product, and people can instead *change* (or remove them) if they wish. I guess that's a much more intuitive default.  For others wondering why they are missing, just add these two to your keybindings:  Just to add to the discussion: the built-in Terminal in VS Code supports this out of the box too, which is awesome."
technical,"If you bind <kbdCtrl+C</kbd, it will be ""smart"". There is no way to make <kbdCtrl+V</kbd smart. If it will be ""smart"" already, then it's even more puzzling why this isn't already bound by default (again, as it is in VS Code terminal, where I think people seem to be mostly happy with it)"
technical,"Totally agree on making this ""smart"" depending on whether there's a selection at all.  BTW, this is *exactly* how the integrated terminal in VS Code behaves, which is awesome. If you bind <kbdCtrl+C</kbd, it will be ""smart"". There is no way to make <kbdCtrl+V</kbd smart."
technical,"Totally agree on making this ""smart"" depending on whether there's a selection at all.  BTW, this is *exactly* how the integrated terminal in VS Code behaves, which is awesome. My (admittedly uneducated) guess is the percentage of users expecting Ctrl+V to result in paste is substantially larger than the percentage of users expecting Ctrl+V to result in ""verbatim insert mode."""
technical,"Does Enter now copy text by default? I realized when switching from cmd.exe to the new terminal that I use that to copy text a lot. I'm not sure if there is any significant disadvantage to making that the default, and it seems that the new terminal should, where possible, ease the transition from cmd.exe. nope, but that is a lot less common of a scenario for our users then <kbdCtrl+C</kbd for copy. There's a balance we need to strike between ""keeping the old behavior of conhost"" and ""making space to create a better experience"". For those of our users who actually do want copy on enter, adding it isn't terribly difficult:  but I'd bet most people weren't even aware that feature existed in the original conhost ˜"
technical,"Does Enter now copy text by default? I realized when switching from cmd.exe to the new terminal that I use that to copy text a lot. I'm not sure if there is any significant disadvantage to making that the default, and it seems that the new terminal should, where possible, ease the transition from cmd.exe. Okay great. Now how do we disable this?  -- sad linux user  Edit: Nevermind. Found it in settings.json and deleted it."
technical,"For the curious,  here is the PR where they added the default keybindings for copy and paste and they discuss the challenges of using Ctrl+C by default.  There is also #2285 that has extensive discussion about using Ctrl+C for copy vs emitting SIGINT. Please don't do this. Ctrl+C already has meaning inside terminals that long outdates copy/paste."
technical,"The problem with a terminal application commandeering these shortcuts is that when you run applications inside of the terminal and find yourself with a need to enter that key combination inside the terminal, your SOL. Registered voter against the new default, but I guess the change is in, so will go delete it. The fact people think this keybinding is obscure demonstrates ignorance. I recommend learning to use the standard keybindings before bashing them. (heh)  What ctrl+v does depends on what terminal application is running. For people who use terminals on a daily basis, this usually means ""page down"" for pagers and editors (reading a man page, emacs, vim). alt-v is usually page up. Counter to your intuition, this is  quite frequently used .  At this point, why not steal the keybindings for ctrl-a, ctrl-t, and crtl-w, since windows users expect them to be select all, new tab, and close tab? The fact is, these are also very useful keybindings in terminal applications. We should encourage people to leave their comfort zone and learn the terminal, instead of defaulting to a Frankenstein with inconsistent exceptions."
technical,"Coming from Linux, it was confusing to ctrl-v in vim and see my last pasta. Had to google on how to get the standard behavior. The problem with a terminal application commandeering these shortcuts is that when you run applications inside of the terminal and find yourself with a need to enter that key combination inside the terminal, your SOL. Registered voter against the new default, but I guess the change is in, so will go delete it."
technical,"I'm curious if we have any data or telemetry to indicate what percentage of users prefer Ctrl+C and Ctrl+V to copy/paste. My intuition tells me it'll be the majority. Curious how many people turn on QuickEdit mode in cmd.exe. The same way as powershell, ctrl+c should copy if anything selected and break if nothing selected. Even cmd have it - Enter to copy if there is any selection and execute if there is no selection. Though Enter never been really convenient.  There is specifically Break button to enforce break of execution in case of emergency, if there is selection."
technical,"Hi there, while win+c is working great, win+v does not, I disabled win+v shortcut in Explorer registry section since win+v is special paste in Windows now, but terminal does not respond to win+v So my goal to mimic cmd+c and cmd+v behavior, and it's  99% ready, the only thing left  is just win+v handling with windows terminal this isn't the right issue for your comment, this issue is for <kbdctrl</kbd."
technical,"My (admittedly uneducated) guess is the percentage of users expecting Ctrl+V to result in paste is substantially larger than the percentage of users expecting Ctrl+V to result in ""verbatim insert mode."" To be honest, for users used to using Terminal from Linux, Ubuntu, OSX, etc. I don't expect ctrl-V to paste. And seeing how a lot of the purpose of terminal is geared in that direction (to finally have Windows be able to start having a culture around working in the terminal instead of in GUI apps, catering towards those users seems preferable)."
technical,"I feel pretty strongly about this -- I'm a UNIX user (also the author of the popular tcell package for building console interface apps in Golang), and understand the need to separate copy from delivering the control-c to the application.  There are challenging trade-offs here.  However, I really liked legacy conhost's solution to this -- if I have text selected, then Ctrl-C is copy just like windows.  And Ctrl-V is paste.  If I have no selected text, then CTRL-C is passed through to the application.  IMO, these should be the default settings.  They should be customizable, in case a user wants to change them.  (On macOS its CMD-C and CMD-V, and I've bound those for Windows Terminal, but what really messes me up is that in every *other* app on Windows its CTRL. )  To be honest what I really wish is that I could change the rest of Windows to use ALT-C and ALT-V (or META-C and META-V) for copy/paste, which would not collide with most other uses but keep the control bindings available for terminal windows.  I do understand why that's not practical -- given the fact that each application manages it's own key bindings.  Another possible solution is to offer some extra modifier (e.g. SHIFT-CTRL-) that would send the control sequence to the application running in the window.  Even as a UNIX user, I know for a fact that I use CTRL-C and CTRL-V occasionally in the terminal to control my apps, but far less frequently than I use copy-paste.   So I'd prefer to require keyboard-twister in the uncommon case, and be able to use copy-paste like I do everywhere else by default. Totally agree on making this ""smart"" depending on whether there's a selection at all.  BTW, this is *exactly* how the integrated terminal in VS Code behaves, which is awesome."
technical,"This is a delightful vignette about why we have rebindable key actions, and why we don't want to be prescriptive with user experience where ""terminal input"" is concerned. We can 100% bind <kbdCtrl+C</kbd by default, now that we have support for passing through bindings that didn't trigger an action. We can **100% not** bind <kbdCtrl+V</kbd by default."
technical,"This is a delightful vignette about why we have rebindable key actions, and why we don't want to be prescriptive with user experience where ""terminal input"" is concerned. :thinking: it looks a duplicate of this."
technical,"Thanks for your comments. Just pushed another commit to fix typo. If this get approved, I'll squash the rest commits. At Daimler, our workaround was patching x/net to keep PingTimeout default at 15 seconds but enforce a ReadIdleTimeout of 30 seconds. I guess this is sufficient for Kubernetes work loads, and at the same time should not really trigger ""additional"" keep-alive efforts in production too often. If I get it right, this PR sets the default to 90/2=45 seconds, and t1.IdleConnTimeout is never set in the entire Kubernetes organization to anything but the default values. maybe it is me, but I prefer hardcoded timeouts than formulas, is not easy to me to understand which values I'm using this way. Personally, as I user, I'd like to use the defaults timeouts provided or the ones I set directly, is my mistake if I set them wrong. I found weird that I set pingTimeout to 3 seconds for whatever reason and something configures it to 15 seconds without I'm noticing, or I'm not understanding correctly it?"
technical,"At Daimler, our workaround was patching x/net to keep PingTimeout default at 15 seconds but enforce a ReadIdleTimeout of 30 seconds. I guess this is sufficient for Kubernetes work loads, and at the same time should not really trigger ""additional"" keep-alive efforts in production too often. If I get it right, this PR sets the default to 90/2=45 seconds, and t1.IdleConnTimeout is never set in the entire Kubernetes organization to anything but the default values. maybe it is me, but I prefer hardcoded timeouts than formulas, is not easy to me to understand which values I'm using this way. Personally, as I user, I'd like to use the defaults timeouts provided or the ones I set directly, is my mistake if I set them wrong. I found weird that I set pingTimeout to 3 seconds for whatever reason and something configures it to 15 seconds without I'm noticing, or I'm not understanding correctly it? For now, http2 Transport configuration is purely internal. We do not expose such configuration to users, that's probably why we don't need release note. So I think we'd better make it reasonable even with some confusing formula.(still can be explained by sufficient comments in code, It should not be a big problem.)"
technical," GitHub didn't allow me to request PR reviews from the following users. Note that only kubernetes members and repo collaborators can review this PR, and authors cannot review their own PRs."
technical,:thinking: it looks a duplicate of this. he is right. please check the other one out and ensure that fixes your problem as well.
technical,"GitHub didn't allow me to request PR reviews from the following users. Note that only kubernetes members and repo collaborators can review this PR, and authors cannot review their own PRs. Hi. Thanks for your PR.  I'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step. Once the patch is verified, the new status will be reflected by the ok-to-test label.  I understand the commands that are listed here. Instructions for interacting with me using PR comments are available here.  If you have questions or suggestions related to my behavior, please file an issue against the kubernetes/test-infra repository."
technical,"Thx, I am on mobile right now. Will check out for another PR later. I think it doesn' t hurt if we test it in the meantime :sweat smile:"
technical,"Thx, I am on mobile right now. Will check out for another PR later. Looks like the update to golang.org/x/sys package breaks azure disk driver? It says event for azuredisk-volume-tester. Code problem should not cause operation timeout. I think it maybe overload of test infra. I will retry next morning."
technical,"Thx, I am on mobile right now. Will check out for another PR later. Not really, original solution proposed in #94844 uses a modified http2 package which breaks backward compatibility and is not merged into golang/net master. And he didn't push another fix till I proposed this fix. To be honest(no offense), code and commit history in #94844 looks nasty..."
technical,"Thx, I am on mobile right now. Will check out for another PR later. Other options are also affected:"
technical,"Thx, I am on mobile right now. Will check out for another PR later. please comment on your concerns in the other PR. that is already approved and is likely to merge first."
technical,"Thx, I am on mobile right now. Will check out for another PR later. thank you both for working to resolve the network connection issue. However, the discussion in this PR and in #94844 is distracting from the main goal of fixing the issue, and is violating the Kubernetes code of conduct with insulting/derogatory comments. I've locked the discussion here and in #94844 for that reason, and will ask them to take a look at the approaches in the two PRs and settle on a way to make use of the PingTimeout capability he added."
technical,"Other options are also affected: Thanks for your comments. Just pushed another commit to fix typo. If this get approved, I'll squash the rest commits."
technical,"I think it doesn' t hurt if we test it in the meantime :sweat smile: Thanks, seem one test job got OOMKilled. I will re-trigger it later."
technical,I think it doesn' t hurt if we test it in the meantime :sweat smile: This is not a competition and please respect the code of conduct. There are certain comments in this thread that are clearly uncalled-for.
technical,"please comment on your concerns in the other PR. that is already approved and is likely to merge first. Thx, I am on mobile right now. Will check out for another PR later."
technical,"Thanks to everyone for the feedback, that's already a lot of food on our plate, we won't be able to implement *all* your requests, but we'll definitively do our best to fulfill your expectations. We're currently focusing on performance. As we will need your help, we might pick some priority issues and offer rewards thanks to our Open Collective sponsors. Stay tuned. :wave: Hello everyone! Summer is coming, and so is the implementation period for Jekyll 4.0. That's right, it's time to get some breaking changes in. To accommodate for this, we're opening this issue to collect interesting ideas that people would like to see implemented in 4.0.   Keep in mind that even if an idea receives a lot of support, there's no guarantee that it'll get implemented   ” that depends on if someone is free to actually implement it. We're all volunteers here, keep that in mind.  Feel free to revive old feature requests, too, just not something that we've explicitly rejected.  For an organized view of how we're consolidating ideas and features, check out our Project board"
technical,"It would be great if include would take preference over exclude  I've submitted a PR that should handle what you're looking for  Just wanted to share that we confirmed that that PR solves the include vs exclude issue, so it would be very neat if that could land in 4.0 (I suggest this without understanding what would actually be involved in doing it)  I'd love to see performance work done regarding glob patterns in frontmatter defaults. I think it's a tremendously useful feature, but it's hampered by the fact that the more useful it is for a project, the more detrimental it is to build times.  I use this feature heavily in scenarios where I have a collection of documents which represents different variations of a type of content. For example, in the Sentry marketing resources ""resources"" is a collection, which is broken up into folders representing podcasts, videos, and pdfs. Each of those folders has it's own defaults that are appropriate for the given medium.  In smaller collections there isn't much of an issue, but on our docs site, where a collection is 250+ pages, using a wildcard added 2s to a .8s build."
technical,I think having an automated tool handle the sass and javascript bits may speed up the compile time of Jekyll  How is that..?? A big reason for me when doing frontend work and the Sass folder has many nested folders and files Jekyll sass can be slow. I have found letting web pack handle the sass instead of Jekyll improves speed and developer happiness.
technical,"Oh, and this wishlist reminded me on an argument I had with pathawks over in  a couple of months ago.  Would be handy to have a way to support internationalisation! (Including hreflangs for SEO purposes) Add the ability to place the post and its files in the same folder."
technical,"I vote for #6293 Markdown links bidi support, in by default. Adding fetch / get for datafiles to core (or as an ""official"" addon/plugin) would be great, see the unloved / public domain source. The code itself is about 40 lines."
technical,"Convincible Hugo is much faster because it's written in natively compiled Go rather than interpreted Ruby. We're not planning to change that. Ah, fair enough “ but then, within the confines of interpreted Ruby, speed is on my wishlist! Mainly an issue when you get to 1,000+ page blogs/sites... I'd love to keep using Jekyll even here and beyond."
technical,"it would be great to have more interactions with github, like pulling profile information (contributions, avatar, ), releases data - and convert it for proper publication. this would be vital for software website to create team and download pages that would not require multiple code changes. currently working with current available plugins, this solution lacks stability since there's too many plugins that should be constantly updated. would be glad to help if this will go further! also background section could get some improvements - building website that will look good on regular displays and retina quiet challenging. creating multiple backgrounds for could be the solution to decrease loading time. or middle state like page loading to comfort browsing"
technical,"Include all enabled plugins in site.plugins, including those available from the Gemfile.   Can you add a bit more detail here? How is this different from the current state?  Plugins using the group :jekyll plugins in a Gemfile are not visible in liquid as site.plugins. Only plugins in the  config.yml are visible in site.plugins.  This is useful for conditional behaviour like alzeih This cannot be implemented because Liquid is parsed and rendered by the liquid gem. Jekyll only provides the means to use and extend Liquid constructs for use in template files.  Ah, I see.  This mentions using partials as a workaround Having all plugins available in site.plugins would allow for this."
technical,"We'll rely on sassc implementation, that is currently removing dependency to Ruby sass Awesome. Thank you!"
technical,build in support for content blocks and components Can you elaborate? Do you mean something like includes?
technical,"Why can't you do this at present? Convincible At present, static files within a  posts directory are ignored.."
technical,"Oh, and: the only thing that keeps me continually considering whether I should switch to Hugo is speed. Can we find ways to make Jekyll: * Much faster at generating the site? (why is Hugo so many times faster? - IDK) * Much better at doing it incrementally? (perhaps with manual dependency hints) Convincible Hugo is much faster because it's written in natively compiled Go rather than interpreted Ruby. We're not planning to change that."
technical,"there's a plugin you can use that allows you to place static files in the same folder as the post. Convincible On the hardware side of things, when there is a complaint about Jekyll taking a long time to generate pages, one could respond by saying get a processor with more cores. But from what I understand, Jekyll does not seem to leverage well multi-core processing when it comes to generating files. It's a petty since taking advantage of many cores could decrease page generation by 4-8X considering that many personal computers these days have processors with about that many cores/threads.  So leveraging the multi-thread capabilities of modern processors when generating pages is the suggestion."
technical,What are the plans for (Dart) Sass support in Jekyll 4.0 now that the Ruby implementation is deprecated? I'm  very  interested to hear the responses from the maintainers. Could libsass be used instead?
technical,"You can use Disqus or some similar software for that, it's way outside the scope of Jekyll as a  static  site generator. Could you consider supporting multi-language & language switching by processing files in po format, or by some other means?"
technical,"Using the quik library for scaffolding. If you scaffold a new jekyll theme or plugin now all the code is ""hard-coded"" / ""hard-wired"". The scaffolding code itself is also ""hard-wired""  / ""hard-coded"", that is, not (re)usable for other projects. Using a (simple) ""generic"" scaffolding library such as quik you can turn any git(hub) repo (or directory/folder or zip archive) into a parametrized and scripted template scaffold. See the Jekyll Quick Starter Template / Scaffold - Build Your Own (Gem-Packaged) Theme as an example.  PS: Background / References - Talk Notes - Quik - The Missing Project Scaffolder (Library) for Ruby - Quick Start Your Ruby Gems, Jekyll Websites, Jekyll Themes 'n' More Custom HTTP headers compatibility for Github Pages"
technical,"It looks like so, as Jekyll is fully agnostic when it comes what's get generated in the front-end, it just transforms files into HTML. You can use plugins like jekyll-assets and/or jekyll-cloudinary to help you deal with responsive images. DirtyF thanks for the input! im looking for every jekyll plugin i can find, put majority is out of date and has no support, so it pushed me to propose these functions into core (not counting jekyll-originated plugins) should i delete my comments? don't want to overload the tread since im thinking it will grow and going tho useless comments would add some discomfort"
technical,"alzeih This cannot be implemented because Liquid is parsed and rendered by the liquid gem. Jekyll only provides the means to use and extend Liquid constructs for use in template files.  Ah, I see.  This mentions using partials as a workaround Having all plugins available in site.plugins would allow for this. Don't parse these conditional blocks unless the plugin is available, so code inside doesn't crash if the plugin isn't available.   alzeih This cannot be implemented because Liquid is parsed and rendered by the liquid gem. Jekyll only provides the means to use and extend Liquid constructs for use in template files.  This would be such a useful feature, I wonder if we could find a workaround. Maybe instead of using the regular {% if %} tag, we create a new tag that acts like the {% raw %} tag (so Liquid does not parse the contents), and then  Jekyll  will render the contents if the plugin is installed.  This is just an ideas list. Let's not let implementation details get in the way at this stage. We are programmers,  anything  is possible."
technical,"Include all enabled plugins in site.plugins, including those available from the Gemfile.  Can you add a bit more detail here? How is this different from the current state? Don't parse these conditional blocks unless the plugin is available, so code inside doesn't crash if the plugin isn't available.  alzeih This cannot be implemented because Liquid is parsed and rendered by the liquid gem. Jekyll only provides the means to use and extend Liquid constructs for use in template files."
technical,also background section could get some improvements - building website that will look good on regular displays and retina quiet challenging. creating multiple backgrounds for could be the solution to decrease loading time. or middle state like page loading to comfort browsing GitHub data should be part of GitHub-metadata plugin not Jekyll-core
technical,"I would like to see reading metadata in separate files Great suggestion. I'll add a use case: - Create a collection of static image files - Add metadata (like alt text, captions, dimensions) in sidecar files - Loop through them to create a rich image gallery  Jekyll CMS services like Siteleaf could make great use of this feature. Siteleaf uploads static assets to an  uploads collection. cc sskylar"
technical,"Could you consider supporting multi-language & language switching by processing files in po format, or by some other means? Hello Jekyll team!  I love working with collections, but the current option output: true|false enforces generation of either all or none of the pages. Having more control about which item of the collection is output would be appreciated.  The current way of putting published: false suppresses output, but also makes the document inaccessible (it does not show up while looping through the collection).  Thank you for the good work so far!"
technical,"I'd love to see Docker volume mounts work with Jekyll and Docker for Windows.  Rails, Flask, Phoenix and Node applications that I've worked with all work fine in Docker for Windows. You volume mount in the code, make a change, and in milliseconds your change is reflected and ready to be seen in a browser.  With Jekyll 3.8, this does not happen. The file change is reflected inside of the container, but the jekyll serve command doesn't regenerate the file. Using --force polling also doesn't work.  Everything works fine under WSL (without Docker) but Jekyll is the only reason I have Ruby installed on my machine, mainly because every other app from every other language / framework works wonderfully with Docker.  Call me crazy but I don't think 4.0 should ship until this is addressed. Not being able to Dockerize a Jekyll site for development with Docker for Windows is a huge bummer. I have found letting web pack handle the sass instead of Jekyll improves speed  Jekyll Core doesn't handle pre-processing css and javascript. We have two plugins (both are written in Ruby): jekyll-sass-converter (included in jekyll by default) and jekyll-coffeescript.  The speed improvements are probably due to the difference between ruby-sass and node-sass"
technical,"I'd love to see automatic generation of tag and category archive pages in core plugins like jekyll-archivesdo this, but none are whitelisted in GitHub Pages). I wrote up #6952 with my detailed rationale and a proposal of how it could work.  Here's the TL,DR:  Having vanilla Jekyll auto-generate archive pages for every tag and category is probably a good idea now, and I think I've come up with a good, clean way to do it while staying true to Jekyll's philosophy.  Code-wise, this snippet gets most of it across: I know that this is markdown and not necessarily jekyll related, but my life would have been a thousand times easier if jekyll supported inline footnotes like so  [Footnote, p. 123] as apposed to the more tedious:   This is first reference[one], this is the second[two]"
technical,"tyler-insight, I have a PR [open][] for this in Minima, the default theme you get if you run jekyll new. In that case, I based my cache off of lib/site template, although I probably should go through the Cookbook to make sure I've covered all bases. I proposed this in the old Jekyll 4 wishlist thread, but it'd be really nice to be able to have schemas for data, collections, frontmatter, etc."
technical,I would like to see an automated tool like Webpack integrated into Jekyll to handle Sass and Javascript. Some of the biggest reasons are auto-prefixing for Sass and the ability to use ES6 syntax that can automatically be polyfilled with babel. I think having an automated tool handle the sass and javascript bits may speed up the compile time of Jekyll. I think having an automated tool handle the sass and javascript bits may speed up the compile time of Jekyll  How is that..??
technical,"letrastudio So the more I've looked into implementing i18n in Core, the less feasible it became practically. There's huge performance drawbacks, even for sites that only use one language, the time spent rewriting a huge part of the Core internals would very likely be better spent working on other features, and we've garnered feedback from some big users of Jekyll that for them, performance improvements are more important than a huge new breaking feature. Obviously this shouldn't influence our decision as a project all that much, but it did provide us with insight into why a full i18n implementation in Core wouldn't be feasible. What I think we   could   do is provide baseline APIs upon which a (maybe officially supported) plugin could operate. If you have any such ideas, feel free to shoot them my direction! I understand, and I feel the same way about performance. However, I am not completely discouraged! Wouldn't performance impact depend largely on how the feature is designed? I think  that my proposed solution wouldn't significantly impact performance for single-language sites (and it wouldn't break Jekyll 3 sites at all). Big disclaimer though:  am not a Ruby programmer so I could be completely wrong.  I think I'll write up my ideas anyway and post them as a new issue ” even if they're not feasible for Core, I hope they'll contribute to the discussion, maybe even inspire some intrepid plugin developer. Or they might help spark some ideas for the baseline APIs you suggested (such low-level discussion is probably a bit out of my league).  And still, I've found that i18n is quite doable with vanilla Jekyll 3. IMHO content management can actually be better than with existing plugins, though it's a bit complex to work with on the Liquid side. Some small tweaks to make that easier might be worth implementing, I'd happily settle for making it feel like less of a hack."
technical,What would a Jekyll site look like as a Progressive Web App? Can you elaborate? I updated the initial comment/request with my thoughts on why this is a good thing to implement in the upcoming Jekyll 4.0 with some reference on how it affects the looks.
technical," I vote for #6293 Markdown links bidi support, in by default."
technical,"The plugin mentioned in the StackOverflow you linked to, jekyll-include-cache is the best option available out there. However, there is a related proposal for getting similar support in Core: #7108 .. ..and another (very distantly) related proposal at #7136  You may *subscribe* to the above PRs to stay notified about any developments on the proposals I was reading over the thoughts about Open Collective in the blog post HERE  Moving towards a means to maintain the list of plugins better might be to move those lists to yaml files and/or individual yaml files.  If each entry in a section of the list here: Available Plugins Then we can make it easier for maintenance. You would simply add a yaml file in the appropriate location that corresponds to your section. The file would have the fields needed to be listed. This would be name, url, description... Part of why that helps things is no merge conflicts even become possible. Refactoring that page becomes less impacted by people adding to the lists, and we can add in additional fields, like repository url that can be used to track the repository of the project in question. When a new breaking change comes into a PR, we can potentially use a bot/script to pull in all those files and if the repository url exists, that it checks for a given pattern (and ambitiously, opens a PR for an automated fix).  All this can potentially also be achieved with a yaml file per section, but we dont gain as much in terms of potential merge conflicts, but this is a fairly straightforward data set at that point, so this might not even be a concern. Opening hundreds of yaml files does have the potential of adding on time to generate the site.  Side note: This also makes it extremely easy to keep the list alphabetically ordered.  I know this is not exactly a solution to the idea: ""Create a comprehensive official plugin and theme directory site"" but it does cause a shift in how we store the data we already have thats more inline with recommendations Jekyll users would be used to. It also makes it a lot easier to pull that data out into some other repository to make a dedicated official plugin directory whenever that task might be taken on.  EDIT: Apologies for my poor wording....I regret not having proofed this better."
technical,"Would love to see a standardized implementation of a service worker built into Jekyll utilizing the methods in The offline cookbook. A way of configuring it so that you could easily designate what the 'shell app' is so that those files load from offline cache first, and then designating dynamic content (such as blog posts) that load from network first and only cache so many entries (since caching 100 blog posts is ridiculous).  I realize these things can be done now, but finding a way to configure it to work perfectly with Jekyll would be amazing and save a lot of people the struggle of figuring out how to properly implement a service worker for it. I would be grateful if you could resolve #6410."
technical,"It would be really beneficial for Jekyllers around the world if you guys could implement some sort of semi-automatic generation on .yml files based on a folder structure. This would be a massive improvement and a great way to document distributed services. I would like some nice to have plugin improvements:  - Include all enabled plugins in site.plugins, including those available from the Gemfile. - Something like {% if plugin ""foo"" %}... for code is that is conditional on having certain plugins available. - Don't parse these conditional blocks unless the plugin is available, so code inside doesn't crash if the plugin isn't available."
technical,"It would be really beneficial for Jekyllers around the world if you guys could implement some sort of semi-automatic generation on .yml files based on a folder structure. This would be a massive improvement and a great way to document distributed services. I would like to see a plugin that lets you loop through any directory's files. This could let you list, display, or include the files in the directory. This might look like"
technical,"I was reading over the thoughts about Open Collective in the blog post HERE  Moving towards a means to maintain the list of plugins better might be to move those lists to yaml files and/or individual yaml files.  If each entry in a section of the list here: Available Plugins Then we can make it easier for maintenance. You would simply add a yaml file in the appropriate location that corresponds to your section. The file would have the fields needed to be listed. This would be name, url, description... Part of why that helps things is no merge conflicts even become possible. Refactoring that page becomes less impacted by people adding to the lists, and we can add in additional fields, like repository url that can be used to track the repository of the project in question. When a new breaking change comes into a PR, we can potentially use a bot/script to pull in all those files and if the repository url exists, that it checks for a given pattern (and ambitiously, opens a PR for an automated fix).  All this can potentially also be achieved with a yaml file per section, but we dont gain as much in terms of potential merge conflicts, but this is a fairly straightforward data set at that point, so this might not even be a concern. Opening hundreds of yaml files does have the potential of adding on time to generate the site.  Side note: This also makes it extremely easy to keep the list alphabetically ordered.  I know this is not exactly a solution to the idea: ""Create a comprehensive official plugin and theme directory site"" but it does cause a shift in how we store the data we already have thats more inline with recommendations Jekyll users would be used to. It also makes it a lot easier to pull that data out into some other repository to make a dedicated official plugin directory whenever that task might be taken on.  EDIT: Apologies for my poor wording....I regret not having proofed this better. I would love to see better environment support.  **Update  For example site.url:  Right now site.url has an implicit env pattern that is hard coded for development (Source). This is very implizit, hard to understand and remember and not extendable. #5142  I would love to see this extended. Example:   The pattern could be: When jekyll calls a variable, it checks for <var-name.<env-name. The fallback is <var-name. So it does not matter if I configure just url order url.<env-name.  (This is a copy)"
technical,"One more suggestions (I'd be happy to help along) - The Jekyll Documention is fantastic and outstanding. But, what's wrong (or better how to make it more awesome)?  Currently the documentation is part of the jekyll repos and the layouts, styling, build scripts etc. are part of the documentation too - it's all mixed together. What's wrong with that?  I'd say with its own dedicated markdown files in a repo it would be easier to contribute / edit / retarget etc. and with a ""proper"" docu (remote) theme  in its own repo it would be easy to reuse and change the theme too.   A while ago (dare I say years) I started to ""clean up"" as an example the documentation to use ""plain vanilla"" markdown files in the manuscripts (book/documentation) format. See the Hyde Press Bookshelf live and all the source repos. For the suggestion for Jekyll 4.0 it would just be a  new theme repo and a new docs repo (with markdown only). Would be great to see some booklets / guides / tutorials that could be made easily from the new docs repo using some (selected) pages etc.   Let me (us) know what you think. I'd like an additional YAML file that defines site-wide variables so other programs can generate the file safely (instead of appending to  config.yml).  It could be better if we can have something like {% include vars.yml %} in  config.yml but I don't think it's necessary. Just allow loading extra variables from another file, even if the file name is hard-coded, would be enough.  A typical use case would be when I write a custom script that pre-processes some file and show in the information generated site with something like {{ site.my preprocessor.info }}. My current approach of appending to  config.yml would pollute the file. With another file enabled, I can keep my configuration file safe by ignoring the dedicated variable YML in .gitignore."
technical,"I proposed this in the old Jekyll 4 wishlist thread, but it'd be really nice to be able to have schemas for data, collections, frontmatter, etc. I'd like to see Multiple Outputs baked in so for each .md file we can output the usual html as well as say amp and json files too. For example:  layout: page, page-amp, page-json referencing page.html, page-amp.html, and page.json.json layout files respectively.  It might also be nice to to be able to specify to output the same html page at different locations/paths."
technical,"Let's keep this restricted to Jekyll features. We don't have any control over GitHub Pages. I'd love to see automatic generation of tag and category archive pages in core plugins like jekyll-archivesdo this, but none are whitelisted in GitHub Pages). I wrote up #6952 with my detailed rationale and a proposal of how it could work.  Here's the TL,DR:  Having vanilla Jekyll auto-generate archive pages for every tag and category is probably a good idea now, and I think I've come up with a good, clean way to do it while staying true to Jekyll's philosophy.  Code-wise, this snippet gets most of it across:"
technical,"Hello Jekyll team!  I love working with collections, but the current option output: true|false enforces generation of either all or none of the pages. Having more control about which item of the collection is output would be appreciated.  The current way of putting published: false suppresses output, but also makes the document inaccessible (it does not show up while looping through the collection).  Thank you for the good work so far! I'd say: **making it easier to filter content by date**.  At the moment, my understanding is posts/collection date field is coerced into a Ruby Date object. Comparison with where exp is impossible without workaround (casting a variable in  config.yml). There is a bit of background about this in #6581. I see this from a Jekyll user standpoint and I don't know whether this sits better as a where exp logic or as a date operator logic.  Thanks for making Jekyll such a nice tool :-) A +1 comment on geraldb proposal: I find remote data to be quite useful to avoid monolithic Jekyll setup. My use case would be to reference sub-projects on a main Jekyll websites, from remote JSON file and Atom feed."
technical,"Make internal linking easier. Like generating a JSON of internals links including anchors. iBug If I understand correctly, you could do this sort of thing using two (or more) config files. You can use this build command flag to specify multiple config files:   Settings in later files override settings in earlier files.  Granted, this option is only available as a command line flag, so it wouldn't work if you're using GitHub Pages to build your site."
technical,"So This is just a thought, I can try and make an issue for it with more details and thought through functionality, but I have noticed a potential gap in functionality.  If you use the  drafts folder, or have a post that is unpublished in your  posts folder, then you still need to move things around for your content, like images, to not get published to the generated site.  Since a lot of users use /assets/imgs/ folders for the beginning position of images associated to a post, I was thinking that we could make a new special key to use in posts that you add to the frontmatter.  The key would be something like asset paths: or asset path: that we merge to an array, and it can define any of the assets that should not be published if they do not match up to any other posts that are being published.  So if I have the following in my frontmatter: Then the post when in the drafts folder would not copy over any of that folder located. However, if a post is in  posts that contains the same frontmatter key/value, then it would get published as it was included in a post that is getting published. This could be done for individual file levels too.  The last part would be that unless included and exclusively used for an unpublished post, that the folder/files in question would perform /be acted on exactly as they already are now, where they will be included or excluded based on the  config.yml settings. In Reply  I am aware of that solution, but it only causes new problems.  **The Scenario is:** I have a config-files with 100 lines of config. One, maybe two line of this config needs to be changed for different environments.  With your solution, I need to duplicate 99 lines in three files (dev, staging, production) and manually sync them every time I make a change. **This is bound to fail!**  People will forget to sync the changes and the main idea of a staging system (to behave like the production system) will be lost.  **Other solutions would be** a. The one I describe  b. Allowing more than one config, one general-config-file for all env, and one config-file for each env. This is similar to the way rails does it. The env-config will overwrite the general-config in case that is needed. c. Your solution, but with the ability to ""include"" or ""reference"" other config files inside the one I call with the jekyll build command. For this I would say ""build with staging-config"" and inside staging config ""use this 1 line and include/use all other lines from this other config-file"". d. ?"
technical,"Don't parse these conditional blocks unless the plugin is available, so code inside doesn't crash if the plugin isn't available.  alzeih This cannot be implemented because Liquid is parsed and rendered by the liquid gem. Jekyll only provides the means to use and extend Liquid constructs for use in template files. Include all enabled plugins in site.plugins, including those available from the Gemfile.   Can you add a bit more detail here? How is this different from the current state?  Plugins using the group :jekyll plugins in a Gemfile are not visible in liquid as site.plugins. Only plugins in the  config.yml are visible in site.plugins.  This is useful for conditional behaviour like"
technical,"I would like some nice to have plugin improvements:  - Include all enabled plugins in site.plugins, including those available from the Gemfile. - Something like {% if plugin ""foo"" %}... for code is that is conditional on having certain plugins available. - Don't parse these conditional blocks unless the plugin is available, so code inside doesn't crash if the plugin isn't available. Include all enabled plugins in site.plugins, including those available from the Gemfile.  Can you add a bit more detail here? How is this different from the current state?"
technical,"You can set up all kinds of fallbacks by using multiple config files, as I mentioned in this comment. I've come across this use case myself ” I use  config.yml for production values, and an additional  config dev.yml for development overrides. When building locally I run: Is anyone working on i18n support? I noticed enthusiasm for it, and a lot of interest. I think I can make a significant contribution at the proposal/spec stage.  I've been working on a multilingual Jekyll site, and I've come up with a pretty good plugin-free solution! It's up at openhousemacau.com, hosted on GitHub Pages with Jekyll 3.7.3. It's still in active development, but the i18n part is fully functional, and can support any number of locales (not just the two it has now).  It's quite a complete solution, featuring sane (DRY) content management with fallbacks between locales, string translation, and date format localization. It even supports permalink localization, though we ended up not using it for this site. It works by using:  - Parallel collections for everything ( posts and  posts zh, for example) - A locales key in  config.yml, set up in a similar way to collections - Scope path glob patterns in front matter defaults - Localized strings in site.data - A liquid include for localizing dates - Another liquid include for setting up a bunch of relationships and variables  The biggest issue is that the last liquid include gets called a lot (especially when looping through documents), so site builds get slow quickly. While I can't open source the entire site, I could open source the i18n system by making a demo site, if there's interest.  Throughout development I've kept Jekyll 4.0 in mind, trying to think about how a native solution could eliminate the problems I've come up against. I think I'm pretty qualified at this point to submit a complete top-to-bottom specification proposal for discussion. Should I do it?"
technical,"Hello all, my first post so go easy. I want to just say that I love Jekyll, I'm now using it on a lot of my web builds. One thing that has bothered me though, is that I've not been able to take advantage of the latest Jekyll features because a lot of the 3rd party CMS solutions I rely on for my clients use (cloudcannon, siteleaf and forestry) simply don't run the latest versions of Jekyll themselves. This forces me to have my folder structures in a really messy state and not take advantage of the latest features when developing locally. So if anyone at Jekyll could bend a few arms to get them to update that would be fab. It always take some time to update to the latest version for services like GitHub Pages, Forestry, CloudCannon or Siteleaf because they need to run tests and adapt their tools. There's nothing Jekyll's core team can do about it."
technical,"my bad. guess i misfired with backgrounds as well? It looks like so, as Jekyll is fully agnostic when it comes what's get generated in the front-end, it just transforms files into HTML. You can use plugins like jekyll-assets and/or jekyll-cloudinary to help you deal with responsive images."
technical,"It would be great if include would take preference over exclude  kvz I've submitted a PR that should handle what you're looking for, as a side-effect.. Would you be able to give that branch a test-run..? ruby Feedback invited at the PR's url It would be great if include would take preference over exclude  I've submitted a PR that should handle what you're looking for  Just wanted to share that we confirmed that that PR solves the include vs exclude issue, so it would be very neat if that could land in 4.0"
technical,"It would be great if include would take preference over exclude. So that you could exclude: *, and include include: [ homepage.html ] and do fast iterations over a single page. This is also how e.g. rsync and other unix tools operate, and more useful than the (current) other way around. This was also reported in ticket but stalled as it would be a breaking change. Seems like 4.0 would be a perfect moment. What do you say envygeeks? (cced as he was planning to work on it) It would be great if include would take preference over exclude  kvz I've submitted a PR that should handle what you're looking for, as a side-effect.. Would you be able to give that branch a test-run..? ruby Feedback invited at the PR's url"
technical,"Awesome. Thank you! It would be great if include would take preference over exclude. So that you could exclude: *, and include include: [ homepage.html ] and do fast iterations over a single page. This is also how e.g. rsync and other unix tools operate, and more useful than the (current) other way around. This was also reported in ticket but stalled as it would be a breaking change. Seems like 4.0 would be a perfect moment. What do you say envygeeks? (cced as he was planning to work on it)"
technical,I would be grateful if you could resolve #6410. It would be really beneficial for Jekyllers around the world if you guys could implement some sort of semi-automatic generation on .yml files based on a folder structure. This would be a massive improvement and a great way to document distributed services.
technical,To benefit from PWA features I'd recommend the use of jekyll-pwa plugin  based on workbox 3 by Google. You'll score 100/100 in LightHouse if you add a manifest.json. jekyll-pwais more of a Service Worker thingy than manifest.json thingy. ˜‚ I am talking about vanilla manifest.json and not Service Workers.
technical,Custom HTTP headers compatibility for Github Pages Let's keep this restricted to Jekyll features. We don't have any control over GitHub Pages.
technical,"Wanted to add a comment about what I noted in comment that this is essentially a solution for what Harrix requested, but a bit more segregated to the standards that we expect blogs to keep for folder/file structure. Instead of making a change to store the content with the post, this just allows you to specify it directly. letrastudio So the more I've looked into implementing i18n in Core, the less feasible it became practically. There's huge performance drawbacks, even for sites that only use one language, the time spent rewriting a huge part of the Core internals would very likely be better spent working on other features, and we've garnered feedback from some big users of Jekyll that for them, performance improvements are more important than a huge new breaking feature. Obviously this shouldn't influence our decision as a project all that much, but it did provide us with insight into why a full i18n implementation in Core wouldn't be feasible. What I think we   could   do is provide baseline APIs upon which a (maybe officially supported) plugin could operate. If you have any such ideas, feel free to shoot them my direction!"
technical,"templates pure, or at least atomic  Do elaborate further merlinpatt For example, an index.md could have a {% link random-page.md %}"
technical,"I would like to see a plugin that lets you loop through any directory's files. This could let you list, display, or include the files in the directory. This might look like merlinpatt You can already do this by looping over site.static files  I wrote up an example of doing this to build an image gallery, but it could be adapted to access any static files."
technical,"merlinpatt You can already do this by looping over site.static files  I wrote up an example of doing this to build an image gallery, but it could be adapted to access any static files. mmistakes The issue with that is that the files have to then be outputted into the main site.  My main use case here is that I would like to be able to loop over files in a directory that won't be outputted or accessible to the public.  For example, I have a slide deck split into various parts, which I then use include relative to put back in the main file.  ### Current method - repeated includes   ### Preferred method - loop over directory  But I have a lot of parts, and add to it / move it around a lot, so I'd rather do something like this:  By using this method, I could reorganize my files, add new ones, or delete them and I wouldn't have to change the slide deck itself."
technical,"This is off-topic, but FYI GitHub-metadata already provides access to these data through site.github namespace. my bad. guess i misfired with backgrounds as well?"
technical,"I'd like to see Multiple Outputs baked in so for each .md file we can output the usual html as well as say amp and json files too. For example:  layout: page, page-amp, page-json referencing page.html, page-amp.html, and page.json.json layout files respectively.  It might also be nice to to be able to specify to output the same html page at different locations/paths. My two biggest wishlist items give two new degrees of freedom/flexibility:  1. Multiple Outputs, see: #3041 2. Multiple Content Sections, see: #246  Currently setting out to implement manually! ˜"
technical,"Speaking of Hugo  something like Shortcodes (in Hugo or in WordPress) would writing Markdown easier to me, because I can defer HTML constructs to another file (for example, generating HTML from JSON via a given partial, but denote where it should show up in my Markdown).  Downside in my experience is, that the HTML looks broken once the plugin for the shortcode is removed  (because the shortcode is interpreted as text).  So Jekyll should offer  hooks  for shortcodes. The implementation should happen in plugins. Oh, and this wishlist reminded me on an argument I had with pathawks over in  a couple of months ago.  Would be handy to have a way to support internationalisation! (Including hreflangs for SEO purposes)"
technical,"I'd say: **making it easier to filter content by date**.  At the moment, my understanding is posts/collection date field is coerced into a Ruby Date object. Comparison with where exp is impossible without workaround (casting a variable in  config.yml). There is a bit of background about this in #6581. I see this from a Jekyll user standpoint and I don't know whether this sits better as a where exp logic or as a date operator logic.  Thanks for making Jekyll such a nice tool :-) A +1 comment on geraldb proposal: I find remote data to be quite useful to avoid monolithic Jekyll setup. My use case would be to reference sub-projects on a main Jekyll websites, from remote JSON file and Atom feed. Oh, and: the only thing that keeps me continually considering whether I should switch to Hugo is speed. Can we find ways to make Jekyll: * Much faster at generating the site? (why is Hugo so many times faster? - IDK) * Much better at doing it incrementally? (perhaps with manual dependency hints)"
technical,"I would love to see an offically supported page generator from data files. With the stellar rise of ""headless"" (= API only) SaaS CMS systems [static site generators ](url)have become the tool of choice for types of sites that they have not been considered for in the past.  e.g. contentful.com  provides an official jekyll plugin that pulls all the CMS content into the Jekyll  data folder as yaml.  You can then use that content inside pages and include you've already created, but you can't actively create new pages from the SaaS CMS.  This is possible with other generators, but I'd like to stick with Jekyll due to your maturity and ecosystem.  e.g. the site I'm maintaining is a mix of content maintained by techies directly in the jekyll git and content written and uploaded by less technical people in contentful. contentful triggers the redeploy on netlify.com  via  webhook when a new article was published.  Overall a great stack, just that we have to rely on a somewhat unmaintained hacky ""page generator"" jekyll plugin to stitch it together. One more suggestions (I'd be happy to help along) - The Jekyll Documention is fantastic and outstanding. But, what's wrong (or better how to make it more awesome)?  Currently the documentation is part of the jekyll repos and the layouts, styling, build scripts etc. are part of the documentation too - it's all mixed together. What's wrong with that?  I'd say with its own dedicated markdown files in a repo it would be easier to contribute / edit / retarget etc. and with a ""proper"" docu (remote) theme  in its own repo it would be easy to reuse and change the theme too.   A while ago (dare I say years) I started to ""clean up"" as an example the documentation to use ""plain vanilla"" markdown files in the manuscripts (book/documentation) format. See the Hyde Press Bookshelf live and all the source repos. For the suggestion for Jekyll 4.0 it would just be a  new theme repo and a new docs repo (with markdown only). Would be great to see some booklets / guides / tutorials that could be made easily from the new docs repo using some (selected) pages etc.   Let me (us) know what you think."
technical,I would like to see the tags of collections included in site.tags. Thanks! Please please support for i18n. At least 2 languages. Many plug-ins break or don't work with ghpages.
technical,"merlinpatt For example, an index.md could have a {% link random-page.md %} Proper i18n support. There are many plug-ins but none of them is supported by GitHub Pages."
technical,"merlinpatt For example, an index.md could have a {% link random-page.md %} PWA support out of the box. There are some Jekyll plug-ins that do this already and their workaround and implemention is dead simple.  These are my arguments on why to add this feature: 1. Jekyll is already doing great on many factors on benchmark tests: This is the Lighthouse score for a fresh website with these characteristics:  Built with Jekyll Uses Hyde as a theme Hosted on GitHub Pages As you can see Jekyll is already doing great in Performance, Accessibility, and SEO with 99, 95, 100 out of 100 respectively. But it is not doing good in PWA.  2. PWA is a new web standard accepted by many international web organizations ( Read more:W3C, MDN). I personally think with a new major release, Jekyll has to standardize itself. It would be a huge advantage for a static site generator to support the latest methods available.  3. Replying to pathawks question regarding the looks: PWA let's you assign values like theme color, orientation, icons, display, and background color for example:  This value repeats what is already available in the site's CSS, but can be used by browsers to draw the background color of a shortcut when the manifest is available before the stylesheet has loaded.   This hides the browser's user-agent.  It's clear what it does IMHO.  Defines the default theme color for an application. This sometimes affects how the OS displays the site (e.g., on Android's task switcher, the theme color surrounds the site).  PWA is useful and these are only values related to the looks.  4. It won't break Jekyll. It is totally compatible. 5. Some of the information related to manifest.json are already available in  config.yml, e.g. ""name"" , ""start url"", ""description"". 6. It is easy to implement. We can automate a task which reads  config.yml and if there is something like the below code it generates the manifest.json at the root of the published  site:  Workaround suggestions: Give this a read: Using a Service Worker with Jekyll"
technical,"So leveraging the multi-thread capabilities of modern processors when generating pages is the suggestion. We can't leverage multiple cores because Ruby (the language Jekyll is written in), itself doesn't leverage them. Ruby certainly does support threads. The trick is making sure that we only render things in parallel that **do not** depend on each other, and how can you know that two pages do not interfere with each other without rendering them?"
technical,"Convincible On the hardware side of things, when there is a complaint about Jekyll taking a long time to generate pages, one could respond by saying get a processor with more cores. But from what I understand, Jekyll does not seem to leverage well multi-core processing when it comes to generating files. It's a petty since taking advantage of many cores could decrease page generation by 4-8X considering that many personal computers these days have processors with about that many cores/threads.  So leveraging the multi-thread capabilities of modern processors when generating pages is the suggestion. So leveraging the multi-thread capabilities of modern processors when generating pages is the suggestion. We can't leverage multiple cores because Ruby (the language Jekyll is written in), itself doesn't leverage them."
technical,"Is anyone working on i18n support? I noticed enthusiasm for it, and a lot of interest. I think I can make a significant contribution at the proposal/spec stage.  I've been working on a multilingual Jekyll site, and I've come up with a pretty good plugin-free solution! It's up at openhousemacau.com, hosted on GitHub Pages with Jekyll 3.7.3. It's still in active development, but the i18n part is fully functional, and can support any number of locales (not just the two it has now).  It's quite a complete solution, featuring sane (DRY) content management with fallbacks between locales, string translation, and date format localization. It even supports permalink localization, though we ended up not using it for this site. It works by using:  - Parallel collections for everything ( posts and  posts zh, for example) - A locales key in  config.yml, set up in a similar way to collections - Scope path glob patterns in front matter defaults - Localized strings in site.data - A liquid include for localizing dates - Another liquid include for setting up a bunch of relationships and variables  The biggest issue is that the last liquid include gets called a lot (especially when looping through documents), so site builds get slow quickly. While I can't open source the entire site, I could open source the i18n system by making a demo site, if there's interest.  Throughout development I've kept Jekyll 4.0 in mind, trying to think about how a native solution could eliminate the problems I've come up against. I think I'm pretty qualified at this point to submit a complete top-to-bottom specification proposal for discussion. Should I do it? So This is just a thought, I can try and make an issue for it with more details and thought through functionality, but I have noticed a potential gap in functionality.  If you use the  drafts folder, or have a post that is unpublished in your  posts folder, then you still need to move things around for your content, like images, to not get published to the generated site.  Since a lot of users use /assets/imgs/ folders for the beginning position of images associated to a post, I was thinking that we could make a new special key to use in posts that you add to the frontmatter.  The key would be something like asset paths: or asset path: that we merge to an array, and it can define any of the assets that should not be published if they do not match up to any other posts that are being published.  So if I have the following in my frontmatter: Then the post when in the drafts folder would not copy over any of that folder located. However, if a post is in  posts that contains the same frontmatter key/value, then it would get published as it was included in a post that is getting published. This could be done for individual file levels too.  The last part would be that unless included and exclusively used for an unpublished post, that the folder/files in question would perform /be acted on exactly as they already are now, where they will be included or excluded based on the  config.yml settings."
technical,I have found letting web pack handle the sass instead of Jekyll improves speed  Jekyll Core doesn't handle pre-processing css and javascript. We have two plugins (both are written in Ruby): jekyll-sass-converter (included in jekyll by default) and jekyll-coffeescript.  The speed improvements are probably due to the difference between ruby-sass and node-sass something like this. Here is my post content.
technical,"GitHub data should be part of GitHub-metadata plugin not Jekyll-core sorry, i just started with jekyll a week ago) this repo is on the list. at this time, i guess that this info could be pulled by metadata, but shouldn't jekyll convert it for better (more universal) usage?"
technical,"Ah, fair enough “ but then, within the confines of interpreted Ruby, speed is on my wishlist! Mainly an issue when you get to 1,000+ page blogs/sites... I'd love to keep using Jekyll even here and beyond. Speaking of Hugo  something like Shortcodes (in Hugo or in WordPress) would writing Markdown easier to me, because I can defer HTML constructs to another file (for example, generating HTML from JSON via a given partial, but denote where it should show up in my Markdown).  Downside in my experience is, that the HTML looks broken once the plugin for the shortcode is removed  (because the shortcode is interpreted as text).  So Jekyll should offer  hooks  for shortcodes. The implementation should happen in plugins."
technical,"what's an example where two pages depend on each other? I can see this being an issue for includes or layouts but pages themselves should be independent of each other templates pure, or at least atomic  Do elaborate further"
technical,"iBug If I understand correctly, you could do this sort of thing using two (or more) config files. You can use this build command flag to specify multiple config files:   Settings in later files override settings in earlier files.  Granted, this option is only available as a command line flag, so it wouldn't work if you're using GitHub Pages to build your site. Thanks for the response. Command line flags is surely not a problem for me because I use Travis CI to build my site."
technical,"(I suggest this without understanding what would actually be involved in doing it)  I'd love to see performance work done regarding glob patterns in frontmatter defaults. I think it's a tremendously useful feature, but it's hampered by the fact that the more useful it is for a project, the more detrimental it is to build times.  I use this feature heavily in scenarios where I have a collection of documents which represents different variations of a type of content. For example, in the Sentry marketing resources ""resources"" is a collection, which is broken up into folders representing podcasts, videos, and pdfs. Each of those folders has it's own defaults that are appropriate for the given medium.  In smaller collections there isn't much of an issue, but on our docs site, where a collection is 250+ pages, using a wildcard added 2s to a .8s build. Thanks to everyone for the feedback, that's already a lot of food on our plate, we won't be able to implement *all* your requests, but we'll definitively do our best to fulfill your expectations. We're currently focusing on performance. As we will need your help, we might pick some priority issues and offer rewards thanks to our Open Collective sponsors. Stay tuned."
technical,"jekyll-pwais more of a Service Worker thingy than manifest.json thingy. ˜‚ I am talking about vanilla manifest.json and not Service Workers. The ability to render includes only once. This question on Stack demonstrates the idea. To demonstrate why this is a great idea, here is a scenario that I have: say you want to have a tag cloud in every one of your posts. You have tag-cloud.html in your  includes, which allows you to generate a tag cloud on every post so the user can have a better time navigating the site. Instead of generating a tag cloud for *every* post, it will be nice to have the ability to generate an include *only once* and then have it included in every post. In my case, this could decrease the generation of my blog from 1 minute to 4 seconds. Curent solutions include plugins and hacks, but this does not apear to be hard to implement natively in Jekyll."
technical,"Thanks for the response. Command line flags is surely not a problem for me because I use Travis CI to build my site. The ability to specify that collection items with the attribute published: false should still have their YAML data exposed in the backend site.my collection object. Presently, setting published: false makes the data entirely inaccessible, even on the backend."
technical,"The ability to render includes only once. This question on Stack demonstrates the idea. To demonstrate why this is a great idea, here is a scenario that I have: say you want to have a tag cloud in every one of your posts. You have tag-cloud.html in your  includes, which allows you to generate a tag cloud on every post so the user can have a better time navigating the site. Instead of generating a tag cloud for *every* post, it will be nice to have the ability to generate an include *only once* and then have it included in every post. In my case, this could decrease the generation of my blog from 1 minute to 4 seconds. Curent solutions include plugins and hacks, but this does not apear to be hard to implement natively in Jekyll. The plugin mentioned in the StackOverflow you linked to, jekyll-include-cache is the best option available out there. However, there is a related proposal for getting similar support in Core: #7108 .. ..and another (very distantly) related proposal at #7136  You may *subscribe* to the above PRs to stay notified about any developments on the proposals"
technical,"Convincible At present, static files within a  posts directory are ignored.. there's a plugin you can use that allows you to place static files in the same folder as the post."
technical,"I know that this is markdown and not necessarily jekyll related, but my life would have been a thousand times easier if jekyll supported inline footnotes like so  [Footnote, p. 123] as apposed to the more tedious:   This is first reference[one], this is the second[two] This depends on the Markdown engine. Please ask  on Kramdown or CommonMark"
technical,"sorry, i just started with jekyll a week ago) this repo is on the list. at this time, i guess that this info could be pulled by metadata, but shouldn't jekyll convert it for better (more universal) usage? This is off-topic, but FYI GitHub-metadata already provides access to these data through site.github namespace."
technical,"Yes, Ruby does support threads. But because of the Global Interpreter Lock (GIL), threads in Ruby are never actually run in parallel,..  *edit: I meant never run simultaneously..* threads in Ruby are never actually run in parallel"
technical,I updated the initial comment/request with my thoughts on why this is a good thing to implement in the upcoming Jekyll 4.0 with some reference on how it affects the looks. To benefit from PWA features I'd recommend the use of jekyll-pwa plugin  based on workbox 3 by Google. You'll score 100/100 in LightHouse if you add a manifest.json.
technical,"mmistakes The issue with that is that the files have to then be outputted into the main site.  My main use case here is that I would like to be able to loop over files in a directory that won't be outputted or accessible to the public.  For example, I have a slide deck split into various parts, which I then use include relative to put back in the main file.  ### Current method - repeated includes   ### Preferred method - loop over directory  But I have a lot of parts, and add to it / move it around a lot, so I'd rather do something like this:  By using this method, I could reorganize my files, add new ones, or delete them and I wouldn't have to change the slide deck itself. tyler-insight, I have a PR [open][] for this in Minima, the default theme you get if you run jekyll new. In that case, I based my cache off of lib/site template, although I probably should go through the Cookbook to make sure I've covered all bases."
technical,"My two biggest wishlist items give two new degrees of freedom/flexibility:  1. Multiple Outputs, see: #3041 2. Multiple Content Sections, see: #246  Currently setting out to implement manually! ˜ User comments please, filterable and maybe with optional moderation."
technical,"Adding fetch / get for datafiles to core (or as an ""official"" addon/plugin) would be great, see the unloved / public domain source. The code itself is about 40 lines. Using the quik library for scaffolding. If you scaffold a new jekyll theme or plugin now all the code is ""hard-coded"" / ""hard-wired"". The scaffolding code itself is also ""hard-wired""  / ""hard-coded"", that is, not (re)usable for other projects. Using a (simple) ""generic"" scaffolding library such as quik you can turn any git(hub) repo (or directory/folder or zip archive) into a parametrized and scripted template scaffold. See the Jekyll Quick Starter Template / Scaffold - Build Your Own (Gem-Packaged) Theme as an example.  PS: Background / References - Talk Notes - Quik - The Missing Project Scaffolder (Library) for Ruby - Quick Start Your Ruby Gems, Jekyll Websites, Jekyll Themes 'n' More"
technical,"You don't have to duplicate 99 lines for each environment specific config file. When you specify multiple config files at build Jekyll daisy chains them from left to right.  Meaning you could have a production config with all settings, and then for your staging and dev configs just add the lines that are unique or change. Jekyll will use everything from the first config, and override whatever comes next. For example, say you have and then a staging specific config (e.g.  config.staging.yml When you run bundle exec jekyll build you'd get the prod url, if you run You'd get the staging url along with all the other variables set in  config.yml. Wanted to add a comment about what I noted in comment that this is essentially a solution for what Harrix requested, but a bit more segregated to the standards that we expect blogs to keep for folder/file structure. Instead of making a change to store the content with the post, this just allows you to specify it directly."
technical,"You don't have to duplicate 99 lines for each environment specific config file. When you specify multiple config files at build Jekyll daisy chains them from left to right.  Meaning you could have a production config with all settings, and then for your staging and dev configs just add the lines that are unique or change. Jekyll will use everything from the first config, and override whatever comes next. For example, say you have and then a staging specific config (e.g.  config.staging.yml When you run bundle exec jekyll build you'd get the prod url, if you run You'd get the staging url along with all the other variables set in  config.yml. We'll rely on sassc implementation, that is currently removing dependency to Ruby sass"
technical,"I understand, and I feel the same way about performance. However, I am not completely discouraged! Wouldn't performance impact depend largely on how the feature is designed? I think  that my proposed solution wouldn't significantly impact performance for single-language sites (and it wouldn't break Jekyll 3 sites at all). Big disclaimer though:  am not a Ruby programmer so I could be completely wrong.  I think I'll write up my ideas anyway and post them as a new issue ” even if they're not feasible for Core, I hope they'll contribute to the discussion, maybe even inspire some intrepid plugin developer. Or they might help spark some ideas for the baseline APIs you suggested (such low-level discussion is probably a bit out of my league).  And still, I've found that i18n is quite doable with vanilla Jekyll 3. IMHO content management can actually be better than with existing plugins, though it's a bit complex to work with on the Liquid side. Some small tweaks to make that easier might be worth implementing, I'd happily settle for making it feel like less of a hack. What are the plans for (Dart) Sass support in Jekyll 4.0 now that the Ruby implementation is deprecated? I'm  very  interested to hear the responses from the maintainers."
technical,Wow!! That's an awesome idea!!! We can easily create a new tag that is a combination of both raw and if tags... :heart: :tada: What do you think for SanitizeHelper
technical,"threads in Ruby are never actually run in parallel What if we made templates pure, or at least atomic?"
technical,"PWA support out of the box. There are some Jekyll plug-ins that do this already and their workaround and implemention is dead simple.  These are my arguments on why to add this feature: 1. Jekyll is already doing great on many factors on benchmark tests: This is the Lighthouse score for a fresh website with these characteristics:  Built with Jekyll Uses Hyde as a theme Hosted on GitHub Pages As you can see Jekyll is already doing great in Performance, Accessibility, and SEO with 99, 95, 100 out of 100 respectively. But it is not doing good in PWA.  2. PWA is a new web standard accepted by many international web organizations ( Read more:W3C, MDN). I personally think with a new major release, Jekyll has to standardize itself. It would be a huge advantage for a static site generator to support the latest methods available.  3. Replying to pathawks question regarding the looks: PWA let's you assign values like theme color, orientation, icons, display, and background color for example:  This value repeats what is already available in the site's CSS, but can be used by browsers to draw the background color of a shortcut when the manifest is available before the stylesheet has loaded.   This hides the browser's user-agent.  It's clear what it does IMHO.  Defines the default theme color for an application. This sometimes affects how the OS displays the site (e.g., on Android's task switcher, the theme color surrounds the site).  PWA is useful and these are only values related to the looks.  4. It won't break Jekyll. It is totally compatible. 5. Some of the information related to manifest.json are already available in  config.yml, e.g. ""name"" , ""start url"", ""description"". 6. It is easy to implement. We can automate a task which reads  config.yml and if there is something like the below code it generates the manifest.json at the root of the published  site:  Workaround suggestions: Give this a read: Using a Service Worker with Jekyll What would a Jekyll site look like as a Progressive Web App? Can you elaborate?"
technical,"What if we made templates pure, or at least atomic? what's an example where two pages depend on each other? I can see this being an issue for includes or layouts but pages themselves should be independent of each other"
technical,"something like this. Here is my post content. When I see the repo, I though GitHub pages. It has similarity between.  sidebar.md,  footer.md etc.."
technical,Add the ability to place the post and its files in the same folder. Why can't you do this at present?
technical,"The ability to specify that collection items with the attribute published: false should still have their YAML data exposed in the backend site.my collection object. Presently, setting published: false makes the data entirely inaccessible, even on the backend. Would love to see a standardized implementation of a service worker built into Jekyll utilizing the methods in The offline cookbook. A way of configuring it so that you could easily designate what the 'shell app' is so that those files load from offline cache first, and then designating dynamic content (such as blog posts) that load from network first and only cache so many entries (since caching 100 blog posts is ridiculous).  I realize these things can be done now, but finding a way to configure it to work perfectly with Jekyll would be amazing and save a lot of people the struggle of figuring out how to properly implement a service worker for it."
technical,"Ruby certainly does support threads. The trick is making sure that we only render things in parallel that **do not** depend on each other, and how can you know that two pages do not interfere with each other without rendering them? Yes, Ruby does support threads. But because of the Global Interpreter Lock (GIL), threads in Ruby are never actually run in parallel,..  *edit: I meant never run simultaneously..*"
technical,"I would love to see better environment support.  **Update  For example site.url:  Right now site.url has an implicit env pattern that is hard coded for development (Source). This is very implizit, hard to understand and remember and not extendable. #5142  I would love to see this extended. Example:   The pattern could be: When jekyll calls a variable, it checks for <var-name.<env-name. The fallback is <var-name. So it does not matter if I configure just url order url.<env-name.  (This is a copy) You can set up all kinds of fallbacks by using multiple config files, as I mentioned in this comment. I've come across this use case myself ” I use  config.yml for production values, and an additional  config dev.yml for development overrides. When building locally I run:"
technical,"User comments please, filterable and maybe with optional moderation. You can use Disqus or some similar software for that, it's way outside the scope of Jekyll as a  static  site generator."
technical,"In Reply  I am aware of that solution, but it only causes new problems.  **The Scenario is:** I have a config-files with 100 lines of config. One, maybe two line of this config needs to be changed for different environments.  With your solution, I need to duplicate 99 lines in three files (dev, staging, production) and manually sync them every time I make a change. **This is bound to fail!**  People will forget to sync the changes and the main idea of a staging system (to behave like the production system) will be lost.  **Other solutions would be** a. The one I describe  b. Allowing more than one config, one general-config-file for all env, and one config-file for each env. This is similar to the way rails does it. The env-config will overwrite the general-config in case that is needed. c. Your solution, but with the ability to ""include"" or ""reference"" other config files inside the one I call with the jekyll build command. For this I would say ""build with staging-config"" and inside staging config ""use this 1 line and include/use all other lines from this other config-file"". d. ? You don't have to duplicate 99 lines for each environment specific config file. When you specify multiple config files at build Jekyll daisy chains them from left to right.  Meaning you could have a production config with all settings, and then for your staging and dev configs just add the lines that are unique or change. Jekyll will use everything from the first config, and override whatever comes next. For example, say you have and then a staging specific config (e.g.  config.staging.yml When you run bundle exec jekyll build you'd get the prod url, if you run You'd get the staging url along with all the other variables set in  config.yml."
technical,"We have now released this SDK to Beta, see version 9.0.0-beta.1 on npm. Thank you everyone who helped test this in alpha! :wave: We are currently working on a new variant of the Firebase JS SDK which will allow module bundlers like Webpack to strip out unused code and make your app bundle much smaller (see #332)  If you are interested in participating in the Alpha program for this SDK, please fill out this form:  We also created a Getting Started Guide to help you get started on this new SDK.  Please note this is an **Alpha** SDK. There are likely to be bugs and breaking API changes along the way. This SDK is not yet ready for use in production, please only join the Alpha program if you're interested in testing this SDK and providing feedback.  If you're already in the Alpha:  - If you've found a bug, please file a new issue using this template - If you want to discuss this new SDK in general and give us feedback or new ideas, please use our GitHub Discussions board."
technical," We have now released this SDK to Beta, see version 9.0.0-beta.1 on npm. Thank you everyone who helped test this in alpha!"
technical,"I agree on some points. but software is hard, that is why it is expensive. If creating the app you want is that easy, you would have seen smaller salary rates for software developers.  Angular is improving, but it is taking toooooo long, integrating great features which should be a must in this time like web workers are taking years here and it is still not available out of the box.  It is not the team fault. but I believe they should spend more money on the platform so it can improve faster. ... a small notice ... also standardization process takes years ... this is true in cases just as web components, service workers, shadow DOM ... and others ... so for general using it means also waiting for implementation in browsers (on leader platforms at least)."
technical,"says that in our daily work with Angular we solve about the same problems time after time. Almost always there is not enough definition of URL or rout name of at the level of the directive of the type routerLinkActive. Very often there is a lack of a setValidator and a removalValidator. Very much hampered by the need for DI all and everything in the tests. There is not enough opportunity to redefine. There is not enough opportunity to override the concrete method without defining the whole class. In general, all that is needed for daily work in production. This is cool, when we have the opportunity to redefine and adjust everything to our own needs, but not when you are forced to do this in almost every new task. ... a small notice ... what do you mean by ...  You can not inject the same pipe into two different modules."
technical,"... a small notice ... what do you mean by ...  You can not inject the same pipe into two different modules. ... ah, thanks for your explanation. I have to say that this factor of modules architecture is absolutely fine for me and I don't feel it as limitation from any point of view. Maybe it is my personal feeling, maybe affected by my professional historical background.  Here are some points as I see it:  * You always have one shared module at least, usually more ... so it is a natural thing to place such a thing to a correct module. A simple app is just one module usually. * If it is something extra then it is a part of that custom eager or lazy loaded module. * There are exact rules ... and I think very simple in fact ... how modules architecture is structured. I never understand what are the reasons why some developers don't understand them. * Usually the similar discussion is about the topic ... one component / one module. The fact is that the simple module for just one component is only 5 simple lines of code ... possible to place into just one TS file."
technical,"Locking this thread. Please keep contributions and comments positive and about the technology. ## I'm submitting a...  Check one of the following options with ""x"" -- <pre<code[ ] Regression (a behavior that used to work and stopped working in a new release) [ ] Bug report   Please search GitHub for a similar issue or PR before submitting -- [ ] Performance issue [ ] Feature request [ ] Documentation issue or request [ ] Support request  Other... Please describe:  message to the developers of Angular for feedback ## Current behavior  I have been working with Angular since 2013 (I started with AngularJS). I also write articles on habrahabr.ru and medium.com about Angular. So I have been collecting a lot of statistics and feedback from most people over those years.  And what did I find out in 5 years of working with Angular?  AngularJS was considered to be too high-level language and therefore everyone who already had experience with jQuery or other things that work as a simple JavaScript could quickly understand it. Angular 2+ has become too low-level and no longer resembles Java, and it feels like working with C++, when you have to do everything with your hands, keep track of everything and grasp on time things that seem to occasionally drift away. Angular 2+ can also be understood, but not immediately.  1. You forgot to make an unsubscribe in the components when they are destroyed - you get a memory leak. 2. You work with large data streams in the application and did not find out how to optimize your application, and/or did not change components on OnPush “ you get severe performance drop.  3. The customer changed the business requirement, he wants dynamic components, and a small bandle “ say goodbye to the technological stack of Angular. Because it is unreal to do it by hand, when you do not even have the experience, especially while time is running out. Even the original ng-component-outlet did not work as it should, and to write your own solution you must have skills on a Senior level at least. I hope Angular Elements will save the day for us.  4. Junior developer included setInterval in the component and did not wrap the start outside the zone “ again, performance decline for you.  5. In AngularJS there was such property at the directive, as setValidate (or something of that kind), I actively used it, everything was cool. When creating a custom validator, you need to ensure that you do not overwrite existing ones, for what? In doing so, you create more than one validator, and all at once. That is, you cannot simply add a validator if you already have required, for example: firstly, you specify the required validator, and then - your own. And still you have to keep in mind whether he is active, or maybe there is already a field filled. Currently there is no such thing and you have to create custom validator as a whole in a separate class with accessories, and even then you can't be sure if you're doing fine.  6. To create your custom ngModel, you have to rewrite a ton of code, a whole class with ControlValueAccesor and a stock full of methods. But even if you use your own banana-box attribute on the component, if you forgot to make emit from the component of the event, you will not even be warned about it.  7. In Angular CLI, since 1.6.5 strange bugs began to appear and something fails all the time, making it necessary to restart the dev-server. When my project was written on pure Webpack and Angular, I had not experienced such problems before.  8. If I want to use the web-worker platform, I have to do an eject, and on the whole Internet there is only a couple of articles on how to further run web-riches.  9. There are declarative and reactive forms. But when I'm working with one enormous entity, I have to do a lot of work to map fields in both the class and the template. Moreover, if I use declarative forms, I lose the advantage in future, where jet forms could do better.  10. Tests are painful. Angular is the only framework with Dependency Injection built-in, but it brings a lot of pain as well. All these mandatory dependencies for the DI component do not let you live happily while writing tests. Writing tests in Angular is as tedious as nowhere else. The component designer requires you to include everything at all. Even if you're not going to test it.  11. Routing is completely untyped, should I switch the module directories, not even the Webstorm IDEA or VSC will not help me in putting those paths correctly.  Angular is overcomplicated. And at first sight in many cases this is not justified. There are not enough ""convenient"" things. Well, take the router, he has a directive for adding an active class. But why is there no such directive for checking the current route? Not the URL, but the very route. As it is made in UI-router, where you can check individually all the segments at the template level. This is a simple thing, I needed it in all projects, on the first and second Angular both. Without exception. When on one page you show one top hat, and other top hat on the other one. Same thing with background pictures. In general, the range of tasks for this thing is diverse and huge. The day before yesterday, I once again had to make up something with this. But I did it.  Now let's take a person who just wants to study Angular and tries to find out what is frequently asked on an interview, what should he study and in which way? And so, most of the beginning developers, they do not find a one-step solution, they start making their own solution, they see it over-complicated and they switch to Vue. Not because it's simpler, but because of the total number of complications and the lack of detail in Angular. Which, seems, is a framework for robots.  At first it seemed that Angular was poorly designed. But in fact, no, it's not.You get tired very quickly while using it, however, and there are no alternatives yet. Neither Vue nor React seem to suit me, and I do not want to write jQuery any more. Angular does not make it simpler for the developer, not for a single moment. From the very first line, you are doomed to seek solutions, something to redefine and make yourself comfortable with your bare hands.  It's cool when you can do it, when there is such an opportunity, but not when you are forced to do this through all the work process. Modern front-end developers have to solve problems, not to engage in academic research, which framework is better or faster scaled in the first place. It is necessary for it to be easy and reliable. React is easy, but it is not reliable, Angular is reliable, but is not easy. Many people now see the balance in Vue, but it's not balanced as they see it. This is a hellish mixture of 80% React and 20% AngularJS. By reliability, I mean, first of all, the probability of code getting smelly, if you know what I mean. JSX itself is a shitty code, to be fair. Regarding React - statistics confirm that the amount of such poor-quality code is just off-scale.  Continuous complaints. And all companies only seek Seniors, who can fix poor-written code.  About the state of the application. Redux - this is also not a good implementation, I'm getting more and more convinced in this opinion. It actually tends more to the ideology of Angular, where you have to describe the elementary things like it's some kind of multi-volume advanced research study. And it does not protect you against unforeseen changes, plus it forces its architecture upon you. It just appeared before Mobx and Dan Abramov speaks for it, and Dan's opinion influences not very experienced programmers, which are a big group among the React developers  ## Expected behavior  I want to know if there are any plans to reduce complexity and simplify the framework. To make it easier to start developing any projects on Angular at any level of preparation, so that the project can easily be increased, while being based on the technological stack of Angular.  If we had something simple as StencilJS out of the box and could easily scale to full components and modules, I think many would be happy. I would like Angular to be more popular and inquired on all markets, not only more popular or better than AngularJS only.  I like Angular and I want everything to be fine (pin medium publish)."
technical,"you don't think Angular is for production?  I got a Golf Cart, a 1978 Joker Poker pinball machine, a 2017 Wizard of Pinball machine, and a Baby PAC man pinball machine on the way THAT WERE ALLL paid for by Angular production code produced at the following companies: 50/50. I'm writing production on Angular. Angular allows you not to write bad code. But almost always forces you to write a lot of low-level code, which takes away a monstrous amount of time"
technical,"Yes, the Angular team makes a tool for developers. But I would like to pay attention to the little things, because of which it sometimes hurts. AWWWWWWWWWwwwwwwww  Alright I'm often full of sassy comments and I don't have troubles any more cause I mastering this shit!  Here, try my code:  Demos and docs at all... I love this Angular.... Sorry you're having such a tough time making the cash rain from Angular. I be milking the sucker ova her!"
technical,"I do not argue, I'm fine, I just wanted to note the observed phenomena Electromagnetism is a phenomena  I wouldn't put complainers in that category.  It's just complaining cause this Angular works the way you say it don't"
technical,"Thank you, Stephen. I hope in the future Angular will become even better and easier to understand. However, first of all, I am grateful to Angular for making me grow as a developer. How often do you collect feedback from community developers?"
technical,"Understood. Agreed. I will step away and unfollow understanding how my mouth often gets me in trouble.  Farewell and good luck to all of you I agree on some points. but software is hard, that is why it is expensive. If creating the app you want is that easy, you would have seen smaller salary rates for software developers.  Angular is improving, but it is taking toooooo long, integrating great features which should be a must in this time like web workers are taking years here and it is still not available out of the box.  It is not the team fault. but I believe they should spend more money on the platform so it can improve faster."
technical,"Our team learned **ng** as well, the problem is two things for now: 1) router lazy loading doesn't work with relative path (problem leaves since angular 2 - wtf?), 2) unstable **angular/cli** (every time something doesn't work since 1.6.x),  P.S. documentation covers not a lot i'd say, if you want to learn angular, read material's code, it's more convenient than angular's documentation  i just feel that **angular team doesn't care about users feedback at all** I didn't graduate with my high school class. I had to take extra classes in the summer and even then i only graduated with a 2.1. Y'all are making me feel like a scientist though!!!!! I learned Angular all on my own. No teacher. Y'all suck at learning. Angular team will put it more politely but nah me: Y'all are being out smarted by this D student. Make money money! Angular taking me places daily"
technical,"Noooooo. Lazy loading works, the cli works too great. I know they work, everyday I know it. And the Angular team talks allllllllllll the time (it's just scheduled). Not gonna argue with flat earthers nor am I gonna debate with y'all. I'll make the money. You write the complaints I do not argue, I'm fine, I just wanted to note the observed phenomena"
technical,I didn't graduate with my high school class. I had to take extra classes in the summer and even then i only graduated with a 2.1. Y'all are making me feel like a scientist though!!!!! I learned Angular all on my own. No teacher. Y'all suck at learning. Angular team will put it more politely but nah me: Y'all are being out smarted by this D student. Make money money! Angular taking me places daily I just think he wanted to say that the project got out of control
technical,"... ah, thanks for your explanation. I have to say that this factor of modules architecture is absolutely fine for me and I don't feel it as limitation from any point of view. Maybe it is my personal feeling, maybe affected by my professional historical background.  Here are some points as I see it:  * You always have one shared module at least, usually more ... so it is a natural thing to place such a thing to a correct module. A simple app is just one module usually. * If it is something extra then it is a part of that custom eager or lazy loaded module. * There are exact rules ... and I think very simple in fact ... how modules architecture is structured. I never understand what are the reasons why some developers don't understand them. * Usually the similar discussion is about the topic ... one component / one module. The fact is that the simple module for just one component is only 5 simple lines of code ... possible to place into just one TS file. I just want to say that I've seen a lot of projects where people come from React. They do not understand the modular architecture to the end, and all the components, pipes, services are shoved into the module or the shared module. And then on the way out we have a huge bundle. For example, I do not have such a feature as lazy components (maybe Angular Elements is it)."
technical,"I feel your points. They burn us, too.  My guess is, the angular team is very occupied with big topics (elements, ivy, bazel, documentation, ng-conf). The ivy renderer in particular targets some of the pain points you mentioned.  We have two projects running in parallel, one angularjs and one angular5. And nobody here likes to touch the angularjs codebase. angular5 and the ecosystem around it is just wonderful (full template typecheck and linting, clean component interface, typescript by default, the template syntax and one way data flow). So we admire the work the team is doing with the framework. Also that there is always an upgrade path (and not a hard cut like angularjs=angular2).  My hope is that the team gets back to productive boosting features as soon as the dirty plumbing is done. And no need for another major rewrite (Router4? ˜) I like Angular. It is quite easy to understand when moving from project to project. But the number of people who are new, who would like to learn it for some reason, is decreasing. I popularize only Angular in Russia, but it becomes difficult every day. While other frameworks have the same number of functional functions, and the number of libraries is increasing every day.  In Russia, I wanted to work in a large company in Yandex, but they write only on React. I wanted to work in Vkontakte, there too on React.  Large companies are opposed to the use of Angular. My friends in mid-level companies, mostly one full-stack developers, and they say that the quicker to sketch the layouts and start using Vue, without going into and diving.  Especially many front-end developers in Russia do not understand the benefits of RxJS. Allegedly they only want to use Promise.  For example, in our company, a component library is already tied to Primeng, it is badly written (if you open source), people do not even write tests. And half the time you have to kill for cutting styles. But when it is no longer possible to cut styles and is strongly tied to a component, even very difficult somehow without crutches to redefine the internal logic of the third-party component.  Overall, I like Angular, I really grew up as a programmer. However, I still feel like a weak expert, since Angular is huge."
technical,"It is necessary to say that the whole web environment is not the easy world, and it'll be harder, not easier in a year or two. If I look at some JS code which was written 5 or more years back it is hard to believe where we are today. Angular as is today, it's not the simplest tool for a standalone beginner because of abstractions and new concepts (especially on the edge it means that when you analyze any design then the best is to think that all is async as the real world is in fact) but it is the very easy and super efficient tool when such beginners have a good teacher. i'm Software Engineer too and i like angular.  But now he wants to become a robot framework. It is good to use for academic purposes, like a barbell that helps build muscle. But in production with it is more difficult than with react or vue. It does not help to solve problems, it simply does what it does.  I really hope that in the future it will be what the framework for an engineer should be - a tool in production, not a bar in the gym"
technical,"I like Angular. It is quite easy to understand when moving from project to project. But the number of people who are new, who would like to learn it for some reason, is decreasing. I popularize only Angular in Russia, but it becomes difficult every day. While other frameworks have the same number of functional functions, and the number of libraries is increasing every day.  In Russia, I wanted to work in a large company in Yandex, but they write only on React. I wanted to work in Vkontakte, there too on React.  Large companies are opposed to the use of Angular. My friends in mid-level companies, mostly one full-stack developers, and they say that the quicker to sketch the layouts and start using Vue, without going into and diving.  Especially many front-end developers in Russia do not understand the benefits of RxJS. Allegedly they only want to use Promise.  For example, in our company, a component library is already tied to Primeng, it is badly written (if you open source), people do not even write tests. And half the time you have to kill for cutting styles. But when it is no longer possible to cut styles and is strongly tied to a component, even very difficult somehow without crutches to redefine the internal logic of the third-party component.  Overall, I like Angular, I really grew up as a programmer. However, I still feel like a weak expert, since Angular is huge. It is necessary to say that the whole web environment is not the easy world, and it'll be harder, not easier in a year or two. If I look at some JS code which was written 5 or more years back it is hard to believe where we are today. Angular as is today, it's not the simplest tool for a standalone beginner because of abstractions and new concepts (especially on the edge it means that when you analyze any design then the best is to think that all is async as the real world is in fact) but it is the very easy and super efficient tool when such beginners have a good teacher."
technical,"I just want to say that I've seen a lot of projects where people come from React. They do not understand the modular architecture to the end, and all the components, pipes, services are shoved into the module or the shared module. And then on the way out we have a huge bundle. For example, I do not have such a feature as lazy components (maybe Angular Elements is it). Its complex? yeah. Impossible to learn? nope. Beginner friendly? Only if you invest some time reading/doing tutorials."
technical,"Its complex? yeah. Impossible to learn? nope. Beginner friendly? Only if you invest some time reading/doing tutorials. Lazy loading in Angular is super dope. You can do by component or entire routes.  , are you using some Angular that a crack dealer sold you? That or you're not reading enough because several things you mention you can do and I'm doing it with ease. Don't bother to ask how cause I feel this chat is compacted with complaining newbies that just need to read and learn how to do things outside React"
technical,"Lazy loading in Angular is super dope. You can do by component or entire routes.  , are you using some Angular that a crack dealer sold you? That or you're not reading enough because several things you mention you can do and I'm doing it with ease. Don't bother to ask how cause I feel this chat is compacted with complaining newbies that just need to read and learn how to do things outside React Learning takes time. Lots of time. That's all. Bye."
technical,"... a small notice ... also standardization process takes years ... this is true in cases just as web components, service workers, shadow DOM ... and others ... so for general using it means also waiting for implementation in browsers (on leader platforms at least). Locking this thread. Please keep contributions and comments positive and about the technology."
technical,"I just think he wanted to say that the project got out of control Noooooo. Lazy loading works, the cli works too great. I know they work, everyday I know it. And the Angular team talks allllllllllll the time (it's just scheduled). Not gonna argue with flat earthers nor am I gonna debate with y'all. I'll make the money. You write the complaints"
technical,"Soooooooooooo you're setting Angular on fire and saying bold shit in bold like the Angular team don't care about you cause you found relative pathing has an issue is an odd area. What did Ionic do to get it right? I'm NOT going to create some plunked to split hairs with you. I felt Angular 2 to Angular 4 stuck it to me in the ace hole butttttttttttttt alas past my bitching, it all was for the better. So try to call me out and split Cee U Next Tuesday's hairs with me........ The topic here has just been a bunch of bitching where I have found success in the CLI and lazy loading. See U. Next Tuesday please lower your tone. I think its not in the angular community spirit to refer to the concerns/inquiries of other ppl in such a way, no matter how frustrating those could be.  Furthermore Id like to point out, that despite the fact that angular as a whole has a lot of room for improvement (thats why there are 1.9k+ issues in the tracker), the current features of the framework outweight by far any existing issues IMHO, and its, for me at least, a recurrent source of sw. architecture guidelines/patterns."
technical,"we talk to developers about Angular every day. But maybe I'm misunderstanding your question? says that in our daily work with Angular we solve about the same problems time after time. Almost always there is not enough definition of URL or rout name of at the level of the directive of the type routerLinkActive. Very often there is a lack of a setValidator and a removalValidator. Very much hampered by the need for DI all and everything in the tests. There is not enough opportunity to redefine. There is not enough opportunity to override the concrete method without defining the whole class. In general, all that is needed for daily work in production. This is cool, when we have the opportunity to redefine and adjust everything to our own needs, but not when you are forced to do this in almost every new task."
technical,"When the title of this issue was grammatically incorrect, it was evidence of the sheer agitation you have with Angular. But your body comments were well typed and thought out.  Angular makes me feel on a high like the Beck WoW song. Thanks so much for creating this issue. I don't have any concrete reactions to the above yet (there's a lot to unpack, and we're already working on some of it), but I want you to know that we are listening and thinking about these problems deeply. We care a lot, and we spend a lot of time reflecting on issues like this.  We've shared several times our goals about making Angular simpler, but still helping developers build state of the art applications that are scalable (teams, code, etc), correct (you can understand what's going on under the hood), with a learning journey that adds complexity at the right times, etc. Thanks for sharing your hopes around these things."
technical,How often do you collect feedback from community developers? we talk to developers about Angular every day. But maybe I'm misunderstanding your question?
technical,"50/50. I'm writing production on Angular. Angular allows you not to write bad code. But almost always forces you to write a lot of low-level code, which takes away a monstrous amount of time Well said. Agreed. I'm just banging it out of the park here in South Florida in the Healthcare app industry.... Rocking Cordova back to life with ""PWAs"" cause web-apps got a bad taste so now we gotta call doing it right a PWA"
technical,"AWWWWWWWWWwwwwwwww  Alright I'm often full of sassy comments and I don't have troubles any more cause I mastering this shit!  Here, try my code:  Demos and docs at all... I love this Angular.... Sorry you're having such a tough time making the cash rain from Angular. I be milking the sucker ova her! When the title of this issue was grammatically incorrect, it was evidence of the sheer agitation you have with Angular. But your body comments were well typed and thought out.  Angular makes me feel on a high like the Beck WoW song."
technical,"Well said. Agreed. I'm just banging it out of the park here in South Florida in the Healthcare app industry.... Rocking Cordova back to life with ""PWAs"" cause web-apps got a bad taste so now we gotta call doing it right a PWA Yes, the Angular team makes a tool for developers. But I would like to pay attention to the little things, because of which it sometimes hurts."
technical,"Your last comment mostly speaks to popularity.... And I'm sure you can agree NOT always the most popular packages on NPM are the best ones.  I hear ya man... I choose Angular for my own sanity, not others. I will deal with the less jobs and packages, nor now. you don't think Angular is for production?  I got a Golf Cart, a 1978 Joker Poker pinball machine, a 2017 Wizard of Pinball machine, and a Baby PAC man pinball machine on the way THAT WERE ALLL paid for by Angular production code produced at the following companies:"
technical,Electromagnetism is a phenomena  I wouldn't put complainers in that category.  It's just complaining cause this Angular works the way you say it don't You want to say that you cannot reproduce it ? Don't say that it works man
technical,"i'm Software Engineer too and i like angular.  But now he wants to become a robot framework. It is good to use for academic purposes, like a barbell that helps build muscle. But in production with it is more difficult than with react or vue. It does not help to solve problems, it simply does what it does.  I really hope that in the future it will be what the framework for an engineer should be - a tool in production, not a bar in the gym Your last comment mostly speaks to popularity.... And I'm sure you can agree NOT always the most popular packages on NPM are the best ones.  I hear ya man... I choose Angular for my own sanity, not others. I will deal with the less jobs and packages, nor now."
technical,"ok - so include playbook would be a feature request? Or can we track it here? E.g., I'm running a git-checkout task on localhost, and then want to include playbook: This should include (lazy-load) the updated playbook from that repository, AFTER pulling the latest commit from the remote. Currently, import playbook imports the ""old"" playbook, then pulls, and then runs the outdated playbook. ... please read the subject of this ticket, that is EXACTLY what we are tracking here"
technical,"ok - so include playbook would be a feature request? Or can we track it here? E.g., I'm running a git-checkout task on localhost, and then want to include playbook: This should include (lazy-load) the updated playbook from that repository, AFTER pulling the latest commit from the remote. Currently, import playbook imports the ""old"" playbook, then pulls, and then runs the outdated playbook. +1 as well, would love to see this feature"
technical,"Include conditionals are very useful for creating branches in our playbooks.  Will it be ensured that import playbook will support conditionals in the next release citing this issue?  If not, you are losing a lot of power and will end up creating a lot of hacks.  Not to mention breaking a ton of include playbook conditionals in end user plays. +1 on implementing a ""when"" conditional."
technical,"The bot added that label, and it looks like it was a false positive.  No work is being done on this feature, nor are there any plans to work on it currently. any plans for this one ?"
technical,"Is this going to be fixed soon? I find it as a serious source of problems because the optional playbook code ca be huge when it comes to number of tasks, causing over **extensive console/log verbosity** of tasks that are never supposed to be loaded. The skip reason"": ""Conditional result was False"" is not of much help either because the user will not see any condition on those tasks, the condition being few nested includes/imports away in another file. It helps nobody that Ansible will list hundreds of lines of files that were never supposed to be run, making much harder to investigate them. As a workaround until this is possible, if the use case is to support different host groups it is possible to use an include task with a conditional on group names, i.e. a task include for a group foo is included when a file tasks directory/foo.yml is provided. The playbook including this construct has to run for all hosts."
technical,"I'm trying to do what he was, using ansible-galaxy to install roles, and then using the roles.  I found a decent workaround for the all-in-one playbook, which was to place the playbooks, in the order that you want them to execute, on the command line. In Watsonb's case, that would look like. Any variables that you set on the command line are passed to the playbooks, sequentially.  This might also solve their problem, also, using file globbing on the command line instead of in the playbook. decet It could, but that would basically mean using a top level bash script as the entrypoint and spliting the main ansible playbook into multiple stage files. Not too horrible, just ugly."
technical," Files identified in the description. If these files are inaccurate, please update the component name section of the description or use the !component bot command. click here for bot help."
technical,"I (still) agree with subcan that this feature should exist. This would be a great thing to get implemented while we're all on COVID-19 lockdown! ,)  As to options for managing multiple configurations, there is another option in between Ansible vault files and full-blown Hashicorp Vault deployment. I wrote a tool I named ""python secrets"" and it works well with Ansible to manage multiple sets of variables outside of a Git repository (including one with Ansible playbooks). I document how to use it this way and have described it in several public talks listed on my home page (my talk at WSLConf from earlier this month will be added as soon as the videos are released). Files identified in the description. If these files are incorrect, please update the component name section of the description or use the !component bot command. click here for bot help"
technical,"This is a frustrating ""missing feature"" Since there is no plan to allow for conditional vars prompt, the only other way to conditionally get user input at start of playbook would be with conditional ""import playbook"" based on whether a tag was sent in on commandline. I understand that we want a playbook to require no user input, but this is just not a realistic scenario. When you have several users of a given playbook, then you need to ask for passwords. The other option is having dozens of ""ansible vault"" files (nightmare), or by implementing hashicorp vault (first get everything working as desired then implement another level of integration)... I (still) agree with subcan that this feature should exist. This would be a great thing to get implemented while we're all on COVID-19 lockdown! ,)  As to options for managing multiple configurations, there is another option in between Ansible vault files and full-blown Hashicorp Vault deployment. I wrote a tool I named ""python secrets"" and it works well with Ansible to manage multiple sets of variables outside of a Git repository (including one with Ansible playbooks). I document how to use it this way and have described it in several public talks listed on my home page (my talk at WSLConf from earlier this month will be added as soon as the videos are released)."
technical,"Just to explain why include tasks is not enough. The system is clustered and the tasks talk to localhost, other physical hosts and virtual machines. We could in theory use delegate to if it supported host groups. I also have this need in order to import different playbooks to configure Vagrant guests differently depending on the active hypervisor. Alternate to when is something like (this is also not supported):"
technical,"I've overcome this in my own way as follows.  First, my typical playbook directory structure.  My .gitignore ignores most sane OS/language/IDE things, but also ignores everything in the roles/ and playbooks/ folders except for the requirements.yml files in each folder.  The requirements.yml file within the playbooks folder is similar to your Galaxy-style requirements.yml, but rather than calling out dependent playbooks by Galaxy owner.name, I specify the full Git source (Galaxy supports this of course).  The requirements.yml within the roles/ folder is just your traditional Galaxy-style requirements.  I have this prerequisites.yml playbook, that looks like this. And, assuming that my ""big bang"" create.yml depends on a playbook and its roles from another playbook project, I import it like this: And so, the work-flow to run my ""big bang"" (e.g. create.yml) is a 3-liner. You could, of course, wrap the above 3-liner in a create.sh shell script for convenience.  This method has served me well for some fairly complex playbook projects that depend on other playbook projects.  This forces us to keep roles and playbooks fairly self-contained and re-usable and factor variables out into their own inventory projects.  When performed with discipline, it makes it really easy to migrate unaltered roles/playbooks to other environments, then just update inventory variables that are unique to that environment.  **This doesn't solve the conditional import problem**, mind you, but does help me use the import playbook statement for something that may not exist just yet.  It kind of gets around a conditional in my very specific use-case.  I make use of tagging on the import playbook to leverage the command-line --tags and --skip-tags features if I need scalpel-like precision at run-time. But if you had to make an import decision based on some other conditional logic (e.g., OS family), well, we still need that as a language feature I think.  For now, I just handle those cases with sub-playbooks and chain them together ensuring I target the appropriate hosts/groups that should or should not be targeted based on how I've setup my inventory (yes, it can get messy).  This is all pretty wild and requires a high degree if what I commonly refer to as ""4th dimensional thinking"", especially when you consider branches/versions of things and running them from CI/CD platforms like Jenkins or even AWX.  But I still find Ansible fascinating and use it daily. I see that this issue now has a ""has pr"" label. I searched a lot, but I can't find the PR. Can someone link it here?"
technical,soon-ish we approaching 1 y since this request was open and no progress so far ...  any chance this get some attention? much thanks ! I would like to vote a '+1'as well.
technical,"any plans for this one ? I'd just like to mention that there is a bit in the documentation that insinuates that the feature requested already exists. See. Specifically, the note mentions that Ansible allows when to work with playbook includes since version 2.0. Perhaps this documentation should also be amended."
technical,"true - the part the confused me is ""conditionally import..."". import playbook and include playbook are the feature we want. I'm trying to do what he was, using ansible-galaxy to install roles, and then using the roles.  I found a decent workaround for the all-in-one playbook, which was to place the playbooks, in the order that you want them to execute, on the command line. In Watsonb's case, that would look like. Any variables that you set on the command line are passed to the playbooks, sequentially.  This might also solve their problem, also, using file globbing on the command line instead of in the playbook."
technical,"true - the part the confused me is ""conditionally import..."". import playbook and include playbook are the feature we want. I've locked this to contributors for now.  Adding +1 comments is too noisy.  For future reference, add a reaction to the issue body, and don't comment."
technical,"decet It could, but that would basically mean using a top level bash script as the entrypoint and spliting the main ansible playbook into multiple stage files. Not too horrible, just ugly. I've overcome this in my own way as follows.  First, my typical playbook directory structure.  My .gitignore ignores most sane OS/language/IDE things, but also ignores everything in the roles/ and playbooks/ folders except for the requirements.yml files in each folder.  The requirements.yml file within the playbooks folder is similar to your Galaxy-style requirements.yml, but rather than calling out dependent playbooks by Galaxy owner.name, I specify the full Git source (Galaxy supports this of course).  The requirements.yml within the roles/ folder is just your traditional Galaxy-style requirements.  I have this prerequisites.yml playbook, that looks like this. And, assuming that my ""big bang"" create.yml depends on a playbook and its roles from another playbook project, I import it like this: And so, the work-flow to run my ""big bang"" (e.g. create.yml) is a 3-liner. You could, of course, wrap the above 3-liner in a create.sh shell script for convenience.  This method has served me well for some fairly complex playbook projects that depend on other playbook projects.  This forces us to keep roles and playbooks fairly self-contained and re-usable and factor variables out into their own inventory projects.  When performed with discipline, it makes it really easy to migrate unaltered roles/playbooks to other environments, then just update inventory variables that are unique to that environment.  **This doesn't solve the conditional import problem**, mind you, but does help me use the import playbook statement for something that may not exist just yet.  It kind of gets around a conditional in my very specific use-case.  I make use of tagging on the import playbook to leverage the command-line --tags and --skip-tags features if I need scalpel-like precision at run-time. But if you had to make an import decision based on some other conditional logic (e.g., OS family), well, we still need that as a language feature I think.  For now, I just handle those cases with sub-playbooks and chain them together ensuring I target the appropriate hosts/groups that should or should not be targeted based on how I've setup my inventory (yes, it can get messy).  This is all pretty wild and requires a high degree if what I commonly refer to as ""4th dimensional thinking"", especially when you consider branches/versions of things and running them from CI/CD platforms like Jenkins or even AWX.  But I still find Ansible fascinating and use it daily."
technical,"I'd just like to mention that there is a bit in the documentation that insinuates that the feature requested already exists. See. Specifically, the note mentions that Ansible allows when to work with playbook includes since version 2.0. Perhaps this documentation should also be amended. If only unfixed issues aged like fine wine instead of like bait fish.... But seriously is this ever going to either be fixed or closed as won't fix? The has pr label typically indicates at fix is being gestated, but in this case there is no hint as to where we should be looking for the fix."
technical,"I'd just like to mention that there is a bit in the documentation that insinuates that the feature requested already exists. See. Specifically, the note mentions that Ansible allows when to work with playbook includes since version 2.0. Perhaps this documentation should also be amended. If someone is interested how to use play-level variables for conditional playbook-import: 1. Set up that variable as a fact in the play 2. Use when with import playbook to check this variable (with full path, hostvars.hostname.a variable)  If someone is interested, I managed to make import playbook be conditional on --limit in the command line: An example."
technical,"Is this still being worked at all?  I'm kind of implementing something like an ansible-galaxy style method of ""installing"" playbooks into a playbooks sub-directory and then I want to include playbook a playbook that was just ""installed"".  Here is what I've got so far.  If I run this as-is, I get an import error because the playbook to be imported isn't there yet. I suppose this could be split up into two (2) separate playbooks in the same Git repo.  The first would be called prepare.yml or maybe prerequisites.yml to ""install"" the other needed playbooks and the second main playbook (playbook.yml) will do the necessary imports, etc.  I was really wanting to make this a ""one-shot"" playbook. Include conditionals are very useful for creating branches in our playbooks.  Will it be ensured that import playbook will support conditionals in the next release citing this issue?  If not, you are losing a lot of power and will end up creating a lot of hacks.  Not to mention breaking a ton of include playbook conditionals in end user plays."
technical,"I also have this need in order to import different playbooks to configure Vagrant guests differently depending on the active hypervisor. Alternate to when is something like (this is also not supported): Is this going to be fixed soon? I find it as a serious source of problems because the optional playbook code ca be huge when it comes to number of tasks, causing over **extensive console/log verbosity** of tasks that are never supposed to be loaded. The skip reason"": ""Conditional result was False"" is not of much help either because the user will not see any condition on those tasks, the condition being few nested includes/imports away in another file. It helps nobody that Ansible will list hundreds of lines of files that were never supposed to be run, making much harder to investigate them."
technical,"If someone is interested how to use play-level variables for conditional playbook-import: 1. Set up that variable as a fact in the play 2. Use when with import playbook to check this variable (with full path, hostvars.hostname.a variable)  If someone is interested, I managed to make import playbook be conditional on --limit in the command line: An example. is this issue/request still up to date in a more current version of ansible? Iam using ansible 2.6.1 and the when condition doesn't seem to work when I use the import playbook function."
technical,"As a workaround until this is possible, if the use case is to support different host groups it is possible to use an include task with a conditional on group names, i.e. a task include for a group foo is included when a file tasks directory/foo.yml is provided. The playbook including this construct has to run for all hosts. Is this still being worked at all?  I'm kind of implementing something like an ansible-galaxy style method of ""installing"" playbooks into a playbooks sub-directory and then I want to include playbook a playbook that was just ""installed"".  Here is what I've got so far.  If I run this as-is, I get an import error because the playbook to be imported isn't there yet. I suppose this could be split up into two (2) separate playbooks in the same Git repo.  The first would be called prepare.yml or maybe prerequisites.yml to ""install"" the other needed playbooks and the second main playbook (playbook.yml) will do the necessary imports, etc.  I was really wanting to make this a ""one-shot"" playbook."
technical,"this is a feature request, you currently CANNOT conditionally import playbooks, the conditions above happen to skip all the tasks in one, but this is not a supported behaviour and not guaranteed to work across versions of Ansible. just to be clear: import or include? because I think the current implementation or naming is actually wrong, based on the definition in. Does import playbook actually ""lazy load"" or does it get loaded & parsed with the yaml file?"
technical,"We have similar need. We have a main playbook that prepares the system, but we allow the user to provide some extra steps by creating a playbook files in a well known directories. We do not know what files will be present there in advance. For this we would really like to have include playbook that supports with fileglob (or with items). Just to explain why include tasks is not enough. The system is clustered and the tasks talk to localhost, other physical hosts and virtual machines. We could in theory use delegate to if it supported host groups."
technical,"You need to initialize variables before doing when. Just add some random task to random host (before doing first 'import playbook'). F.e., do set fact on localhost, as in example above. many thanks for the quick response.   Unfortunately it doesn't seem to work for me. Even if answer the prompt with ""no"" the playbook ""vmware createsnap.yml"" gets invoked. The other tasks are working as expected. What I am doing wrong? My playbook:"
technical,"neither, it gets loaded at 'playbook compile time' which is before execution but not on file load  there is no include playbook, that is the whole purpose of this feature request, to add one mhm that sounds even more wrong - or am I missing the big picture here? I would have expected that import task and include task have an * playbook sibling..."
technical,"If only unfixed issues aged like fine wine instead of like bait fish.... But seriously is this ever going to either be fixed or closed as won't fix? The has pr label typically indicates at fix is being gestated, but in this case there is no hint as to where we should be looking for the fix. My previous comment still stands as an answer to these questions."
technical,"just to be clear: import or include? because I think the current implementation or naming is actually wrong, based on the definition in. Does import playbook actually ""lazy load"" or does it get loaded & parsed with the yaml file? neither, it gets loaded at 'playbook compile time' which is before execution but not on file load  there is no include playbook, that is the whole purpose of this feature request, to add one"
technical,"the engine never supported that, why include: was very misleading and we had to separate it into the different include X/import X options and make each behaviour explicit. So include X is dynamic aka runtime, while import X is 'static' aka 'compile time'. ok - so include playbook would be a feature request? Or can we track it here? E.g., I'm running a git-checkout task on localhost, and then want to include playbook: This should include (lazy-load) the updated playbook from that repository, AFTER pulling the latest commit from the remote. Currently, import playbook imports the ""old"" playbook, then pulls, and then runs the outdated playbook."
technical,"+1 on implementing a ""when"" conditional. soon-ish we approaching 1 y since this request was open and no progress so far ...  any chance this get some attention? much thanks !"
technical,"I see that this issue now has a ""has pr"" label. I searched a lot, but I can't find the PR. Can someone link it here? The bot added that label, and it looks like it was a false positive.  No work is being done on this feature, nor are there any plans to work on it currently."
technical,"mhm that sounds even more wrong - or am I missing the big picture here? I would have expected that import task and include task have an * playbook sibling... the engine never supported that, why include: was very misleading and we had to separate it into the different include X/import X options and make each behaviour explicit. So include X is dynamic aka runtime, while import X is 'static' aka 'compile time'."
technical,"many thanks for the quick response.   Unfortunately it doesn't seem to work for me. Even if answer the prompt with ""no"" the playbook ""vmware createsnap.yml"" gets invoked. The other tasks are working as expected. What I am doing wrong? My playbook: this is a feature request, you currently CANNOT conditionally import playbooks, the conditions above happen to skip all the tasks in one, but this is not a supported behaviour and not guaranteed to work across versions of Ansible."
technical,"many thanks for the quick response.   Unfortunately it doesn't seem to work for me. Even if answer the prompt with ""no"" the playbook ""vmware createsnap.yml"" gets invoked. The other tasks are working as expected. What I am doing wrong? My playbook: This is a frustrating ""missing feature"" Since there is no plan to allow for conditional vars prompt, the only other way to conditionally get user input at start of playbook would be with conditional ""import playbook"" based on whether a tag was sent in on commandline. I understand that we want a playbook to require no user input, but this is just not a realistic scenario. When you have several users of a given playbook, then you need to ask for passwords. The other option is having dozens of ""ansible vault"" files (nightmare), or by implementing hashicorp vault (first get everything working as desired then implement another level of integration)..."
technical,"... please read the subject of this ticket, that is EXACTLY what we are tracking here true - the part the confused me is ""conditionally import..."". import playbook and include playbook are the feature we want."
technical,"Files identified in the description. If these files are inaccurate, please update the component name section of the description or use the !component bot command. click here for bot help. We have similar need. We have a main playbook that prepares the system, but we allow the user to provide some extra steps by creating a playbook files in a well known directories. We do not know what files will be present there in advance. For this we would really like to have include playbook that supports with fileglob (or with items)."
technical,"is this issue/request still up to date in a more current version of ansible? Iam using ansible 2.6.1 and the when condition doesn't seem to work when I use the import playbook function. You need to initialize variables before doing when. Just add some random task to random host (before doing first 'import playbook'). F.e., do set fact on localhost, as in example above."
technical,"I agree that SPIR-V is a step forward. However, it does not address all issues of exhaustive jitting.   you do not need special knowledge of the very details of the hardware to run on  Is this a copy&paste from OpenCL 1.0 marketing, which claimed exactly the same? You will  always  need to go down to the details of the underlying hardware if you aim for maximum performance. This is especially the case in the context of fast tensor contractions. ...as  scott-gray demonstrated with neon"
technical,"If you are going to write an article, I guess it wouldn't hurt to also explain (took 3 hours to get the whole picture):  * TF has in fact a SYCL 1.2 backend. No \*actual\* opencl.  in turn, you have two implementations of the standard (trisycl looks cool, but it's limited * In the end, ComputeCpp 'hooks' SPIR/SPIR-V (in addition to PTX, but this is really another story And this is what eventually gets you straight to your bloody yearned OpenCL 1.2 (w/  cl khr spir ext) HIP instead is yet another backend, sits opposite to SYCL, and targets only and exclusively ROCm (or well, lol, even in turn cuda if you have an nvidia gpu.. but this is again another story)  AMD are building an alternative path that's fully open source - translating the CUDA code to OpenCL code with a compiler toolchain. Nope. You are talking about HIP, and.. that's actually it, what you eventually convert your code *to*. Which is not OpenCL. HIP then runs on ROCm as I was saying... ROCm which is *also* what runs OpenCL for you (on supported cards), but please I'd stress everybody to notice how the relations is only forward from ROCm, never ""intra-sub-layers"" What you are perhaps thinking about could be coriander.   I'm not sure what the status of that is for the older cards like mine though.  Summed up here: fully fledged AMDGPU-PRO, amdgpu-pro-opencl-only driver as you are doing now ... Or continuing to wait until the end of the decade for somebody to finally make clover usable.  Also, fglrx... But if that's hard to recommend for pre-gcn cards, I guess it's just better to draw a veil over. 1. I'm not concerned with pre-GCN cards. Mine's a Sea Islands and I'm not planning on acquiring anything older. Then again, I'm not planning on acquiring another AMD GPU either. ,-) 2. I don't know whether ROCm will run on my workstation - there's no open source hardware tester that can give me a yes-or-no answer. I've opened an issue for that and received no response. 3. SPIR-V is a compiler target - I took a look at it and threw my hands up, not having a budget to hire a compiler writer.  So that leaves SYCL ... or throwing up my other two hands and doing everything with Keras, which has TensorFlow, Theano (which is getting frozen), CNTK or PlaidML back ends. From a purely engineering economics point of view, Keras / PlaidML is a big winner provided I get TensorBoard somehow."
technical,Codeplay has made a lot of progress on the opencl front. Stay tuned for a big push to in the next few weeks. A fast implementation of the convolution operation would be extremely helpful in TensorFlow. Looking forward to it.
technical,"The official (but experimental) OpenCL Caffe branch can be made to run on Android GPUs, however the performance at the moment is far from optimal. A real alternative to cudnn could be the extension of OpenVx standard objects with support to Tensor, NdConvolution, NdPooling operators and (probably) some other operator that could be considered standardizable. Also cudnn team need to make some choice on what new API and operators they will introduce in every release. Of course a standard can not move as fast as cudnn releases but I think some operations and objects has enough ""citations history"" to be standardized."
technical,Great to read this is being worked on. It would help if Beignet 2.0 were polished. Lots of potential with Skylake and Iris right now. A recent pull request was added at if somebody want to take a look.
technical,"Will Vulkan and Opencl be fused? according to Raja's Reddit AMA, Vega will support Tensorflow, Cafe2, Cafe, Torch7 and MxNet via MIOpen."
technical,What's the latest on this? According to TF 1.1 Release Notes (Nvidia only) support has been deprecated. Let's hope this will help to improve OpenCL approach (not very confident on this).
technical,"there are few commits that you have to apply to get it compiling. I would suggest using tip of dev/amd gpu or if you don't want to change your current branch.. you can merge dev/amd gpu to it. Actually I m working on my unofficial Debian/Ubuntu package so I'm trying to keep close to official 1.3.1 release. I can live without OpenCL support but I'd to be ready to enable it as soon as it's correctly supported. Maybe I'll update packages against your branch for testing purposes, but that's enough for today ,)"
technical,"Yes, no problem. Contact us via infostreamcomputing.eu After reading through all this I'm guessing there's no solid solution yet for using OpenCL on macOS/OS X? I tried to compile Tensorflow C++ with OpenCL support ( which I assume requires ComputeCpp for SYCL 1.2 as someone pointed out ). I looked around and couldn't seem to locate where to download, compile or build the SYCL library. Is it here? I'm unsure really how to proceed, thanks..."
technical,"Depends on what you mean by ""general AMD GPU support"". If you mean really old dGPU or APUs, I don't know. But if you have newer (2nd Gen GCN or newer), hipTensorFlow (v1.0.1) running on ROCm was working pretty well. Ah yes I have seen AMD's work on ROCm. Unfortunately they only support Linux though, and it doesn't even seem like support for any other OS is on their roadmap. I'm hoping for something that supports Windows."
technical,"Yes in addition to the OpenCL branch above,  will be publishing a book (very soon) on using it for real-time inference on commodity hardware using amd and hd graphics. Ah, I was talking about hipCaffe, not the OpenCL branch: Installing ROCm to build/test hipCaffe required me to uninstall AMDGPU-pro, perhaps I'll try the vanilla branch again. It's poorly documented, unfortunately.. I suppose I'll try a blind ""make"" and see. So, I'm still interested to hear others' experiences with the AMD ROCm/HIP stack, if they're working on a Tensorflow fork it'd be great, provided it actually works on more than 3/4 models of AMD card in the wild."
technical,"If TensorFlow on OpenCL would help you in your work let me know, to the extent I can help advance research and practice of deep learning I'd like to help. My company has built an OpenCL back end for TensorFlow tuned for a variety of GPUs as part of our work in on-device inference. We have tested on the major mobile & desktop GPU families including common configurations on Windows & Mac. If there's enough interest we may do some kind of public distribution. We also have Metal (Apple GPU) and LLVM (CPU), along with a way to do zero-dependency deployment. The idea here being to give every device great support for deep learning. all of that sounds incredibly useful and helpful. My personal project  would greatly benefit from OpenCL on OS X, as well as Metal for iOS and Desktop deployment. If its possible for this to be introduced to Tensorflow proper I think it would be a tremendous boon for tons of developers.Thank you."
technical,"Nice, lucky you! Wish there was such studies when I was at the Heig-vd would have continued to a MSc certainly.  Yeah... That's what I figured. So much work, so little human resources available in these fields. All that stuff sounds great, but let's refocus the discussion on having TensorFlow working with OpenCL SYCL and not only vendor-specific solutions... :-) I hope RoC and other HiP have their own GitHub to discuss their own issues... at least i am still in the OpenCL realm. Join the club again! :-)"
technical,"Yeah I am also trying to get some MIOpen and TensorFlow stuff working on my RX 480, but I don't want to destroy my main development rig, so instead I use IOMMU virtualization and use an Ubuntu 16.04 virtual machine that can use the RX 480. The AMD drivers are very friendly to virtualization (unlike nVidia drivers made for the gaming cards - only the Quadro drivers do). All you gotta do is sudo apt-get install rocm miopen-hip"
technical,"The Tensorflow AMD OpenCL performance is very slow according to my tests. So i did some basic tests with an other Deep Learning framework. You will find my setup and benchmarks on my GitHub page here.  Long story short. The other Deep Learning framework is about 10 times faster than Tensorflow AMD OpenCL currently. AlphasCodes  I know the TF team prefer to keep the thread TF-only, we're happy to host the PlaidML-specific conversation over on the PlaidML project. That said, we do hope to eventually support TensorFlow itself as well as non-OpenCL platforms (e.g. Apple's Metal for iOS which currently exists in prototype form)."
technical,The Tensorflow AMD OpenCL performance is very slow according to my tests. So i did some basic tests with an other Deep Learning framework. You will find my setup and benchmarks on my GitHub page here.  Long story short. The other Deep Learning framework is about 10 times faster than Tensorflow AMD OpenCL currently. Also N4355 in c++17 could enter in the game soon or later
technical,"py.test is as good a solution as any, it's just a pip away and that's part of the process for installing tensorflow anyway. I've discovered a few interesting things since starting my tests, and they may not be debuggable using Python output alone, however: * Different calls to the same script may crash early, or may ""hang"" (no output, no progress, no response to Ctrl-C, process needs to be pkill -9'd), or may crash late either at the validation part or after the script completes successfully. Crashes (segfaults) may take down Xorg. * The results vary for seemingly no reason: I may call a script and have it segfault, then call it again and it will work. * Hangs can occur in portions of code that were working literally moments ago, I've had one hang occur within or after a training batch, after several hundred batches just happened successfully.  So, it might be that there's unresolved stuff on the GPU side, and that a good segfault is needed to clear it out? I don't know much about the GPU model or OpenCL yet, so I can't contribute much here. But, it might be that GPU debugging output is needed to properly explore what's happening. Also, I thought you were with AMD from your github, but it seems you're a ""rogue agent"" doing this whole CUDA-on-CL thing on your own time. Thanks sincerely for spearheading this! Is there some way that I and others can contribute for your efforts, perhaps by crowdfunding you a GPU? Or you could set up a Patreon, I'm happy to sign up for a monthly contribution to the project?"
technical,"We have experience with HIP, here at Stream. Let me take a look.  Agree on the ""my company is better"" arguing. I would like to know which GPUs TensorFlow should be targeting. It has to be pragmatic and useful. For instance Intel GPUs or embedded GPUs (Qualcomm, ARM, Imagination), RaspBerry Pi - yes or no? AMD Radeon Vega Frontier Edition. We continue to aggressively improve our ROCm open software platform and machine learning libraries.  We're also supporting open machine intelligence frameworks like Caffe (released in April). Later this quarter we plan to offer support for Torch, and **Tensor Flow** is in the works."
technical,"I do hope they are working on a hip backend, not a full fork. That would be sad and only split the working effort. There is enough tooling already :/ AMD's efforts are a bit poorly coordinated at the moment (yes, all forks). Also it doesn't seem to work that well on a large variety of devices yet, unlike the OpenCL branch of Caffe, for example..."
technical,"I do hope they are working on a hip backend, not a full fork. That would be sad and only split the working effort. There is enough tooling already :/ An interesting initiative at  also if here we rely on Eigen tensor extension."
technical,"Great, thanks for the update. I hope you are right about getting it done fully-featured in less than a year. Another step of Streamexecutor in LLVM"
technical,Another step of Streamexecutor in LLVM any chance of getting acceleration on the rx 480?
technical,"ComputeCpp 0.3.2 can build. Upstream is missing a patch to Eigen that fixes it. Any idea how to inject this Eigen patch during bazel build ? Maybe we should bump somewhere Eigen tgz version to get the fixed one ? Thanks, Adam."
technical,tf-coriander works Any tensorflow 1.3 gpu/opencl support out there on macos?
technical,Sure - I'm heading to dinner but I'll file it when I get back Any way to use tensorflow GPU in Mac with AMD processor ?
technical,"Honestly someone smarter than I should look into integrating Mac OS 10.13's MPS - Metal Performance Shaders which have support for a large set of neural network primitives out of the box. This would allow up to date, high performance GPU for mobile and Desktop iOS and macOS Tensorflow inference deployment. You can't train with Apples primitives as I understand it (they don't provide anything), but with Tensorflow support maybe you could? I imagine it for folks on the Apple platform that would be a boon. I don't think Google would provide this internally, and I don't have nearly the requisite skills to attempt it myself. Posting this idea so folks more talented than I might take it on. :) Apple is solely aimed to sell Apple devices. Google is aimed to hire Google massive services. If you're willing to do AI (learning) with one single device, like one Apple Laptop, you'll do ""Superficial Learning"" instead of ""Deep Learning"", so you'd better give up doing anything but tutorials. Inference results in a trained model for one single user, in a single device (even in a not-too-many-multicore phone), could be neat to be done through GPUs, but is perfectly doable only with CPUs. On the other side, GPUs are absolutely needed if you're gonna feed extremely large datasets for learning or you're gonna serve trained inference to extremely large concurrent customer groups. Even though, doing it in such scale, is not that easy due to the network issues. Just have a look to the TPU-Pods physical architecture. It's in the antipode of a laptop (several GPUs per memory-overloaded multi-core server, with dedicated optical-fiber for inter-server communications). I have a MacBook Pro. It's a nice terminal to get to the cloud :-D"
technical,"Honestly someone smarter than I should look into integrating Mac OS 10.13's MPS - Metal Performance Shaders which have support for a large set of neural network primitives out of the box. This would allow up to date, high performance GPU for mobile and Desktop iOS and macOS Tensorflow inference deployment. You can't train with Apples primitives as I understand it (they don't provide anything), but with Tensorflow support maybe you could? I imagine it for folks on the Apple platform that would be a boon. I don't think Google would provide this internally, and I don't have nearly the requisite skills to attempt it myself. Posting this idea so folks more talented than I might take it on. :) APUs do support OpenCL for both the CPU and GPU part. This should work pretty much out of the box when the OpenCL support is ready. Meanwhile, if you already have an APU and want to try out another ML framework, BVLC OpenCL Caffe already works."
technical,"What's your approach to benchmarking? We time the models included with Keras and normalize against  because that is a common and well optimized configuration. Our methodology is similar to Max Woolf, it's not much code but we'd be happy to share it. We have some throughput numbers on our web site, our code is very slightly faster than TF 1.2 on Xception inference and it would be interesting to compare more approaches side-by-side. Are there any Windows solutions? I would install Ubuntu on my PC but I currently don't have enough space to do it."
technical,"hi all,  we will coordinate the effort of porting Eigen's tensor module to SYCL for OpenCL as we already have something mostly working, but it's not ready for review yet.  We are in favour of this approach as it will introduce less invasion to the code base. SYCL supports the single-source C++ templated model that eigen already uses.  Road map design is in progress so it shouldn't be too long now.  Thanks, Luke Are you working or in contact with upstream? Do you think will be accepted upstream in Eigen?"
technical,"After reading through all this I'm guessing there's no solid solution yet for using OpenCL on macOS/OS X? I tried to compile Tensorflow C++ with OpenCL support ( which I assume requires ComputeCpp for SYCL 1.2 as someone pointed out ). I looked around and couldn't seem to locate where to download, compile or build the SYCL library. Is it here? I'm unsure really how to proceed, thanks... As far as I know that there is still not a ComputeCpp for macOS. So that means OpenCL for macOS is not ready."
technical,"Is there any GPU accelerated tensorflow package available for Windows? I thought, if you want GPU accelerated deeplearning, Linux was only choice. If TensorFlow was ported to OpenCL, would that make it easier to port to Windows? I'm not sure why TensorFlow is not available on windows with GPU acceleration when CUDA is supported there.  I guess this is now off topic, but if anyone know of TensorFlow and/or PyTorch for windows that is GPU accelerated, I'd like to know about it as well... As far as I know, Tensorflow already supports Nvidia GPU acceleration on Windows."
technical,"thanks for the good summary with all the links. I think you have not wasted your 3 hours... :-) As I told you quite some times, no it won't work. Pre GCN 3rd gen gpus simply lack the hardware for ROCm to either perform or even work at all.  SPIR(-V).. I'm not sure what you are talking about. It's not your job to care about that. Computecpp makes it from SYCL ""commands"", and then it's all (opencl) driver business.  You have what I'm tentatively calling amdgpu-pro-opencl-only, and I'm not sure what's the problem then. EDIT: would be also cool to have some sort of ETA for Luke's code to land"
technical,"you might want to look  to learn how some functions are mapped. At my company StreamComputing we have various GPUs for build-testing and benchmarking, which we use for our customer-projects. I could hook your Github into our Jenkins to do a weekly run."
technical,"A real alternative to cudnn could be the extension of OpenVx standard objects with support to Tensor, NdConvolution, NdPooling operators and (probably) some other operator that could be considered standardizable. Also cudnn team need to make some choice on what new API and operators they will introduce in every release. Of course a standard can not move as fast as cudnn releases but I think some operations and objects has enough ""citations history"" to be standardized. At the moment, I haven't tried any deep learning library, I'm just doing some scouting to see which library I could potentially use. Have you tried cltorch and DeepCL on Android? I just assumed cltorch did work on Android, since there is an implementation of Torch that is dedicated specifically for Android. And why would you have such an implementation if there already was one that both worked on Android and used OpenCL, right? But maybe I should have known better."
technical,"A real alternative to cudnn could be the extension of OpenVx standard objects with support to Tensor, NdConvolution, NdPooling operators and (probably) some other operator that could be considered standardizable. Also cudnn team need to make some choice on what new API and operators they will introduce in every release. Of course a standard can not move as fast as cudnn releases but I think some operations and objects has enough ""citations history"" to be standardized. At the very least, the Eigen library would have to support OpenCL."
technical,henline and jlebar are the experts to answer the difference between streamexecutor? Axcell and StreamExecutor are separate projects.  There are no current plans to merge them.  I leave it up to the TensorFlow folks to say whether or not they plan to switch.
technical,"I just assumed Benoit because he self assigned the feature, but I think you've got it Junli! Maybe start with an email or forum thread of interested parties? benoitsteiner knows more about interested parties that may not have shown up in this thread (or this issue). I'd wait for him to coordinate to make sure we avoid duplicating work"
technical,"I'm really interested in karlrupp and  opinions. I hope they want to join in the discussion on the new google  group. benoitsteiner Thank you for the update. It would be wonderful if all involved partners in KhronosGroup (Google, Nvidia, Amd, Intel, Codeplay, Xilinx etc.) will promote a cudnn like API in a standardized way. A sort of Khronos openvx computer vision standardization effort but for deep learning."
technical,"bensander Thanks, I'll upgrade. bensander Anything else I need from the AMD stack? All I have now is the AMD proprietary opencl library that uses the open source ""amdgpu"" driver."
technical,is the officially supported way to run tensor flow on AMD hardware. bensander Does the ROCm runtime work on Ubuntu 16.04.3? I haven't been able to get it working.  P.S.: Do you have any insight if / when the AMDGPU-Pro setup will work on Ubuntu 16.04.3? I need that for another project.
technical,"masahi - if you install the rocm and rocm-libs (i.e. ""apt-get install rocm rocm-libs"") that should be all your need.  The rocm docs at the repot has full instructions including expected results. bensander how do I know if I am running rocm 1.6.4 correctly (and not 1.6.3) ?"
technical,"masahi  - make sure you have rocm 1.6.4 base installed. bensander Thanks, I'll upgrade."
technical,"Me too. I can't afford to buy a machine with NVIDIA graphic card. Besides the above 2 posts, I'd like to add that now AMD's Vega GPUs (including the ones inside Raven Ridge APUs) can do FP16 at twice the FLOPS, so if TF could support them (through OpenCL) it would really help people with less budget. Also a lot of these people would be students, and if we get them to use TF as the starting point of their DNN journey, they would probably stick with TF down the road, and even tell others about TF, it's a great way to help expand this project."
technical,"The website  is created to support open source porting projects just like these! We're currently installing all necessary tools at the website and have space for repositories at  later on we're adding build-servers to test for several types of hardware and can provide our expertise in how to write code that runs at full speed on numerous hardware.  We're launching a porting initiative for GEGL next week, but we're happy to also support you. bhack from that thread and here it seems like  is looking into it. I think we have enough willing people to work on it, we just need benoitsteiner,  or gujunli to coordinate. Benoit has been quiet, maybe he's on holiday."
technical,"I really hope that could be compiled also with an opensource tool.  how it is going with your new Opencl branch bhack It would be nice to see if it can work with triSYCL in CPU OpenMP host device mode first. But I do not have the bandwidth to enter the TensorFlow/Eigen build system rnow. :-( If someone wants to try, feel free to do so. :-)  should allow to run OpenCL kernels soon in OpenCL interoperability mode, but not in the SYCL single source mode we all dream about because we do not have the Clang/LLVM outliner yet to extract the kernels from SYCL. But Khronos open-sourced recently the components from AMD & Intel to support OpenCL C++ 2.2 & SPIR-V that would be the basis of it. So it is ""just"" a question of time..."
technical,"My apologies for not contributing more to this discussion recently, my plate has been more than full these past 2 weeks.  I'll be coordinating the OpenCL effort on the TensorFlow side. Our current thinking is: - TensorFlow relies on c++11 and has taken a ""single source"" approach, so SYCL seems like a great fit. - We don't have a lot of OpenCL experience in house, so we're collaborating closely with Codeplay to bridge this gap. In particular, Codeplay is currently leading the effort to add support for SYCL to the Eigen tensor library. - TensorFlow relies on the cuDNN library to compute convolutions on NVidia GPUs. If somebody is interested in contributing an OpenCL equivalent, we'd be happy to help.  In order to help structure the effort, I created a mailing list: tensorflow-openclgooglegroups.com. bhack sure I have some interest for high-end C++ on FPGA :-) TensorFlow sounds like a good validation use-case for triSYCL too. By the way, if some people here are looking for some internships on this subject, I have some positions. It looks like Codeplay is looking for some people too, if I trust their web site."
technical,"Thank you for the inputs BTW, AMD are building an alternative path that's fully open source - translating the CUDA code to OpenCL code with a compiler toolchain. I'm not sure what the status of that is for the older cards like mine though."
technical,"Yes I have still a bit of confusion between StreamExecutor, SyCL in eigen, XLA (that actually has only a CUDA backend, other than CPU and opencl in some slides) Bump"
technical,"Are there any Windows solutions? I would install Ubuntu on my PC but I currently don't have enough space to do it. But if I use Eigen skcl guide can run on cpu with OpenCL support. (Also this guide code is a little out of date, need some modify) Any people can help to check how tensorflow python interface can also run with OpenCL support. And build tensorflow with this opt set will not really generate tensorflow binary.  --config=sycl Just build tensorflow in this command: Maybe I build forget --config=sycl I will try build command and verify whether it can call OpenCL library. After get result, I will post here."
technical,"Otherwise OpenCL 2.0+ and SYCL 2.2 support SVM, if you want to keep the same software architecture. OpenCL 2.0+ is supported by AMD and Intel GPU for example. In the embedded world, it is often supported by side effect even with OpenCL 1.x, since the host and device memories are often the same for cost reasons. But the most notable platforms, Linux + the new AMD GPUs (RX 480, upcoming Vega) do only support OpenCL 1.2 for now... and who knows when that's gonna change (my bet is on in a year). Beignet (opensource Linux Intel) for OpenCL 2.0 is also still a buggy mess, the stable version has 1.2. Also considering all the smaller companies that make OpenCL compatible chips are barely pulling 1.2 support. So I guess anything relying on OpenCL 2.0 will see very bad adaption rates in practice."
technical,Streamexecutor was renamed in LLVM parallel-libs and now is acxxel Can any Google member explain the difference and roadmaps between streamexecutor and ?
technical,"The SYCL specs describe in section 5.8 (""Address-space deduction"") how an implementation needs to deal with different memory types. This is similar to previous work done for PlayStation 3 and described in this paper: Offload “ Automating Code Migration to Heterogeneous Multicore Systems or C++ on Accelerators: Supporting Single-Source SYCL and HSA Programming Models Using Clang  hope that helps. Can I compile your tensorflow-opencl repo code  to apply my ARM board?  My ARM board has Imagination GPU which support opencl 1.2 ."
technical,"I thought, that SPIR-V would directly replace CUDA as a cross-hardware alternative Why does Google still rely on CUDA? Can these help?  OpenCL random number generation(Thomas Wang's):"
technical,"I thought, that SPIR-V would directly replace CUDA as a cross-hardware alternative Why does Google still rely on CUDA? Can we push long log on gist to let the thread to be still readable?"
technical,It seems AMD is working on that: Can XLA backends LLVM IR be converted to SPIR-V with
technical,It's in the Arch User Repository but it doesn't install - it doesn't install from GitHub source either. It looks like something simple - it can't find a few dependencies. Can you create an issue herEso that we can discuss there? Thanks!
technical,"I absolutely agree. To be honest, they really seem to lack human resources, they are working on all the fronts. By the way: Didn't know ETH had a Neuroinformatics ,) nice ! Can you elaborate on the ROCm/HIP stack for a layman like myself. I've been playing AMGPU-pro and AMDGPU with my Sea Islands cards so I'm sure I could post some useful results."
technical,"Hi, I also have interest in implementing TensorFlow on FPGA, using high level programming languages like Xilinx C++ or OpenCL. I am with pleasure to contribute if you have some plan. Can you explain what will be the role of StreamExecutor on Opencl and of relevant Canned operations for Tensorflow. I still cannot see how this will integrate with SyCL plans on Eigen and cudnn (replacement?)"
technical,"Congrats! Be sure to check the HIP project, as they tried to solve the same problem. They chose to create a new language called HIP, which defines what manually needs to be converted (like checking double precision support by checking compute level). While the project advances, the amound of manual translations would go down.   My suggestion for you is to use HIP and fix some bugs that are blocking for advancing Tensorflow or your own goals, as you now have the understanding of LLVM to do it. This way you don't have to solve the problems they already fixed. can't build python module with your fork following this I'm on ubuntu 16.04, dirname is from coreutils-8.25-2ubuntu2"
technical,"For those who want to try TensorFlow on AMD hardware using ROCm, I wrote a blog describing how to run Fast.ai notebooks using AMD Fury Nano. can't wait for this!"
technical,"For those who want to try TensorFlow on AMD hardware using ROCm, I wrote a blog describing how to run Fast.ai notebooks using AMD Fury Nano. Canned operations StreamExecutor provides several predefined kernels for common data-parallel operations. The supported classes of operations are: - BLAS: basic linear algebra subprograms, - DNN: deep neural networks, - FFT: fast Fourier transforms, and - RNG: random number generation."
technical,"plaidML is super cool. Works on keras. Of course I had to transfer some tf code to pure keras in order to work on plaidML backend (for example tf.image.ssim) But result - my code works on VIDIA and AMD cards.  Also plaidML is heaven for researchers. It automatically generates gradient for any function you will write on ""Tile"" language and it will work on your GPU with 80% speed of tensorflow. So I cannot understand why ML researchers still using PyTorch ? Let's boost ML science with Intel's plaidML ? Care to know why practically no one uses PlaidML ? 1. It runs pitifully slow on AMD's OpenCL implementations compared to Tensorflow's CUDA backend so there goes at least half the reason to use it. Performance so bad that using Tensorflow with CPUs is competitive or even outright beats their hardware using PlaidML ? 2. Nobody is interested in maintaining their specialized Tile programming language in which only someone like a pure maths professor would concoct so PlaidML's code quality just goes down the drain and no serious programmers in their right mind would want to deal with overly clever code ... 3. This pretty much ties into #2 but ever since Intel bought out Vertex.AI, they don't care about PlaidML anymore. Intel's solution for GPU compute accelerated machine learning is introducing a new compiler specifically for deep learning now known as nGraph to target Tensorflow, PyTorch or other deep learning frameworks as a backend for them. No reason for them to keep developing PlaidML anymore as their intermediary when they have nGraph ... People use PyTorch for other reasons such as maintainability or other features so to sum it up PlaidML is Intel's tool and they probably don't intend for it to play in any role of the final parts of their plans. nGraph's current Intel GPU backend is based off of OpenCL 2.1 of which only Intel has a conformant implementation so Intel only exists to look out for themselves rather than purely for the betterment of machine learning. When Intel goes on to further developing nGraph, I can't see them continue basing off their GPU backend on OpenCL 2.1 alone since many deep learning frameworks have templated kernels which are not compatible with OpenCL, Metal or Vulkan's separate source programming models so it's probably only for experimentation purposes. Intel's final GPU backend is probably going to either be based off of SYCL 2.2 or something else entirely different like OpenMP and maybe they'll even bring a vendor specific solution ... As for AMD, who cares ? OpenCL is irrelevant to them and they're finally showing some results with their work on HIP ..."
technical,for Eigen/OpenCL/SYCL Certainly would be interested in contributing. Please let me know when you plan to start.
technical,"As far as I know, Tensorflow already supports Nvidia GPU acceleration on Windows. CL tensofrflow is already a mess on linux, don't expect anything any soon. If you want to accelerate stuff there, there's only plaidML. (and please, we are already at 500 comments.. let's try to only post if really, really necessary)"
technical,"I am the author of a C++11 OpenCL BLAS library and am currently implementing half-precision support there. I am happy to contribute on the BLAS/GEMM part of this project and/or modify CLBlast to fit your needs better. CLBlast is now also available in OpenCL-Caffe, have you seen that? :) Did you also have a chance to look at the libDNN convolutions?"
technical,"What instructions are you following? Have you run clinfo? Have you run computecpp info? Does that indicate that your OpenCL drivers are installed as expected? The instructions for Ubuntu 14.04 are here and if you are using 16.04 there are some experimental instructions here. clinfo and clpeak both run. I haven't done this recently, but when I build caffe from source and run the tests it definitely hits the GPU. So I'm pretty sure the OpenCL / GPU drivers / libraries are working.  I'm on Arch Linux - kernel is their LTS - linux-lts 4.9.52-1. If it matters, the ""Bonaire"" peaks about 1.7 TFLOPS in 32-bit mode and is in the ""Sea Island"" family of AMD GPUs."
technical,"Is the eigen-opencl ready enough to support an opencl tensor flow development ?  Does tensorflow depend only on Eigen tensors or are there any other dependencies of Eigen ? Codeplay has just released partial support for OpenCL to Eigen Tensors. The code is available in this bitbucket repository. For the most part, TensorFlow depends on Eigen tensors. There are additional dependencies on Eigen for the linear algebra operations, but we don't have to provide an OpenCL compatible implementation of these operations (at least not initially). Therefore we're in a very good position to start supporting OpenCL in TensorFlow.  If you are interested in contributing, I have started to track what needs to be done in this spreadsheet."
technical,"I'm soon releasing a cuDNN replacement (only the convolution part, as it is the most performance and memory critical to have this done) for OpenCL on Caffe. Maybe it will also be to some use for the Tensorflow port. Codeplay has made a lot of progress on the opencl front. Stay tuned for a big push to in the next few weeks."
technical,"I'm soon releasing a cuDNN replacement (only the convolution part, as it is the most performance and memory critical to have this done) for OpenCL on Caffe. Maybe it will also be to some use for the Tensorflow port. Come to my talk next week at Arm TechCon !   You will need Mali drivers with SPIR-V support, which is probably not easily available, yet. And you will need a ComputeCpp runtime for Android with Arm CPU support and SPIR-V support, which is also not available (yet). So, you will have to be just a  little  bit patient."
technical,"I'm soon releasing a cuDNN replacement (only the convolution part, as it is the most performance and memory critical to have this done) for OpenCL on Caffe. Maybe it will also be to some use for the Tensorflow port. Comparing only 2 numbers is no information - who cares if OpenCL on NVidia runs at half speed if it runs at 4x speed on other GPUs?  I think we'd need these benchmarks: 1. CUDA on NV GPUs (reference benchmarks) 1. tf-coriander on AMD, Nvidia and Intel GPUs 1. tensorflow-opencl on AMD, Nvidia and Intel GPUs 1. tensorflow on AMD, Nvidia and Intel GPUs  The reference benchmarks are easy to be found. We have some high end GPUs here, so only need a place to put the numbers in (with links to building-docs)."
technical,"Is that a ComputeCpp version that can build TF at the moment ? I tried various versions between 0.3.2 and 0.1.4 and none worked. They all ended up with the ""multiple overloads of 'global ptr' instantiate to the same signature"" error. Btw, I cannot find the TensorDeviceSycl.h file in TF sources, is that a renamed one ? Is it possible to apply the patch to current sources ? Thanks in advance. ComputeCpp 0.3.2 can build. Upstream is missing a patch to Eigen that fixes it."
technical,"mirh to clarify the ""acronyms of magically random technologies [...] making [you] mad"":  In the Khronos Group realm, OpenCL is the low-level non-single source API and SYCL is the high-level *single-source* C++ domain-specific embedded language (DSeL). SYCL is expected to be built on top of OpenCL, so by transitivity when you use SYCL, often you use OpenCL.  Since TensorFlow uses Eigen which uses a *single-source* C++ approach with *single-source* CUDA, when it was later ported to OpenCL, SYCL was chosen because it is the Khronos Group standard way to have *single-source* C++.  But if you think about CUDA, it is even more subtle.  Almost everybody uses the high-level  *single-source* version of CUDA which is actually named ""CUDA **Runtime** API"". This is somehow similar to SYCL. But there is actually a less known low-level *non single-source* version of CUDA which is called ""CUDA **Driver** API"", similar to OpenCL, and used for example by the ""CUDA **Runtime** API"" implementation itself.  Since it is a kind of FAQ, I clarified a little bit ComputeCpp which is the SYCL implementation you are using with TensorFlow does not yet support Ubuntu 17.10. You would need to stick to Ubuntu 16.04 which is the current LTS. Instructions and pre-requisites are here.  As an aside, OpenCL support for TensorFlow does not mean just AMD device support. The SYCL integration is also enabling other OpenCL devices. As part of the work we are doing with TensorFlow, support for ARM and Intel GPUs will be available when the latest drivers from these companies are available. We are also working to enable support for Renesas accelerator processors too for the R-Car platform."
technical,"Also, I thought you were with AMD from your github, but it seems you're a ""rogue agent"" doing this whole CUDA-on-CL thing on your own time. Thanks sincerely for spearheading this! Is there some way that I and others can contribute for your efforts, perhaps by crowdfunding you a GPU? Or you could set up a Patreon, I'm happy to sign up for a monthly contribution to the project? Concerning AMD GPUs, we're a partner of AMD. See my message of 8 days ago, which you might have missed: At my company StreamComputing we have various GPUs for build-testing and benchmarking, which we use for our customer-projects. I could hook your Github into our Jenkins to do a weekly run."
technical,"Yes, I hope they discuss some DeepLearning beneficial stuff :) Congrats! Be sure to check the HIP project, as they tried to solve the same problem. They chose to create a new language called HIP, which defines what manually needs to be converted (like checking double precision support by checking compute level). While the project advances, the amound of manual translations would go down.   My suggestion for you is to use HIP and fix some bugs that are blocking for advancing Tensorflow or your own goals, as you now have the understanding of LLVM to do it. This way you don't have to solve the problems they already fixed."
technical,"Initial OpenCL/SyCL support was merged in master Congratulations!  Btw, what happened to the triSYCL repository? It seems to be gone and I can only find a reference to Khronos' Gitlab which is not publicly accessible.  EDIT: I found your private clone, only the one from amd is gone."
technical,"bhack It would be nice to see if it can work with triSYCL in CPU OpenMP host device mode first. But I do not have the bandwidth to enter the TensorFlow/Eigen build system rnow. :-( If someone wants to try, feel free to do so. :-)  should allow to run OpenCL kernels soon in OpenCL interoperability mode, but not in the SYCL single source mode we all dream about because we do not have the Clang/LLVM outliner yet to extract the kernels from SYCL. But Khronos open-sourced recently the components from AMD & Intel to support OpenCL C++ 2.2 & SPIR-V that would be the basis of it. So it is ""just"" a question of time... Could someone provide estimates for when Tensorflow might be able to run with OpenCL (AMD GPUs)? And what the curve of performance/usability looks like over time? It's difficult to parse all the past information into actionable hardware buying information. :)  Thanks in advance!"
technical,"what is your AMD GPU ?  If you have dual GPUs, disable the unsupported one from BIOS. Couldn't read the whole thread... what's the present status for tensorflow on OpenCL on MacOS (sierra +)? Specifically, I have an Intell Iris GPU and was hoping if I could build from source Tf+Open CL support for it. Also, tf corrainder seems to run fine, at version 1.2."
technical,"Just trying to get a sense of the state of this issue. Am I right to say that this repo:  ...built with ComputeCpp, is the current best option for building Tensorflow with general AMD GPU support? And if so, is there any benchmark evidence that this build provides a speedup over CPU? Depends on what you mean by ""general AMD GPU support"". If you mean really old dGPU or APUs, I don't know. But if you have newer (2nd Gen GCN or newer), hipTensorFlow (v1.0.1) running on ROCm was working pretty well."
technical,"I don't think VirtualBox exports OpenCL from a Mac host into a Linux guest, and Ubuntu 16.04.3 doesn't work right now. I don't have a Mac so I don't have any way of testing things. Did anyone successfully try out working TensorFlow on AMD via OpenCL  and succeed ?."
technical,Is Intel GPUs supported? I think my Iris Pro can speed up the age-long training for a bit. Discuss the lack of intel gpu (or amd cpu) support here
technical,Is Intel GPUs supported? I think my Iris Pro can speed up the age-long training for a bit. Do the OpenCL Caffe branch and the OpenCL Caffe implementation by AMD have anything more in common besides the name? Have you compared the two or do you know if there is any difference in performance? You write that the OpenCL branch is far from optimal performance. What does that mean and what would be necessary in order to improve it? It would be interesting to try it on Android.
technical,"There's a new version of the setup guide for compiling TensorFlow with ComputeCpp, Codeplay's implementation of SYCL, so that OpenCL devices can be used. We would appreciate any feedback you can give us on it. do you have any idea what the success rate is for getting this working on untested AMD gpus?  I'm specifically interested if it has been tested for AMD Radeon Pro 460  .  I would be happy to spend a few hours  getting ubuntu running on my Macbook laptop if there is any hope with an untested GPU"
technical,"there is a lot of politics behind all these huge software/hardware infrastructures... :-( Even if we can dream of a clean-slate solution based on pure scientific criteria, I think we have to be pragmatic. Google does not want to change too much of the TensorFlow architecture. This is why the OpenCL-based architecture has to be very similar, requiring single-source C++ like ""CUDA runtime"" instead  of the lower-level non-single-source OpenCL C solution. In the Khronos realm, the single-source C++ version of OpenCL is called SYCL. Let's discuss this when you drop by Dublin, for example, since you look to be based in Ireland too. :-) In the meantime, feel free to contribute to the TensorFlow & Eigen branches dealing with SYCL... Do you know if also XLA:GPU:OpenCL is planned on SyCL?"
technical,For Intel GPU (Gen9) under Linux we have seen significantly better DNN performance with Intel's open source Beignet implementation vs the closed source one when benchmarking with common vision nets on PlaidML. Beignet is also easier to install which is nice. Does it support intel graphics hd615 (7th gen cpu) on ubuntu17.10?  The opencl dirver SRB5.0 for linux64 is running well on ubuntu17.10.  And It has not been updated for a long time
technical,"Yes there is no machine-learning style proposal in SG14 yet. But participation is open, so you can send some proposals. :-) But perhaps SG6 (numerics topics) is more relevant. I do not think they have their own mailing-list/forum yet. Does OpenCL Caffe run on Android? Sorry for asking this here but I didn't find anywhere else to ask it :) Would be great with a deep learning library that ran on Android devices  and  could use the GPU but it seems like there are no at the moment. (Correct me if I'm wrong!)"
technical,"Congratulations!  Btw, what happened to the triSYCL repository? It seems to be gone and I can only find a reference to Khronos' Gitlab which is not publicly accessible.  EDIT: I found your private clone, only the one from amd is gone. does the opencl-docker support in mac platform?"
technical,We are in process of upsreaming changes to Eigen that will fix issue you are seeing.  As well we are preparing to upstream performance improvements to Eigen - this is bit tricky and needs coordination with benoitsteiner to avoid stream of merge conflicts - but we are getting there.  For AMD users I would suggest trying out my fork: Set Up instructions for Ubuntu 16.04 can be found here:  All the changes will be upstream to tensorflow after mentioned earlier Eigen changes are in place.  Hope that helps. Does your fork only support AMD R9 Nano / AMD FirePro GPU?
technical,"In this news about the release of SYCL 1.2.1 it says : ** The new specification incorporates significant experience gained from three separate implementations and feedback from developers of machine learning frameworks such as TensorFlow, which now supports SYCL alongside the original CUDA accelerator back-end. **  Does that mean it is now possible to ""easily"" run TensorFlow on AMD GPU that support OpenCL 1.2 on which SYCL is built ? Easily in the sense that some low-level software / drivers / libraries for the AMD hardware are where most of the broken stuff is, not in the hardware or TensorFlow or the OpenCL standards or SYCL. ,-) If you've got working AMD GPU drivers and working OpenCL libraries you've got TensorFlow on AMD GPUs.  My working setup for an AMD Bonaire (Sea Islands architecture):  Arch Linux with the amdgpu kernel module loaded and the radeon kernel module blacklisted The Arch User Repository package opencl-amd The ComputeCpp library TensorFlow built from source on my workstation using 's fork:"
technical,"So don't you still have a counterpart internally? What about ""Canned operators""? Eigen enables you to write an expression that describes the computation you want to perform once, and automatically generate a kernel (which we call the evaluator) to evaluate that expression on CPU and another kernel to evaluate the expression on a CUDA device. Once we have support for OpenCL in Eigen (we're getting close), it will be possible to also generate the OpenCL kernel automatically. For a few TensorFlow operations that are performance critical (such as convolution), we use hand optimized kernels and/or third party libraries. In these cases, we'll need a good OpenCL implementation of these operations."
technical,"I'm new here. Very interested in seeing TF support OpenCL. How do I get updates from this thread? emmm...Interesting, but why? I mean why Tensorflow choose cuda but opencl at the beginning? Some commercial reason I guess?"
technical,"There already was support for nvidia GPUs on Mac OS. It was just removed in 1.2. Note: As of version 1.2, TensorFlow no longer provides GPU support on Mac OS X. I've cancelled my order for an eGPU (Sonnet's) and will just dual boot Linux on my gaming rig, but this it's kind of bad to just stop supporting something that people were using. Was really hoping to do this on my mac with an eGPU (model training), but I guess that won't happen now: Er, you know CoreML supports importing tensor flow models by way of Keras? Apple doesnt 'hate' Google, business is business, One of Apples suppliers is Samsung. Read into that for a moment. Google, Apple, Samsung are businesses and will do what makes money. As a side note. My MacBook Pro hasn't melted from running inference on thousands of movies by the way..  I suspect CUDA was super convenient to adopt and continued support from Nvidia and missed opportunities from AMD got us to where we are. I don't think its nefarious, just cost of making a change vs performance deltas vs cost of staying the course.  I suspect some genius will come along to help solve the issue."
technical,"As I told you quite some times, no it won't work. Pre GCN 3rd gen gpus simply lack the hardware for ROCm to either perform or even work at all.  SPIR(-V).. I'm not sure what you are talking about. It's not your job to care about that. Computecpp makes it from SYCL ""commands"", and then it's all (opencl) driver business.  You have what I'm tentatively calling amdgpu-pro-opencl-only, and I'm not sure what's the problem then. EDIT: would be also cool to have some sort of ETA for Luke's code to land everyone  I have (L)Ubuntu 17.10 incl. kernel 4.14.x and the OpenCL library parts from the AMDGPU Pro 17.40 driver running and can run OpenCL applications like clinfo or Boinc (e.g. EngimaHome, MilkywayHome) without issue on my AMD A12-9800E APU.  I can also successfull compile and use the tensorflow (currently version 1.4.1) CPU version. But i fail to successfully compile the OpenCL version of tensorflow. I use computecpp 0.5 (the current one i can download without need to registering) together with vanilla tensorflow 1.4.1 and with the ""dev/amd gpu"" branch from 's fork.  So could please someone who successfully compiled the OpenCL version of tensorflow provide some information which version of the computecpp library and which branch of which tensorflow git he/she is using?  Thank you"
technical,"The build and test seem to be working on my Bonaire. I am using Python 3.6, though, and the instructions use Python 2.7. Do I need to use 2.7 or will 3.6 work? Following it seems Python 3.6 should be working - I haven not tried it though"
technical,"Thank you andrewrichards  fo your attention and your session speech. But for now for me(a graduate student), to build an app using Tensorflow on Android device and want GPU(Mali-T720) activated, what are required to abtain Mali driver with SPIP-V support and ComputeCpp runtime for Android with Arm CPU support and SPIR-V support. Since I've downloaded ComputeCpp(Ubuntu16.04 x64 with bin/ doc/ include/ lib/) on CodePlay homepage, yesterday I run:  incompatible, so I consider maybe I need ComputeCpp for Android with Arm CPU support and SPIR-V support, but I could not find any source code to build an Android ComputeCpp, there are only samples at github. And you've said ComputeCpp for Android is now not available, so is there any plan to support Android device or how can I get it if supported. For AMD gpu and linux users, AMD recently released HIP port of tensorflow here. You might be interested.  I haven't tested it, though."
technical,"Thank you andrewrichards  fo your attention and your session speech. But for now for me(a graduate student), to build an app using Tensorflow on Android device and want GPU(Mali-T720) activated, what are required to abtain Mali driver with SPIP-V support and ComputeCpp runtime for Android with Arm CPU support and SPIR-V support. Since I've downloaded ComputeCpp(Ubuntu16.04 x64 with bin/ doc/ include/ lib/) on CodePlay homepage, yesterday I run:  incompatible, so I consider maybe I need ComputeCpp for Android with Arm CPU support and SPIR-V support, but I could not find any source code to build an Android ComputeCpp, there are only samples at github. And you've said ComputeCpp for Android is now not available, so is there any plan to support Android device or how can I get it if supported. For deep learning inference on mobile devices with GPU/OpenCL support, you can checkout MACE, which is optimized for Adreno, Mali and PowerVR GPUs. Here are some benchmark results."
technical,"Thank you andrewrichards  fo your attention and your session speech. But for now for me(a graduate student), to build an app using Tensorflow on Android device and want GPU(Mali-T720) activated, what are required to abtain Mali driver with SPIP-V support and ComputeCpp runtime for Android with Arm CPU support and SPIR-V support. Since I've downloaded ComputeCpp(Ubuntu16.04 x64 with bin/ doc/ include/ lib/) on CodePlay homepage, yesterday I run:  incompatible, so I consider maybe I need ComputeCpp for Android with Arm CPU support and SPIR-V support, but I could not find any source code to build an Android ComputeCpp, there are only samples at github. And you've said ComputeCpp for Android is now not available, so is there any plan to support Android device or how can I get it if supported. for Eigen/OpenCL/SYCL"
technical,Sorry I cannot help with Intel OpenCL because i have only AMD OpenCL hardware.  I don't use jupyter yet. I use a plain bash shell and a virtual Python 3 environment (see my Python 3 +  Tensorflow setup). But i cannot reproduce the issue. There is no CPU usage by a python process after the compute has been completed. Thank you for the information. Is it possible to speed up the initial compile time? e.g. using all available CPU threads instead of only 50%. For Intel GPU (Gen9) under Linux we have seen significantly better DNN performance with Intel's open source Beignet implementation vs the closed source one when benchmarking with common vision nets on PlaidML. Beignet is also easier to install which is nice.
technical,"I'm very sad about the abandon of GPU support for macOS. Still looking for OpenCL support for GPU on macOS because Apple will, obviously, not change to Nvidia GPUs any time soon. Tensorflow is my my engine of choice. Using GPU acceleration locally on my MacBook Pro or future iMac Pro would be awesome. For Microsoft it would make sense to sabotage Apple, but since Google has no desktop OS they are only hurting themselves."
technical,"I see TF on Metal can extend to iOS too. If anyone interested in picking it up, I recommend adding Metal support to Eigen first (can use OpenCL as reference). For school I had to use Tensorflow for training models, not just for evaluating a model. And I will have to repeat this again in near future. For students like me, having training on GPU is a must, saving a lot of time. It not just a matter to serve multiple concurrent user."
technical,"At the moment, I haven't tried any deep learning library, I'm just doing some scouting to see which library I could potentially use. Have you tried cltorch and DeepCL on Android? I just assumed cltorch did work on Android, since there is an implementation of Torch that is dedicated specifically for Android. And why would you have such an implementation if there already was one that both worked on Android and used OpenCL, right? But maybe I should have known better. For some reason I imagined that torch-android was an official Torch implementation for Android, meaning that no other Torch implementation (at least not official) was likely to run smoothly on Android, including cltorch. I don't know why I thought that, it of course doesn't make any sense."
technical,"Does it support intel graphics hd615 (7th gen cpu) on ubuntu17.10?  The opencl dirver SRB5.0 for linux64 is running well on ubuntu17.10.  And It has not been updated for a long time For the love of god, can't you read just 2 (two!) posts above? Discuss the lack of intel gpu (or amd cpu) support here"
technical,"Hi, I just want to mention that I was able to get hiptensorflow working, thanks to bensander and other folks at AMD. I can run all examples in their quickstart guide.  Thanks For those who want to try TensorFlow on AMD hardware using ROCm, I wrote a blog describing how to run Fast.ai notebooks using AMD Fury Nano."
technical,"Hi, I just want to mention that I was able to get hiptensorflow working, thanks to bensander and other folks at AMD. I can run all examples in their quickstart guide.  Thanks from which planet you are came from? How you can compare tf-CPU and AMD GPU ? AMD GPU on plaidML x30 faster than tf-CPU   1. It runs pitifully slow on AMD's OpenCL implementations compared to Tensorflow's CUDA backend so there goes at least half the reason to use it  in my deepfakes tests OpenCL slower only by 20%, but in some mini networks OpenCL is 20% FASTER.  My project DeepFaceLab has many users that have been waiting for the support of AMD. How many people were delighted when deepfakes can finally be trained on AMD cards. Also plaidML is the only backend for keras that supports AMD/IntelHD out of the box. If a new AMD backend for keras appears, of course my project will switch to it. PyTorch has no future.  What to maintain in plaidML ? Ops are auto differentiable, there is nothing to maintain. Tile programming language in which only someone like a pure maths professor would concoct  Machine learning is invented by professors of mathematics, isn't it?"
technical,"I did not even knew you could use pre-GCN hardware for TF. With my Tahiti I only have support threw one distro (ubuntu 14.01.x) as AMD proprietary drivers only work with older linux kernels for GCN1. (I get TF + openCL via SYCL (untested on 7970) ) Where I work the entire R&D department runs green team. They all have PHDs and all but none wrote a single line of cuda (nor OCL). But the tooling is here to accelerate their Keras workload. I'm kind of an oddball with my recycled mining GPUs trying to squeeze a second life out of them. tl,dr other than green team support will only show up if the AMD GPU market share shows up. It's a chicken and egg problem. I have hopes for vega  but yeah FWIW here's the GitHub post that got me going this weekend after thrashing and Googling since April: That was my original issue - trying to use my Bonaire to accelerate linear algebra on R."
technical,"Using a finicky closed source thing that only works on one very old combination of Kernel/OS (codeplay): yes. Using an old version of tensorflow and without support for some nonlinearities yet (tf-coriander): yes. Really: not officially. Though AMD are porting to HIP, so I'd expect progress within 3 months or so. Other frameworks already work well due to their efforts. FWIW I believe the recent versions of PyGpu can use either CUDA or OpenCL. I have all of the software installed on my Arch box but I haven't tested it yet."
technical,"Couldn't read the whole thread... what's the present status for tensorflow on OpenCL on MacOS (sierra +)? Specifically, I have an Intell Iris GPU and was hoping if I could build from source Tf+Open CL support for it. Also, tf corrainder seems to run fine, at version 1.2. FWIW there's an Intel SDK for OpenCL - I've got it on my ancient Sandy Bridge laptop but I'm sure it'll work on your machine."
technical,"Thanks! I have this working on Arch Linux (4.14.4 kernel) with the opencl-amd library from the Arch User Repository. The card is a Bonaire (GCN 2.0). I'll run the tests on that page to verify that it's doing what it should. GCN 2nd gen (aka 1.1) if any, 2.0 doesn't exist. (should stoop to be so pedantic)"
technical,"I have been using Caffe OpenCL branch on AMD's GPUs and it works just fine. make run test passed all tests except one Good to hear, may I ask about your HW/SW setup? E.g., what card you're using, what distro/version of Linux, etc?  I previously had AMDGPU-pro, but uninstalled it when installing ROCm. It's possible there's some legacy thing interfering with me."
technical,"I have been using Caffe OpenCL branch on AMD's GPUs and it works just fine. make run test passed all tests except one Great news , let us know of any help you need.  I'll guess you are using your own implementation of SYCL - will that be available for developers/researchers? On what platforms?"
technical,I have been using Caffe OpenCL branch on AMD's GPUs and it works just fine. make run test passed all tests except one Great to read this is being worked on. It would help if Beignet 2.0 were polished. Lots of potential with Skylake and Iris right now.
technical,"I have been using Caffe OpenCL branch on AMD's GPUs and it works just fine. make run test passed all tests except one Great, thanks for the update. I hope you are right about getting it done fully-featured in less than a year."
technical,"Thanks for publishing your results. With regard to the initialisation time, the way OpenCL works is that the code is compiled before execution, so the startup time is the compilation process.  For Intel devices, there is a thread with mirh here that explains how to remove the restrictions around running devices. We have seen issues with Intel drivers which is why these device types are restricted, but we are hoping that updated drivers will be available soon for Intel devices that improve the support. In the meantime you can re-compile TensorFlow with the change to test your own Intel hardware. We are looking at removing the device restrictions in the codebase. Guys, I apologize for perhaps naive question but why is this build AMD GPU only ? Isn't OpenCL supposed to be standard ? Do I understand it correctly that it won't work on my Intel Carbon X1 with installed OpenCL 2.0 drivers ?"
technical,"Thanks for publishing your results. With regard to the initialisation time, the way OpenCL works is that the code is compiled before execution, so the startup time is the compilation process.  For Intel devices, there is a thread with mirh here that explains how to remove the restrictions around running devices. We have seen issues with Intel drivers which is why these device types are restricted, but we are hoping that updated drivers will be available soon for Intel devices that improve the support. In the meantime you can re-compile TensorFlow with the change to test your own Intel hardware. We are looking at removing the device restrictions in the codebase. Guys, I'm not going to read this entire thread, but if someone could answer my question that'd be great! Can I use Tensorflow with an AMD GPU yet. If so in what operating system, and can I do it with RX Vega? Thanks!"
technical,"What about replacing the openCL port with the HIP port backed by AMD ? Haha! Life is very strange actually... Are you working for the HiP marketing team of AMD ? :-) Please look at the subject of this issue : ""OpenCL support"".  This means it is about the Khronos standard (and the other SYCL standard from the OpenCL Khronos working group appears at the end of the ""Overview"" section).  Of course there is a world outside of this issue, but it is... *outside*! :-)  Please try not to increase inconsiderately the entropy of the universe by posting some random posts on this already too lengthy discussion... :-) This comment applies to some other posters here, not only you, by the way. This is a GitHub issue to solve a *technical* problem: having TensorFlow running on devices supporting the OpenCL standard, not a FaceBook page about how people like or dislike tool A or B. :-) But please feel free to send some git commits related to this issue we can look at..."
technical,Bump Has anyone in Google spoken to Apple or AMD to ease this? I guess AMD people is so lost they don't even know the problem is there and they are still wondering why Nvidia has such a huge market share. I guess too Apple AI team would be more than happy to help in here... if OpenCL wasn't an abandonware from their side since 2013
technical,I know quite a bit about the big Movidius unit - it's inference only and it runs either TensorFlow or Caffe pre-compiled models. IIRC they're all in 16 bit mode.  The Movidius chip itself is much more powerful but you have to be a qualified partner to get the SDK. Have some link for other that try to have tensor opencl Maybe worth to check also: Feel free add working projects.
technical,"This thread is very interesting. I've been trying to get caffe to work on android. The results seem to be surprising: caffe running with Mali gpu seems to be 2-3 slower than cpu, but about 4-5x more energy efficient. The test was run on Galaxy S6 (Mali T760, Peak Performance 200 GFlops).  Since GEMM is the core of convolution in caffe, I decided to profile its performance on Android. It seems that ViennaCL is not as efficient as some simple kernels. Now I am able to get GPU run as fast as CPU for large matrices (2k x 2k). This is still counter-intuitive, since normally we expect GPUs to be much faster. See: The kernel implementations can be found here: OpenCL kernels for GEMM.Any thoughts? Have you already followed this thread?"
technical,"This thread is very interesting. I've been trying to get caffe to work on android. The results seem to be surprising: caffe running with Mali gpu seems to be 2-3 slower than cpu, but about 4-5x more energy efficient. The test was run on Galaxy S6 (Mali T760, Peak Performance 200 GFlops).  Since GEMM is the core of convolution in caffe, I decided to profile its performance on Android. It seems that ViennaCL is not as efficient as some simple kernels. Now I am able to get GPU run as fast as CPU for large matrices (2k x 2k). This is still counter-intuitive, since normally we expect GPUs to be much faster. See: The kernel implementations can be found here: OpenCL kernels for GEMM.Any thoughts? Have you tried the last sgemm version in the MALI SDK?"
technical,"yes SYCL 1.2 is a priori for OpenCL 1.2 and SYCL 2.2 is a priori for OpenCL 2.2. I said ""a priori"" since, if you do not use anything requiring the OpenCL-compatibility mode of SYCL, SYCL does not really require OpenCL at all. Actually SYCL is a very generic model for heterogeneous computing and can run on top of anything. But of course a real implementation may require OpenCL too. Hello, I am learning/working with TensorFlow and Keras for the time being and I would be interested to get the OpenCL support working under macOS ... Is there some news on the work done around macOS ?I succeeded to compile TensorFlow but if I try to configure for OpenCL it ask me for the computeCpp 1.2 location, and there is no ComputeCpp for macOS it seems to me."
technical,"What about slide 40? Hello, that I planning that tensorflow will support the new AMD Radeon Instinct?"
technical,"Hello, I am learning/working with TensorFlow and Keras for the time being and I would be interested to get the OpenCL support working under macOS ... Is there some news on the work done around macOS ?I succeeded to compile TensorFlow but if I try to configure for OpenCL it ask me for the computeCpp 1.2 location, and there is no ComputeCpp for macOS it seems to me. Hello. By no means an expert in ML / Tensorflow / or even OpenCL, but I'm an experienced Mac graphics dev who desperately wants faster performance of Tensorflow on systems with integrated and AMD GPU's using built in libraries and simple dependencies :) How can I help?"
technical,here you go henline and jlebar are the experts to answer the difference between streamexecutor?
technical,Can any Google member explain the difference and roadmaps between streamexecutor and ? here you go
technical,"I am willing to help you for testing. I hava a Windows PC with an AMD card, a MBP with an AMD card, An MB with an Intel integrated GPU Hey - I am going through the test set above, this evening, on an AMD R9 390 8GB. So far I've already got one different result, logistic regression.py trains and doesn't return nan. So, good! It segfaults at the end, so I'll investigate whether the script or the cl code is at fault. Where should I push my results, where they can be most useful to you? Perhaps we could get a standard ""test script"" that generates a standard set of results that volunteers can push to you (or set up on local CIs or whatever)?"
technical,"Yes, I just thought it would be nice if you could look over it and maybe give some improvement or tuning tips on libdnn.  It is using GEMM, but implicit (not through a BLAS, only small GEMM's on a workgroup level) so that a higher level of parallelism is possible, as well as no intermediate buffer are necessary (to unroll the data into a GEMM scheme). Hey all, thanks for mentioning our push! Hope it will be useful! To compile this code, you need a SYCL compiler. Currently, the only supported compiler is Codeplay's ComputeCpp, which is available via a Codeplay's evaluation program. ComputeCpp will be made available for free as a public open beta, later in 2016 and then released with a free version (the ComputeCpp Community Edition) in 2017. This will let anyone compile and develop TensorFlow on OpenCL devices, such as AMD or Intel GPUs and CPUs. btw. shouldn't this issue have OpenCL label? :) Thanks, Luke"
technical,"I've an AMD GPU and an Intel GPU in the laptop. I think both have OpenCL drivers and AMD's support seems to be much better. I'd have higher performance, because I've 2 OpenCL devices. I hope you make it scale with OpenCL devices. Hi all,  Thanks for the interest! At this point we are getting our testing infrastructure set up to make sure that nothing that we do introduces regression. We are in touch with benoitsteiner to make sure we are in sync with what he's done so far.  We are still in compiling a road map for the integration process - it should be done in couple weeks time, as there is a couple of business details to clarify.  Our goal is to bring the OpenCL to TensorFlow via Eigen by end of this year.  Thanks,"
technical,"I would love to help contribute with this initiative. hi all,  we will coordinate the effort of porting Eigen's tensor module to SYCL for OpenCL as we already have something mostly working, but it's not ready for review yet.  We are in favour of this approach as it will introduce less invasion to the code base. SYCL supports the single-source C++ templated model that eigen already uses.  Road map design is in progress so it shouldn't be too long now.  Thanks, Luke"
technical,"I wonder if you might have the possibility of setting up a CI server, that runs on each commit? No problem. I probably need write-access to the project, so Jenkins can write the log-file into a build-log directory. I just spammend you, so we can discuss. Hi all, As you probably see already, a bunch of SYCL stuff has been pushed to TensorFlow. We are not complete yet,  and there is plenty to do. But we are progressing to get there. If you are interested in contributing or just curious on the current state, check the breakdown below. **Infrastructure** Google kindly donated two machines that are set up to test benoitsteiner's fork of TensorFlow periodically. Both have AMD GPUs. We at Codeplay are looking to dedicate machine(s) next year too. To improve the OpenCL device diversity coverage. We are looking for contributors on that front if anyone is interested in providing a test build server for relevant platforms that we support. Currently, the requirements are: - Ubuntu 14.04 - OpenCL drivers that support SPIR ( Intel CPU / GPU or AMD GPU )  perhaps you culd help out with that?  **Tests** On the Fiji machine we are facing 164 fails. On the Hawaii machine we are down to 56 fails. We are looking into fixing the failing gradient tests and investigating the origins of the additional fails on the Fiji machine. **Eigen** For the past few months we have been actively implementing features needed by TensorFlow including: Reshaping, Slicing, Basic Reduction etc. Currently we are implementing Contraction. A detailed breakdown can be found in the Eigen Tensor tab. **TensorFlow** A lot of Coefficient-wise operations have been implemented including Abs, Floor, IsFinite, Log, Pow, Mul, etc., as well as Tensor Manipulations like Reshape, Shape, Identity, Fill etc. A detailed breakdown can be found in the TensorFlow Kernels tab. **Organisation** The above spreadsheed has several tabs that categorise the efforts of the project like: Overall Plan, Eigen Tensor, TensorFlow Kernels, Models. If you would like to get involved, please put your name next to the item you are working on or add anything important that is missing."
technical,"Certainly would be interested in contributing. Please let me know when you plan to start. Hi all, here at Codeplay we are looking into Eigen's tensor running on GPU using SYCL (a modern C++ layer on top of OpenCL). From what we have gathered so far, GPU tensor design is very closely coupled with CUDA and it will require interface changes for another programming model and particularly a SYCL and OpenCL 1.2 version. If anyone is interested in digging deeper / helping out, we are most certainly interested in contributing. Thanks, Luke"
technical,"Certainly would be interested in contributing. Please let me know when you plan to start. Hi all, Just to keep you posted, we are still investigating how we can change the Eigen interface to better fit the SYCL/OpenCL 1.2 programming model. Once we come up with a reasonable approach that targets heterogeneous programming models ( not only OpenCL / SYCL )  we will create a proposal. Thanks, Luke"
technical,"I have tried figuring out whats happening but unfortunately i wasn't able to. I'd appreciate if someone who knows about the following can help me come up to speed!  Most of the above discussion pertained to getting Tensorflow running with OpenCL acceleration on AMD chips. Am I correct in saying this? If I want to get gpu accelerated tensorflow using my integrated graphics card (intel HD 5000) which supports opencl, what should be my approach?  Thanks in advance! Hi Ed, thanks for replying. I have gotten OpenCL downloaded and running on my system. But my question was - how can I compile tensorflow to actually use the OpenCL libraries?"
technical,"I'll bring this up at the sig-arch meeting today, however as neolit noted * our API guarantees prevent these sorts of renames on short time scales (we might introduce a new name, but the old name would be preserved effectively for the rest of the v1 lifetime) * the term is well established in both having common descriptive usage (outside of the less-common vulgar usage) so I think it's unlikely we would change this.  Will update after the meeting. hi everyone, my name is ricardo , i am a C++ programmer with many years in C++ experience, and little on Cuda, i will be glade in contribute to this effort. How can i contribute to this job?"
technical,"emmm...Interesting, but why? I mean why Tensorflow choose cuda but opencl at the beginning? Some commercial reason I guess? Hi, has created Coriander that could run NVIDIA CUDA  code on OpenCL 1.2 devices. You might want to take a look if that suits your need to connect TF to OpenCL 1.2 devices. Kindly attribute his name and his contribution in case if you plan to use his work."
technical,"Canned operations StreamExecutor provides several predefined kernels for common data-parallel operations. The supported classes of operations are: - BLAS: basic linear algebra subprograms, - DNN: deep neural networks, - FFT: fast Fourier transforms, and - RNG: random number generation. Hi, I also have interest in implementing TensorFlow on FPGA, using high level programming languages like Xilinx C++ or OpenCL. I am with pleasure to contribute if you have some plan."
technical,"masahi - just open an issue over there and we'll get you set up. Hi, I just want to mention that I was able to get hiptensorflow working, thanks to bensander and other folks at AMD. I can run all examples in their quickstart guide.  Thanks"
technical,"Hello, that I planning that tensorflow will support the new AMD Radeon Instinct? Hi, is there any progress in the TF-OpenCL support for FPGAs?"
technical,"bensander Does the ROCm runtime work on Ubuntu 16.04.3? I haven't been able to get it working.  P.S.: Do you have any insight if / when the AMDGPU-Pro setup will work on Ubuntu 16.04.3? I need that for another project. Hmm, I don't (and wouldn't) fun Ubuntu anywhere but I do have a CentOS 7 w/ repos and a GTX1080TI in it, running kernel 4.14.x and the latest Nvidia beta driver, so I could help test it out on there at some point today if it helps?"
technical,"Guys, I'm not going to read this entire thread, but if someone could answer my question that'd be great! Can I use Tensorflow with an AMD GPU yet. If so in what operating system, and can I do it with RX Vega? Thanks! Hmmm ... I haven't been following the thread but it looks like there's possibly testable OpenCL code now, at least on *CPU* OpenCL. I have an older AMD GPU (""Bonaire"") and I have OpenCL running on both the GPU and CPU, so I can test this. I might take a shot at it over the weekend, I really want OpenCL TensorFlow on my GPU."
technical,"For Microsoft it would make sense to sabotage Apple, but since Google has no desktop OS they are only hurting themselves. Honestly someone smarter than I should look into integrating Mac OS 10.13's MPS - Metal Performance Shaders which have support for a large set of neural network primitives out of the box. This would allow up to date, high performance GPU for mobile and Desktop iOS and macOS Tensorflow inference deployment. You can't train with Apples primitives as I understand it (they don't provide anything), but with Tensorflow support maybe you could? I imagine it for folks on the Apple platform that would be a boon. I don't think Google would provide this internally, and I don't have nearly the requisite skills to attempt it myself. Posting this idea so folks more talented than I might take it on. :)"
technical,"This will have an impact on the strategy:  EDIT: ""StreamExecutor is currently used as the runtime for the vast majority of Google's internal GPGPU applications, and a snapshot of it is included in the open-source TensorFlow  project, where it serves as the GPGPU runtime."" Hope people working on it manage to overcome the CUDNN alternative problem by the time tensorflow gets close to 1.0"
technical,What is the current state on opencl convolutions support? Are you planning on leveraging the exiting kernels directly? What about matrix multiplications?  Any ETA? How about using HIP to port CUDA code to platform agnostic one?
technical,"Nice catch, they got the chair How Can I simply remove cuda implementation? because '#ifdef GOOGLE CUDA' is so complicated. It sometimes means CUDA, sometimes means GPU."
technical,That's awesome! Thanks for the info! how did you get tf 1.2 to build with cuDNN 6? Did you use LLVM? GCC? I only managed to get it to build with cuDNN 5...
technical,"to go back to the discussion on OpenCL 2 or not, at some point real things have to be delivered... Otherwise there is already nVidia GPU and CUDA with TensorFlow running... :-) But of course, a version of TensorFlow without SVM has some interest. How much of the Vulkan SPIR-V work on drivers (that has already a good devices coverage) do you think will push modern Opencl versions?"
technical,"AMD's efforts are a bit poorly coordinated at the moment (yes, all forks). Also it doesn't seem to work that well on a large variety of devices yet, unlike the OpenCL branch of Caffe, for example... I absolutely agree. To be honest, they really seem to lack human resources, they are working on all the fronts. By the way: Didn't know ETH had a Neuroinformatics ,) nice !"
technical,"You are right, OpenCL and CUDA are too different programming approaches. The single-source aspect found for example in CUDA and OpenMP 4.5 is extremely powerful from a software engineering perspective. This is why there is this SYCL standard for the real C++ programmers. SYCL can be seen as CUDA on steroids without any language extension and with some OpenMP aspects (the tasks). A typical SYCL device compiler is expected to generate SPIR-V kernels.  Your concerns about portability are less an issue with the SPIR-V standard (kind of portable equivalent of nVidia PTX/AMDIL/... in the Vulkan & OpenCL world) which is mandatory to accept in OpenCL 2.1 and Vulkan. So the beauty is that if you have a front-end that generates SPIR-V, you do not need special knowledge of the very details of the hardware to run on. There is a Khronos open-source bidirectional translator between LLVM IR and SPIR-V, so it opens quite new territories. I agree that SPIR-V is a step forward. However, it does not address all issues of exhaustive jitting.   you do not need special knowledge of the very details of the hardware to run on  Is this a copy&paste from OpenCL 1.0 marketing, which claimed exactly the same? You will  always  need to go down to the details of the underlying hardware if you aim for maximum performance. This is especially the case in the context of fast tensor contractions."
technical,An interesting initiative at  also if here we rely on Eigen tensor extension. I also would like to contribute.  can you organize it?
technical,"Easily in the sense that some low-level software / drivers / libraries for the AMD hardware are where most of the broken stuff is, not in the hardware or TensorFlow or the OpenCL standards or SYCL. ,-) If you've got working AMD GPU drivers and working OpenCL libraries you've got TensorFlow on AMD GPUs.  My working setup for an AMD Bonaire (Sea Islands architecture):  Arch Linux with the amdgpu kernel module loaded and the radeon kernel module blacklisted The Arch User Repository package opencl-amd The ComputeCpp library TensorFlow built from source on my workstation using 's fork: I am a bit surprise by what you said "" If you've got working AMD GPU drivers and working OpenCL libraries you've got TensorFlow on AMD GPUs"". I had understood that TensorFlow ""Official"" version was not running on OpenCL (CUDA only). Seems I got confused. I was quite happy to find the PlaidML project that at least allow for some Keras code to run on my iMac with AMD Redeon HD 6970. AFAIK you have also tried that Framework. I will give a go running TensorFlow on the Ubuntu VirtualBox were Tensorflow is already running (CPU only)."
technical,"OK i did some more troubleshooting.  - i used the Tensorflow MNIST example, see the Validate a Tensorflow Setup - i used to check/watch the iGPU clock/load and ""top"" to check the CPU load - the intialization phase until Step 0 took about 6 minutes, the iGPU load was about 0%, the iGPU clock at 300 MHz (the minimal available clock) and the python process CPU usage was about 200% (= 2 threads) - starting with Step 0 the iGPU load was about 90%, the iGPU clock switched always from 654 MHz - 720 MHz - 800 MHz - 900 MHz (max available clock) and back, the python process CPU usage was about 100% (= 1 CPU thread) I am still trying to get things to compile on Arch.  What I used yesterday After 14 hours (yes, my potato is very slow) I got this binary, if you want to try."
technical,"Codeplay has just released partial support for OpenCL to Eigen Tensors. The code is available in this bitbucket repository. For the most part, TensorFlow depends on Eigen tensors. There are additional dependencies on Eigen for the linear algebra operations, but we don't have to provide an OpenCL compatible implementation of these operations (at least not initially). Therefore we're in a very good position to start supporting OpenCL in TensorFlow.  If you are interested in contributing, I have started to track what needs to be done in this spreadsheet. I am the author of a C++11 OpenCL BLAS library and am currently implementing half-precision support there. I am happy to contribute on the BLAS/GEMM part of this project and/or modify CLBlast to fit your needs better."
technical,"Thank you for the answer, I will go back on the subject at work this week, with specific scripts. My use cases are around text/syntaxic matching analysis, using Gensim and Keras/tensorflow in my experiments. I am willing to help you for testing. I hava a Windows PC with an AMD card, a MBP with an AMD card, An MB with an Intel integrated GPU"
technical,"Thank you for the feedback. I think that benoitsteiner worked at the tensor extension part of eigen. I can help code some OpenCL/SYCL if someone makes a plan, divides work into tasks etc. I recommend using Boost.Compute as a wrapper for OpenCL (it makes running kernels, testing, templating easier)."
technical,"For AMD gpu and linux users, AMD recently released HIP port of tensorflow here. You might be interested.  I haven't tested it, though. I can test it - stay tuned. Looks like it's failing CI though."
technical,"Tensorflow port to AMD GPU:It works great for me.  My hardware setting: The key is mother board and CPU have to support PCIe v3  Its performance is similar to Nvidia 980Ti I can't even get the ""supported"" AMD drivers to work on my ""supported"" Ubuntu 16.04 LTS install. Planned obsolescence?"
technical,"I joined the Google Group but it seems to be quiet. So I'll rant here ,-) 1. Machine learning / high performance / GPU computing is a fiercely competitive market. NVidia, like it or not, dominates the market *and* keeps their cards and software close to the vest. If you've got a budget and a deadline, you're more or less stuck with NVidia for now. 2. I've got an old-ish AMD card (""Bonaire"") and *zero* budget - hobbyist. I've got caffe running with the proprietary AMD OpenCL 2 implementation on Arch Linux as of yesterday, and I just got AMD's open source MIOpen running the same way this morning. That'll let me train some models, the Bonaire peaks around 1800 GFLOPS single-precision. So if TensorFlow won't run with OpenCL on the Bonaire I won't be using TensorFlow. 3. If a budget should magically appear I would buy an Intel CPU and an NVidia card and run vendor-supported proprietary software. I'm done doing unpaid QA for vendors like Google, Red Hat, Canonical and AMD. It's taken me three months (and three distros - Fedora 25, Ubuntu 16.04 LTS and Arch) to get something out of a GPU I've had for three years. There are unfixed bugs in Fedora's bug tracker with my name on them. Same goes for Ubuntu and Freedesktop.org. Most of the people who'd be fixing them aren't getting paid either, or they're getting paid to do something else. Yes, AMD's new CPUs are impressive, and yes, most of their software is open source, but budgets and deadlines change things. Support is key. Support is everything! I did not even knew you could use pre-GCN hardware for TF. With my Tahiti I only have support threw one distro (ubuntu 14.01.x) as AMD proprietary drivers only work with older linux kernels for GCN1. (I get TF + openCL via SYCL (untested on 7970) ) Where I work the entire R&D department runs green team. They all have PHDs and all but none wrote a single line of cuda (nor OCL). But the tooling is here to accelerate their Keras workload. I'm kind of an oddball with my recycled mining GPUs trying to squeeze a second life out of them. tl,dr other than green team support will only show up if the AMD GPU market share shows up. It's a chicken and egg problem. I have hopes for vega  but yeah"
technical,"SUCCESS!  The latest ""dev/amd gpu"" branch commits in  fork fixed my Tensorflow OpenCL compile issue. I assume it was the SysCL 1.2.1 related commits.  I successfully compiled a Tensorflow OpenCL version and can use it. See my Tensorflow Setup documents for details.  I also added a benchmarks page where you can find some benchmarks of my setup under different Tensorflow setups (non CPU optimized, CPU optimized, OpenCL) in the future.  The AMDGPU Pro driver version 17.50 is also working for me. I updated the related AMD OpenCL Setup document  Thank you to all contributors. I did some benchmarks and it seems the iGPU is slower than the 4 available CPU threads except of the matmul bench.pybenchmark.  The initialization of a OpenCL Tensorflow run is also much slower than a CPU only OpenCL Tensorflow run. Something like 5 seconds for CPU vs 1-2 minutes for OpenCL.  Can anybody confirm such results?"
technical,"what is the open source compiler you propose to use then? It is difficult to find an open source OpenCL-compliant solution to address a lot of devices in the wild... We need to bootstrap a solution at some point somehow... I didn't say it was an easy fix. But, OpenCL isn't the problem, there. After all, CUDA is entirely proprietary, much worse than even the OpenCL option TensorFlow chose. That all said, there are options for a CL-or-cuda system if you're starting from scratch, including portable middleware runtimes or arrayfire, etc. Tensorflow is too tied to CUDA though. I find it frustrating that people are willing to write kernels in CUDA but balk at doing it for CL, even though it would reach more users and seriously grow the market ecosystem. There are direct and indirect benefits to an open platform for everyone, possibly leading to big cost savings for everyone in the long run. If SYSCL is how that eventually happens, great: so why aren't some big names putting money on an open SYSCL distribution instead of buying into fringey proprietary options, which kind of defeat the purpose of an open standard?."
technical,does the opencl-docker support in mac platform? I don't have an OSX platform but I think that adapting a little bit the launching command could works.
technical,"everyone  I have (L)Ubuntu 17.10 incl. kernel 4.14.x and the OpenCL library parts from the AMDGPU Pro 17.40 driver running and can run OpenCL applications like clinfo or Boinc (e.g. EngimaHome, MilkywayHome) without issue on my AMD A12-9800E APU.  I can also successfull compile and use the tensorflow (currently version 1.4.1) CPU version. But i fail to successfully compile the OpenCL version of tensorflow. I use computecpp 0.5 (the current one i can download without need to registering) together with vanilla tensorflow 1.4.1 and with the ""dev/amd gpu"" branch from 's fork.  So could please someone who successfully compiled the OpenCL version of tensorflow provide some information which version of the computecpp library and which branch of which tensorflow git he/she is using?  Thank you I don't have anything running on Ubuntu - all my working things are on Arch. I do have the machine dual-booted with Ubuntu 16.04.3 but the AMD proprietary libraries don't work there yet. As far as I know they're not supported on 17.10, but if you've got the OpenCL piece working on 17.10 I might add a third boot - I have plenty of disk space. ,-)  What kind of errors are you getting? If they're build errors, you might have a Bazel incompatibility. Bazel is constatnly moving forward like TensorFlow is and sometimes one will get ahead of the other."
technical,"Perhaps benoitsteiner could drop by, since he is local. But before this event there is the full C++ F2F at the end of the month in Jacksonville, Florida. Unfortunately I will not be able to attend any of them. I don't know if CppCon 2015 talk C++ Multi-dimensional Arrays for Computational Physics and Applied Mathematics generated some paper follow-up."
technical,"What about using HIP ? ""Your wish is being granted, Eigen is being ported over AMD GPU via HIP. The second part of your request is can we bring standardized tool supporting FLOAT16 that ships with all our GFX8 GPU's, wish granted."" Our development branch of AMDGPU compiler now support's both Float16 and Int16 native instruction, instead of emulating FP16/Int16 with up convert & down convert instructions to convert from FP16/Int16 to Float and back.  This is f16 tests on Fiji hardware successfully executing a matrix multiplication with half types with conversion and with Native instructions.""  Also, not related but you should use syCL/openCL 2.0 instead of 1.2, because nvidia is already supported via CUDA. And openCL 2.0 is supported on both AMD and Intel Windows drivers. Also AMD has said that they will soon opensource un openCL 2.0 driver for Linux (which could be used by Intel, opensource magic) (and Intel already has a Linux openCL 2.0 implementation which Just need maturation.) if you ask Intel and AMD,  maybe they could speed up the work, because tensorflow is important for their economic interests. And they already have said in this comment section that they wanted to help. Also all the major ARM makers support openCL 2.0. This could open a lot of opportunitys for Android (which is in the economic interest of Google) , raspberry like, smart TVs, etc  And in mid term we could eventually develop an opencl 1.2 fallback layer for non supported hardware. And the implementation should use also openVX (which is now supoorted by all major hardware makers, and AMD has an opensource implementation) and with  And the all with Spir-V (which can be use simultaneously by Vulkan and openGL). You could say that I'm making a duplicate of what was already said, but synthetizing is important. And finally, could tensorflow use HSA ? HSA would be awesome on Android. I don't know if HIP would be useful or not. It is only supported on some AMD cards so that we need an OpenCL implementation anyway if we want to support all devices. It might still be worth it if the HIP implementation is notably faster. This might be the case but I haven't seen many benchmarks (HIP vs. OpenCL) yet. Another reason might be MLOpen (which is written in HC) as an replacement for cudnn but again I have no idea how fast that is or which features it supports.  TensorFlow would not use HSA directly because it is quite low-level. But HC (and HIP) is implemented on top of it and you can also implement OpenCL on top of if (pocl does that)."
technical,"I am a bit surprise by what you said "" If you've got working AMD GPU drivers and working OpenCL libraries you've got TensorFlow on AMD GPUs"". I had understood that TensorFlow ""Official"" version was not running on OpenCL (CUDA only). Seems I got confused. I was quite happy to find the PlaidML project that at least allow for some Keras code to run on my iMac with AMD Redeon HD 6970. AFAIK you have also tried that Framework. I will give a go running TensorFlow on the Ubuntu VirtualBox were Tensorflow is already running (CPU only). I don't think VirtualBox exports OpenCL from a Mac host into a Linux guest, and Ubuntu 16.04.3 doesn't work right now. I don't have a Mac so I don't have any way of testing things."
technical,Seems that the next f2f SG14 meeting in March will be hosted by Google. Will be any tensorflow internal there? I found a solution !
technical,"I was going to try ComputeCpp SYCL however they only provide the ubuntu installer(I am also on arch) and the aur install script is broken.  It is good to hear that it can work.  If I get desperate enough I may try it out. bensander That looks like exactly what I need to get ADM support however I am worried by the fact that this code has not been back-ported to TF and that its code was last updated over 2 months ago given that my code targets TF 1.4.0 It seems like at the moment tensorflow basically ties you to Nvidia, at least for us ""mortal"" programmers.  Lack of documentation/updated roadmap doesn't help.  I wouldn't mind helping out in any way I could however I've had little success getting things working so far. I got the ComputeCpp SYCL working on Arch - there was a binary tarball on their website when I did it."
technical,"Still can't make it working on Ubuntu 16.04 with AMD card and catalyst driver. Is there any howto? I had to look at output before trying to use TF compiled with OpenCL support. In my case it showing now there is 2 choices for running TF on GPU: good working on limited (by vendor) number of devices, but proprietary CUDA bad working on limited (by computecpp developers) number of devices and also proprietary computecpp Still no OpenCL support."
technical,"Actually I m working on my unofficial Debian/Ubuntu package so I'm trying to keep close to official 1.3.1 release. I can live without OpenCL support but I'd to be ready to enable it as soon as it's correctly supported. Maybe I'll update packages against your branch for testing purposes, but that's enough for today ,) I have around ten different varieties of AMD GPUs in my mining rigs. (from 7970 to RX 480 running ubuntu 16.04 and amdgpu-pro). Let me know if I can contribute by testing anything."
technical,"They've already released Caffe, would be very interested to hear others on this thread sharing their experiences with building/testing: I've started installing but hit a roadblock where anything requiring CL just freezes, even clinfo. Not sure if this is because of some software issue, or if my card (R9 390) simply isn't supported by ROCm. I have been using Caffe OpenCL branch on AMD's GPUs and it works just fine. make run test passed all tests except one"
technical,Did anyone successfully try out working TensorFlow on AMD via OpenCL  and succeed ?. I have the  fork working (Arch Linux) . I'm waiting on some more AMDGPU-Pro work before I publish a blog post
technical,"I am still trying to get things to compile on Arch.  What I used yesterday After 14 hours (yes, my potato is very slow) I got this binary, if you want to try. I have tried figuring out whats happening but unfortunately i wasn't able to. I'd appreciate if someone who knows about the following can help me come up to speed!  Most of the above discussion pertained to getting Tensorflow running with OpenCL acceleration on AMD chips. Am I correct in saying this? If I want to get gpu accelerated tensorflow using my integrated graphics card (intel HD 5000) which supports opencl, what should be my approach?  Thanks in advance!"
technical,I'm doubtful it found GPU device in your case because in mine it printed debug info in the beginning about selecting GPU device. Perhaps you can recompile with added printf to console somewhere I joined the discussion Google Group and posted a request for documentation. I'm going to wait to see if anyone responds before I put more effort into this.
technical,"tscholak I wont post it here to keep this on OpenCL but ill summarise the steps here] I joined the Google Group but it seems to be quiet. So I'll rant here ,-) 1. Machine learning / high performance / GPU computing is a fiercely competitive market. NVidia, like it or not, dominates the market *and* keeps their cards and software close to the vest. If you've got a budget and a deadline, you're more or less stuck with NVidia for now. 2. I've got an old-ish AMD card (""Bonaire"") and *zero* budget - hobbyist. I've got caffe running with the proprietary AMD OpenCL 2 implementation on Arch Linux as of yesterday, and I just got AMD's open source MIOpen running the same way this morning. That'll let me train some models, the Bonaire peaks around 1800 GFLOPS single-precision. So if TensorFlow won't run with OpenCL on the Bonaire I won't be using TensorFlow. 3. If a budget should magically appear I would buy an Intel CPU and an NVidia card and run vendor-supported proprietary software. I'm done doing unpaid QA for vendors like Google, Red Hat, Canonical and AMD. It's taken me three months (and three distros - Fedora 25, Ubuntu 16.04 LTS and Arch) to get something out of a GPU I've had for three years. There are unfixed bugs in Fedora's bug tracker with my name on them. Same goes for Ubuntu and Freedesktop.org. Most of the people who'd be fixing them aren't getting paid either, or they're getting paid to do something else. Yes, AMD's new CPUs are impressive, and yes, most of their software is open source, but budgets and deadlines change things. Support is key. Support is everything!"
technical,"tscholak I wont post it here to keep this on OpenCL but ill summarise the steps here] I just assumed Benoit because he self assigned the feature, but I think you've got it Junli! Maybe start with an email or forum thread of interested parties?"
technical,"There is a TensorRT that supports Movidius Pi Hat. And that Movidius Pi Hat is Google's 45 AIY Vision Kit. Google links to Target to buy it.  This doesn't have any ties to CUDA or Nvidia? Says it uses an Intel chip. At its heart, maybe the chip is a FPGA? Anyone know anything more about it? I know quite a bit about the big Movidius unit - it's inference only and it runs either TensorFlow or Caffe pre-compiled models. IIRC they're all in 16 bit mode.  The Movidius chip itself is much more powerful but you have to be a qualified partner to get the SDK."
technical,"Hey all, thanks for mentioning our push! Hope it will be useful! To compile this code, you need a SYCL compiler. Currently, the only supported compiler is Codeplay's ComputeCpp, which is available via a Codeplay's evaluation program. ComputeCpp will be made available for free as a public open beta, later in 2016 and then released with a free version (the ComputeCpp Community Edition) in 2017. This will let anyone compile and develop TensorFlow on OpenCL devices, such as AMD or Intel GPUs and CPUs. btw. shouldn't this issue have OpenCL label? :) Thanks, Luke I really hope that could be compiled also with an opensource tool.  how it is going with your new Opencl branch"
technical,"CLBlast is now also available in OpenCL-Caffe, have you seen that? :) Did you also have a chance to look at the libDNN convolutions? I saw it, yes! :) I haven't looked at libDNN at all so far, but I am not sure what you mean exactly. I assume convolution is implemented using GEMM?"
technical,"Apple is solely aimed to sell Apple devices. Google is aimed to hire Google massive services. If you're willing to do AI (learning) with one single device, like one Apple Laptop, you'll do ""Superficial Learning"" instead of ""Deep Learning"", so you'd better give up doing anything but tutorials. Inference results in a trained model for one single user, in a single device (even in a not-too-many-multicore phone), could be neat to be done through GPUs, but is perfectly doable only with CPUs. On the other side, GPUs are absolutely needed if you're gonna feed extremely large datasets for learning or you're gonna serve trained inference to extremely large concurrent customer groups. Even though, doing it in such scale, is not that easy due to the network issues. Just have a look to the TPU-Pods physical architecture. It's in the antipode of a laptop (several GPUs per memory-overloaded multi-core server, with dedicated optical-fiber for inter-server communications). I have a MacBook Pro. It's a nice terminal to get to the cloud :-D I see TF on Metal can extend to iOS too. If anyone interested in picking it up, I recommend adding Metal support to Eigen first (can use OpenCL as reference)."
technical,"You can move on to the latest Linux distros & use a recent custom compiled kernel like 4.11/4.12 with AMDGPU drivers enabled, RADEON disabled and with CONFIG DRM AMDGPU SI=Y and/or CONFIG DRM AMDGPU CIK=Y set in the kernel configuration, plus AMD firmware for 7970 (Tahiti) in the initramfs = newest AMDGPU-PRO OpenCL will work on any GCN cards. Forget about FGLRX (on older Linux distros) and Clover via RADEON drivers, both are sub-par. Forget about pre-GCN cards also. I tested them using OpenCL on Windows for Caffe, the performance is not worth making an effort for such old cards. As all AMD cards post 2012 should be GCN anyways. I spent a few hours yesterday trying to get AMD's open source stack working. I got MIOpen and its dependencies but hcc is still missing some bits. I may need to do a custom kernel build to get everything. I don't much care about porting CUDA code or running compiled C++ on the GPU - I want to do number crunching on it. ,-)  I also saw something on their website about programming it in assembler - *that* I might be interested in, because it's easy to go from assembler to FORTH. ,-)"
technical,"Can I compile your tensorflow-opencl repo code  to apply my ARM board?  My ARM board has Imagination GPU which support opencl 1.2 . I stumbled on this thread while searching for tf/intel support.  I have an intel MacBook Pro, how can I help? I don't know c/c++, but I can follow build/compile/test instructions and pass back (pastebin) results"
technical,"Indeed it's failing. Still at an early stage, I guess. I tested it, got segfault in MNIST example immediately. Don't know what I am doing wrong here."
technical,"I wanted to give it a try but failed. Could not find a version that satisfies the requirement plaidml (from versions: ) No matching distribution found for plaidml I think it would be more appropriate to make an issue at plaidml repository instead of here, since this issue is about supporting OpenCL in tensorflow. Additionally by looking at the installation instructions there your pip install command may be incorrect."
technical,There are also some papers at WG21. As you can see jfbastien at Google has  some activity at WG21 and also helped to host the SG14 f2f meeting at Google in March. I think it would be worth taking this discussion to the SG14 mailing list as the details aren't related to OpenCL / tensorflow.
technical,"That would be a great discussion to be had at yesterday's TensorFlow Dev Summit. Do you ask about the resources available / type of programmers needed to contribute? If so, in OpenCL/SYCL approach C++ programmers / OpenCL C programmers can quickly be brought up to speed and be able to contribute. XLA approach requires a compiler / llvm experience. XLA is Google's internal project by extension they have more resouces associated with it. But, on the other hand their task is way bigger too.. Writing a compiler is not an easy task. Otherwise, if you are asking about the model: As I mentioned earlier we are seeing both efforts as complementary approches and both having different use cases. I still stand with that statement. For instance tatatodd in his presentation mentioned that some of the Ops will never have XLA as a target. I believe that we are possible to fill that gap. Other things to consider are new platforms. I will use mobile and embedded enviroment for this argument's sake as a new chips tend to pop out more frequently than GPUs ( the principle is the same ). If the semiconductor support SYCL / OpenCL you get TF support out of the box ( some performance tweaks might be required ). If the architecture is exotic and there is no LLVM backend for it yet XLA needs to add it ( that might not happen too often but still ). What happens more often is the architecture changes a bit and then new optimisation passes need to be added or existing one must be modified to gain the benefit. Tweaking kernel code is easier. I haven't looked very deep to XLA but I assume that XLA has to call into the CUDA API somehow to run the PTX kernel code, so would have to be ported to OpenCL or Vulkan to run SPIR-V kernels instead - that I assume would go through StreamExecutor - another framework to get familliar with - probably quite a big effort. In short, we are providing an unified / stable platform in very fragmented / diverted ecosystem that both semiconductor companies and developers can target. Where as XLA would have to commit to support. benoitsteiner or drpngx  might be able to give more inside knowledge of XLA as I am working with a lot of assumptions / conclusions based on conversations. Oh, as well I have created slack channel to eas up communication Edit: Slack link is no longer valid. Please ping me if you'd like to join. I think that is correct and partially will depend in which direction the semiconductors producers will be oriented. ""These backends emit the LLVM IR necessary to represent the XLA HLO computation in an efficient manner, and then invoke LLVM to emit native code from this LLVM IR."" So LLVM IR could be converted to SPIR-V. But Opencl SPIRV dialect it is different from Vulkan. Streamexecutor is being pushed in LLVM parallel-lib the original plan seems to cover opencl."
technical,"That would be a great discussion to be had at yesterday's TensorFlow Dev Summit. Do you ask about the resources available / type of programmers needed to contribute? If so, in OpenCL/SYCL approach C++ programmers / OpenCL C programmers can quickly be brought up to speed and be able to contribute. XLA approach requires a compiler / llvm experience. XLA is Google's internal project by extension they have more resouces associated with it. But, on the other hand their task is way bigger too.. Writing a compiler is not an easy task. Otherwise, if you are asking about the model: As I mentioned earlier we are seeing both efforts as complementary approches and both having different use cases. I still stand with that statement. For instance tatatodd in his presentation mentioned that some of the Ops will never have XLA as a target. I believe that we are possible to fill that gap. Other things to consider are new platforms. I will use mobile and embedded enviroment for this argument's sake as a new chips tend to pop out more frequently than GPUs ( the principle is the same ). If the semiconductor support SYCL / OpenCL you get TF support out of the box ( some performance tweaks might be required ). If the architecture is exotic and there is no LLVM backend for it yet XLA needs to add it ( that might not happen too often but still ). What happens more often is the architecture changes a bit and then new optimisation passes need to be added or existing one must be modified to gain the benefit. Tweaking kernel code is easier. I haven't looked very deep to XLA but I assume that XLA has to call into the CUDA API somehow to run the PTX kernel code, so would have to be ported to OpenCL or Vulkan to run SPIR-V kernels instead - that I assume would go through StreamExecutor - another framework to get familliar with - probably quite a big effort. In short, we are providing an unified / stable platform in very fragmented / diverted ecosystem that both semiconductor companies and developers can target. Where as XLA would have to commit to support. benoitsteiner or drpngx  might be able to give more inside knowledge of XLA as I am working with a lot of assumptions / conclusions based on conversations. Oh, as well I have created slack channel to eas up communication Edit: Slack link is no longer valid. Please ping me if you'd like to join. I think that it would be revolutionary ,)"
technical,"can't build python module with your fork following this I'm on ubuntu 16.04, dirname is from coreutils-8.25-2ubuntu2 I think that tweaking the TF dockerfile on your repository with this istructions could easy the setup for others."
technical,"All that stuff sounds great, but let's refocus the discussion on having TensorFlow working with OpenCL SYCL and not only vendor-specific solutions... :-) I hope RoC and other HiP have their own GitHub to discuss their own issues... at least i am still in the OpenCL realm. Join the club again! :-) I think the discussion on HIP is valid, if there's a HIP port for Tensorflow in the works. After all, the official Tensorflow-on-CL solution is to use a proprietary SYCL framework with sharply limited platform and kernel support, so it's not really any better than the ""vendor-specific"" HIP solutions that offer a new way out of CUDA.  HIP may be mostly AMD's doing right now, but AFAIK it's an open standard? Perhaps I'm mistaken. If it is though, and if AMD can deliver a tensorflow-on-HIP port, it would immediately be more open than the official tensorflow-on-SYCL one."
technical,Can XLA backends LLVM IR be converted to SPIR-V with I think there is no hope with appbundle
technical,"Besides the above 2 posts, I'd like to add that now AMD's Vega GPUs (including the ones inside Raven Ridge APUs) can do FP16 at twice the FLOPS, so if TF could support them (through OpenCL) it would really help people with less budget. Also a lot of these people would be students, and if we get them to use TF as the starting point of their DNN journey, they would probably stick with TF down the road, and even tell others about TF, it's a great way to help expand this project. I think this thread is mostly meaningless for developers (too much noise - and I'll add some more ,-) but I think many comments are missing the point: **If you want to run Tensorflow with AMD cards OpenCL IS NOT what you are looking for** - please head over and install the ROCm stack. AFAIK **AMD's current strategy is based on ROCm instead of OpenCL for Tensorflow/pytorch**.  Generic OpenCL was too much maintenance/did not give enough performance benefits to be worthwhile for AMD. Therefore **this ticket is only interesting if you are running (e.g.) an ARM platform** which uses OpenCL only.  (Disclaimer: just an outsider, no real inside into Tensorflow development so maybe the information above completely wrong and misleading. Feel free to bash me if you know better.)"
technical,"But the most notable platforms, Linux + the new AMD GPUs (RX 480, upcoming Vega) do only support OpenCL 1.2 for now... and who knows when that's gonna change (my bet is on in a year). Beignet (opensource Linux Intel) for OpenCL 2.0 is also still a buggy mess, the stable version has 1.2. Also considering all the smaller companies that make OpenCL compatible chips are barely pulling 1.2 support. So I guess anything relying on OpenCL 2.0 will see very bad adaption rates in practice. I think.. any hardware vedor has the urgency of consuming SPIR-V? I think that Graphic/Shaders pressure on Vulkan could help Opencl side.."
technical,"Software has to support what it has to support to cover every use case. HIP is all bells and whistles (at least on the paper) if you have supported hardware. But there aren't just ""newer amd and nvidia cards"" to this world.  Now please, for the love of god, complain here for any problem with that. And here for everybody else interested to the continuation of this issue. I thought, that SPIR-V would directly replace CUDA as a cross-hardware alternative Why does Google still rely on CUDA?"
technical,"If the codeplay route fails to deliver, don't miss out on tf-coriander, which is finally at a state of practical usage on Ubuntu/Mac.I'm currently testing it on convnets, bidirectional rnns, etcetera and everything seems to be working great. It runs on ""vanilla"" OpenCL 1.2 so that should enable Tensorflow on a huge range of relatively old hardware. The rub is, for now, that it's based on Tensorflow 0.11. I tried following the steps listed on the link. I get the following error: Actually I am getting the same error if I try to compile tensorflow from source. I have compiled it earlier, not sure what has changed though."
technical,"Jesus, I wish you guys STFU and get back to work instead. I'll have to unsubscribe from the ticket because it's unbearable to get emails with flame wars. Too bad maintainers do not mute the thread. Could you please do something about it ? I understand TensorFlow only supports CUDA. What would need to be done to add in OpenCL support?"
technical,"We (Vertex.AI) have just open sourced PlaidML, our deep learning stack with support for running Keras on OpenCL. TensorFlow support is coming, help there would be welcome. And yes, Mac support is on the way (also Windows). I wanted to give it a try but failed. Could not find a version that satisfies the requirement plaidml (from versions: ) No matching distribution found for plaidml"
technical,"Just to help test it for you!  No worries if you don't need a hand testing, just thought I'd offer. I haven't even tried that machine with cuda TBH, I've only tried it on MacOS where I can't use OpenCL through Docker at the moment. I was going to try ComputeCpp SYCL however they only provide the ubuntu installer(I am also on arch) and the aur install script is broken.  It is good to hear that it can work.  If I get desperate enough I may try it out. bensander That looks like exactly what I need to get ADM support however I am worried by the fact that this code has not been back-ported to TF and that its code was last updated over 2 months ago given that my code targets TF 1.4.0 It seems like at the moment tensorflow basically ties you to Nvidia, at least for us ""mortal"" programmers.  Lack of documentation/updated roadmap doesn't help.  I wouldn't mind helping out in any way I could however I've had little success getting things working so far."
technical,thumbs up and all that. I will be interested in expanding Tensor Flow with OpenCL. As we have already released OpenCL caffe. Hopefully it can get integrated in light way? Is anyone interested in working together on this?
technical,"Concerning AMD GPUs, we're a partner of AMD. See my message of 8 days ago, which you might have missed: At my company StreamComputing we have various GPUs for build-testing and benchmarking, which we use for our customer-projects. I could hook your Github into our Jenkins to do a weekly run. I wonder if you might have the possibility of setting up a CI server, that runs on each commit? No problem. I probably need write-access to the project, so Jenkins can write the log-file into a build-log directory. I just spammend you, so we can discuss."
technical,"Can you explain what will be the role of StreamExecutor on Opencl and of relevant Canned operations for Tensorflow. I still cannot see how this will integrate with SyCL plans on Eigen and cudnn (replacement?) I would like to contribute to this, too."
technical,"bhack from that thread and here it seems like  is looking into it. I think we have enough willing people to work on it, we just need benoitsteiner,  or gujunli to coordinate. Benoit has been quiet, maybe he's on holiday. I would love to help contribute with this initiative."
technical,"let me give it a try. I'll bring this up at the sig-arch meeting today, however as neolit noted * our API guarantees prevent these sorts of renames on short time scales (we might introduce a new name, but the old name would be preserved effectively for the rest of the v1 lifetime) * the term is well established in both having common descriptive usage (outside of the less-common vulgar usage) so I think it's unlikely we would change this.  Will update after the meeting."
technical,"Yeah - it works (doesn't crash) but there's no indication whether or not it found OpenCL. I think the best course of action here is to file a separate issue to request documentation, since it's clear (when you run ./configure building from source) that there is *code* for OpenCL. That's how I found it, anyhow. I'm doubtful it found GPU device in your case because in mine it printed debug info in the beginning about selecting GPU device. Perhaps you can recompile with added printf to console somewhere"
technical,"Yes, when there will be something more functional. Basically it is quite a copy and past of this istructions you have posted. I'm experimenting building this on MacOS 10.10.5 on a MacBook late 2015 with ATI 6770M (OpenCL 1.2). I've installed Xcode 8, Anaconda (Python 3.5), and MacPorts equivalents of clang+llvm: This is as far as I get:"
technical,"Now that you have hyper notified us can you give some feedback on stream executor strategy? ,) I'm just going to blame GitHub for everything, including lack of OpenCL support. ,)  benoitsteiner might be able to comment more though.  I don't really know what you mean by 'stream executor' strategy.  We currently use a version of stream executor and CuDNN and Eigen and they all play well together, so I'm not sure how any plans have changed for the OpenCL side of things"
technical,sorry but isn't this completely off topic here? I don't see how this is relevant in OpenCL TF? I'm more interested here to know if will be taken a tuning approach to matrix multiplication and convolution kernels and if will be a valid open source alternative.
technical,Opensource triSYCL is arriving. I'm new here. Very interested in seeing TF support OpenCL. How do I get updates from this thread?
technical,"bhack sure I have some interest for high-end C++ on FPGA :-) TensorFlow sounds like a good validation use-case for triSYCL too. By the way, if some people here are looking for some internships on this subject, I have some positions. It looks like Codeplay is looking for some people too, if I trust their web site. I'm really interested in karlrupp and  opinions. I hope they want to join in the discussion on the new google  group."
technical,"Is there a plan to push more code? What about sycl compiler? Seems that there are no opensource GPU target implementatons released. I'm soon releasing a cuDNN replacement (only the convolution part, as it is the most performance and memory critical to have this done) for OpenCL on Caffe. Maybe it will also be to some use for the Tensorflow port."
technical,"is there any update/estimate regarding plans? I've an AMD GPU and an Intel GPU in the laptop. I think both have OpenCL drivers and AMD's support seems to be much better. I'd have higher performance, because I've 2 OpenCL devices. I hope you make it scale with OpenCL devices."
technical,"Er, you know CoreML supports importing tensor flow models by way of Keras? Apple doesnt 'hate' Google, business is business, One of Apples suppliers is Samsung. Read into that for a moment. Google, Apple, Samsung are businesses and will do what makes money. As a side note. My MacBook Pro hasn't melted from running inference on thousands of movies by the way..  I suspect CUDA was super convenient to adopt and continued support from Nvidia and missed opportunities from AMD got us to where we are. I don't think its nefarious, just cost of making a change vs performance deltas vs cost of staying the course.  I suspect some genius will come along to help solve the issue. I've created a Google Group for a collaborative discussion about bringing deep learning to new places like OpenCL, Mac, iOS, CoreML, Vulkan, etc. If you'd like to help make these happen please join and post a note with your use case or what part of the problem you're working on. There already are people working really hard on efforts to bring TF to more platforms including MIOpen, Codeplay's work, TF-Coriander, and an internal project at my company (Vertex.AI). It would be great to get developers & users all in one place as these efforts are all closely related."
technical,"This. As for ubuntu, only 16.04.3 is said to be supported (at least officially then, considering even arch can get it to work after some script magic) EDIT: 'complete' AMDGPU-PRO driver requires kernel 4.9, that was likely the problem If anyone cares, the port of AMDGPU-Pro Driver 17.40 to Arch is ongoing and is very active on GitHub,  We really should close this issue, since, as mirh pointed out, TensorFlow uses SYCL, not OpenCL. Maybe we should open another one, ""TensorFlow on AMD cards""??"
technical,"Two new Kronos Group standards are released. If it helps, a better version of isaac is out, and provides significant speed-ups over clBLAS and cuBLAS on Maxwell, Pascal and Fiji. Also provides faster (input-aware) kernels than Tensorflow for 1D and 2D reductions."
technical,"Thanks! I'm following this issue during last months. I'm not confident about the Apple OpenCL commitment, given they're stuck onto OpenCL 1.2 since 2013 (Apple is not providing SPIR 1.2 support yet). If TensorFlow on OpenCL would help you in your work let me know, to the extent I can help advance research and practice of deep learning I'd like to help. My company has built an OpenCL back end for TensorFlow tuned for a variety of GPUs as part of our work in on-device inference. We have tested on the major mobile & desktop GPU families including common configurations on Windows & Mac. If there's enough interest we may do some kind of public distribution. We also have Metal (Apple GPU) and LLVM (CPU), along with a way to do zero-dependency deployment. The idea here being to give every device great support for deep learning."
technical,"we have not tested this but you could give it a try. You will need to use some older AMD drivers with Ubuntu that support the SPIR extension. I haven't yet been able to figure out what drivers those are yet. If the codeplay route fails to deliver, don't miss out on tf-coriander, which is finally at a state of practical usage on Ubuntu/Mac.I'm currently testing it on convnets, bidirectional rnns, etcetera and everything seems to be working great. It runs on ""vanilla"" OpenCL 1.2 so that should enable Tensorflow on a huge range of relatively old hardware. The rub is, for now, that it's based on Tensorflow 0.11."
technical,"BTW, AMD are building an alternative path that's fully open source - translating the CUDA code to OpenCL code with a compiler toolchain. I'm not sure what the status of that is for the older cards like mine though. If you are going to write an article, I guess it wouldn't hurt to also explain (took 3 hours to get the whole picture):  * TF has in fact a SYCL 1.2 backend. No \*actual\* opencl.  in turn, you have two implementations of the standard (trisycl looks cool, but it's limited * In the end, ComputeCpp 'hooks' SPIR/SPIR-V (in addition to PTX, but this is really another story And this is what eventually gets you straight to your bloody yearned OpenCL 1.2 (w/  cl khr spir ext) HIP instead is yet another backend, sits opposite to SYCL, and targets only and exclusively ROCm (or well, lol, even in turn cuda if you have an nvidia gpu.. but this is again another story)  AMD are building an alternative path that's fully open source - translating the CUDA code to OpenCL code with a compiler toolchain. Nope. You are talking about HIP, and.. that's actually it, what you eventually convert your code *to*. Which is not OpenCL. HIP then runs on ROCm as I was saying... ROCm which is *also* what runs OpenCL for you (on supported cards), but please I'd stress everybody to notice how the relations is only forward from ROCm, never ""intra-sub-layers"" What you are perhaps thinking about could be coriander.   I'm not sure what the status of that is for the older cards like mine though.  Summed up here: fully fledged AMDGPU-PRO, amdgpu-pro-opencl-only driver as you are doing now ... Or continuing to wait until the end of the decade for somebody to finally make clover usable.  Also, fglrx... But if that's hard to recommend for pre-gcn cards, I guess it's just better to draw a veil over."
technical,Ok so actually seems that it is an effort of Codeplay with some kind of sync to Google internal. What are the role of AMD and Intel subscribers here? if you have any interest on this from SYCL/FPGA universe
technical,"But if I use Eigen skcl guide can run on cpu with OpenCL support. (Also this guide code is a little out of date, need some modify) Any people can help to check how tensorflow python interface can also run with OpenCL support. And build tensorflow with this opt set will not really generate tensorflow binary.  --config=sycl Just build tensorflow in this command: Maybe I build forget --config=sycl I will try build command and verify whether it can call OpenCL library. After get result, I will post here. If you modify the tf.Session creation with the below it will show a log in the terminal, is this mentioning SYCL anywhere? For the Eigen guide do you have any specific feedback where it is out of date?"
technical,"Guys, I apologize for perhaps naive question but why is this build AMD GPU only ? Isn't OpenCL supposed to be standard ? Do I understand it correctly that it won't work on my Intel Carbon X1 with installed OpenCL 2.0 drivers ? If you read the issue that was linked twice, you'd see there's nothing about amd gpu. Intel's is currently excluded, but it has nothing to do with wanting to force users, and there is a temporary workaround - discuss there if really any."
technical,"The easiest is if you click on ""register your interest"" on our page here: That will get you into our developer program and we can work together on this alephman If you want you can cotribute also to let to compile with an opensource alternative."
technical,"all of that sounds incredibly useful and helpful. My personal project  would greatly benefit from OpenCL on OS X, as well as Metal for iOS and Desktop deployment. If its possible for this to be introduced to Tensorflow proper I think it would be a tremendous boon for tons of developers.Thank you. If your company publish an OpenCL version, or more interesting a Metal version of TensorFlow, I think that this is going to be a great news for a lot of people, I am in the process to build an eGPU with an NVidia card to get TensorFlow / Keras running on my MBP for my job...For people interested ... go to eGPU.io community"
technical,"clinfo and clpeak both run. I haven't done this recently, but when I build caffe from source and run the tests it definitely hits the GPU. So I'm pretty sure the OpenCL / GPU drivers / libraries are working.  I'm on Arch Linux - kernel is their LTS - linux-lts 4.9.52-1. If it matters, the ""Bonaire"" peaks about 1.7 TFLOPS in 32-bit mode and is in the ""Sea Island"" family of AMD GPUs. Impossible for me to build TensorFlow for Sycl/OpenCL !   Training models on my CPU takes ages, I really need OpenCL/GPU acceleration ..."
technical,"I got the ComputeCpp SYCL working on Arch - there was a binary tarball on their website when I did it. In this news about the release of SYCL 1.2.1 it says : ** The new specification incorporates significant experience gained from three separate implementations and feedback from developers of machine learning frameworks such as TensorFlow, which now supports SYCL alongside the original CUDA accelerator back-end. **  Does that mean it is now possible to ""easily"" run TensorFlow on AMD GPU that support OpenCL 1.2 on which SYCL is built ?"
technical,"I can test it - stay tuned. Looks like it's failing CI though. Indeed it's failing. Still at an early stage, I guess."
technical,There's also.  Here's the associated paper. Initial OpenCL/SyCL support was merged in master
technical,What is the triSYCL status? Intel beignet opencl 2.0 support is almost done!
technical,"Hi all,  Thanks for the interest! At this point we are getting our testing infrastructure set up to make sure that nothing that we do introduces regression. We are in touch with benoitsteiner to make sure we are in sync with what he's done so far.  We are still in compiling a road map for the integration process - it should be done in couple weeks time, as there is a couple of business details to clarify.  Our goal is to bring the OpenCL to TensorFlow via Eigen by end of this year.  Thanks, interested. would love to contribute."
technical,"I think the discussion on HIP is valid, if there's a HIP port for Tensorflow in the works. After all, the official Tensorflow-on-CL solution is to use a proprietary SYCL framework with sharply limited platform and kernel support, so it's not really any better than the ""vendor-specific"" HIP solutions that offer a new way out of CUDA.  HIP may be mostly AMD's doing right now, but AFAIK it's an open standard? Perhaps I'm mistaken. If it is though, and if AMD can deliver a tensorflow-on-HIP port, it would immediately be more open than the official tensorflow-on-SYCL one. is a subset of CUDA, so it's as open as CUDA."
technical,"Thanks for the information i edited my message accordingly.   The AMD A12-9800E iGPU should be GCN v3.  The main and only reason for me to do the benchmarks/tests is to find an answer on my question ""Stay with AMD or switch to Nvidia for my Deep Learning adventure"".  And the answer is. I really like the opensource approach of AMD but i will likely switch to Nvidia due to 2 factors. First the Deep Learning software stack (e.g. Tensorflow) is much more mature for Nvidia. Second the graphic card offers for my very specific needs (must fit into a Dan A4 SFX Case and must be very very silent / almost noiseless under full load for hours) is very limited or even non existing on AMD side. Is Intel GPUs supported? I think my Iris Pro can speed up the age-long training for a bit."
technical,"Following it seems Python 3.6 should be working - I haven not tried it though Is that a ComputeCpp version that can build TF at the moment ? I tried various versions between 0.3.2 and 0.1.4 and none worked. They all ended up with the ""multiple overloads of 'global ptr' instantiate to the same signature"" error. Btw, I cannot find the TensorDeviceSycl.h file in TF sources, is that a renamed one ? Is it possible to apply the patch to current sources ? Thanks in advance."
technical,Since this issue has made its way to the roadmap (see  Platforms ): do we roughly have an idea of when OpenCL support will hit TensorFlow? Like version 0.9 / 1.0? Q3/4 2016? Or is 2017 more realistic? Is the eigen-opencl ready enough to support an opencl tensor flow development ?  Does tensorflow depend only on Eigen tensors or are there any other dependencies of Eigen ?
technical,"There is a fork of TensorFlow supporting OpenCL And of course benoitsteiner 's work IMHO, it is ridiculous that mainstream TF still didn't merged their work. Is the focus here on getting-it-to-run-as-lomg-as-it-is-OpenCL, or making it actually run faster? I'd prefer there not a holy war, but focusing on getting it to run fast on several GPUs. LifeIsStrange's focus is on getting it to work on AMD GPUs and then HIP makes good sense. For others the focus is to make it work on Intel GPUs or Android, and then OpenCL makes much more sense. GPU-languages are a mess, so please keep practical,  If I read some of the comments here, performance is an issue with the OpenCL ports. But unfortunately I cannot see many benchmarks around. Are there more benchmarks than this one?"
technical,"pfc Is what currently usable on non-Ubuntu Linux? TensorFlow using OpenCL in general? Or TensorFlow using OpenCL on an AMD GPU? I'll assume the latter, since it's the only reason you'd want to run TensorFlow using OpenCL. For an NVidia GPU you'd use the NVidia drivers / libraries and for CPU-only there's nothing to gain from OpenCL.  I had this working a few weeks ago on Arch Linux, using the proprietary ComputeCpp SYCL library and an AMD ""Bonaire"" (Sea Islands architecture) GPU. There's a new ComputeCpp release that I need to test but I'm guessing it will work.  It turns out that the AMDGPU Pro proprietary libraries you need to make this work don't run on Ubuntu 16.04.3. The upgrade from 16.04.2 brought in a newer Linux kernel and X Server, and AMD has yet to ship something that works on it. see for the details. I have been unable to make AMD OpenCL work on Ubuntu.  There's an experimental AMD version of TensorFlow that uses a compiler to translate CUDA code to OpenCL code but I haven't tested that either. In the absence of a supported driver it's useless. is the officially supported way to run tensor flow on AMD hardware."
technical,"pfc Is what currently usable on non-Ubuntu Linux? TensorFlow using OpenCL in general? Or TensorFlow using OpenCL on an AMD GPU? I'll assume the latter, since it's the only reason you'd want to run TensorFlow using OpenCL. For an NVidia GPU you'd use the NVidia drivers / libraries and for CPU-only there's nothing to gain from OpenCL.  I had this working a few weeks ago on Arch Linux, using the proprietary ComputeCpp SYCL library and an AMD ""Bonaire"" (Sea Islands architecture) GPU. There's a new ComputeCpp release that I need to test but I'm guessing it will work.  It turns out that the AMDGPU Pro proprietary libraries you need to make this work don't run on Ubuntu 16.04.3. The upgrade from 16.04.2 brought in a newer Linux kernel and X Server, and AMD has yet to ship something that works on it. see for the details. I have been unable to make AMD OpenCL work on Ubuntu.  There's an experimental AMD version of TensorFlow that uses a compiler to translate CUDA code to OpenCL code but I haven't tested that either. In the absence of a supported driver it's useless. Is there a list of CUDA dependency libraries that Tensorflow relying on?  This would help to see if we could have immediate OpenCL alternatives."
technical,"pfc Is what currently usable on non-Ubuntu Linux? TensorFlow using OpenCL in general? Or TensorFlow using OpenCL on an AMD GPU? I'll assume the latter, since it's the only reason you'd want to run TensorFlow using OpenCL. For an NVidia GPU you'd use the NVidia drivers / libraries and for CPU-only there's nothing to gain from OpenCL.  I had this working a few weeks ago on Arch Linux, using the proprietary ComputeCpp SYCL library and an AMD ""Bonaire"" (Sea Islands architecture) GPU. There's a new ComputeCpp release that I need to test but I'm guessing it will work.  It turns out that the AMDGPU Pro proprietary libraries you need to make this work don't run on Ubuntu 16.04.3. The upgrade from 16.04.2 brought in a newer Linux kernel and X Server, and AMD has yet to ship something that works on it. see for the details. I have been unable to make AMD OpenCL work on Ubuntu.  There's an experimental AMD version of TensorFlow that uses a compiler to translate CUDA code to OpenCL code but I haven't tested that either. In the absence of a supported driver it's useless. Is there a plan to push more code? What about sycl compiler? Seems that there are no opensource GPU target implementatons released."
technical,Does your fork only support AMD R9 Nano / AMD FirePro GPU? Is there a test case I can use to verify that I'm using the GPU? I can monitor it with radeontop but I'd like something that uses TensorFlow itself.
technical,"Ah yes I have seen AMD's work on ROCm. Unfortunately they only support Linux though, and it doesn't even seem like support for any other OS is on their roadmap. I'm hoping for something that supports Windows. Is there any GPU accelerated tensorflow package available for Windows? I thought, if you want GPU accelerated deeplearning, Linux was only choice. If TensorFlow was ported to OpenCL, would that make it easier to port to Windows? I'm not sure why TensorFlow is not available on windows with GPU acceleration when CUDA is supported there.  I guess this is now off topic, but if anyone know of TensorFlow and/or PyTorch for windows that is GPU accelerated, I'd like to know about it as well..."
technical,"Ah yes I have seen AMD's work on ROCm. Unfortunately they only support Linux though, and it doesn't even seem like support for any other OS is on their roadmap. I'm hoping for something that supports Windows. Is there any interest in this by Intel? I really hope that we don't fragment OPENCL here like in Caffe where we have an AMD fork, Intel unmerged PRs, another semi-unofficial AMD PR, and a long staging user PR (plus two old abandoned Opencl efforts). If somebody is interested in the history can take a look at comments."
technical,Have some link for other that try to have tensor opencl Maybe worth to check also: Feel free add working projects. Is there any update? This issue is over 3 years old.
technical,"We are in contact with benoitsteiner, but we will discuss our proposal with the upstream maintainers before we invest too much effort. We are developing our implementation of SYCL, ComputeCpp . For more information, can you please contact me off-list via the email address on my profile? is there any update/estimate regarding plans?"
technical,FWIW there's an Intel SDK for OpenCL - I've got it on my ancient Sandy Bridge laptop but I'm sure it'll work on your machine. Is this currently in a usable state on non-ubuntu linux systems?  The roadmap page simply links here.
technical,"Hi all, As you probably see already, a bunch of SYCL stuff has been pushed to TensorFlow. We are not complete yet,  and there is plenty to do. But we are progressing to get there. If you are interested in contributing or just curious on the current state, check the breakdown below. **Infrastructure** Google kindly donated two machines that are set up to test benoitsteiner's fork of TensorFlow periodically. Both have AMD GPUs. We at Codeplay are looking to dedicate machine(s) next year too. To improve the OpenCL device diversity coverage. We are looking for contributors on that front if anyone is interested in providing a test build server for relevant platforms that we support. Currently, the requirements are: - Ubuntu 14.04 - OpenCL drivers that support SPIR ( Intel CPU / GPU or AMD GPU )  perhaps you culd help out with that?  **Tests** On the Fiji machine we are facing 164 fails. On the Hawaii machine we are down to 56 fails. We are looking into fixing the failing gradient tests and investigating the origins of the additional fails on the Fiji machine. **Eigen** For the past few months we have been actively implementing features needed by TensorFlow including: Reshaping, Slicing, Basic Reduction etc. Currently we are implementing Contraction. A detailed breakdown can be found in the Eigen Tensor tab. **TensorFlow** A lot of Coefficient-wise operations have been implemented including Abs, Floor, IsFinite, Log, Pow, Mul, etc., as well as Tensor Manipulations like Reshape, Shape, Identity, Fill etc. A detailed breakdown can be found in the TensorFlow Kernels tab. **Organisation** The above spreadsheed has several tabs that categorise the efforts of the project like: Overall Plan, Eigen Tensor, TensorFlow Kernels, Models. If you would like to get involved, please put your name next to the item you are working on or add anything important that is missing. Is this roadmap active?"
technical,"sxpc722 That should work then. By the way, the new machine is Windows 10 and I am not planning to dual-boot it with Linux until I absolutely have to do so! I'm sick of chasing down driver and library bugs for vendors (looking at you, AMD). In fact, I may put a Windows partition on my workstation for the same AMD reason. ,-) It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly."
technical,"For the love of god, can't you read just 2 (two!) posts above? Discuss the lack of intel gpu (or amd cpu) support here it is a goal to make full use of various computing resources(e.g. cpu,gpu,DSP, any other coprocessor). In fact, it depends on the support of hardware vendors: **dirver**  and OS. As i know, you may can't enable both intel GPU and nvida GPU for video during the same time, due to the limitation from vedio driver. (You might be able to switch between them). However, opencl can use them at the same time. They are both ""devices"" in it."
technical,"For the love of god, can't you read just 2 (two!) posts above? Discuss the lack of intel gpu (or amd cpu) support here It is in Google's interest to support OpenCL, by having a specific (company/brand/vendor)'s specific hardware as a dependency for your software, you enforce yourself to pay more for hardware, market competition lowers costs. Google has always been about commodity hardware since the very beginning which was and still crucial for Google's success (market dominance), having lower data center operating costs, enabled the revolutionary generous essentially free services offerings like Gmail (storage space) and Google Photos (storage space and auto-tagging)."
technical,How about using HIP to port CUDA code to platform agnostic one? It seems AMD is working on that:
technical,"Hi, has created Coriander that could run NVIDIA CUDA  code on OpenCL 1.2 devices. You might want to take a look if that suits your need to connect TF to OpenCL 1.2 devices. Kindly attribute his name and his contribution in case if you plan to use his work. It seems hopes of ever seeing OpenCL support for Mac have gone from little to tf.zero. I just read that TensorFlow Mac will no longer have **ANY** GPU support apparently (1.2+): Note: As of version 1.2, TensorFlow no longer provides GPU support on Mac OS X."
technical,"Looking at the last compile failure on OS X in the travis log - it looks like running might a solve? It should re-link the /usr/include directory. I had this issue myself when updating Xcode beta to release and had issues compiling some C++ code. It seems like the XLA compiler will provide LLVM code generation from dataflow graphs. This means very easy access to spir-v and therefore Vulkan's compute API. With code generation sorted out, I can't imagine Google not providing Vulkan compatibility given the high number of unused integrated GPUs running on Android."
technical,"Yes, plainly passing any   global structure (like array or struct) containing pointers is incorrect just because those pointers can point to memory of another device (OpenCL supports multi-device paradigm where one device can't access memory of another). But it seems to be possible to overcome on IR level, w/o intermediate translation to OpenCL code - that's what I assumed :) it suggests the streamexecutor has supported the CL version canned operation(like DNN, BLAS) out-of-box. Does it suggest google has already has the clDNN, clBLAS implementation ready for Tensorflow, but just not open source it yet?"
technical,"Maybe OpenCL 2.0's SVM feature could solve the pointer issue? Since everyone besides Nvidia(AMD, Intel, ARM, Qualcomm) is starting to support OpenCL 2.0. Maybe it's a good solution? it's a blas implementation itself. It implements some of the symbols in clblas and cublas headers so no recompilation and code modification. is necessary. I could also implement some of the symbols for clblast.h, since it uses a different header. Some advantages of Isaac are: - Entirely dynamic, so that it can use either/both CUDA or OpenCL without recompilation. - Input-aware , it doesn't tune kernels for large square matrices. It should perform well on all shapes you can think of without retuning. - C++ API similar to numpy/arrayfire. Some fusion for combining elementwise operation with reductions"
technical,"For school I had to use Tensorflow for training models, not just for evaluating a model. And I will have to repeat this again in near future. For students like me, having training on GPU is a must, saving a lot of time. It not just a matter to serve multiple concurrent user. it's about the ability to develop models and solutions locally on a mac"
technical,All you gotta do is sudo apt-get install rocm miopen-hip It's in the Arch User Repository but it doesn't install - it doesn't install from GitHub source either. It looks like something simple - it can't find a few dependencies.
technical,"Just so you are aware ROCm uses OpenCL, it is AMD's userspace OpenCL driver on Linux (that is why is why it doesn't support windows), so if you are not aware of how AMD's driver ecosystem on linux works, they have their kernel side drivers AMDGPU and AMDKFD(which is now getting merged into AMDGPU) then there is the userspace drivers RadeonSI(for OpenGL) RadV/AMDVLK(for Vulkan) and ROCm(for OpenCL). Judging by the dynamics of this bug and other forks Google has zero interest in this and will **never** implement this in the official repository. I would vote for closing this issue (or locking it) at all to not give any false hopes for everyone."
technical,"I think this thread is mostly meaningless for developers (too much noise - and I'll add some more ,-) but I think many comments are missing the point: **If you want to run Tensorflow with AMD cards OpenCL IS NOT what you are looking for** - please head over and install the ROCm stack. AFAIK **AMD's current strategy is based on ROCm instead of OpenCL for Tensorflow/pytorch**.  Generic OpenCL was too much maintenance/did not give enough performance benefits to be worthwhile for AMD. Therefore **this ticket is only interesting if you are running (e.g.) an ARM platform** which uses OpenCL only.  (Disclaimer: just an outsider, no real inside into Tensorflow development so maybe the information above completely wrong and misleading. Feel free to bash me if you know better.) Just a thought, what about llvm with the new GPU offload?  That would put a great level of abstraction between tensorflow and cuda specific code."
technical,"What about all of you reading just 10 posts above and noticing there already is a fork by /codeplaysoftware you can try ? (also my hats off to xiaomi for, once, contributing some serious kind of open source effort) Just so you are aware ROCm uses OpenCL, it is AMD's userspace OpenCL driver on Linux (that is why is why it doesn't support windows), so if you are not aware of how AMD's driver ecosystem on linux works, they have their kernel side drivers AMDGPU and AMDKFD(which is now getting merged into AMDGPU) then there is the userspace drivers RadeonSI(for OpenGL) RadV/AMDVLK(for Vulkan) and ROCm(for OpenCL)."
technical,"Why would you run an NVidia GPU with OpenCL when there are perfectly good CUDA libraries for it? Just to help test it for you!  No worries if you don't need a hand testing, just thought I'd offer. I haven't even tried that machine with cuda TBH, I've only tried it on MacOS where I can't use OpenCL through Docker at the moment."
technical,"Discuss the lack of intel gpu (or amd cpu) support here Just trying to get a sense of the state of this issue. Am I right to say that this repo:  ...built with ComputeCpp, is the current best option for building Tensorflow with general AMD GPU support? And if so, is there any benchmark evidence that this build provides a speedup over CPU?"
technical,"I've created a Google Group for a collaborative discussion about bringing deep learning to new places like OpenCL, Mac, iOS, CoreML, Vulkan, etc. If you'd like to help make these happen please join and post a note with your use case or what part of the problem you're working on. There already are people working really hard on efforts to bring TF to more platforms including MIOpen, Codeplay's work, TF-Coriander, and an internal project at my company (Vertex.AI). It would be great to get developers & users all in one place as these efforts are all closely related. justinrmiller I have a eGPU on Sierra (Titan Xp in a Sonnet enclosure) running Tensorflow 1.2.1 (CUDA 8, cuDNN 6) which wasnt too much trouble if you dont mind building from scratch. If you have any trouble let me know."
technical,"yeah exactly. I was going to use my new egpu enclosure to ditch my windows box for good. Keeping in mind that although Nvidia cards work in eGPU enclosures, Apple will only officially support the RX580 in their dev kit, so the need for OpenCL will not go away."
technical,"How much of the Vulkan SPIR-V work on drivers (that has already a good devices coverage) do you think will push modern Opencl versions? Khronos meeting is next week in Seoul with both OpenCL and Vulkan people, but discussions are not public. But that sounds like a good idea to have each world to improve the other, and at some point benefits to TensorFlow. :-)"
technical,"Any tensorflow 1.3 gpu/opencl support out there on macos? Latest news: I have successfully built TensorFlow 1.3.1 with OpenCL from the GitHub source. There are quite a few missing pieces in the documentation, and I haven't tried to run anything in the GPU yet, but it is at least working for non-OpenCL CPU. BTW, I do *not* have CPU OpenCL installed, just GPU OpenCL.  Does anyone have any test cases for TensorFlow with an OpenCL GPU? I'll have to build one for myself eventually but I was hoping for a quick check."
technical,"Thanks for your instructions! I try to compile your cuda-on-cl on arm platform. Following your cuda-on-cl's  guide: My ARM board info: arm64,  gcc 4.9 , clang and llvm 3.5, openCL 1.2   Do I have to use clang++-3.8 version? let me give it a try."
technical,"Hello. By no means an expert in ML / Tensorflow / or even OpenCL, but I'm an experienced Mac graphics dev who desperately wants faster performance of Tensorflow on systems with integrated and AMD GPU's using built in libraries and simple dependencies :) How can I help? Looking at the last compile failure on OS X in the travis log - it looks like running might a solve? It should re-link the /usr/include directory. I had this issue myself when updating Xcode beta to release and had issues compiling some C++ code."
technical,"Yes I love multidimensional arrays. Also in our domain of interest, there is the SG14 in the C++ committee that tries to have all the people interested in these issues to converge into **the** standard. Of course SYCL is in the discussions. :-) Mainly on cudnn for pooling and convolution. I think that if every vendor will produce an API with its own hardware for this operations with its own binary assembly will not be a so scalable approach. That is why I think some performance crucial API calls would be better to be standardized in some way."
technical,"I tested it, got segfault in MNIST example immediately. Don't know what I am doing wrong here. masahi  - make sure you have rocm 1.6.4 base installed."
technical,"bensander Anything else I need from the AMD stack? All I have now is the AMD proprietary opencl library that uses the open source ""amdgpu"" driver. masahi - if you install the rocm and rocm-libs (i.e. ""apt-get install rocm rocm-libs"") that should be all your need.  The rocm docs at the repot has full instructions including expected results."
technical,"right, I'm getting off topic. I stop here. Anyway, I couldn't get hiptensorflow working on my system. I will try later with clean Ubuntu install. masahi - just open an issue over there and we'll get you set up."
technical,"bensander how do I know if I am running rocm 1.6.4 correctly (and not 1.6.3) ? masahi just a guess : you should ask the question on a more related place for your issue, such as AMD or RoCM project rather than here..."
technical,"I think that it would be revolutionary ,) Maybe it would help if you join forces with these guys."
technical,"seems you have more chances to write CUDA compiler for any OpenCL device, rather than CUDA-OpenCL translator. Maybe OpenCL 2.0's SVM feature could solve the pointer issue? Since everyone besides Nvidia(AMD, Intel, ARM, Qualcomm) is starting to support OpenCL 2.0. Maybe it's a good solution?"
technical,OpenCL is more suitable for us too. Me too. I can't afford to buy a machine with NVIDIA graphic card.
technical,"Well... Soumith kind of coordinates torch development.  He works at Facebook AI Research.  So, since torch-android repo belongs to Soumith, I would say it's fairly close to official.  But it maybe is not part of core for some reason.  I guess you can ask the question as an issue in that repo, or in. Actually, since Soumith is kind of the main person that handles the requests in , I reckon you probably want to post your question there. meaning that no other Torch implementation (at least not official) was likely to run smoothly on Android, including cltorch  Note that cltorch is not an implementation of torch.  It's a plugin, that provides OpenCL.  You need both."
technical,"OK i don't know if i have enough time to do the build again and provide the compile errors until the weekend. But i added my existing documentation to my new github repo. mirh to clarify the ""acronyms of magically random technologies [...] making [you] mad"":  In the Khronos Group realm, OpenCL is the low-level non-single source API and SYCL is the high-level *single-source* C++ domain-specific embedded language (DSeL). SYCL is expected to be built on top of OpenCL, so by transitivity when you use SYCL, often you use OpenCL.  Since TensorFlow uses Eigen which uses a *single-source* C++ approach with *single-source* CUDA, when it was later ported to OpenCL, SYCL was chosen because it is the Khronos Group standard way to have *single-source* C++.  But if you think about CUDA, it is even more subtle.  Almost everybody uses the high-level  *single-source* version of CUDA which is actually named ""CUDA **Runtime** API"". This is somehow similar to SYCL. But there is actually a less known low-level *non single-source* version of CUDA which is called ""CUDA **Driver** API"", similar to OpenCL, and used for example by the ""CUDA **Runtime** API"" implementation itself.  Since it is a kind of FAQ, I clarified a little bit"
technical,"if you have any interest on this from SYCL/FPGA universe My apologies for not contributing more to this discussion recently, my plate has been more than full these past 2 weeks.  I'll be coordinating the OpenCL effort on the TensorFlow side. Our current thinking is: - TensorFlow relies on c++11 and has taken a ""single source"" approach, so SYCL seems like a great fit. - We don't have a lot of OpenCL experience in house, so we're collaborating closely with Codeplay to bridge this gap. In particular, Codeplay is currently leading the effort to add support for SYCL to the Eigen tensor library. - TensorFlow relies on the cuDNN library to compute convolutions on NVidia GPUs. If somebody is interested in contributing an OpenCL equivalent, we'd be happy to help.  In order to help structure the effort, I created a mailing list: tensorflow-openclgooglegroups.com."
technical,if you have any interest on this from SYCL/FPGA universe My company has been working on OpenCL deep learning for a while and we have some early results to show. We are focusing on Keras in the near term however we also have built (very) experimental TensorFlow support and will revisit that after our initial release. More details here including initial throughput numbers on AMD:
technical,"triSYCL is mainly developed at Xilinx now. Still adding more and more features. The Clang/LLVM-based outlining compiler is still in development to have a full single-source experience on a device. But the OpenCL compatibility mode, already implemented, has some value too, by simplifying the communications between host and kernels with the SYCL runtime doing the lazy transfers according to the dependencies expressed by the accessors. My mac is OpenCL compatible, so how can I run my tensorflow with openCL? I just found that opencl had been supported in tensorflow, when I configure the new codes."
technical,"This message was created automatically by mail delivery software.  A message that you sent could not be delivered to one or more of its recipients. This is a temporary error. The following address(es) deferred:  acmeidealgmail.com Domain biomassiv.es has exceeded the max emails per hour (111/100 (111%)) allowed.  Message will be reattempted later New here. Wanted to ask if there will be OpenCL support in tensorflow in the future, would that mean there will be support to run tensorflow on FPGA? Thank you"
technical,"This message was created automatically by mail delivery software.  A message that you sent could not be delivered to one or more of its recipients. This is a temporary error. The following address(es) deferred:  acmeidealgmail.com Domain biomassiv.es has exceeded the max emails per hour (111/100 (111%)) allowed.  Message will be reattempted later Next time a creative unit for naming ,) but probably was not a lack of creativity motivation but just an upstreaming effort of an internal tool that diverged to the one maintained in TF.."
technical,"A fast implementation of the convolution operation would be extremely helpful in TensorFlow. Looking forward to it. Nice catch, they got the chair"
technical,I will be interested in expanding Tensor Flow with OpenCL. As we have already released OpenCL caffe. Hopefully it can get integrated in light way? Is anyone interested in working together on this? Nice to see AMD here.
technical,"Would the relooper algorithm be helpful here? Nice to see you have some progress with your compiler, but I think it becomes off-topic for TensorFlow. You should start many smaller discussion threads on the GitHub page of your compiler project instead. It would be more focused and productive I guess."
technical,"They sponsored my original Caffe OpenCL project, and unfortunately didn't coordinate well, so AMD research and an independent guy at AMD also worked on OpenCL ports in parallel - the former AMD research team is now disfunct and most of them actually work for Tesla (self driving car project) now... so an unfortunate chain of events. I'm still in collaboration & contact with them though. Vega is going to be interesting :) Nice, lucky you! Wish there was such studies when I was at the Heig-vd would have continued to a MSc certainly.  Yeah... That's what I figured. So much work, so little human resources available in these fields."
technical,"It is in Google's interest to support OpenCL, by having a specific (company/brand/vendor)'s specific hardware as a dependency for your software, you enforce yourself to pay more for hardware, market competition lowers costs. Google has always been about commodity hardware since the very beginning which was and still crucial for Google's success (market dominance), having lower data center operating costs, enabled the revolutionary generous essentially free services offerings like Gmail (storage space) and Google Photos (storage space and auto-tagging). No, it isn't necessarily in Google's interest. They make their own hardware - something called a ""TensorBoard"", IIRC. They can bypass OpenCL and CUDA / CUDnn and make the board run raw TensorFlow code."
technical,"If anyone cares, the port of AMDGPU-Pro Driver 17.40 to Arch is ongoing and is very active on GitHub,  We really should close this issue, since, as mirh pointed out, TensorFlow uses SYCL, not OpenCL. Maybe we should open another one, ""TensorFlow on AMD cards""?? No, it's totally legit. You want tensorflow to run *eventually* on opencl devices, that's the aim. Legit and end. Saying it was actually using SYCL was only a technical nitpick I made, because all these acronyms of magically random technologies were making me mad. EDIT: I'd also like to thank all codeplay guys for their egregious work  If you want something all-that-specifically-crafted for amd, I'd recommend to check their hiptensorflow. ROCm-only though. And please, let's leave behind this argument."
technical,"Is there a test case I can use to verify that I'm using the GPU? I can monitor it with radeontop but I'd like something that uses TensorFlow itself. no, not only.. We currently test on AMD ( R9 380, R9 Nano, FirePro ). We know Intel GPU exposes some driver bugs, but there are fixes coming. And we have announced Renesas R-Car and expect more to follow.  I believe that Xilinx is upstreaming support for triSYCL - so FPG's (?) -  should know more about that"
technical,"YES THERE IS JUST LOOK AT THE LAST HANDFUL OF POSTS. no, there are no updates and will never be in any foreseeable future - probability of that is lower than of alien invasion and finding a way to travel back in time."
technical,"it's a blas implementation itself. It implements some of the symbols in clblas and cublas headers so no recompilation and code modification. is necessary. I could also implement some of the symbols for clblast.h, since it uses a different header. Some advantages of Isaac are: - Entirely dynamic, so that it can use either/both CUDA or OpenCL without recompilation. - Input-aware , it doesn't tune kernels for large square matrices. It should perform well on all shapes you can think of without retuning. - C++ API similar to numpy/arrayfire. Some fusion for combining elementwise operation with reductions Not really. AMD went back to 1.2 support on the AMDGPU-PRO drivers. Might be a while until full 2.0 support is widespread. Definitely not a short-term solution there."
technical,"Oh GitHub Now that you have hyper notified us can you give some feedback on stream executor strategy? ,)"
technical,"Oh GitHub Nvidia should soon support opencl 2.0 on both Linux and Windows, this is YUGE !"
technical,"You can't always use the same commit comments in different repository ,) Oh GitHub"
technical,"I did some benchmarks and it seems the iGPU is slower than the 4 available CPU threads except of the matmul bench.pybenchmark.  The initialization of a OpenCL Tensorflow run is also much slower than a CPU only OpenCL Tensorflow run. Something like 5 seconds for CPU vs 1-2 minutes for OpenCL.  Can anybody confirm such results? OK i did some more troubleshooting.  - i used the Tensorflow MNIST example, see the Validate a Tensorflow Setup - i used to check/watch the iGPU clock/load and ""top"" to check the CPU load - the intialization phase until Step 0 took about 6 minutes, the iGPU load was about 0%, the iGPU clock at 300 MHz (the minimal available clock) and the python process CPU usage was about 200% (= 2 threads) - starting with Step 0 the iGPU load was about 90%, the iGPU clock switched always from 654 MHz - 720 MHz - 800 MHz - 900 MHz (max available clock) and back, the python process CPU usage was about 100% (= 1 CPU thread)"
technical,"No, it's totally legit. You want tensorflow to run *eventually* on opencl devices, that's the aim. Legit and end. Saying it was actually using SYCL was only a technical nitpick I made, because all these acronyms of magically random technologies were making me mad. EDIT: I'd also like to thank all codeplay guys for their egregious work  If you want something all-that-specifically-crafted for amd, I'd recommend to check their hiptensorflow. ROCm-only though. And please, let's leave behind this argument. OK i don't know if i have enough time to do the build again and provide the compile errors until the weekend. But i added my existing documentation to my new github repo."
technical,interested. would love to contribute. Ok so actually seems that it is an effort of Codeplay with some kind of sync to Google internal. What are the role of AMD and Intel subscribers here?
technical,"hi everyone, my name is ricardo , i am a C++ programmer with many years in C++ experience, and little on Cuda, i will be glade in contribute to this effort. How can i contribute to this job? Ok, i have an Odroid Xu3 with a Mali-T628 MP6(OpenGL ES 3.0/2.0/1.1 and OpenCL 1.1 Full profile) running on OS: LUbuntu 1404 64 bits I will make a complete installation and post the result on this platform. About bugs, there is a list of bugs (something like Bugzilla?) or an spreadsheet with a list of bugs? Cheers!"
technical,"CL tensofrflow is already a mess on linux, don't expect anything any soon. If you want to accelerate stuff there, there's only plaidML. (and please, we are already at 500 comments.. let's try to only post if really, really necessary) OpenCL Caffe does work on Windows. Sure it's not TensorFlow in terms of features, but pretty solid for Software that has to be deployed everywhere."
technical,OpenCL support please! OpenCL is more suitable for us too.
technical,"Yes I know.. I was just curious if was discussed in some meetings. OpenCL is radically different from CUDA. I would however definitively see this ported to HIP instead. So +1 For all of you that suggested it.  HIP allows developers to convert CUDA code to portable C++. The same source code can be compiled to run on NVIDIA or AMD GPUs  Not many people know about HIP. **Side note:** I don't think we should fight / brag about Nvidia vs AMD. Those are respectable companies that make amazing hardware and software period. We should instead focus on delivering tensorflow to a larger user base. Targeting many languages through bindings is already a good starting point, but we also need to target as many hardware as possible. (Even if cloud solutions are amazing, they aren't always the answer)"
technical,"Remember also that Noveau guys are working independently on Opencl with SPIR-V. The status is a little bit outdated but there are fresh commits. Opencl isn't inherently slower than Cuda, it's Just nvidia virtually locking the market by crippling his opencl driver. But nvidia leading is finally coming to an end and even their ammoral anticompetitives practices will not save them. With the impressive Cuda autotranslator HIP. The upcoming vega apus and dgpus and ARM coming to Windows, Nvidia has no future, this is why the industry NEED to support opencl/syCL/HIP/HSA very soon and massively."
technical,"Keeping in mind that although Nvidia cards work in eGPU enclosures, Apple will only officially support the RX580 in their dev kit, so the need for OpenCL will not go away. OpenCL on Mac is 1.2 which means that there seems to be no active driver development. I think adding Metal support to TF is a painstaking process (enabling Eigen and stream executor) but doable."
technical,"Comparing only 2 numbers is no information - who cares if OpenCL on NVidia runs at half speed if it runs at 4x speed on other GPUs?  I think we'd need these benchmarks: 1. CUDA on NV GPUs (reference benchmarks) 1. tf-coriander on AMD, Nvidia and Intel GPUs 1. tensorflow-opencl on AMD, Nvidia and Intel GPUs 1. tensorflow on AMD, Nvidia and Intel GPUs  The reference benchmarks are easy to be found. We have some high end GPUs here, so only need a place to put the numbers in (with links to building-docs). OpenCL support It must become true.  cuda too limited,and nvidia dont want to share it. cuda only work for Nv gpus. that is dead end for TensorFlow, if another ""TensorFlow"" come out but more support than TensorFlow. if TensorFlow still only support cuda in windows. you have to realize TensorFlow not the only choose."
technical,"Tensorflow is supported by Google Brain, and Google has partnership with nVidia, I guess that we shall not expect from Tensorflow to support OpenCL Big OpenCL community effort is needed OpenCL support please!"
technical,"The issue as I understand is Bazel is trying to compile Protobuf and its not finding or downloading the sub directories. I did a pull with recursive-submodule still it has the same issues. And it has the same issue without --config = sycl. In fact I am facing the same issue when I do a git pull from tensorflow main project. I dont think this is linked with openCL, its some issues with the way I am doing the pull. When I manually download the project zip from your repo without git and compile, it compiles properly,  but then I get a segmentation fault. I have already raised this issue on your GIT project and we are talking there, I will give the updates related to the segmentation fault on that thread (no point duplicating things). Thanks for your response. Opensource triSYCL is arriving."
technical,"it suggests the streamexecutor has supported the CL version canned operation(like DNN, BLAS) out-of-box. Does it suggest google has already has the clDNN, clBLAS implementation ready for Tensorflow, but just not open source it yet? Otherwise OpenCL 2.0+ and SYCL 2.2 support SVM, if you want to keep the same software architecture. OpenCL 2.0+ is supported by AMD and Intel GPU for example. In the embedded world, it is often supported by side effect even with OpenCL 1.x, since the host and device memories are often the same for cost reasons."
technical,"raw TensorFlow code.  There is no such thing - it's not like unprocessed food. TPUs need their own DNN-library that handles the different types of calls.  It seems it's time for compressing the above discussion into one list again: - CodePlay is working on a SYCL backend - Hugh Perkins is working on tf-coriander - AMD is working on a HIP backend - PlaidML only supports CPUs at the moment. - Status of support for Intel GPUs is unclear.  So choose a project you like and start supporting them. Maybe each of the groups can give a status-update on their project?  Do understand that OpenCL has been transformed from a full language to a language-definition/hardware-specification that is represented in SPIRV (kernels), which then can be run on top of a platform like OpenCL-drivers and later also on Vulkan-drivers (platforms). So by supporting SYCL, you also support OpenCL. Perfect sum-up, but plaidml does run on gpus too. It's just that at the moment they are a backend for keras, not tensorflow. So it's kinda OT there."
technical,"Nvidia should soon support opencl 2.0 on both Linux and Windows, this is YUGE ! Performance wise it's likely to be slower than CUDA though."
technical,"I found a solution ! Perhaps benoitsteiner could drop by, since he is local. But before this event there is the full C++ F2F at the end of the month in Jacksonville, Florida. Unfortunately I will not be able to attend any of them."
technical,"Is this currently in a usable state on non-ubuntu linux systems?  The roadmap page simply links here. pfc Is what currently usable on non-Ubuntu Linux? TensorFlow using OpenCL in general? Or TensorFlow using OpenCL on an AMD GPU? I'll assume the latter, since it's the only reason you'd want to run TensorFlow using OpenCL. For an NVidia GPU you'd use the NVidia drivers / libraries and for CPU-only there's nothing to gain from OpenCL.  I had this working a few weeks ago on Arch Linux, using the proprietary ComputeCpp SYCL library and an AMD ""Bonaire"" (Sea Islands architecture) GPU. There's a new ComputeCpp release that I need to test but I'm guessing it will work.  It turns out that the AMDGPU Pro proprietary libraries you need to make this work don't run on Ubuntu 16.04.3. The upgrade from 16.04.2 brought in a newer Linux kernel and X Server, and AMD has yet to ship something that works on it. see for the details. I have been unable to make AMD OpenCL work on Ubuntu.  There's an experimental AMD version of TensorFlow that uses a compiler to translate CUDA code to OpenCL code but I haven't tested that either. In the absence of a supported driver it's useless."
technical,"This intel initiative PlaidML works reasonably well, worth checking it out. It runs on opencl OR metal on mac. It works with Macbook Pro AMD gpus, which is what I was looking for. Meanwhile, could you guys help vote for Pytorch support in PlaidML? PlaidML is certainly all nice and dandy (I, for one, somehow could get more performance on an nvidia gpu on opencl than with tf's cuda itself).. But it's a backend for keras? In complete replacement to tensorflow, which you know, it's the repo we are discussing this in? (for as much as I seem to understand latest tf versions can export models directly to keras? so there's that..)  Anyway, for the fourth damn time, if you want a recent solution **on opencl** and something still being actively developed (*and also* the thing with the actual chances to be merged here for real one day), there's just codeplay stack"
technical,"This intel initiative PlaidML works reasonably well, worth checking it out. It runs on opencl OR metal on mac. It works with Macbook Pro AMD gpus, which is what I was looking for. Meanwhile, could you guys help vote for Pytorch support in PlaidML? plaidML is super cool. Works on keras. Of course I had to transfer some tf code to pure keras in order to work on plaidML backend (for example tf.image.ssim) But result - my code works on VIDIA and AMD cards.  Also plaidML is heaven for researchers. It automatically generates gradient for any function you will write on ""Tile"" language and it will work on your GPU with 80% speed of tensorflow. So I cannot understand why ML researchers still using PyTorch ? Let's boost ML science with Intel's plaidML ?"
technical,"respectfully disagree. While cloud based multi-GPU solutions work great for internet services, I'm targeting professional video production pipelines where inference is being run on hours and hours of pro-res and uncompressed HD, 2K, 4K footage, which a) no production house is going to upload to a cloud, b) they don't want google or whomever to have their data, c) they have rooms full of multi GPU capable systems (Mac and Windows) locally which they would like to leverage, and d) while inference on a single image is fine on CPU, running entire movies for inference through multiple graphs 100% sees an increase in perf using something like MPS vs CPU. Because the community has declined to support / embrace standards and instead uses Nvidia only code, real world use cases get pigeon holed and its really a shame. This isn't an idle request from someone who is a hobbyist running tutorials - GPU inference is important as is supporting diverse GPU / CPU families for diverse workloads on real world hardware. I really hope Google takes this seriously, because it would be great to be able to stick with a single library like TF, which is awesome.Thank you for hearing me, I'm not trying to rant, but to provide an alternate point of view to the community. please don't get me wrong. I'd love to have it, and if you search in this thread I joined as a supporter, but as far as I've been following it I reached to the conclusion I wrote, not just because I think it so (I guess a MacBook would melt down if trained with thousands of videos :-D), but with the actual industry facts. Don't wait to have it solved in a short timeframe (IMHO, it won't be solved ever because Apple and Google hate each other since iPhone/Android issue)."
technical,"Yeah, sorry for hijacking this thread. I just didn't know where to ask the question. please raise an issue about it on the Caffe branch, flag it with Android and OpenCL. Then we can discuss this further. Thanks."
technical,"Can these help?  OpenCL random number generation(Thomas Wang's): Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the contributions welcome label. Thank you."
technical,"Hey - I am going through the test set above, this evening, on an AMD R9 390 8GB. So far I've already got one different result, logistic regression.py trains and doesn't return nan. So, good! It segfaults at the end, so I'll investigate whether the script or the cl code is at fault. Where should I push my results, where they can be most useful to you? Perhaps we could get a standard ""test script"" that generates a standard set of results that volunteers can push to you (or set up on local CIs or whatever)? py.test is as good a solution as any, it's just a pip away and that's part of the process for installing tensorflow anyway. I've discovered a few interesting things since starting my tests, and they may not be debuggable using Python output alone, however: * Different calls to the same script may crash early, or may ""hang"" (no output, no progress, no response to Ctrl-C, process needs to be pkill -9'd), or may crash late either at the validation part or after the script completes successfully. Crashes (segfaults) may take down Xorg. * The results vary for seemingly no reason: I may call a script and have it segfault, then call it again and it will work. * Hangs can occur in portions of code that were working literally moments ago, I've had one hang occur within or after a training batch, after several hundred batches just happened successfully.  So, it might be that there's unresolved stuff on the GPU side, and that a good segfault is needed to clear it out? I don't know much about the GPU model or OpenCL yet, so I can't contribute much here. But, it might be that GPU debugging output is needed to properly explore what's happening."
technical,"It seems like the XLA compiler will provide LLVM code generation from dataflow graphs. This means very easy access to spir-v and therefore Vulkan's compute API. With code generation sorted out, I can't imagine Google not providing Vulkan compatibility given the high number of unused integrated GPUs running on Android. Quickly: Right now I am running Inception v3 on a custom C++ / Object-C codebase and passing in decoded video frames to the network. I don't know enough about TF to know low level needs, but high level: load models, run session, expect stuff to work. I think that means 100% compatibility to be really honest. I know thats of no help in prioritizing. Basically the C++ Image Recognition using TF /InceptionV3 was my starting point. cuda-on-cl running on Mac: I've checked out the repo and can help debug and run builds on my systems and verify results on a variety of hardware:I have access to AMD Mac Pros with Dual D700s, Nvidia Mac Laptops and Desktop systems. Thanks for your detailed feedback. I'll monitor the repo, try to follow along, and try to help best I can."
technical,"No, it isn't necessarily in Google's interest. They make their own hardware - something called a ""TensorBoard"", IIRC. They can bypass OpenCL and CUDA / CUDnn and make the board run raw TensorFlow code. raw TensorFlow code.  There is no such thing - it's not like unprocessed food. TPUs need their own DNN-library that handles the different types of calls.  It seems it's time for compressing the above discussion into one list again: - CodePlay is working on a SYCL backend - Hugh Perkins is working on tf-coriander - AMD is working on a HIP backend - PlaidML only supports CPUs at the moment. - Status of support for Intel GPUs is unclear.  So choose a project you like and start supporting them. Maybe each of the groups can give a status-update on their project?  Do understand that OpenCL has been transformed from a full language to a language-definition/hardware-specification that is represented in SPIRV (kernels), which then can be run on top of a platform like OpenCL-drivers and later also on Vulkan-drivers (platforms). So by supporting SYCL, you also support OpenCL."
technical,Performance wise it's likely to be slower than CUDA though. Remember also that Noveau guys are working independently on Opencl with SPIR-V. The status is a little bit outdated but there are fresh commits.
technical,"it's about the ability to develop models and solutions locally on a mac respectfully disagree. While cloud based multi-GPU solutions work great for internet services, I'm targeting professional video production pipelines where inference is being run on hours and hours of pro-res and uncompressed HD, 2K, 4K footage, which a) no production house is going to upload to a cloud, b) they don't want google or whomever to have their data, c) they have rooms full of multi GPU capable systems (Mac and Windows) locally which they would like to leverage, and d) while inference on a single image is fine on CPU, running entire movies for inference through multiple graphs 100% sees an increase in perf using something like MPS vs CPU. Because the community has declined to support / embrace standards and instead uses Nvidia only code, real world use cases get pigeon holed and its really a shame. This isn't an idle request from someone who is a hobbyist running tutorials - GPU inference is important as is supporting diverse GPU / CPU families for diverse workloads on real world hardware. I really hope Google takes this seriously, because it would be great to be able to stick with a single library like TF, which is awesome.Thank you for hearing me, I'm not trying to rant, but to provide an alternate point of view to the community."
technical,"masahi just a guess : you should ask the question on a more related place for your issue, such as AMD or RoCM project rather than here... right, I'm getting off topic. I stop here. Anyway, I couldn't get hiptensorflow working on my system. I will try later with clean Ubuntu install."
technical,"masahi just a guess : you should ask the question on a more related place for your issue, such as AMD or RoCM project rather than here... Sad because now with an eGPU and n Nvidia 980 Ti inside we get driver working and Cuda working I didn't have the time for now to try Tensor Flow in my configuration yet.webdriver and Cuda toolkit installed on my computer and the Cuda samples work well"
technical,"Which new Google group?  Other than that, OpenCL and CUDA are too different programming approaches. CUDA works the way it is because one company has full control over everything, so it can embed binary blobs and who knows what in the final executable. This cannot be done with OpenCL, unless one goes down the SyCL path (I have my concerns...) and the SyCL compiler vendor has full control over all possible target architectures (unlikely or impossible in practice). Overall, my opinion is that a good OpenCL-enabled library needs more than just a few tweaks here and there. Probably not what you wanted to hear, but you asked for my opinion :-) See at the end for the google group. I asked your opinion cause you have a great experience with ViennaCL interfacing an algebra library with multiple backends (CPU, GPU, MIC). Tensorflow rely on Eigein library and its new tensor extension contributed by Google upstream (but only with CUDA backend). I think that they don't experienced much all the pitfall you have already encountered with ViennaCL in this years of development."
technical,"I don't have an OSX platform but I think that adapting a little bit the launching command could works. see my comment about mac above, if you point me to the build instructions I'll have a go"
technical,"please raise an issue about it on the Caffe branch, flag it with Android and OpenCL. Then we can discuss this further. Thanks. Seems that the next f2f SG14 meeting in March will be hosted by Google. Will be any tensorflow internal there?"
technical,"If it helps, a better version of isaac is out, and provides significant speed-ups over clBLAS and cuBLAS on Maxwell, Pascal and Fiji. Also provides faster (input-aware) kernels than Tensorflow for 1D and 2D reductions. seems you have more chances to write CUDA compiler for any OpenCL device, rather than CUDA-OpenCL translator."
technical,"If it helps, a better version of isaac is out, and provides significant speed-ups over clBLAS and cuBLAS on Maxwell, Pascal and Fiji. Also provides faster (input-aware) kernels than Tensorflow for 1D and 2D reductions. SInce I do not work for Google, I have no idea about XLA details..."
technical,"How Can I simply remove cuda implementation? because '#ifdef GOOGLE CUDA' is so complicated. It sometimes means CUDA, sometimes means GPU. Since this issue has made its way to the roadmap (see  Platforms ): do we roughly have an idea of when OpenCL support will hit TensorFlow? Like version 0.9 / 1.0? Q3/4 2016? Or is 2017 more realistic?"
technical,Axcell and StreamExecutor are separate projects.  There are no current plans to merge them.  I leave it up to the TensorFlow folks to say whether or not they plan to switch. So also StreamExecutor and StreamExecutor llvm was not the same projects?
technical,"So is it still needed to write two kernels, one for CUDA and one for Opencl? So don't you still have a counterpart internally? What about ""Canned operators""?"
technical,"StreamExecutor provides functionality equivalent to that of the CUDA runtime and some of the CUDA libraries (such as cublas or cudnn). However you still need to write your GPU kernels, which is what we use Eigen for. So is it still needed to write two kernels, one for CUDA and one for Opencl?"
technical,"Why is OpenCL better than HIP? I think OpenCL has failed to gain traction and supporting OpenCL at this point in time probably is counter productive and waist of resources for the whole comunity/industry. I'd rather see TensorFlow support HIP directly and let the compiler/tool/library to take care of the portability.  Isn't it better for software to support 1 language/programming model? Software has to support what it has to support to cover every use case. HIP is all bells and whistles (at least on the paper) if you have supported hardware. But there aren't just ""newer amd and nvidia cards"" to this world.  Now please, for the love of god, complain here for any problem with that. And here for everybody else interested to the continuation of this issue."
technical,"thank you very much for the clarification & sorry for my misinformation! This sounds very good & promising! I will definitely have a look at ComputeCpp then. I am really looking forward to OpenCL support for TensorFlow, because this offers a lot of new possiblities for robotics (which is the field where I am researching and using TensorFlow for deep learning applications). I will at least have a look at early releases and try to test / debug. We have some Intel Chips plus a number of ARM CPUs that are waiting for tests ,) sorry but isn't this completely off topic here? I don't see how this is relevant in OpenCL TF?"
technical,"We are working on bringing the OpenCL to the TensorFlow via the SYCL for OpenCL 1.2. Please have a look for ""todos"" and progress. Recently we released a compiler for SYCL called ComputeCpp Comunity Edition. People can try it out! As well, we are focusing on the Eigen library  - getting it to the stage required by TensorFlow's MNIST - there are a couple things remaining. As for constraints, the current ComputeCpp CE release has been tested for Intel (CPU, GPU) and AMD (CPU, GPU) as for the platforms we support Ubuntu 14.04 64bit and CentOS 64bit. ComptueCpp is downloadable for free and can be used in commercial and open source projects. Because we <3 open communities :) Sorry for discussing/asking this here in the thread, but I think it may be of interest to others as well: I understand that Codeplay is highly interested in the SYCL for OpenCL implementation and I already heard others being interested in this work of you, too. I read some post by a Movidius official for example. However, I would like to ask what Google's contribution to this really is? Since Movidius, besides AMD and others, are listed as Codeplay's partners I can understand that they encourage or even support SYCL for OpenCL, but as far as I am aware of it, Google is not your partner and has not contributed so far?!  Do not get me wrong, I really like your work, but wouldn't it be a good idea to consolidate your efforts, pool the resources and try to work together with Google? To me it looks like many different parties are interested in OpenCL for TensorFlow, but a huge potential is not used, because these parties do not develop together?!  I may be wrong and please apologize myself if this has been discussed sufficiently, but I am still unaware of any major attempts by Google (or other parties) to work together on this and, as a result, I am still unaware of how the community could help or support (like single individuals), either via direct contributions, testing or other things."
technical,"When i use the amd gpu branch with a jupyter notebook, there seems to be a left over thread. python still uses 100% of one cpu, even after the computation has finished. Restarting the kernel finishes the stray thread. Does anybody else experience this? Sorry I cannot help with Intel OpenCL because i have only AMD OpenCL hardware.  I don't use jupyter yet. I use a plain bash shell and a virtual Python 3 environment (see my Python 3 +  Tensorflow setup). But i cannot reproduce the issue. There is no CPU usage by a python process after the compute has been completed. Thank you for the information. Is it possible to speed up the initial compile time? e.g. using all available CPU threads instead of only 50%."
technical,As far as I know that there is still not a ComputeCpp for macOS. So that means OpenCL for macOS is not ready. Still can't make it working on Ubuntu 16.04 with AMD card and catalyst driver. Is there any howto?
technical,"I would like to contribute to this, too. StreamExecutor provides functionality equivalent to that of the CUDA runtime and some of the CUDA libraries (such as cublas or cudnn). However you still need to write your GPU kernels, which is what we use Eigen for."
technical,"thanks for the quick response So, does this mean that there is no support, or that correct operation is not guaranteed? Also, from what I read on this thread I see that the testings are mainly on AMD GPUs... is anyone training nets on Nvidia GPUs with this OpenCL port? Streamexecutor was renamed in LLVM parallel-libs and now is acxxel"
technical,"GCN 2nd gen (aka 1.1) if any, 2.0 doesn't exist. (should stoop to be so pedantic) SUCCESS!  The latest ""dev/amd gpu"" branch commits in  fork fixed my Tensorflow OpenCL compile issue. I assume it was the SysCL 1.2.1 related commits.  I successfully compiled a Tensorflow OpenCL version and can use it. See my Tensorflow Setup documents for details.  I also added a benchmarks page where you can find some benchmarks of my setup under different Tensorflow setups (non CPU optimized, CPU optimized, OpenCL) in the future.  The AMDGPU Pro driver version 17.50 is also working for me. I updated the related AMD OpenCL Setup document  Thank you to all contributors."
technical,Can you create an issue herEso that we can discuss there? Thanks! Sure - I'm heading to dinner but I'll file it when I get back
technical,"That's interesting to know, we did some work to help enable Beignet but the activity on this project seems to have gone a bit quiet.   Yes any GPU will probably not perform much better on a small problem, glad you are making some progress though!   ComputeCpp with TensorFlow is able to be used by any hardware that supports SPIR OpenCL intermediate instructions which includes Intel GPUs however as in the thread here, we had intentionally prevented it running because we didn't think the current drivers were working at the moment. You can remove that restriction since it sounds like some users have got it working with different Intel drivers. We are also working on enabling this for ARM and Renesas processors that have OpenCL drivers. sxpc722 That should work then. By the way, the new machine is Windows 10 and I am not planning to dual-boot it with Linux until I absolutely have to do so! I'm sick of chasing down driver and library bugs for vendors (looking at you, AMD). In fact, I may put a Windows partition on my workstation for the same AMD reason. ,-)"
technical,"Great news , let us know of any help you need.  I'll guess you are using your own implementation of SYCL - will that be available for developers/researchers? On what platforms? SYCL seems like the right way to go given the amount of template metaprogramming involved with Eigen. I'm an experienced c++ developer with OpenCL experience gained from developing my own neural nets and linear algebra library. I'd love to help with this effort and get started developing with SYCL."
technical,Have you tried the last sgemm version in the MALI SDK? Tensorflow is latee ! Ahah
technical,"Unfortunately the latest TensorFlow is using more advanced features than the current triSYCL can cope with, so you have to use ComputeCpp, currently the only fully compliant SYCL implementation... Tensorflow is supported by Google Brain, and Google has partnership with nVidia, I guess that we shall not expect from Tensorflow to support OpenCL Big OpenCL community effort is needed"
technical,"Unfortunately the latest TensorFlow is using more advanced features than the current triSYCL can cope with, so you have to use ComputeCpp, currently the only fully compliant SYCL implementation... Tensorflow port to AMD GPU:It works great for me.  My hardware setting: The key is mother board and CPU have to support PCIe v3  Its performance is similar to Nvidia 980Ti"
technical,"There is clFFT, clBLAS (alternatively ViennaCL). Random number generator is a bit more tricky (no curand), either use a CPU generator and transfer to GPU or use another existing kernel for RNG. The biggest pitfall will again be efficient convolution implementations (something like cuDNN). There is experience about such issues here: Tensorflow use tensor extension upstreamed to Eigen. So I think that an Opencl/Sycl support to Eigen is needed. See this thread"
technical,"Hmmm ... I haven't been following the thread but it looks like there's possibly testable OpenCL code now, at least on *CPU* OpenCL. I have an older AMD GPU (""Bonaire"") and I have OpenCL running on both the GPU and CPU, so I can test this. I might take a shot at it over the weekend, I really want OpenCL TensorFlow on my GPU. tf-coriander works"
technical,"I think it would be more appropriate to make an issue at plaidml repository instead of here, since this issue is about supporting OpenCL in tensorflow. Additionally by looking at the installation instructions there your pip install command may be incorrect. Thank you andrewrichards  fo your attention and your session speech. But for now for me(a graduate student), to build an app using Tensorflow on Android device and want GPU(Mali-T720) activated, what are required to abtain Mali driver with SPIP-V support and ComputeCpp runtime for Android with Arm CPU support and SPIR-V support. Since I've downloaded ComputeCpp(Ubuntu16.04 x64 with bin/ doc/ include/ lib/) on CodePlay homepage, yesterday I run:  incompatible, so I consider maybe I need ComputeCpp for Android with Arm CPU support and SPIR-V support, but I could not find any source code to build an Android ComputeCpp, there are only samples at github. And you've said ComputeCpp for Android is now not available, so is there any plan to support Android device or how can I get it if supported."
technical,"I think it would be more appropriate to make an issue at plaidml repository instead of here, since this issue is about supporting OpenCL in tensorflow. Additionally by looking at the installation instructions there your pip install command may be incorrect. Thank you for pointing the talk on multi-dimensional arrays. It is interesting and address the real issues but looks too ad-hoc to be ratified in C++ as is. Personally I use Boost.MultiArray and I am more confident in a polished version of Boost.MultiArray."
technical,"At my company StreamComputing we have various GPUs for build-testing and benchmarking, which we use for our customer-projects. I could hook your Github into our Jenkins to do a weekly run. Thank you for the answer, I will go back on the subject at work this week, with specific scripts. My use cases are around text/syntaxic matching analysis, using Gensim and Keras/tensorflow in my experiments."
technical,"Hi all, here at Codeplay we are looking into Eigen's tensor running on GPU using SYCL (a modern C++ layer on top of OpenCL). From what we have gathered so far, GPU tensor design is very closely coupled with CUDA and it will require interface changes for another programming model and particularly a SYCL and OpenCL 1.2 version. If anyone is interested in digging deeper / helping out, we are most certainly interested in contributing. Thanks, Luke Thank you for the feedback. I think that benoitsteiner worked at the tensor extension part of eigen."
technical,I have the  fork working (Arch Linux) . I'm waiting on some more AMDGPU-Pro work before I publish a blog post Thank you for the inputs
technical,"no, not only.. We currently test on AMD ( R9 380, R9 Nano, FirePro ). We know Intel GPU exposes some driver bugs, but there are fixes coming. And we have announced Renesas R-Car and expect more to follow.  I believe that Xilinx is upstreaming support for triSYCL - so FPG's (?) -  should know more about that Thank you I will try it on AMD GPU"
technical,"We at Google have been working closely with Luke and his Codeplay colleagues on this project for almost 12 months now. Codeplay's contribution to this effort has been tremendous, so we felt that we should let them take the lead when it comes down to communicating updates related to OpenCL. This is why you haven't heard much from us on the topic :)  Now that the ComputeCpp compiler is broadly available, we are planning to merge the work that has been done so far. But first we want to put together a comprehensive test infrastructure to make sure that we don't destabilize the exiting codebase.  We welcome all contributions to this effort, so feel free to contact me if you want to help. We're especially interested in high performance OpenCL kernels for matrix multiplication and convolutions. Several candidates have been suggested, but we haven't started looking into the pros and cons of each one or how to integrate them. thank you very much for the clarification & sorry for my misinformation! This sounds very good & promising! I will definitely have a look at ComputeCpp then. I am really looking forward to OpenCL support for TensorFlow, because this offers a lot of new possiblities for robotics (which is the field where I am researching and using TensorFlow for deep learning applications). I will at least have a look at early releases and try to test / debug. We have some Intel Chips plus a number of ARM CPUs that are waiting for tests ,)"
technical,"there is no clinfo instruction in my mac, what can I do for it? But I can compile the test code here for opencl with clang and result the following info: Device Intel(R) Core(TM) i5-5257U CPU  2.70GHz supports OpenCL 1.2. Device Intel(R) Iris(TM) Graphics 6100 supports OpenCL 1.2 Thank you, but I think I had tried the computecpp yesterday, and it's seem that the macbook system is still not supported with computecpp. So, maybe keep waiting for new updates is the only thing I can do (T.T).  BTW, my Iris 6100 is eight generation, which is good for OpenCL 1.2."
technical,"there is no clinfo instruction in my mac, what can I do for it? But I can compile the test code here for opencl with clang and result the following info: Device Intel(R) Core(TM) i5-5257U CPU  2.70GHz supports OpenCL 1.2. Device Intel(R) Iris(TM) Graphics 6100 supports OpenCL 1.2 Thanks for publishing your results. With regard to the initialisation time, the way OpenCL works is that the code is compiled before execution, so the startup time is the compilation process.  For Intel devices, there is a thread with mirh here that explains how to remove the restrictions around running devices. We have seen issues with Intel drivers which is why these device types are restricted, but we are hoping that updated drivers will be available soon for Intel devices that improve the support. In the meantime you can re-compile TensorFlow with the change to test your own Intel hardware. We are looking at removing the device restrictions in the codebase."
technical,"Have you already followed this thread? thanks for sharing. this thread looks very interesting. i tried to turn of the DVFS as suggested, but no significant performance was seen for sgemm in ViennaCL."
technical,"APUs do support OpenCL for both the CPU and GPU part. This should work pretty much out of the box when the OpenCL support is ready. Meanwhile, if you already have an APU and want to try out another ML framework, BVLC OpenCL Caffe already works. Thanks for the clarification.  I'm looking at cost effective hardware/software combinations to locally run Tensorflow and will definitely follow the OpenCL development progress."
technical,"APUs do support OpenCL for both the CPU and GPU part. This should work pretty much out of the box when the OpenCL support is ready. Meanwhile, if you already have an APU and want to try out another ML framework, BVLC OpenCL Caffe already works. Thanks for the correction, I based my comment off of the system requirements in the MIOpen documentation  but happy to update if there's a better link."
technical,"1. I'm not concerned with pre-GCN cards. Mine's a Sea Islands and I'm not planning on acquiring anything older. Then again, I'm not planning on acquiring another AMD GPU either. ,-) 2. I don't know whether ROCm will run on my workstation - there's no open source hardware tester that can give me a yes-or-no answer. I've opened an issue for that and received no response. 3. SPIR-V is a compiler target - I took a look at it and threw my hands up, not having a budget to hire a compiler writer.  So that leaves SYCL ... or throwing up my other two hands and doing everything with Keras, which has TensorFlow, Theano (which is getting frozen), CNTK or PlaidML back ends. From a purely engineering economics point of view, Keras / PlaidML is a big winner provided I get TensorBoard somehow. thanks for the good summary with all the links. I think you have not wasted your 3 hours... :-)"
technical,"AlphasCodes  I know the TF team prefer to keep the thread TF-only, we're happy to host the PlaidML-specific conversation over on the PlaidML project. That said, we do hope to eventually support TensorFlow itself as well as non-OpenCL platforms (e.g. Apple's Metal for iOS which currently exists in prototype form). Thanks for the information i edited my message accordingly.   The AMD A12-9800E iGPU should be GCN v3.  The main and only reason for me to do the benchmarks/tests is to find an answer on my question ""Stay with AMD or switch to Nvidia for my Deep Learning adventure"".  And the answer is. I really like the opensource approach of AMD but i will likely switch to Nvidia due to 2 factors. First the Deep Learning software stack (e.g. Tensorflow) is much more mature for Nvidia. Second the graphic card offers for my very specific needs (must fit into a Dan A4 SFX Case and must be very very silent / almost noiseless under full load for hours) is very limited or even non existing on AMD side."
technical,"was removed some months ago and Streamexecutor roadmap it is not clear. thanks for the quick response So, does this mean that there is no support, or that correct operation is not guaranteed? Also, from what I read on this thread I see that the testings are mainly on AMD GPUs... is anyone training nets on Nvidia GPUs with this OpenCL port?"
technical,"I stumbled on this thread while searching for tf/intel support.  I have an intel MacBook Pro, how can I help? I don't know c/c++, but I can follow build/compile/test instructions and pass back (pastebin) results Thanks for your instructions! I try to compile your cuda-on-cl on arm platform. Following your cuda-on-cl's  guide: My ARM board info: arm64,  gcc 4.9 , clang and llvm 3.5, openCL 1.2   Do I have to use clang++-3.8 version?"
technical,"We don't yet support TensorFlow running on OpenCL. It is a work in progress. Currently we are working on AMD GPUs. PowerVR support should follow. If you want to contribute to the development you should contact us (Codeplay) directly. If you want to run TensorFlow on PowerVR you should wait for a little more progress. thanks,  it looks  similar the OpenGL that hides the implementation of vendor specific.  I love to contribute the development, how to contact with you?"
technical,"We don't yet support TensorFlow running on OpenCL. It is a work in progress. Currently we are working on AMD GPUs. PowerVR support should follow. If you want to contribute to the development you should contact us (Codeplay) directly. If you want to run TensorFlow on PowerVR you should wait for a little more progress. Thanks, I was looking specifically"
technical,"ComputeCpp which is the SYCL implementation you are using with TensorFlow does not yet support Ubuntu 17.10. You would need to stick to Ubuntu 16.04 which is the current LTS. Instructions and pre-requisites are here.  As an aside, OpenCL support for TensorFlow does not mean just AMD device support. The SYCL integration is also enabling other OpenCL devices. As part of the work we are doing with TensorFlow, support for ARM and Intel GPUs will be available when the latest drivers from these companies are available. We are also working to enable support for Renesas accelerator processors too for the R-Car platform. Thanks! I have this working on Arch Linux (4.14.4 kernel) with the opencl-amd library from the Arch User Repository. The card is a Bonaire (GCN 2.0). I'll run the tests on that page to verify that it's doing what it should."
technical,"You can follow also the status of the PR. Thanks! I'm following this issue during last months. I'm not confident about the Apple OpenCL commitment, given they're stuck onto OpenCL 1.2 since 2013 (Apple is not providing SPIR 1.2 support yet)."
technical,"Tensorflow use tensor extension upstreamed to Eigen. So I think that an Opencl/Sycl support to Eigen is needed. See this thread Thanks. Yeah, I don't think there is a viable alternative for cuDNN for OpenCL right now."
technical,"What are the differences between the sycl development effort and XLA targetting SPIR-V other than PTX in the middle term vision? That would be a great discussion to be had at yesterday's TensorFlow Dev Summit. Do you ask about the resources available / type of programmers needed to contribute? If so, in OpenCL/SYCL approach C++ programmers / OpenCL C programmers can quickly be brought up to speed and be able to contribute. XLA approach requires a compiler / llvm experience. XLA is Google's internal project by extension they have more resouces associated with it. But, on the other hand their task is way bigger too.. Writing a compiler is not an easy task. Otherwise, if you are asking about the model: As I mentioned earlier we are seeing both efforts as complementary approches and both having different use cases. I still stand with that statement. For instance tatatodd in his presentation mentioned that some of the Ops will never have XLA as a target. I believe that we are possible to fill that gap. Other things to consider are new platforms. I will use mobile and embedded enviroment for this argument's sake as a new chips tend to pop out more frequently than GPUs ( the principle is the same ). If the semiconductor support SYCL / OpenCL you get TF support out of the box ( some performance tweaks might be required ). If the architecture is exotic and there is no LLVM backend for it yet XLA needs to add it ( that might not happen too often but still ). What happens more often is the architecture changes a bit and then new optimisation passes need to be added or existing one must be modified to gain the benefit. Tweaking kernel code is easier. I haven't looked very deep to XLA but I assume that XLA has to call into the CUDA API somehow to run the PTX kernel code, so would have to be ported to OpenCL or Vulkan to run SPIR-V kernels instead - that I assume would go through StreamExecutor - another framework to get familliar with - probably quite a big effort. In short, we are providing an unified / stable platform in very fragmented / diverted ecosystem that both semiconductor companies and developers can target. Where as XLA would have to commit to support. benoitsteiner or drpngx  might be able to give more inside knowledge of XLA as I am working with a lot of assumptions / conclusions based on conversations. Oh, as well I have created slack channel to eas up communication Edit: Slack link is no longer valid. Please ping me if you'd like to join."
technical,"justinrmiller I have a eGPU on Sierra (Titan Xp in a Sonnet enclosure) running Tensorflow 1.2.1 (CUDA 8, cuDNN 6) which wasnt too much trouble if you dont mind building from scratch. If you have any trouble let me know. That's awesome! Thanks for the info!"
technical,"it is a goal to make full use of various computing resources(e.g. cpu,gpu,DSP, any other coprocessor). In fact, it depends on the support of hardware vendors: **dirver**  and OS. As i know, you may can't enable both intel GPU and nvida GPU for video during the same time, due to the limitation from vedio driver. (You might be able to switch between them). However, opencl can use them at the same time. They are both ""devices"" in it. That's interesting to know, we did some work to help enable Beignet but the activity on this project seems to have gone a bit quiet.   Yes any GPU will probably not perform much better on a small problem, glad you are making some progress though!   ComputeCpp with TensorFlow is able to be used by any hardware that supports SPIR OpenCL intermediate instructions which includes Intel GPUs however as in the thread here, we had intentionally prevented it running because we didn't think the current drivers were working at the moment. You can remove that restriction since it sounds like some users have got it working with different Intel drivers. We are also working on enabling this for ARM and Renesas processors that have OpenCL drivers."
technical,"Yes, you are right, #22 is almost 8 months old and has over 100 posts! The information can get swamped!  TensorFlow uses the Eigen library for tensor computation (in the Tensor module). We have committed a partial implementation for OpenCL 1.2 using SYCL (branch Codeplay). The reason we used SYCL for this work is that this section of TensorFlow uses C++ expression trees, which is possible with SYCL for OpenCL, but not possible with OpenCL C directly. Other components of TensorFlow, such as convolutions or BLAS, could use OpenCL C directly.  Currently, I am working on integrating ComputeCpp (Codeplay's SYCL compiler) into the bazel build system. This should be ready soon ( follow this repo: ). After that is done, TensorFlow should be accelerated on systems that support OpenCL SPIR (such as AMD or Intel) with ComputeCpp. Further work will continue on accelerating more of TensorFlow, as well as supporting more OpenCL implementations and the triSYCL open-source SYCL. SYCL and OpenCL are multi-vendor, royalty-free open standards, so there are lots of platforms and devices that can be supported using this approach (not just AMD GPUs).  The ComputeCpp Community Edition compiler will be available for free later in 2016 (in beta form: full conformance will be released for free early 2017).  The work on accelerating the non-C++ parts of TensorFlow (e.g. BLAS and convolutions) could be done without SYCL and implemented separately. Different hardware vendors may have their own optimized libraries for these features which could aid acceleration. Or, we could use Eigen with C++ for these features. We believe the performance will improve steadily. To accelerate on a wide variety of devices, we need to manage the data more efficiently, which is why there is a ""managed tensor"" item of work, so that data movement can be more efficiently managed between host and multiple devices. It is hard to predict how the performance will vary over a wide range of devices, right now. Currently, very little is accelerated, but we are putting the infrastructure is in place to allow open-standard acceleration in TensorFlow. The basic operations should be here very soon. We are putting the basic infrastructure in place within the code to support open standards-based acceleration. We believe that with community support, an accelerated and usable version will be ready in much less than a year. ComputeCpp will be available publicly, for free, in 2016. The open-source triSYCL support should follow behind. Open-source OpenCL is already supported with pocl, Shamrock, Clover, Beignet."
technical,"Thank you I will try it on AMD GPU The build and test seem to be working on my Bonaire. I am using Python 3.6, though, and the instructions use Python 2.7. Do I need to use 2.7 or will 3.6 work?"
technical,"thanks,  it looks  similar the OpenGL that hides the implementation of vendor specific.  I love to contribute the development, how to contact with you? The easiest is if you click on ""register your interest"" on our page here: That will get you into our developer program and we can work together on this alephman"
technical,"A recent pull request was added at if somebody want to take a look. The Imagination(GPU)'s OpenCL SDK needs NDA to get accessible, we only have the shared library.  Is it possible to run tensorflow based on this libs?"
technical,"what is that gets included? As well do you get it when compiling without --config=sycl ? The issue as I understand is Bazel is trying to compile Protobuf and its not finding or downloading the sub directories. I did a pull with recursive-submodule still it has the same issues. And it has the same issue without --config = sycl. In fact I am facing the same issue when I do a git pull from tensorflow main project. I dont think this is linked with openCL, its some issues with the way I am doing the pull. When I manually download the project zip from your repo without git and compile, it compiles properly,  but then I get a segmentation fault. I have already raised this issue on your GIT project and we are talking there, I will give the updates related to the segmentation fault on that thread (no point duplicating things). Thanks for your response."
technical,"Does OpenCL Caffe run on Android? Sorry for asking this here but I didn't find anywhere else to ask it :) Would be great with a deep learning library that ran on Android devices  and  could use the GPU but it seems like there are no at the moment. (Correct me if I'm wrong!) The official (but experimental) OpenCL Caffe branch can be made to run on Android GPUs, however the performance at the moment is far from optimal."
technical,"Can we push long log on gist to let the thread to be still readable? The SYCL specs describe in section 5.8 (""Address-space deduction"") how an implementation needs to deal with different memory types. This is similar to previous work done for PlayStation 3 and described in this paper: Offload “ Automating Code Migration to Heterogeneous Multicore Systems or C++ on Accelerators: Supporting Single-Source SYCL and HSA Programming Models Using Clang  hope that helps."
technical,It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly. The Tensorflow AMD OpenCL performance is very slow according to my tests. So i did some basic tests with an other Deep Learning framework. You will find my setup and benchmarks on my GitHub page here.  Long story short. The other Deep Learning framework is about 10 times faster than Tensorflow AMD OpenCL currently.
technical,"Also N4355 in c++17 could enter in the game soon or later The tensorflow approach is to rely on a hardware abstraction (the tensor module) for the majority of the operations needed in by a typical neural network, while relying on specialized libraries (such as cudnn) for the few operations that are really critical performance wise. The hardware abstraction enables us to implement most TensorFlow operations once and have them run on an accelerator with more than good enough performance."
technical,"Thanks. Yeah, I don't think there is a viable alternative for cuDNN for OpenCL right now. The website  is created to support open source porting projects just like these! We're currently installing all necessary tools at the website and have space for repositories at  later on we're adding build-servers to test for several types of hardware and can provide our expertise in how to write code that runs at full speed on numerous hardware.  We're launching a porting initiative for GEGL next week, but we're happy to also support you."
technical,"please don't get me wrong. I'd love to have it, and if you search in this thread I joined as a supporter, but as far as I've been following it I reached to the conclusion I wrote, not just because I think it so (I guess a MacBook would melt down if trained with thousands of videos :-D), but with the actual industry facts. Don't wait to have it solved in a short timeframe (IMHO, it won't be solved ever because Apple and Google hate each other since iPhone/Android issue). There already was support for nvidia GPUs on Mac OS. It was just removed in 1.2. Note: As of version 1.2, TensorFlow no longer provides GPU support on Mac OS X. I've cancelled my order for an eGPU (Sonnet's) and will just dual boot Linux on my gaming rig, but this it's kind of bad to just stop supporting something that people were using. Was really hoping to do this on my mac with an eGPU (model training), but I guess that won't happen now:"
technical,Thank you for pointing the talk on multi-dimensional arrays. It is interesting and address the real issues but looks too ad-hoc to be ratified in C++ as is. Personally I use Boost.MultiArray and I am more confident in a polished version of Boost.MultiArray. There are also some papers at WG21. As you can see jfbastien at Google has  some activity at WG21 and also helped to host the SG14 f2f meeting at Google in March.
technical,"Well sadly, that's clearly not sufficient, here're some of the next build failures: there are few commits that you have to apply to get it compiling. I would suggest using tip of dev/amd gpu or if you don't want to change your current branch.. you can merge dev/amd gpu to it."
technical,"Mainly on cudnn for pooling and convolution. I think that if every vendor will produce an API with its own hardware for this operations with its own binary assembly will not be a so scalable approach. That is why I think some performance crucial API calls would be better to be standardized in some way. There are really interesting topics for Matrix/Tensor  in the new SG14 c++ specially in vector/SIMD calls agenda. But seems that nobody talked  of convolution, pooling, and others useful ""stabilized"" deep learning interfaces. Also seems to me that in this specific standardization subgroups there are people from Nvidia, Intel, Amd, CodePlay etc.. but not from Google also if it is in others groups."
technical,"I had to look at output before trying to use TF compiled with OpenCL support. In my case it showing now there is 2 choices for running TF on GPU: good working on limited (by vendor) number of devices, but proprietary CUDA bad working on limited (by computecpp developers) number of devices and also proprietary computecpp Still no OpenCL support. There in an OpenCL specific section  in the overall TensorFlow documentation. This will be published on the tensorflow.org site soon."
technical,"Haha! Life is very strange actually... Are you working for the HiP marketing team of AMD ? :-) Please look at the subject of this issue : ""OpenCL support"".  This means it is about the Khronos standard (and the other SYCL standard from the OpenCL Khronos working group appears at the end of the ""Overview"" section).  Of course there is a world outside of this issue, but it is... *outside*! :-)  Please try not to increase inconsiderately the entropy of the universe by posting some random posts on this already too lengthy discussion... :-) This comment applies to some other posters here, not only you, by the way. This is a GitHub issue to solve a *technical* problem: having TensorFlow running on devices supporting the OpenCL standard, not a FaceBook page about how people like or dislike tool A or B. :-) But please feel free to send some git commits related to this issue we can look at... There is a fork of TensorFlow supporting OpenCL And of course benoitsteiner 's work IMHO, it is ridiculous that mainstream TF still didn't merged their work."
technical,"Haha! Life is very strange actually... Are you working for the HiP marketing team of AMD ? :-) Please look at the subject of this issue : ""OpenCL support"".  This means it is about the Khronos standard (and the other SYCL standard from the OpenCL Khronos working group appears at the end of the ""Overview"" section).  Of course there is a world outside of this issue, but it is... *outside*! :-)  Please try not to increase inconsiderately the entropy of the universe by posting some random posts on this already too lengthy discussion... :-) This comment applies to some other posters here, not only you, by the way. This is a GitHub issue to solve a *technical* problem: having TensorFlow running on devices supporting the OpenCL standard, not a FaceBook page about how people like or dislike tool A or B. :-) But please feel free to send some git commits related to this issue we can look at... There is a TensorRT that supports Movidius Pi Hat. And that Movidius Pi Hat is Google's 45 AIY Vision Kit. Google links to Target to buy it.  This doesn't have any ties to CUDA or Nvidia? Says it uses an Intel chip. At its heart, maybe the chip is a FPGA? Anyone know anything more about it?"
technical,"Is there a list of CUDA dependency libraries that Tensorflow relying on?  This would help to see if we could have immediate OpenCL alternatives. There is clFFT, clBLAS (alternatively ViennaCL). Random number generator is a bit more tricky (no curand), either use a CPU generator and transfer to GPU or use another existing kernel for RNG. The biggest pitfall will again be efficient convolution implementations (something like cuDNN). There is experience about such issues here:"
technical,"My mac is OpenCL compatible, so how can I run my tensorflow with openCL? I just found that opencl had been supported in tensorflow, when I configure the new codes. there is no clinfo instruction in my mac, what can I do for it? But I can compile the test code here for opencl with clang and result the following info: Device Intel(R) Core(TM) i5-5257U CPU  2.70GHz supports OpenCL 1.2. Device Intel(R) Iris(TM) Graphics 6100 supports OpenCL 1.2"
technical,"the discussion is clearly valid, but not here... You are on GitHub here, on the track discussing the port of TensorFlow & Eigen using Khronos Group standards. This is not Twitter or your Facebook wall... :-) So please contribute with some commits on these projects ! :-) There's a new version of the setup guide for compiling TensorFlow with ComputeCpp, Codeplay's implementation of SYCL, so that OpenCL devices can be used. We would appreciate any feedback you can give us on it."
technical,"the discussion is clearly valid, but not here... You are on GitHub here, on the track discussing the port of TensorFlow & Eigen using Khronos Group standards. This is not Twitter or your Facebook wall... :-) So please contribute with some commits on these projects ! :-) There's also.  Here's the associated paper."
technical,"Why does tensorflow no longer support GPUs on OS X? I was planning on using Tensorflow with an eGPU setup I have on order. they claim they can't test it anymore on mac os, and therefore decided to stop the support. however, I'm having a hard time believing that. Going forward with the advert of egpus on high sierra and with new nvidia drivers, this will no longer be the case."
technical,"Can you elaborate on the ROCm/HIP stack for a layman like myself. I've been playing AMGPU-pro and AMDGPU with my Sea Islands cards so I'm sure I could post some useful results. They sponsored my original Caffe OpenCL project, and unfortunately didn't coordinate well, so AMD research and an independent guy at AMD also worked on OpenCL ports in parallel - the former AMD research team is now disfunct and most of them actually work for Tesla (self driving car project) now... so an unfortunate chain of events. I'm still in collaboration & contact with them though. Vega is going to be interesting :)"
technical,"AMD Radeon Vega Frontier Edition. We continue to aggressively improve our ROCm open software platform and machine learning libraries.  We're also supporting open machine intelligence frameworks like Caffe (released in April). Later this quarter we plan to offer support for Torch, and **Tensor Flow** is in the works. They've already released Caffe, would be very interested to hear others on this thread sharing their experiences with building/testing: I've started installing but hit a roadblock where anything requiring CL just freezes, even clinfo. Not sure if this is because of some software issue, or if my card (R9 390) simply isn't supported by ROCm."
technical,"Any idea how to inject this Eigen patch during bazel build ? Maybe we should bump somewhere Eigen tgz version to get the fixed one ? Thanks, Adam. This commit would be enough to get it built ?"
technical,"no, there are no updates and will never be in any foreseeable future - probability of that is lower than of alien invasion and finding a way to travel back in time. This intel initiative PlaidML works reasonably well, worth checking it out. It runs on opencl OR metal on mac. It works with Macbook Pro AMD gpus, which is what I was looking for. Meanwhile, could you guys help vote for Pytorch support in PlaidML?"
technical,"no, there are no updates and will never be in any foreseeable future - probability of that is lower than of alien invasion and finding a way to travel back in time. This message was created automatically by mail delivery software.  A message that you sent could not be delivered to one or more of its recipients. This is a temporary error. The following address(es) deferred:  acmeidealgmail.com Domain biomassiv.es has exceeded the max emails per hour (111/100 (111%)) allowed.  Message will be reattempted later"
technical,"very interesting topic. Hope it coming soon. This thread is very interesting. I've been trying to get caffe to work on android. The results seem to be surprising: caffe running with Mali gpu seems to be 2-3 slower than cpu, but about 4-5x more energy efficient. The test was run on Galaxy S6 (Mali T760, Peak Performance 200 GFlops).  Since GEMM is the core of convolution in caffe, I decided to profile its performance on Android. It seems that ViennaCL is not as efficient as some simple kernels. Now I am able to get GPU run as fast as CPU for large matrices (2k x 2k). This is still counter-intuitive, since normally we expect GPUs to be much faster. See: The kernel implementations can be found here: OpenCL kernels for GEMM.Any thoughts?"
technical,I also would like to contribute.  can you organize it? This was included in the Roadmap but also tagged as contribution so a direction/bootstrap could be really useful.
technical,"Tensorflow is latee ! Ahah This will have an impact on the strategy:  EDIT: ""StreamExecutor is currently used as the runtime for the vast majority of Google's internal GPGPU applications, and a snapshot of it is included in the open-source TensorFlow  project, where it serves as the GPGPU runtime."""
technical,"What you mean, ""not supported"" ? This. As for ubuntu, only 16.04.3 is said to be supported (at least officially then, considering even arch can get it to work after some script magic) EDIT: 'complete' AMDGPU-PRO driver requires kernel 4.9, that was likely the problem"
technical,"What you mean, ""not supported"" ? thumbs up and all that."
technical,"I think.. any hardware vedor has the urgency of consuming SPIR-V? I think that Graphic/Shaders pressure on Vulkan could help Opencl side.. to go back to the discussion on OpenCL 2 or not, at some point real things have to be delivered... Otherwise there is already nVidia GPU and CUDA with TensorFlow running... :-) But of course, a version of TensorFlow without SVM has some interest."
technical,"Intel beignet opencl 2.0 support is almost done! triSYCL is mainly developed at Xilinx now. Still adding more and more features. The Clang/LLVM-based outlining compiler is still in development to have a full single-source experience on a device. But the OpenCL compatibility mode, already implemented, has some value too, by simplifying the communications between host and kernels with the SYCL runtime doing the lazy transfers according to the dependencies expressed by the accessors."
technical,Intel beignet opencl 2.0 support is almost done! tscholak I wont post it here to keep this on OpenCL but ill summarise the steps here]
technical,Intel beignet opencl 2.0 support is almost done! Two new Kronos Group standards are released.
technical,"SInce I do not work for Google, I have no idea about XLA details... unfortunately all this is a work-in-progress..."
technical,"which version of tensorflow and trisycl are required for integration. I am having trouble building tensorflow (1.9) with latest trisycl release. Unfortunately the latest TensorFlow is using more advanced features than the current triSYCL can cope with, so you have to use ComputeCpp, currently the only fully compliant SYCL implementation..."
technical,"Wait, I'v been reading this thread/issue for 10 mins now. I got halfway through and I skipped through the rest. Are AMD GPUs supported yet? Using a finicky closed source thing that only works on one very old combination of Kernel/OS (codeplay): yes. Using an old version of tensorflow and without support for some nonlinearities yet (tf-coriander): yes. Really: not officially. Though AMD are porting to HIP, so I'd expect progress within 3 months or so. Other frameworks already work well due to their efforts."
technical,Yes probably it is no more so strictly confined here with all the details. Other than Eigen/sycl support Is there a plan for the cudnn calls? very interesting topic. Hope it coming soon.
technical,"Thanks for the correction, I based my comment off of the system requirements in the MIOpen documentation  but happy to update if there's a better link. Wait, I'v been reading this thread/issue for 10 mins now. I got halfway through and I skipped through the rest. Are AMD GPUs supported yet?"
technical,"Hi, is there any progress in the TF-OpenCL support for FPGAs? was removed some months ago and Streamexecutor roadmap it is not clear."
technical,"Come to my talk next week at Arm TechCon !   You will need Mali drivers with SPIR-V support, which is probably not easily available, yet. And you will need a ComputeCpp runtime for Android with Arm CPU support and SPIR-V support, which is also not available (yet). So, you will have to be just a  little  bit patient. We (Vertex.AI) have just open sourced PlaidML, our deep learning stack with support for running Keras on OpenCL. TensorFlow support is coming, help there would be welcome. And yes, Mac support is on the way (also Windows)."
technical,"Next time a creative unit for naming ,) but probably was not a lack of creativity motivation but just an upstreaming effort of an internal tool that diverged to the one maintained in TF.. we *did* change the name, precisely when we realized that we did not think it made sense to move StreamExecutor into LLVM wholesale.  It's now called ""Acxxel"". I'm sorry for the confusion and I appreciate the feedback..  It was a learning process for sure."
technical,"See at the end for the google group. I asked your opinion cause you have a great experience with ViennaCL interfacing an algebra library with multiple backends (CPU, GPU, MIC). Tensorflow rely on Eigein library and its new tensor extension contributed by Google upstream (but only with CUDA backend). I think that they don't experienced much all the pitfall you have already encountered with ViennaCL in this years of development. We are currently at the face-to-face meeting in Seattle this week but of course I cannot say whether we are talking about DNN libraries or not... :-)"
technical,"Thanks, I was looking specifically we are discussing here about porting TensorFlow to OpenCL, a standard from Khronos Group, and actually more OpenCL SYCL, the post-modern C++ single source standard from Khronos Group. ROCm looks like yet-another-non-standard-solution-from-some-vendor. If you are interested in proprietary solutions, there is already a CUDA version of TensorFlow which looks working well. :-)"
technical,Do the OpenCL Caffe branch and the OpenCL Caffe implementation by AMD have anything more in common besides the name? Have you compared the two or do you know if there is any difference in performance? You write that the OpenCL branch is far from optimal performance. What does that mean and what would be necessary in order to improve it? It would be interesting to try it on Android. We are going off topic
technical,"SYCL seems like the right way to go given the amount of template metaprogramming involved with Eigen. I'm an experienced c++ developer with OpenCL experience gained from developing my own neural nets and linear algebra library. I'd love to help with this effort and get started developing with SYCL. We are in contact with benoitsteiner, but we will discuss our proposal with the upstream maintainers before we invest too much effort. We are developing our implementation of SYCL, ComputeCpp . For more information, can you please contact me off-list via the email address on my profile?"
technical,"Impossible for me to build TensorFlow for Sycl/OpenCL !   Training models on my CPU takes ages, I really need OpenCL/GPU acceleration ... We are in process of upsreaming changes to Eigen that will fix issue you are seeing.  As well we are preparing to upstream performance improvements to Eigen - this is bit tricky and needs coordination with benoitsteiner to avoid stream of merge conflicts - but we are getting there.  For AMD users I would suggest trying out my fork: Set Up instructions for Ubuntu 16.04 can be found here:  All the changes will be upstream to tensorflow after mentioned earlier Eigen changes are in place.  Hope that helps."
technical,"Yes can be an issue, but I think parts such as im2col/col2im and other convolution implementations could also be plugged in as external APIs if it's really an issue with the GCLA. This may also be better for the original authors of such work. We are working on bringing the OpenCL to the TensorFlow via the SYCL for OpenCL 1.2. Please have a look for ""todos"" and progress. Recently we released a compiler for SYCL called ComputeCpp Comunity Edition. People can try it out! As well, we are focusing on the Eigen library  - getting it to the stage required by TensorFlow's MNIST - there are a couple things remaining. As for constraints, the current ComputeCpp CE release has been tested for Intel (CPU, GPU) and AMD (CPU, GPU) as for the platforms we support Ubuntu 14.04 64bit and CentOS 64bit. ComptueCpp is downloadable for free and can be used in commercial and open source projects. Because we <3 open communities :)"
technical,"Sorry for discussing/asking this here in the thread, but I think it may be of interest to others as well: I understand that Codeplay is highly interested in the SYCL for OpenCL implementation and I already heard others being interested in this work of you, too. I read some post by a Movidius official for example. However, I would like to ask what Google's contribution to this really is? Since Movidius, besides AMD and others, are listed as Codeplay's partners I can understand that they encourage or even support SYCL for OpenCL, but as far as I am aware of it, Google is not your partner and has not contributed so far?!  Do not get me wrong, I really like your work, but wouldn't it be a good idea to consolidate your efforts, pool the resources and try to work together with Google? To me it looks like many different parties are interested in OpenCL for TensorFlow, but a huge potential is not used, because these parties do not develop together?!  I may be wrong and please apologize myself if this has been discussed sufficiently, but I am still unaware of any major attempts by Google (or other parties) to work together on this and, as a result, I am still unaware of how the community could help or support (like single individuals), either via direct contributions, testing or other things. We at Google have been working closely with Luke and his Codeplay colleagues on this project for almost 12 months now. Codeplay's contribution to this effort has been tremendous, so we felt that we should let them take the lead when it comes down to communicating updates related to OpenCL. This is why you haven't heard much from us on the topic :)  Now that the ComputeCpp compiler is broadly available, we are planning to merge the work that has been done so far. But first we want to put together a comprehensive test infrastructure to make sure that we don't destabilize the exiting codebase.  We welcome all contributions to this effort, so feel free to contact me if you want to help. We're especially interested in high performance OpenCL kernels for matrix multiplication and convolutions. Several candidates have been suggested, but we haven't started looking into the pros and cons of each one or how to integrate them."
technical,"Is there any interest in this by Intel? I really hope that we don't fragment OPENCL here like in Caffe where we have an AMD fork, Intel unmerged PRs, another semi-unofficial AMD PR, and a long staging user PR (plus two old abandoned Opencl efforts). If somebody is interested in the history can take a look at comments. We do have interest in this. Thanks for letting me know. If there is a proposal for Eigen's OpenCL/SYCL implementation, we will see what we can do from Intel side."
technical,You don't need vendor specific header files to build any OpenCL program. Try cl.hpp from any other SDK. For example - I have at least 3 OpenCL platform and all it works with one shared We don't yet support TensorFlow running on OpenCL. It is a work in progress. Currently we are working on AMD GPUs. PowerVR support should follow. If you want to contribute to the development you should contact us (Codeplay) directly. If you want to run TensorFlow on PowerVR you should wait for a little more progress.
technical,"OpenCL is radically different from CUDA. I would however definitively see this ported to HIP instead. So +1 For all of you that suggested it.  HIP allows developers to convert CUDA code to portable C++. The same source code can be compiled to run on NVIDIA or AMD GPUs  Not many people know about HIP. **Side note:** I don't think we should fight / brag about Nvidia vs AMD. Those are respectable companies that make amazing hardware and software period. We should instead focus on delivering tensorflow to a larger user base. Targeting many languages through bindings is already a good starting point, but we also need to target as many hardware as possible. (Even if cloud solutions are amazing, they aren't always the answer) We have experience with HIP, here at Stream. Let me take a look.  Agree on the ""my company is better"" arguing. I would like to know which GPUs TensorFlow should be targeting. It has to be pragmatic and useful. For instance Intel GPUs or embedded GPUs (Qualcomm, ARM, Imagination), RaspBerry Pi - yes or no?"
technical,do you have any idea what the success rate is for getting this working on untested AMD gpus?  I'm specifically interested if it has been tested for AMD Radeon Pro 460  .  I would be happy to spend a few hours  getting ubuntu running on my Macbook laptop if there is any hope with an untested GPU we have not tested this but you could give it a try. You will need to use some older AMD drivers with Ubuntu that support the SPIR extension. I haven't yet been able to figure out what drivers those are yet.
technical,"yes, you should be able to cherry-pick that Well sadly, that's clearly not sufficient, here're some of the next build failures:"
technical,"For some reason I imagined that torch-android was an official Torch implementation for Android, meaning that no other Torch implementation (at least not official) was likely to run smoothly on Android, including cltorch. I don't know why I thought that, it of course doesn't make any sense. Well... Soumith kind of coordinates torch development.  He works at Facebook AI Research.  So, since torch-android repo belongs to Soumith, I would say it's fairly close to official.  But it maybe is not part of core for some reason.  I guess you can ask the question as an issue in that repo, or in. Actually, since Soumith is kind of the main person that handles the requests in , I reckon you probably want to post your question there."
technical,"Just a thought, what about llvm with the new GPU offload?  That would put a great level of abstraction between tensorflow and cuda specific code. What about all of you reading just 10 posts above and noticing there already is a fork by /codeplaysoftware you can try ? (also my hats off to xiaomi for, once, contributing some serious kind of open source effort)"
technical,"from which planet you are came from? How you can compare tf-CPU and AMD GPU ? AMD GPU on plaidML x30 faster than tf-CPU   1. It runs pitifully slow on AMD's OpenCL implementations compared to Tensorflow's CUDA backend so there goes at least half the reason to use it  in my deepfakes tests OpenCL slower only by 20%, but in some mini networks OpenCL is 20% FASTER.  My project DeepFaceLab has many users that have been waiting for the support of AMD. How many people were delighted when deepfakes can finally be trained on AMD cards. Also plaidML is the only backend for keras that supports AMD/IntelHD out of the box. If a new AMD backend for keras appears, of course my project will switch to it. PyTorch has no future.  What to maintain in plaidML ? Ops are auto differentiable, there is nothing to maintain. Tile programming language in which only someone like a pure maths professor would concoct  Machine learning is invented by professors of mathematics, isn't it? What about ARM or Broadcom ? The former probably has subpar OpenCL implementation and the latter doesn't even officially provide OpenCL drivers! It's not Google's responsibility to create and maintain a competent compute stack for hardware vendors ...  You realize that training neural nets with embedding layers on PlaidML is painful, right ? PlaidML also has a bunch of other limitations as well such as not being all that well suited for DenseNets or the fact that it's computation graphs are static and does PlaidML even work well with RNNs ?  As for your project, don't worry about it. You'll move on to something better like Tensorflow since AMD will soon offer a native GPU backend for it once MIOpen gets upstreamed which is their GPU accelerated library of primitives for deep neural networks similar to their competitor's cuDNN library both of which will leave PlaidML in the dust in terms of performance. Who cares about Intel iGPUs anyway ? If Intel is truly committed to delivering high performance deep learning on their future discrete graphics hardware then they'll offer a single source option just like the others (AMD/HIP and Nvidia/CUDA) did before them ...   PyTorch has no future. Envy much ? PyTorch is 10x more popular than PlaidML, newest techniques in DL are implemented easily on PyTorch, tons of different contributors and is actively developed by Facebook all the while Intel hasn't contributed to PlaidML in nearly a month ?   What to maintain in plaidML ? Ops are auto differentiable, there is nothing to maintain. So I take it from you that PlaidML shouldn't receive any new fixes or new features in the future going forward ? If you don't see the value in improving code then there's no point in convincing you to acknowledge PlaidML's glaring flaws ...   Machine learning is invented by professors of mathematics, isn't it? Doesn't mean we have to take up whatever programming language they make up especially in the case of Tile where elegance is clearly favoured over readability. It's no wonder why so many potential contributors are scared away from contributing ..."
technical,"OpenCL Caffe does work on Windows. Sure it's not TensorFlow in terms of features, but pretty solid for Software that has to be deployed everywhere. What about replacing the openCL port with the HIP port backed by AMD ?"
technical,"Opencl isn't inherently slower than Cuda, it's Just nvidia virtually locking the market by crippling his opencl driver. But nvidia leading is finally coming to an end and even their ammoral anticompetitives practices will not save them. With the impressive Cuda autotranslator HIP. The upcoming vega apus and dgpus and ARM coming to Windows, Nvidia has no future, this is why the industry NEED to support opencl/syCL/HIP/HSA very soon and massively. What about slide 40?"
technical,"Ok, i have an Odroid Xu3 with a Mali-T628 MP6(OpenGL ES 3.0/2.0/1.1 and OpenCL 1.1 Full profile) running on OS: LUbuntu 1404 64 bits I will make a complete installation and post the result on this platform. About bugs, there is a list of bugs (something like Bugzilla?) or an spreadsheet with a list of bugs? Cheers! What about using HIP ? ""Your wish is being granted, Eigen is being ported over AMD GPU via HIP. The second part of your request is can we bring standardized tool supporting FLOAT16 that ships with all our GFX8 GPU's, wish granted."" Our development branch of AMDGPU compiler now support's both Float16 and Int16 native instruction, instead of emulating FP16/Int16 with up convert & down convert instructions to convert from FP16/Int16 to Float and back.  This is f16 tests on Fiji hardware successfully executing a matrix multiplication with half types with conversion and with Native instructions.""  Also, not related but you should use syCL/openCL 2.0 instead of 1.2, because nvidia is already supported via CUDA. And openCL 2.0 is supported on both AMD and Intel Windows drivers. Also AMD has said that they will soon opensource un openCL 2.0 driver for Linux (which could be used by Intel, opensource magic) (and Intel already has a Linux openCL 2.0 implementation which Just need maturation.) if you ask Intel and AMD,  maybe they could speed up the work, because tensorflow is important for their economic interests. And they already have said in this comment section that they wanted to help. Also all the major ARM makers support openCL 2.0. This could open a lot of opportunitys for Android (which is in the economic interest of Google) , raspberry like, smart TVs, etc  And in mid term we could eventually develop an opencl 1.2 fallback layer for non supported hardware. And the implementation should use also openVX (which is now supoorted by all major hardware makers, and AMD has an opensource implementation) and with  And the all with Spir-V (which can be use simultaneously by Vulkan and openGL). You could say that I'm making a duplicate of what was already said, but synthetizing is important. And finally, could tensorflow use HSA ? HSA would be awesome on Android."
technical,"Ok, i have an Odroid Xu3 with a Mali-T628 MP6(OpenGL ES 3.0/2.0/1.1 and OpenCL 1.1 Full profile) running on OS: LUbuntu 1404 64 bits I will make a complete installation and post the result on this platform. About bugs, there is a list of bugs (something like Bugzilla?) or an spreadsheet with a list of bugs? Cheers! What are the differences between the sycl development effort and XLA targetting SPIR-V other than PTX in the middle term vision?"
technical,What are the differences between the sycl development effort and XLA targetting SPIR-V other than PTX in the middle term vision? What are the differences between the sycl development effort and XLA targetting SPIR-V other than PTX in the middle term vision?
technical,"I didn't say it was an easy fix. But, OpenCL isn't the problem, there. After all, CUDA is entirely proprietary, much worse than even the OpenCL option TensorFlow chose. That all said, there are options for a CL-or-cuda system if you're starting from scratch, including portable middleware runtimes or arrayfire, etc. Tensorflow is too tied to CUDA though. I find it frustrating that people are willing to write kernels in CUDA but balk at doing it for CL, even though it would reach more users and seriously grow the market ecosystem. There are direct and indirect benefits to an open platform for everyone, possibly leading to big cost savings for everyone in the long run. If SYSCL is how that eventually happens, great: so why aren't some big names putting money on an open SYSCL distribution instead of buying into fringey proprietary options, which kind of defeat the purpose of an open standard?. What I want to ask in this context is this: So some deep learning frameworks like Tensorflow are somewhat tepidly exploring the use of opencl as an alternative to CUDA. Of course CUDA is just the ""language"" that cuDNN was developped on, and that's (if my understanding is correct) is what most deep learning languages are actually using. In this context, I am not sure what the opencl version of cuDNN is. Also AMD has been talking about open-source alternatives to CUDA which they are continuously developing and are calling rocM. They are also talking about miOpen to be the cuDNN equivalent (handcrafted assembler libraries for common deep learning functions), which however has not been released yet. The AMD approach is somewhat more holistic: We are not just exporting heavy-compute to the GPU. In this context, I am genuinely confused. How do opencl efforts like the ones listed above fit together? For NVIDIA GPUs, it's easy....there is CUDA, and there is cuDNN written in CUDA. For non-NVIDIA/or in this case AMD, it seems so much less clear. When is HIP preferred? When is using the HCC preferred? When is using opencl preferred? Any insights would truly be appreciated!"
technical,I joined the discussion Google Group and posted a request for documentation. I'm going to wait to see if anyone responds before I put more effort into this. What instructions are you following? Have you run clinfo? Have you run computecpp info? Does that indicate that your OpenCL drivers are installed as expected? The instructions for Ubuntu 14.04 are here and if you are using 16.04 there are some experimental instructions here.
technical,"I tried following the steps listed on the link. I get the following error: Actually I am getting the same error if I try to compile tensorflow from source. I have compiled it earlier, not sure what has changed though. what is that gets included? As well do you get it when compiling without --config=sycl ?"
technical,There in an OpenCL specific section  in the overall TensorFlow documentation. This will be published on the tensorflow.org site soon. What is the current state on opencl convolutions support? Are you planning on leveraging the exiting kernels directly? What about matrix multiplications?  Any ETA?
technical,Maybe it would help if you join forces with these guys. what is the open source compiler you propose to use then? It is difficult to find an open source OpenCL-compliant solution to address a lot of devices in the wild... We need to bootstrap a solution at some point somehow...
technical,"yes, triSYCL moved from AMD to Xilinx  but you are right, the version on my GitHub workspace works too.  We have not tried triSYCL on TensorFlow yet. There is already a big build config work to do just to try... What is the triSYCL status?"
technical,"I can't even get the ""supported"" AMD drivers to work on my ""supported"" Ubuntu 16.04 LTS install. Planned obsolescence? what is your AMD GPU ?  If you have dual GPUs, disable the unsupported one from BIOS."
technical,"I don't have anything running on Ubuntu - all my working things are on Arch. I do have the machine dual-booted with Ubuntu 16.04.3 but the AMD proprietary libraries don't work there yet. As far as I know they're not supported on 17.10, but if you've got the OpenCL piece working on 17.10 I might add a third boot - I have plenty of disk space. ,-)  What kind of errors are you getting? If they're build errors, you might have a Bazel incompatibility. Bazel is constatnly moving forward like TensorFlow is and sometimes one will get ahead of the other. What you mean, ""not supported"" ?"
technical,Has anyone in Google spoken to Apple or AMD to ease this? I guess AMD people is so lost they don't even know the problem is there and they are still wondering why Nvidia has such a huge market share. I guess too Apple AI team would be more than happy to help in here... if OpenCL wasn't an abandonware from their side since 2013 What's the latest on this?
technical,"yes, this issue has been around on for a while now. The effort is massive and a lot of people are trying to ""get it working"" as  mentioned. A bit of an update from our side. You should be able to use ComputeCpp 0.3.0 on the AMDGPU-pro driver stack for Ubuntu 16.04 the instructions can be found here. As well, we are focusing now on performance improvements for different models - there is a lot to do but we are getting there. What's your approach to benchmarking? We time the models included with Keras and normalize against  because that is a common and well optimized configuration. Our methodology is similar to Max Woolf, it's not much code but we'd be happy to share it. We have some throughput numbers on our web site, our code is very slightly faster than TF 1.2 on Xception inference and it would be interesting to compare more approaches side-by-side."
technical,"If you read the issue that was linked twice, you'd see there's nothing about amd gpu. Intel's is currently excluded, but it has nothing to do with wanting to force users, and there is a temporary workaround - discuss there if really any. When i use the amd gpu branch with a jupyter notebook, there seems to be a left over thread. python still uses 100% of one cpu, even after the computation has finished. Restarting the kernel finishes the stray thread. Does anybody else experience this?"
technical,"benoitsteiner Thank you for the update. It would be wonderful if all involved partners in KhronosGroup (Google, Nvidia, Amd, Intel, Codeplay, Xilinx etc.) will promote a cudnn like API in a standardized way. A sort of Khronos openvx computer vision standardization effort but for deep learning. Which new Google group?  Other than that, OpenCL and CUDA are too different programming approaches. CUDA works the way it is because one company has full control over everything, so it can embed binary blobs and who knows what in the final executable. This cannot be done with OpenCL, unless one goes down the SyCL path (I have my concerns...) and the SyCL compiler vendor has full control over all possible target architectures (unlikely or impossible in practice). Overall, my opinion is that a good OpenCL-enabled library needs more than just a few tweaks here and there. Probably not what you wanted to hear, but you asked for my opinion :-)"
technical,"For deep learning inference on mobile devices with GPU/OpenCL support, you can checkout MACE, which is optimized for Adreno, Mali and PowerVR GPUs. Here are some benchmark results. which version of tensorflow and trisycl are required for integration. I am having trouble building tensorflow (1.9) with latest trisycl release."
technical,"For deep learning inference on mobile devices with GPU/OpenCL support, you can checkout MACE, which is optimized for Adreno, Mali and PowerVR GPUs. Here are some benchmark results. Why does tensorflow no longer support GPUs on OS X? I was planning on using Tensorflow with an eGPU setup I have on order."
technical,"OpenCL support It must become true.  cuda too limited,and nvidia dont want to share it. cuda only work for Nv gpus. that is dead end for TensorFlow, if another ""TensorFlow"" come out but more support than TensorFlow. if TensorFlow still only support cuda in windows. you have to realize TensorFlow not the only choose. Why is OpenCL better than HIP? I think OpenCL has failed to gain traction and supporting OpenCL at this point in time probably is counter productive and waist of resources for the whole comunity/industry. I'd rather see TensorFlow support HIP directly and let the compiler/tool/library to take care of the portability.  Isn't it better for software to support 1 language/programming model?"
technical,Hope people working on it manage to overcome the CUDNN alternative problem by the time tensorflow gets close to 1.0 why is this issues closed ?  I don't think your commit fixes this.
technical,Hope people working on it manage to overcome the CUDNN alternative problem by the time tensorflow gets close to 1.0 Why not implement a CL version first while waiting for the SYCL port to be ready? I assume there are quite a few folks here willing to help out. One year just sounds too long.
technical,"Hmm, I don't (and wouldn't) fun Ubuntu anywhere but I do have a CentOS 7 w/ repos and a GTX1080TI in it, running kernel 4.14.x and the latest Nvidia beta driver, so I could help test it out on there at some point today if it helps? Why would you run an NVidia GPU with OpenCL when there are perfectly good CUDA libraries for it?"
technical,"OK, fine, HIP-the-API is a subset of CUDA-the-API, but unless NVidia is craven enough to start  channeling Oracle I doubt that will be an issue. I was referring to the runtime/compilers for HIP, of which I think AMDs are open.  *edit*: Sorry if the above came out sounding rude, just trying to clarify my position above! Will Vulkan and Opencl be fused?"
technical,"OK, fine, HIP-the-API is a subset of CUDA-the-API, but unless NVidia is craven enough to start  channeling Oracle I doubt that will be an issue. I was referring to the runtime/compilers for HIP, of which I think AMDs are open.  *edit*: Sorry if the above came out sounding rude, just trying to clarify my position above! Will your fork support AMD GPUs on macOS as well?"
technical,Nice to see AMD here. would be great.
technical,I don't know if HIP would be useful or not. It is only supported on some AMD cards so that we need an OpenCL implementation anyway if we want to support all devices. It might still be worth it if the HIP implementation is notably faster. This might be the case but I haven't seen many benchmarks (HIP vs. OpenCL) yet. Another reason might be MLOpen (which is written in HC) as an replacement for cudnn but again I have no idea how fast that is or which features it supports.  TensorFlow would not use HSA directly because it is quite low-level. But HC (and HIP) is implemented on top of it and you can also implement OpenCL on top of if (pocl does that). Would the relooper algorithm be helpful here?
technical,"Yeah, there is test app in issue that I reported. Could you please let me know if it works in your case ? Yeah - it works (doesn't crash) but there's no indication whether or not it found OpenCL. I think the best course of action here is to file a separate issue to request documentation, since it's clear (when you run ./configure building from source) that there is *code* for OpenCL. That's how I found it, anyhow."
technical,"they claim they can't test it anymore on mac os, and therefore decided to stop the support. however, I'm having a hard time believing that. Going forward with the advert of egpus on high sierra and with new nvidia drivers, this will no longer be the case. yeah exactly. I was going to use my new egpu enclosure to ditch my windows box for good."
technical,"I spent a few hours yesterday trying to get AMD's open source stack working. I got MIOpen and its dependencies but hcc is still missing some bits. I may need to do a custom kernel build to get everything. I don't much care about porting CUDA code or running compiled C++ on the GPU - I want to do number crunching on it. ,-)  I also saw something on their website about programming it in assembler - *that* I might be interested in, because it's easy to go from assembler to FORTH. ,-) Yeah I am also trying to get some MIOpen and TensorFlow stuff working on my RX 480, but I don't want to destroy my main development rig, so instead I use IOMMU virtualization and use an Ubuntu 16.04 virtual machine that can use the RX 480. The AMD drivers are very friendly to virtualization (unlike nVidia drivers made for the gaming cards - only the Quadro drivers do)."
technical,"We are going off topic Yeah, sorry for hijacking this thread. I just didn't know where to ask the question."
technical,"Latest news: I have successfully built TensorFlow 1.3.1 with OpenCL from the GitHub source. There are quite a few missing pieces in the documentation, and I haven't tried to run anything in the GPU yet, but it is at least working for non-OpenCL CPU. BTW, I do *not* have CPU OpenCL installed, just GPU OpenCL.  Does anyone have any test cases for TensorFlow with an OpenCL GPU? I'll have to build one for myself eventually but I was hoping for a quick check. Yeah, there is test app in issue that I reported. Could you please let me know if it works in your case ?"
technical,"Thanks for the clarification.  I'm looking at cost effective hardware/software combinations to locally run Tensorflow and will definitely follow the OpenCL development progress. Yes can be an issue, but I think parts such as im2col/col2im and other convolution implementations could also be plugged in as external APIs if it's really an issue with the GCLA. This may also be better for the original authors of such work."
technical,"we *did* change the name, precisely when we realized that we did not think it made sense to move StreamExecutor into LLVM wholesale.  It's now called ""Acxxel"". I'm sorry for the confusion and I appreciate the feedback..  It was a learning process for sure. Yes I have still a bit of confusion between StreamExecutor, SyCL in eigen, XLA (that actually has only a CUDA backend, other than CPU and opencl in some slides)"
technical,unfortunately all this is a work-in-progress... Yes I know.. I was just curious if was discussed in some meetings.
technical,"The tensorflow approach is to rely on a hardware abstraction (the tensor module) for the majority of the operations needed in by a typical neural network, while relying on specialized libraries (such as cudnn) for the few operations that are really critical performance wise. The hardware abstraction enables us to implement most TensorFlow operations once and have them run on an accelerator with more than good enough performance. Yes I love multidimensional arrays. Also in our domain of interest, there is the SG14 in the C++ committee that tries to have all the people interested in these issues to converge into **the** standard. Of course SYCL is in the discussions. :-)"
technical,"New here. Wanted to ask if there will be OpenCL support in tensorflow in the future, would that mean there will be support to run tensorflow on FPGA? Thank you yes if the OpenCL or SYCL version & source code is supported by the FPGA environment. But since TensorFlow is perhaps the most ported framework with various means, it might already have some part running on an FPGA already somewhere..."
technical,"New here. Wanted to ask if there will be OpenCL support in tensorflow in the future, would that mean there will be support to run tensorflow on FPGA? Thank you Yes in addition to the OpenCL branch above,  will be publishing a book (very soon) on using it for real-time inference on commodity hardware using amd and hd graphics."
technical,I think it would be worth taking this discussion to the SG14 mailing list as the details aren't related to OpenCL / tensorflow. Yes probably it is no more so strictly confined here with all the details. Other than Eigen/sycl support Is there a plan for the cudnn calls?
technical,"Thank you, but I think I had tried the computecpp yesterday, and it's seem that the macbook system is still not supported with computecpp. So, maybe keep waiting for new updates is the only thing I can do (T.T).  BTW, my Iris 6100 is eight generation, which is good for OpenCL 1.2. yes SYCL 1.2 is a priori for OpenCL 1.2 and SYCL 2.2 is a priori for OpenCL 2.2. I said ""a priori"" since, if you do not use anything requiring the OpenCL-compatibility mode of SYCL, SYCL does not really require OpenCL at all. Actually SYCL is a very generic model for heterogeneous computing and can run on top of anything. But of course a real implementation may require OpenCL too."
technical,Is there any update? This issue is over 3 years old. YES THERE IS JUST LOOK AT THE LAST HANDFUL OF POSTS.
technical,"Is there any update? This issue is over 3 years old. Yes there is no machine-learning style proposal in SG14 yet. But participation is open, so you can send some proposals. :-) But perhaps SG6 (numerics topics) is more relevant. I do not think they have their own mailing-list/forum yet."
technical,"Khronos meeting is next week in Seoul with both OpenCL and Vulkan people, but discussions are not public. But that sounds like a good idea to have each world to improve the other, and at some point benefits to TensorFlow. :-) Yes, I hope they discuss some DeepLearning beneficial stuff :)"
technical,"I saw it, yes! :) I haven't looked at libDNN at all so far, but I am not sure what you mean exactly. I assume convolution is implemented using GEMM? Yes, I just thought it would be nice if you could look over it and maybe give some improvement or tuning tips on libdnn.  It is using GEMM, but implicit (not through a BLAS, only small GEMM's on a workgroup level) so that a higher level of parallelism is possible, as well as no intermediate buffer are necessary (to unroll the data into a GEMM scheme)."
technical,"Is this roadmap active? Yes, no problem. Contact us via infostreamcomputing.eu"
technical,"Is this roadmap active? Yes, plainly passing any   global structure (like array or struct) containing pointers is incorrect just because those pointers can point to memory of another device (OpenCL supports multi-device paradigm where one device can't access memory of another). But it seems to be possible to overcome on IR level, w/o intermediate translation to OpenCL code - that's what I assumed :)"
technical,"FWIW I believe the recent versions of PyGpu can use either CUDA or OpenCL. I have all of the software installed on my Arch box but I haven't tested it yet. yes, this issue has been around on for a while now. The effort is massive and a lot of people are trying to ""get it working"" as  mentioned. A bit of an update from our side. You should be able to use ComputeCpp 0.3.0 on the AMDGPU-pro driver stack for Ubuntu 16.04 the instructions can be found here. As well, we are focusing now on performance improvements for different models - there is a lot to do but we are getting there."
technical,"see my comment about mac above, if you point me to the build instructions I'll have a go yes, triSYCL moved from AMD to Xilinx  but you are right, the version on my GitHub workspace works too.  We have not tried triSYCL on TensorFlow yet. There is already a big build config work to do just to try..."
technical,"I think that tweaking the TF dockerfile on your repository with this istructions could easy the setup for others. Yes, when there will be something more functional. Basically it is quite a copy and past of this istructions you have posted."
technical,"Why not implement a CL version first while waiting for the SYCL port to be ready? I assume there are quite a few folks here willing to help out. One year just sounds too long. Yes, you are right, #22 is almost 8 months old and has over 100 posts! The information can get swamped!  TensorFlow uses the Eigen library for tensor computation (in the Tensor module). We have committed a partial implementation for OpenCL 1.2 using SYCL (branch Codeplay). The reason we used SYCL for this work is that this section of TensorFlow uses C++ expression trees, which is possible with SYCL for OpenCL, but not possible with OpenCL C directly. Other components of TensorFlow, such as convolutions or BLAS, could use OpenCL C directly.  Currently, I am working on integrating ComputeCpp (Codeplay's SYCL compiler) into the bazel build system. This should be ready soon ( follow this repo: ). After that is done, TensorFlow should be accelerated on systems that support OpenCL SPIR (such as AMD or Intel) with ComputeCpp. Further work will continue on accelerating more of TensorFlow, as well as supporting more OpenCL implementations and the triSYCL open-source SYCL. SYCL and OpenCL are multi-vendor, royalty-free open standards, so there are lots of platforms and devices that can be supported using this approach (not just AMD GPUs).  The ComputeCpp Community Edition compiler will be available for free later in 2016 (in beta form: full conformance will be released for free early 2017).  The work on accelerating the non-C++ parts of TensorFlow (e.g. BLAS and convolutions) could be done without SYCL and implemented separately. Different hardware vendors may have their own optimized libraries for these features which could aid acceleration. Or, we could use Eigen with C++ for these features. We believe the performance will improve steadily. To accelerate on a wide variety of devices, we need to manage the data more efficiently, which is why there is a ""managed tensor"" item of work, so that data movement can be more efficiently managed between host and multiple devices. It is hard to predict how the performance will vary over a wide range of devices, right now. Currently, very little is accelerated, but we are putting the infrastructure is in place to allow open-standard acceleration in TensorFlow."
technical,"This commit would be enough to get it built ? yes, you should be able to cherry-pick that"
technical,"Try to push the cause in Seattle ,) You are right, OpenCL and CUDA are too different programming approaches. The single-source aspect found for example in CUDA and OpenMP 4.5 is extremely powerful from a software engineering perspective. This is why there is this SYCL standard for the real C++ programmers. SYCL can be seen as CUDA on steroids without any language extension and with some OpenMP aspects (the tasks). A typical SYCL device compiler is expected to generate SPIR-V kernels.  Your concerns about portability are less an issue with the SPIR-V standard (kind of portable equivalent of nVidia PTX/AMDIL/... in the Vulkan & OpenCL world) which is mandatory to accept in OpenCL 2.1 and Vulkan. So the beauty is that if you have a front-end that generates SPIR-V, you do not need special knowledge of the very details of the hardware to run on. There is a Khronos open-source bidirectional translator between LLVM IR and SPIR-V, so it opens quite new territories."
technical,According to TF 1.1 Release Notes (Nvidia only) support has been deprecated. Let's hope this will help to improve OpenCL approach (not very confident on this). You can follow also the status of the PR.
technical,"FWIW here's the GitHub post that got me going this weekend after thrashing and Googling since April: That was my original issue - trying to use my Bonaire to accelerate linear algebra on R. You can move on to the latest Linux distros & use a recent custom compiled kernel like 4.11/4.12 with AMDGPU drivers enabled, RADEON disabled and with CONFIG DRM AMDGPU SI=Y and/or CONFIG DRM AMDGPU CIK=Y set in the kernel configuration, plus AMD firmware for 7970 (Tahiti) in the initramfs = newest AMDGPU-PRO OpenCL will work on any GCN cards. Forget about FGLRX (on older Linux distros) and Clover via RADEON drivers, both are sub-par. Forget about pre-GCN cards also. I tested them using OpenCL on Windows for Caffe, the performance is not worth making an effort for such old cards. As all AMD cards post 2012 should be GCN anyways."
technical,"why is this issues closed ?  I don't think your commit fixes this. You can't always use the same commit comments in different repository ,)"
technical,"The Imagination(GPU)'s OpenCL SDK needs NDA to get accessible, we only have the shared library.  Is it possible to run tensorflow based on this libs? You don't need vendor specific header files to build any OpenCL program. Try cl.hpp from any other SDK. For example - I have at least 3 OpenCL platform and all it works with one shared"
technical,"Quickly: Right now I am running Inception v3 on a custom C++ / Object-C codebase and passing in decoded video frames to the network. I don't know enough about TF to know low level needs, but high level: load models, run session, expect stuff to work. I think that means 100% compatibility to be really honest. I know thats of no help in prioritizing. Basically the C++ Image Recognition using TF /InceptionV3 was my starting point. cuda-on-cl running on Mac: I've checked out the repo and can help debug and run builds on my systems and verify results on a variety of hardware:I have access to AMD Mac Pros with Dual D700s, Nvidia Mac Laptops and Desktop systems. Thanks for your detailed feedback. I'll monitor the repo, try to follow along, and try to help best I can. you might want to look  to learn how some functions are mapped."
technical,"Sad because now with an eGPU and n Nvidia 980 Ti inside we get driver working and Cuda working I didn't have the time for now to try Tensor Flow in my configuration yet.webdriver and Cuda toolkit installed on my computer and the Cuda samples work well you said that you test convnets on tf-coriander, but it doesn't seem like convnets are working yet. Can you please clarify if you managed to get convnets to run on the GPU using tf-coriander?"
technical,Sad because now with an eGPU and n Nvidia 980 Ti inside we get driver working and Cuda working I didn't have the time for now to try Tensor Flow in my configuration yet.webdriver and Cuda toolkit installed on my computer and the Cuda samples work well ...Dissenter detected ...Stifling discussion ...Summoning egamma ...Issue closed in 3... 2... 1...
technical,...Dissenter detected ...Stifling discussion ...Summoning egamma ...Issue closed in 3... 2... 1... Thanks for creating this issue. We think this issue is unactionable or unrelated to the goals of this project. Please follow our issue reporting guidelines. Happy Coding!
technical,"Sounds like that's indeed the change you meant, then - I wasn't trying to rehash the debate, just trying to clarify what you were referring to. In that case, I think that the ecosystem benefited hugely from having a single authority for that, since it would have greatly fragmented the package ecosystem if many package authors had gotten the wrong idea and assumed peer deps were optional by default. ...having a single authority for that  That depends entirely on the point of view and on how accountable that authority is to the community. Even with the RFC process, npm's roadmap is still controlled by a singular commercial interest. Perhaps the better forum for such discussions would be the OpenJS Foundation, where a spec and reference implementation for such things could live while allowing competition on specific implementations. But... that's a separate conversation.  iarna does raise a valid point about ensuring that bugs that need to be fixed in the 1.x stream are fixed in 1.x. As I understand it, there's no intent on shipping yarn2 in core which means we cannot interpret ""fixed in 2"" as being meaningful here. arcanis addresses that concern rather well here. So long as there is a demonstrated commitment to ensuring that security and critical bug fixes are made, and that yarn1 remains functional/compatible with Node.jslatest, then I'm not sure what else can be said there that would be convincing. Specifically, no one has asserted that the yarn versions shipped in node.js would be ""unupdatable"".  I would not expect Node.js to just accept that unquestioningly. I would expect Node.js to hold Yarn accountable to that promise that the issues would be addressed, and if it turns out the issues are not addressed and major bugs persist, then we would always have the option of removing yarn from the distribution.  iarna also said:  ...if that lands in a way that's acceptable and adopted the major package managers still under development, it would be very uncomfortable for Node itself to be shipping with one that didn't support it.  But that's not the case here. Node.js would not just be shipping a single package manager option that did not support the feature, it would continue shipping multiple package manager options. It would be up to users to determine what they want to use -- and whether support for that new feature is important to them or not. The users would give us the necessary feedback to determine which is the right path. That said, it would be rather unfriendly to the ecosystem as a whole for a single package manager run by a singular commercial interest to unilaterally ship a new feature that would qualify as a ""must support"" by others."
technical,"While the decision is up for consideration by the TSC, I'm going to go ahead and lock the conversation here to just collaborators. While discussion on a topic is always a good thing, there's really not much more to be said here that's going to add to the discussion. Please allow the TSC time to review and decide what action will be taken, if any. A reminder for the TSC folks that consulting our Technical values and priorities may be helpful in informing your vote on this issue."
technical,"It depends which level of fixes we're talking about. As linked by jasnell, we are already maintaining Yarn 1 with regard to security fixes (ie if we receive a security-related issue, we'll release a fix). If merged in Node, we're also going to follow deprecation paths. And of course, critical bugs that put the computer stability at risk (for example erasing files outside of the project) will always be fixed. I'm not aware of any at the moment.  Generally I believe the answer to your question is that we're reasonable developers, and if there's a reasonable reason to backport a fix, we'll consider it. However, if what you mean is that we should stop our work on Yarn 2+ and resume the v1 trunk as if nothing happened, then it indeed won't happen. Also note that my goal here is to easen the contribution workflow for projects already using Yarn (so that anyone cloning them can start working with them without having to install Yarn). This implies two things:  - It has to be Yarn 1 by design, because that's what those projects already use (Yarn 2 projects check-in the package manager, so they have no need for a Yarn 2 global binary - Yarn 1 works just fine for this purpose). - It shouldn't change significantly how those projects work. Indeed, projects that use Yarn **now** use it because it works. As such, making fixes is more likely to break something than to improve their situation (it won't surprise you that any fix comes with its own risks in terms of regression).  You'll note that this reasoning is also why we switched to a model where Yarn 2 is installed by-project rather than globally. We simply acknowledge that it's a dependency, like any other, and as such that it should be locked like any other. The npm install model (that Yarn 1 inherited) is rather unfortunate in that it merrily upgrades package managers at the same time as you upgrade Node, and this is not something we want to happen in our case. We believe that updates must be controlled, and left at the discretion of the user. Hence why we intend to remain conservative on the Yarn 1 trunk (unlike Yarn 2, where users are in full control of their migrations)."
technical,"Without actually weighing in yes/no, I wanted to add some (IMO very important) background info to the conversation that I don't think has been clearly stated anywhere in this thread so far:  Yarn v1 is both a runtime and install-time dependency of Yarn v2. Edit: slightly more accurate: **Yarn v2 installs are meant to be managed by Yarn v1**. (In more details: Yarn v2 is meant to be installed via Yarn v1. Yarn v2 is meant to be installed locally on a per-project basis -- Yarn v2 is not designed to be installable globally. The binary you hit first when running a yarn command is the Yarn v1 binary. Yarn v1 redirects to run Yarn v2 if the project has set up Yarn v2 locally.)  So, the notion of including ""Yarn v2 instead of Yarn v1"" isn't technically doable with the current design of Yarn v1/v2.  I think that's also a big part of the background info needed to explain why including Yarn v1 with Node would make any sense, and why not including Yarn v2 with Node would make any sense.  From the perspective of a potential Yarn v2 user, and given the current designs of Yarn v1 & v2, this proposal is as close as it gets to ""including Yarn v2 with Node"". (And if I understand correctly, this proposal would seamlessly provide the closest thing to ""including Yarn v3 with Node"" as soon as/if/when that Yarn release exists.)  **EDIT/correction?**: Placing the Yarn v2 binary (having downloaded it with Yarn v1) at my usual global package install location as yarn does seem to yield a usable package manager. It's not the way the Yarn v2 docs instruct users to do things, though. So to be strictly accurate, Yarn v2 appears to be usable without Yarn v1. There's no apparent technical barrier stopping this workflow. But it's not supported by Yarn maintainers, I suppose. It appears to be a minified/bundled file, though. So not the usual global package install. The missing piece of the ""user story"" here is there's no easy way to update ""globally installed Yarn v2"" because Yarn v2 isn't officially meant to be globally installed, and it's not on the registries as a unified package, that I know of. And this, folks, is the real reason why the same group of vocals in this issue (not everyone, mostly those who take care to upvote each other) is against this proposal.  Let's be honest - this isn't about maintenance. None of you even assessed the maintenance cost, or made proposals to decrease it. This isn't about Yarn 1 either - you neither use it, nor recommend it. This isn't about bugs - you'd otherwise be at arms about some other things we won't list here. This isn't about community - you certainly didn't talk to Yarn's users, nor even cared about their opinion. This isn't about Node - it should be fairly obvious that shipping a project owned by a for-profit as part of a free software is a major red flag and liability. This isn't about package management ecosystem - otherwise you'd listen to those actually writing those package managers (you may have missed it, but I'm not even the only one - another ghost is lurking this thread as well). This isn't whatever reason you want to make people believe it is, folks.  What this is is that you believe that modern npm is as good as Yarn, and you don't see how this proposal would do you any good. To you, Yarn should die a fiery death to give back npm all its glory, and you have many amazing ideas to make that happen. It's fine, we all have opinions. I'm more an Emacs user myself, but whatever. The problem is that this, quite literally, isn't the discussion at hand. Right here, right now, we're talking about the existing state of the ecosystem.  For the better or worse, whether you like it or not, Yarn is used. That's it. It's really not up to you (or me) to decide whether its users are right or wrong. The very fact that they use it means it matters. To block this proposal because, to you, ""npm is as good"" would be at best egoistical, at worst worrying meddling (not only would Node tolerate a for-profit project, it would actively fight for it, do you still want to laugh at Deno?). Frankly, the question really isn't ""should Node add Yarn"" - it's rather ""can Node prone inclusivity by persisting in providing a subpar developer experience to a specific set of people, depending on the tool they use?"".  As a side note, out of the 1000 most depended packages on the npm registry, 216 definitely use Yarn, and 291 use npm (the rest being undeterminable)."
technical,"I don't think you understand this diff, or how Yarn is distributed. All versions of Yarn act both as a package manager and a jumper (similar, if you will, to gulp-cli). If it finds a yarnPath entry in .yarnrc.yml, it will use it instead of the global one. In other words, older projects will use Yarn 1 (because that's the global version used as fallback when nothing is found), or the checked-in package manager if it finds the right configuration to do so. Put simply, everyone gets what they ask for. There is no ""involuntarily upgrading users who are on an older version"". On the other hand, it does require *a* Yarn to be globally available.  Let's be clear - you're telling *me* that I don't credit trade-offs or drawbacks, on a project I maintain, for a protocol that was designed a year ago, and for which noone has reported any automated breakage of any system, while still being able to migrate at the time of their choice. At this point I don't know what to tell you. That's answered. Are you 100% certain, without question, that nobody is relying on the global yarn to be a specific version rather than using yarnPath? If there is  anyone  who is and they upgrade, it'll very quickly become our problem. Yes. In the paragraph you wrote - which is clearly what I was replying to - you only focus on the aspirational positives of Yarn v1 being frozen (outside of security updates, which you'd mentioned in a previous comment) without addressing potential trade-offs or drawbacks in a neutral way. I'm not sure how that's seemingly absurd or controversial to you. You point to the Binary Management Discussion, which is somewhat about Corepack but ended up being more about version management. The PR you opened, says this: In short, the intent is to provide standard shims that allow users to run Yarn and pnpm commands without having to explicitly install them first, and without cluttering the Node distribution. I fail to see how this PR is not entirely antithetical to the goal of including Corepack and does not ""clutter the Node distribution"""
technical,"That's what I previously meant by this trust thing. Not only is your comment extremely insulting to the work my team is doing, you keep going and rambling ""we can't trust them they're the devil in disguise"". I find this behavior ridiculous, entitled, and generally not up to the standard our community should strive for. Again, we opened this PR because we saw a way to improve Node by this partnership, and we went with it. We certainly don't have anything tangible to gain from it, except an increased maintenance load. If the response is for a set of people to feel justified in harassing a project, let me say I'm disappointed. As has been stated repeatedly in this thread and throughout the entire conversation, yarn2 is simply, explicitly, intentionally *not* on the table and literally no one has even remotely suggested that it should or would ever be. If someone opens an issue or PR to add it later, we'll deal with that if it happens. But it is safe to say that right now there's an absolute and obvious consensus that it wouldn't land. Assuming something is ""part of the plan"" when literally everyone is repeatedly telling you that it is *not* is not productive and just wastes everyone's time and I'd ask you to please just stop."
technical,"Yes, I do, if that's the expectation going in and we message it responsibly. As someone that has been using Node.js heavily for years, I think adding Yarn 1 to Node.js would be a huge mistake.  - Modern npm is as good as Yarn 1. - Yarn 1 has a lot of critical bugs that will never get fixed. - It would better to focus on npm which is actively maintained. *(Promising to fix critical security bugs does not mean Yarn 1 is actively mainained)* - It will hinder package management evolution, as Yarn 1 will lag behind and block package maintainer from using new features since many users are on Yarn 1. (If it's not in core, package maintainers can use that as an argument, but if it's in core (""blessed""), users will demand support) - It will confuse beginners why there are multiple package managers. - It will add more maintenance work for Node.js core team without any clear benefit. It's easy enough to install Yarn for the ones that want it."
technical,"Opened for this comment by Michael. Closing this as we will not land the PR. Please let us know if you think that was not the right thing to do. Before submitting a pull request, please read Commit message formatting guidelines:  For code changes: 1. Include tests for any bug fixes or new features. 2. Update documentation if relevant. 3. Ensure that make -j4 test (UNIX), or vcbuild test (Windows) passes.  Developer's Certificate of Origin 1.1  By making a contribution to this project, I certify that:  (a) The contribution was created in whole or in part by me and I have the right to submit it under the open source license indicated in the file, or (b) The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate open source license and I have the right under that license to submit that work with modifications, whether created in whole or in part by me, under the same open source license (unless I am permitted to submit under a different license), as indicated in the file, or (c) The contribution was provided directly to me by some other person who certified (a), (b) or (c) and I have not modified it. (d) I understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information I submit with it, including my sign-off) is maintained indefinitely and may be redistributed consistent with this project or the open source license(s) involved. --  Ref this (and this in particular)  This commit adds Yarn to the Node release tarballs and installers. I tested: - Windows (ran the resulting exe) - MacOS (make pkg and ran the resulting pkg) - Linux (untar'd the .tar.gz file) In all cases, running yarn --version yielded the correct Yarn version (1.22.5 for infra reasons, which is for all purposes identical to 1.22.10 minus a postinstall script which has no impact whatsoever in this situation).  * This PR changes close to nothing for existing Node.js users (this PR doesn't remove npm from the Node.js project, and you can still install Yarn separately from Node.js), it's meant to improve the experience of new users. * Reasons for integrating Yarn v1 (rather than v2) are outlined here and more in depth in this. * Please keep the discussion focused on Yarn v1, integration of other package managers can happen in a separate issue."
technical,"No, I'm saying that we would look to yarn to fix those issues and if they do not we retain the option of removing it from the distribution. It's no different than the relationship with npm today. Fixes are made upstream. But, and maybe I'm misunderstanding(?), they are as a matter of  policy not making fixes. It's frozen to the degree that you can't fix bugs in it. There is no plan to have more releases of Yarn 1, is there? Edited to add: If Yarn 1 were actively maintained, receiving bug fixes, I would have no complaints. But ""frozen but not legacy"" isn't enough, imo. All I want is  someone  to be responsible for it."
technical,"No 2.x stable release was ever made on the npm registry. The last one which was, 2.0.0-rc.27 (out of 36), was only assigned to a berry tag. I'm positive no Yarn 2 user relies on a specific global version.  As for Yarn 1 users, past releases were semver-compliant, so 1.22.5 (the latest one) can installs the whole 1.x line. I would also point out that this wouldn't be any different from how npm releases work: you recently upgraded npm from 6 to 7, which is far riskier than Yarn 1.20 to 1.22 - at least semver-wise. Discussed in the meeting today: Consensus seems to be to try to do survey to get more information to help TSC members understand the need before calling a vote. Next step is to find volunteer to drive survey as we had no volunteers in the meeting. More details in files and you can also watch the recording at around 10:00 minutes in."
technical,Yarn 2 by default does not install to node modules/ and does not work with the the node CLI. I verified this 30 minutes ago using the guides on the website. Except... That elephant is explicitly *not* in the room. Literally no one is suggesting that yarn2 is in any way being considered here
technical,"Well yes, that's what we've been saying since the beginning. If you read the thread (for example my very last message) you'll also get one of the reasons why it still matters. For those that aren't aware, what you're actually talking about is Yarn PnP, Yarn 2, just like Yarn 1, supports both PnP and node modules. Yarn doesn't replace the node cli, we simply add the require hook, if you're not a fan of that use the node modules linker."
technical, I am -1 based on the fact that Yarn v1 (a.k.a. anything other than Berry) is frozen. That does not sound like a dependency that we should be taking on imo.  The 1.x line is frozen - features and bugfixes now happen on this.
technical,"This point is discussed (and answered) there. As the maintainer of both release lines I don't mind discussing it further, but I think this linked thread would be a better opportunity to discuss these topics than an implementation PR - both for ergonomic reasons and because it's already been answered.  Just to be clear, I don't mind waiting for a TSC evaluation. I've only contributed to Node a couple times, and processes / ownerships are still a bit fuzzy to me. I'm happy to wait for the decision of whoever group is in charge of this kind of work. At least the implementation exists and provides a concrete picture of what the outcome would be. I appreciate the content there but it does not address my concern. The fact that Yarn v1 is frozen is a massive red flag to me as a 'new' dependency. IMO that context deserves to be held in the PR to add a dependency if it is a concern.  You assert that you expect people to stop using Yarn in favor of Berry, and have data to back that up. Further, you assert that the only changes you're planning on making are security changes - this is relatively well demonstrated by the backlog of PRs that are submitted solving issues that users are having but aren't being merged or addressed, including tiny ones like fixing broken links.  From the perspective of looking at Node.js over the next few years - rather than the short-term benefit of such a change - I frankly cannot see a reason why the Node.js project would take something on that's actively and intentionally being migrated off of as a result of the maintainers' choice in direction for it and isn't addressing problems end-users are facing outside of security issues."
technical,"Surely you understand that the assumption is that a project updates to latest versions of dependencies eventually. Without explicitly stating that Node will not in this case, users (and anyone just observing) will assume it eventually will be. I can understand people having that misconception, yes, which is why it's been reiterated multiple times that yarn2 is quite explicitly not on the table. Sure, that could change at some indeterminate time in the future at which time we can discuss the pros and cons and make a decision, but that's a purely hypothetical argument right now."
technical,TSC vote concluded that we would not include yarn directly and agreed on moving towards Corepack as the way forward instead. See this issue for results of the vote. Closing this as we will not land the PR. Please let us know if you think that was not the right thing to do. I just wanted to say that the TSC decision is a big win for package managers like yarn and having corepack in core sounds like a good way forward to me :) Edit: I just noticed this is still locked - so I will leave discussion to the GitHub discussions and will hide this comment since non-collaborators can't participate here so it's not a good space to have this discussion.
technical,"Sounds good   Only remark I have: as we probably all know, which package managers people use tend to be a quite polarizing topic, where both communities can sometimes display a certain lack of empathy. Years ago it was Emacs v Vim, then Tabs v Space, and nowadays Yarn v npm. I feel the need to mention this, because studies have shown that those who oppose tend to be more vocal than those who support, especially when the opposition is the status quo.  Given that the primary goal (at least for me) is to help current users, not necessarily acquire new ones, I'd be wary of the survey wording. Specifically, I'd be cautious about not making it look like a ""two parties"" question. In my opinion, the survey would benefit by proving not only that a segment of the Node community would be positively impacted by this change, but also that even those who wouldn't directly benefit from it *wouldn't be negatively impacted* either. For instance, rather than ""would you like it? Y/N/neutral"", which could incite non-Yarn users to answer N rather than neutral, perhaps a more suitable question would be ""would it benefit your workflows? Y/N, would it impede your workflows? Y/N"" (perhaps with a ""Why?"" box on each, to get testimonies on both sides). Basically focusing on the effects rather than the feelings. My 2 cents. (One last note in the meantime, jasnell made an informal Twitter poll pretty much about this about a year ago, while the results are incomplete with only 378 answers, it can help set expectations and perhaps give ideas about wording.) I started a test release build so we can check the result on all platforms. Will post the download link when it's ready."
technical,"As has been stated repeatedly in this thread and throughout the entire conversation, yarn2 is simply, explicitly, intentionally *not* on the table and literally no one has even remotely suggested that it should or would ever be. If someone opens an issue or PR to add it later, we'll deal with that if it happens. But it is safe to say that right now there's an absolute and obvious consensus that it wouldn't land. Assuming something is ""part of the plan"" when literally everyone is repeatedly telling you that it is *not* is not productive and just wastes everyone's time and I'd ask you to please just stop. I think you're misinterpreting my comments in this thread. This is not Yarn's repo. I am not dismissing your work or the value of your project in the use cases you are aiming for. I am against Yarn 2 becoming a a built in feature of Node because of (1) poor performance and (2) incompatibility with the ecosystem, something that would be harmful for Node to endorse. As Yarn operates as an independent project I have no view on it, positively or negatively. I follow you on Twitter and think you're an insightful person. I never said there is an issue of trust of you or your team. I don't trust attitudes of  all people  whether in this thread or not not to change once a dependency is merged into core. I think it's perfectly valid to say that merging a dependency does increase the likelihood of a major version being introduced at a later time. I am not trolling by bringing up this concern. I presume that dependency updates are not all treating as one-off decisions in Node. You do not approach a V8 version update as an isolated decision ""should we upgrade this time everyone or just stick to the current version"", do you? I would presume that you approach it with the assumption that you will upgrade, unless there are reasons to not do so.  Merging Yarn 1 increases the likelihood of Yarn 2 being merged at a later date. And for that reason, Yarn 1 should not be merged into core."
technical,"I don't think this achieves the goal of improving the experience for beginners, it just makes it confusing (why are there two package managers that are compatible with one another?) It also makes node.js look confused: why should a distribution ship with two COMPETING package managers? I would expect Node.js to hold Yarn accountable to that promise that the issues would be addressed, and if it turns out the issues are not addressed and major bugs persist, then we would always have the option of removing yarn from the distribution. I can't imagine you reasonably expect us to be able to remove Yarn at some point in the future if it's merged into core? We've seen this time and again - once something gets into core, it's effectively impossible to remove without breaking a massive amount of the ecosystem. I'm curious how this example - especially with the scale of its impact - would be any different?   This  is the root of the reason I think it make zero sense for Yarn v1 to go into Node.js core. Whether it's in a year when we decide for some reason that Yarn v1 isn't a match, or if it's in 7 years when it's gotten zero updates and is one of the Regrets from a current TSC member because we're permanently checking in technical debt, there's no chance we're ever able to remove this without causing massive ecosystem pain. If Yarn v1 were being actively developed and improved, I think this PR would make sense. Thinking about our future selves and our users future selves, I can't find any avenue to understand how one could come to the conclusion that checking in ""frozen"" (I still take issue with the phrasing of this - it's effectively EOL, outside of a commitment by  current maintainers  to provide security fixes  indefinitely ) code is a good choice  if  enhancing the experience of our users is the only motivation,  especially  given that - at least to me - there's zero chance of ever reversing the decision."
technical,"it is waiting on a volunteer to move the survey forward that was discussed in the TSC meeting. Trott also mentioned that he might use it to test out an RFC processes he was thinking of proposing. Not sure if there was discussion in the meeting today as I was not able to be there, but last week we agreed that if there is no progress with 2-3 weeks on either of those we'll just put it to a vote of the TSC. I would really like to push for Yarn not to be included in Node. As a project it has abandoned its original intentions which made Yarn v1 appealing to the Node community. And the new direction of the Berry project has led to massive confusion and division in the community. At the same time, npm has addressed the original concerns that led to the creation of Yarn in the first place, and has shown a far more stable path forward for the community."
technical,"But, and maybe I'm misunderstanding(?), they are as a matter of  policy not making fixes. It's frozen to the degree that you can't fix bugs in it. There is no plan to have more releases of Yarn 1, is there? Edited to add: If Yarn 1 were actively maintained, receiving bug fixes, I would have no complaints. But ""frozen but not legacy"" isn't enough, imo. All I want is  someone  to be responsible for it. It depends which level of fixes we're talking about. As linked by jasnell, we are already maintaining Yarn 1 with regard to security fixes (ie if we receive a security-related issue, we'll release a fix). If merged in Node, we're also going to follow deprecation paths. And of course, critical bugs that put the computer stability at risk (for example erasing files outside of the project) will always be fixed. I'm not aware of any at the moment.  Generally I believe the answer to your question is that we're reasonable developers, and if there's a reasonable reason to backport a fix, we'll consider it. However, if what you mean is that we should stop our work on Yarn 2+ and resume the v1 trunk as if nothing happened, then it indeed won't happen."
technical,"Sorry, you are right I skipped the discussion. I read it and I found the answer to many question. it is waiting on a volunteer to move the survey forward that was discussed in the TSC meeting. Trott also mentioned that he might use it to test out an RFC processes he was thinking of proposing. Not sure if there was discussion in the meeting today as I was not able to be there, but last week we agreed that if there is no progress with 2-3 weeks on either of those we'll just put it to a vote of the TSC."
technical,Discussed in the meeting today: Consensus seems to be to try to do survey to get more information to help TSC members understand the need before calling a vote. Next step is to find volunteer to drive survey as we had no volunteers in the meeting. More details in files and you can also watch the recording at around 10:00 minutes in. it was also suggested that we invite arcanis to a meeting to make a case in favor of this PR before we move to a vote.
technical,"I can understand people having that misconception, yes, which is why it's been reiterated multiple times that yarn2 is quite explicitly not on the table. Sure, that could change at some indeterminate time in the future at which time we can discuss the pros and cons and make a decision, but that's a purely hypothetical argument right now. It's not a misconception, it's how almost all software projects treat their dependencies. You will absolutely get issues filed to upgrade Yarn from users who will just assume that it's part of the plan.  Yarn 2 not being on the table today is not enough, it has to be explicitly off the table in the future. The harm to the ecosystem (and Node itself) is to great to invite in Yarn 1 as a trojan horse without such a commitment."
technical,"I can understand people having that misconception, yes, which is why it's been reiterated multiple times that yarn2 is quite explicitly not on the table. Sure, that could change at some indeterminate time in the future at which time we can discuss the pros and cons and make a decision, but that's a purely hypothetical argument right now. license-builder should be updated to add Yarn's license to our distribution."
technical,"I would really like to push for Yarn not to be included in Node. As a project it has abandoned its original intentions which made Yarn v1 appealing to the Node community. And the new direction of the Berry project has led to massive confusion and division in the community. At the same time, npm has addressed the original concerns that led to the creation of Yarn in the first place, and has shown a far more stable path forward for the community. My main concern about Yarn v1 is that bugs are now being closed as ""fixed in v2"" which is not what you do in a maintained major version. These kinds of bugs are complete show stoppers for some private registries. Shipping an unupdatable version of Yarn as an unchanging artifact in Node leaves Node permanently broken. Bugs happen, but there needs to be some process through which they can be fixed. If this really was just a feature freeze, I would be less concerned, but as can be seen from the issue and associated PR, that's not how it's playing out on the ground. I would also add that the ecosystem needs to be able to add features to package management systems. Blessing a package manager that can't receive feature updates would reduce the ability for the Node community to adapt to changes. For instance, there's the open npm RFC to add registry dependencies -- if that lands in a way that's acceptable and adopted the major package managers still under development, it would be very uncomfortable for Node itself to be shipping with one that didn't support it."
technical,"Are you 100% certain, without question, that nobody is relying on the global yarn to be a specific version rather than using yarnPath? If there is  anyone  who is and they upgrade, it'll very quickly become our problem. Yes. In the paragraph you wrote - which is clearly what I was replying to - you only focus on the aspirational positives of Yarn v1 being frozen (outside of security updates, which you'd mentioned in a previous comment) without addressing potential trade-offs or drawbacks in a neutral way. I'm not sure how that's seemingly absurd or controversial to you. You point to the Binary Management Discussion, which is somewhat about Corepack but ended up being more about version management. The PR you opened, says this: In short, the intent is to provide standard shims that allow users to run Yarn and pnpm commands without having to explicitly install them first, and without cluttering the Node distribution. I fail to see how this PR is not entirely antithetical to the goal of including Corepack and does not ""clutter the Node distribution"" No 2.x stable release was ever made on the npm registry. The last one which was, 2.0.0-rc.27 (out of 36), was only assigned to a berry tag. I'm positive no Yarn 2 user relies on a specific global version.  As for Yarn 1 users, past releases were semver-compliant, so 1.22.5 (the latest one) can installs the whole 1.x line. I would also point out that this wouldn't be any different from how npm releases work: you recently upgraded npm from 6 to 7, which is far riskier than Yarn 1.20 to 1.22 - at least semver-wise."
technical,"Which is exactly why I included this caveat. There were suggestions in that RFC that you expressed support for. I don't believe consensus is impossible. **Are you suggesting that the Node Foundation will be taking on responsibility for updating Yarn1 in the future, if it's included? If that were the case then my concerns would be answered.** Right now the problem is, if you have to patch Yarn1 you're SOL. You have to support floating patches forever and the version users can install independently diverges from the one bundled with Node.  Surely that is true, and also not afaict relevant? No, I'm saying that we would look to yarn to fix those issues and if they do not we retain the option of removing it from the distribution. It's no different than the relationship with npm today. Fixes are made upstream."
technical,"Please do not add yarn to node.  Please consider removing npm from node. When you install node on FreeBSD (using pkg), it installs ... node.  If you want npm or yarn, no problem, that is just another simple install. But new users will fail if npm isn't installed automatically!  Well, if they can't follow a simple recipe and also install npm or yarn, they aren't going to get very far with async/await. But lots of developers prefer yarn!  Well, lots of developers prefer Express, or Fastify, or Hapi, but node does not bundle any of those either. Node has succeeded in large part because of its modularity and composability.  Please don't turn it into a monolith now by prescribing additional must-have packages from on high. Please prefer minimal node.  Do that one thing, and do it well. only rational reply in this godforsaken discussion thread. npm is an optional dependency on various linux distros such as Arch, I think it's better idea to push for yarn to be a optional dependency on  distro packages rather than core nodejs."
technical,I just wanted to say that the TSC decision is a big win for package managers like yarn and having corepack in core sounds like a good way forward to me :) Edit: I just noticed this is still locked - so I will leave discussion to the GitHub discussions and will hide this comment since non-collaborators can't participate here so it's not a good space to have this discussion. Opened for this comment by Michael. Closing this as we will not land the PR. Please let us know if you think that was not the right thing to do.
technical,"The proposed yarn addition are not the sources but a result after it was 'built'. In other words, the javascript here is not the preferred method of editing but just distribution.. This would make it difficult or impossible to patch with upstream cherry-picks. Please do not add yarn to node.  Please consider removing npm from node. When you install node on FreeBSD (using pkg), it installs ... node.  If you want npm or yarn, no problem, that is just another simple install. But new users will fail if npm isn't installed automatically!  Well, if they can't follow a simple recipe and also install npm or yarn, they aren't going to get very far with async/await. But lots of developers prefer yarn!  Well, lots of developers prefer Express, or Fastify, or Hapi, but node does not bundle any of those either. Node has succeeded in large part because of its modularity and composability.  Please don't turn it into a monolith now by prescribing additional must-have packages from on high. Please prefer minimal node.  Do that one thing, and do it well."
technical,"What do we need to do at this point to get this moving forward? I'd like to see this move forward. Sorry, you are right I skipped the discussion. I read it and I found the answer to many question."
technical,"it was also suggested that we invite arcanis to a meeting to make a case in favor of this PR before we move to a vote. Sounds good   Only remark I have: as we probably all know, which package managers people use tend to be a quite polarizing topic, where both communities can sometimes display a certain lack of empathy. Years ago it was Emacs v Vim, then Tabs v Space, and nowadays Yarn v npm. I feel the need to mention this, because studies have shown that those who oppose tend to be more vocal than those who support, especially when the opposition is the status quo.  Given that the primary goal (at least for me) is to help current users, not necessarily acquire new ones, I'd be wary of the survey wording. Specifically, I'd be cautious about not making it look like a ""two parties"" question. In my opinion, the survey would benefit by proving not only that a segment of the Node community would be positively impacted by this change, but also that even those who wouldn't directly benefit from it *wouldn't be negatively impacted* either. For instance, rather than ""would you like it? Y/N/neutral"", which could incite non-Yarn users to answer N rather than neutral, perhaps a more suitable question would be ""would it benefit your workflows? Y/N, would it impede your workflows? Y/N"" (perhaps with a ""Why?"" box on each, to get testimonies on both sides). Basically focusing on the effects rather than the feelings. My 2 cents. (One last note in the meantime, jasnell made an informal Twitter poll pretty much about this about a year ago, while the results are incomplete with only 378 answers, it can help set expectations and perhaps give ideas about wording.)"
technical,"This isn't the time nor place to have this debate with you once more. You're incorrect, and I don't appreciate trying to have the last word on this argument after I told you I don't wish to debate it. Sounds like that's indeed the change you meant, then - I wasn't trying to rehash the debate, just trying to clarify what you were referring to. In that case, I think that the ecosystem benefited hugely from having a single authority for that, since it would have greatly fragmented the package ecosystem if many package authors had gotten the wrong idea and assumed peer deps were optional by default."
technical,"Except... That elephant is explicitly *not* in the room. Literally no one is suggesting that yarn2 is in any way being considered here Surely you understand that the assumption is that a project updates to latest versions of dependencies eventually. Without explicitly stating that Node will not in this case, users (and anyone just observing) will assume it eventually will be."
technical,"I appreciate the content there but it does not address my concern. The fact that Yarn v1 is frozen is a massive red flag to me as a 'new' dependency. IMO that context deserves to be held in the PR to add a dependency if it is a concern.  You assert that you expect people to stop using Yarn in favor of Berry, and have data to back that up. Further, you assert that the only changes you're planning on making are security changes - this is relatively well demonstrated by the backlog of PRs that are submitted solving issues that users are having but aren't being merged or addressed, including tiny ones like fixing broken links.  From the perspective of looking at Node.js over the next few years - rather than the short-term benefit of such a change - I frankly cannot see a reason why the Node.js project would take something on that's actively and intentionally being migrated off of as a result of the maintainers' choice in direction for it and isn't addressing problems end-users are facing outside of security issues. That's because you're deriving an incorrect premise, which is that Yarn 1 doesn't address problems end-users are facing. To reiterate:  **Yarn 1 isn't legacy** Yarn 1, as it is, is fine. It has its flaws, and some people would like more features, but, and that's critical, **its value comes from its stability**. People using Yarn 1.x right now have a project that already works. They may hit edge cases every once in a while, but the whole reason they use it is that they mostly figured out that the tradeoff was still value-positive for their current project. **Being frozen is a feature**, it's a gift that we are giving. It means that existing users won't get accidental regressions or bugs. **Yarn is about stability, and putting our users in control.** You shouldn't need to upgrade just because we release something new.  Yarn 2 is technologically better in many aspects. It would be easier for us to simply say that it should be the global binary at all time and that's it - similar to how npm is distributed. But different projects value different things, and in our case we care a lot about giving each user exactly the version they expect - whether it's Yarn 1, 2, 3, or 42. We care about it so much that we introduced a builtin feature for that three years ago, that we made it the default a year ago, and that we started the discussions around Corepack three months ago.  Of course it means that not all users are necessarily on the latest release. That's fine! Because they already have a tool that solves their needs right now, and because we'll never break semantics by redefining existing fields or ranges - as that would negatively affect past users by forcing them to upgrade. Now, perhaps later their use case will evolve, they'll need better perfs, or to solve a particular bug, or to implement something on their own, and they'll upgrade to the latest release, which will keep receiving regular bugfixes and new features. But perhaps not, and that's fine, because they will still follow an explicitly supported use case."
technical,"I think you're misinterpreting my comments in this thread. This is not Yarn's repo. I am not dismissing your work or the value of your project in the use cases you are aiming for. I am against Yarn 2 becoming a a built in feature of Node because of (1) poor performance and (2) incompatibility with the ecosystem, something that would be harmful for Node to endorse. As Yarn operates as an independent project I have no view on it, positively or negatively. I follow you on Twitter and think you're an insightful person. I never said there is an issue of trust of you or your team. I don't trust attitudes of  all people  whether in this thread or not not to change once a dependency is merged into core. I think it's perfectly valid to say that merging a dependency does increase the likelihood of a major version being introduced at a later time. I am not trolling by bringing up this concern. I presume that dependency updates are not all treating as one-off decisions in Node. You do not approach a V8 version update as an isolated decision ""should we upgrade this time everyone or just stick to the current version"", do you? I would presume that you approach it with the assumption that you will upgrade, unless there are reasons to not do so.  Merging Yarn 1 increases the likelihood of Yarn 2 being merged at a later date. And for that reason, Yarn 1 should not be merged into core. That's just not how the Node.js project works, all updates, as any change, need to go through a code review process. In the case of npm for example, we usually defer to members of npm who are also Node.js collaborators to review a PR updating the npm version bundled with Node.js. If this PR lands, I would expect that we would follow the same process: deferring reviews of PRs updating the bundled yarn version to members of the Yarn project who are also Node.js collaborators. So it is in fact a trust system between Yarn and Node.js. Thank you for expressing your concern, I think you made your point, please stop bringing it up so we can keep the discussion in this thread productive."
technical,"That's because you're deriving an incorrect premise, which is that Yarn 1 doesn't address problems end-users are facing. To reiterate:  **Yarn 1 isn't legacy** Yarn 1, as it is, is fine. It has its flaws, and some people would like more features, but, and that's critical, **its value comes from its stability**. People using Yarn 1.x right now have a project that already works. They may hit edge cases every once in a while, but the whole reason they use it is that they mostly figured out that the tradeoff was still value-positive for their current project. **Being frozen is a feature**, it's a gift that we are giving. It means that existing users won't get accidental regressions or bugs. **Yarn is about stability, and putting our users in control.** You shouldn't need to upgrade just because we release something new.  Yarn 2 is technologically better in many aspects. It would be easier for us to simply say that it should be the global binary at all time and that's it - similar to how npm is distributed. But different projects value different things, and in our case we care a lot about giving each user exactly the version they expect - whether it's Yarn 1, 2, 3, or 42. We care about it so much that we introduced a builtin feature for that three years ago, that we made it the default a year ago, and that we started the discussions around Corepack three months ago.  Of course it means that not all users are necessarily on the latest release. That's fine! Because they already have a tool that solves their needs right now, and because we'll never break semantics by redefining existing fields or ranges - as that would negatively affect past users by forcing them to upgrade. Now, perhaps later their use case will evolve, they'll need better perfs, or to solve a particular bug, or to implement something on their own, and they'll upgrade to the latest release, which will keep receiving regular bugfixes and new features. But perhaps not, and that's fine, because they will still follow an explicitly supported use case. That's not justification for being bundled in Node.js, imo. I can understand where you're coming from here, but this sounds like a pitch to me. It feels like justification of decisions without crediting the potential trade-offs or drawbacks of those decisions.  There's so much to deconstruct in this bit so rather than doing it in paragraphs, I'm going to try to summarize in bullets:  * ""It has its flaws"" there are bug reports and PRs to patch them that are going unmerged. This is a very... simple and kind way to frame this. * ""People using Yarn 1.x right now have a project that already works."" People using pnpm, bower, or jspm also have projects that work. If they're already using it and have a workflow that works for them, why do we need to include Yarn in Node.js? Why is yarn different from those other package managers? * ""They may hit edge cases every once in a while, but the whole reason they use it is that they mostly figured out that the tradeoff was still value-positive for their current project."" Same point as above. If they're already using it and have a workflow around that, what is the reason to add it to Node.js? * ""Being frozen is a feature, it's a gift that we are giving."" Framing a decision you've made as a ""gift"" implies that the recipient should be grateful. If they're not, that sets them up to be the villain. Frankly, I don't really think this framing has a space in this PR nor, honestly, to our users. * ""It means that existing users won't get accidental regressions or bugs. Yarn is about stability, and putting our users in control."" How is this different than permanently pinning to a package manager version across a company? I can use the first-ever published version of Yarn and expect the same, just like I can for npm, pnpm, Bower, or jspm. * ""You shouldn't need to upgrade just because we release something new."" Wasn't your previous assertion that you're only going to be doing security releases from now on?  I am entirely familiar with the arguments around projects being ""done"" and feature complete. I think that those reasons are entirely fine within userland. Where I think we need to be more critical of that is when we're committing it into Node.js in a way that is effectively irrevocable. Again, if they already have the tool why are we putting it in Node.js core? Doing so would  force  them to change their existing workflows - including involuntarily upgrading users who are on an older version - which seems to be antithetical to the stated goals of Yarn."
technical,"This isn't about Yarn 1 either - you neither use it, nor recommend it.  I use it often. My employer also uses it often, and I have worked with the people in those teams often. I've also helped advise on how we can better support it to make a seamless experience for Yarn users in multiple products for customers who do use Yarn.This isn't about bugs - you'd otherwise be at arms about some other things we won't list here. I have cited a concern around bugs, which you dismissed by saying ""Yarn 1, as it is, is fine. It has its flaws, and some people would like more features, but, and that's critical, its value comes from its stability."" As an SME, I trust and believe your input, but you seem to invalidate that as if it never happened.  This isn't about community - you certainly didn't talk to Yarn's users, nor even cared about their opinion. Throughout this I've been pinging friends, colleagues, and peers from across the ecosystem who are experienced Yarn users with my feedback before posting it.  If this is your concern, why didn't you bring it up earlier? Further, why are you solving this problem by adding more things rather than proposing that the alleged red flag and liability be removed? How does shipping Yarn meaningfully address this concern? This isn't about package management ecosystem - otherwise you'd listen to those actually writing those package managers (you may have missed it, but I'm not even the only one - another ghost is lurking this thread as well). A former maintainer of npm and a former maintainer of Yarn have commented -1 against this PR. Why are you ignoring their input?  What this is is that you believe that modern npm is as good as Yarn, and you don't see how this proposal would do you any good. To you, Yarn should die a fiery death to give back npm all its glory, and you have many amazing ideas to make that happen. It's fine, we all have opinions. I'm more an Emacs user myself, but whatever. The problem is that this, quite literally, isn't the discussion at hand. Right here, right now, we're talking about the existing state of the ecosystem.  This is an entirely unproductive comment that assumes bad faith on the part of those who've made objections to a proposal. For the better or worse, whether you like it or not, Yarn is used. That's it. It's really not up to you (or me) to decide whether its users are right or wrong. Nobody disagrees with this. Every single person I know, from Node.js TSC members to package maintainers to end-users to engineering managers, are happy that Yarn is used and that there is more than one option. This is also unrelated to Node.js and this PR. Nobody thinks others are ""wrong"" for their package manager choice, and it's frankly weird that that is being asserted here. (not only would Node tolerate a for-profit project, it would actively fight for it, do you still want to laugh at Deno?)  This is a weird remark. Several Node.js TSC members actively engage with Deno and there's a lot of hope that - like yarn did with npm - Deno can help move the ecosystem forward as a whole, no matter which platform end-users choose. Also, Deno is now a for-profit company using the open core model, in case you missed the news. In that news, they actively attacked how Node.js operates as an open governance project, despite our project leadership's eagerness to work with them. Perhaps not the best example.  Frankly, the question really isn't ""should Node add Yarn"" - it's rather ""can Node prone inclusivity by persisting in providing a subpar developer experience to a specific set of people, depending on the tool they use?"". I don't understand what you're saying here? As a side note, out of the 1000 most depended packages on the npm registry, 216 definitely use Yarn, and 291 use npm (the rest being undeterminable). Your point here is that a majority of projects in the top 1000 most depended packages in the npm registry are undeterminable? The conversation here (on all sides) is devolving into less-than-productive discourse. The question is up for consideration and vote by the TSC, and ""adding Yarn1"" is not the only option. There are other choices on the table being discussed (such as the corepack option). The feedback already discussed here has just established that there is not a clear consensus. (Yes, there are a good number of folks who have disagreed with the idea of adding yarn1 to the distribution but there are a enough who have spoken up in favor that we cannot ignore them)... therefore the question is going to the TSC to resolve.  Further back-and-forth around this is not going to serve the project or community well, so at this point, let's allow the TSC time to weigh the choices. For those who are not aware, the alternative corepack option is to ship a utility that installs ""jumpers"" to the actual binaries. That is, it would install a yarn command-line that would on-demand download and install the yarn binary only if invoked and the user explicitly opts-in. It would also have the ability to install pnpm as well -- but the actual distributions of those package managers would not be included. I (and I believe arcanis if he'll allow me to speak for him here) definitely prefer the corepack approach as opposed to bundling yarn1 directly. The vote that the TSC will be considering will also cover a decision on corepack."
technical,"Thank you for the clarification it helps a lot, and does make me feel better. My concern was because I saw PRs to Yarn 1 being closed because the build breaking bugs fixed by them were addressed in Yarn 2. I don't expect you to drop everything and have core members continue patching Yarn 1. However, I would expect you to continue to accept community provided fixes for outstanding problems. I get the desire not to do this, and as a stand alone project I thought you were totally justified in just saying ""no, we've moved on"". But as something that's going to be bundled with Node and expand the core ecosystem I would expect more I guess. The elephant in the room here is Yarn 2. Yarn 2 should **never** be merged into Node core. For those that aren't aware, Yarn 2 is effectively a soft fork of Node.js. It works by replacing the node cli with its own that runs a ** 10,000 line of code** file before any script you attempt to run. This file monkey patches the module system replacing the Node module resolution logic with its own.  Aside from slowing down startup on scripts, this also results in breakage of packages (as some usage of core modules breaks in Yarn projects). I don't think Node should support an environment where some ecosystem packages don't work.  It would be a major mistake to merge this without one of the following conditions:  1. Agree to never merge Yarn 2 (in which case I don't understand merging Yarn 1 either) 2. Yarn agrees to stop monkey patching the runtime. (Node should probably address their use-case for doing so, but that's another matter).  As (1) and (2) seems unlikely, I don't think this should be merged."
technical,"The whole ""should we add yarn or not"" argument is kind of missing the forest for the trees. It would be really nice to see node move towards not being so heavily dependent on needing  any  package manager, not adding more of them. And then the inevitable ""enterprise"" package manager manager meta stuff that will creep in with it. The proposed yarn addition are not the sources but a result after it was 'built'. In other words, the javascript here is not the preferred method of editing but just distribution.. This would make it difficult or impossible to patch with upstream cherry-picks."
technical,"The conversation here (on all sides) is devolving into less-than-productive discourse. The question is up for consideration and vote by the TSC, and ""adding Yarn1"" is not the only option. There are other choices on the table being discussed (such as the corepack option). The feedback already discussed here has just established that there is not a clear consensus. (Yes, there are a good number of folks who have disagreed with the idea of adding yarn1 to the distribution but there are a enough who have spoken up in favor that we cannot ignore them)... therefore the question is going to the TSC to resolve.  Further back-and-forth around this is not going to serve the project or community well, so at this point, let's allow the TSC time to weigh the choices. For those who are not aware, the alternative corepack option is to ship a utility that installs ""jumpers"" to the actual binaries. That is, it would install a yarn command-line that would on-demand download and install the yarn binary only if invoked and the user explicitly opts-in. It would also have the ability to install pnpm as well -- but the actual distributions of those package managers would not be included. I (and I believe arcanis if he'll allow me to speak for him here) definitely prefer the corepack approach as opposed to bundling yarn1 directly. The vote that the TSC will be considering will also cover a decision on corepack. The whole ""should we add yarn or not"" argument is kind of missing the forest for the trees. It would be really nice to see node move towards not being so heavily dependent on needing  any  package manager, not adding more of them. And then the inevitable ""enterprise"" package manager manager meta stuff that will creep in with it."
technical,"Everyone who disagrees with me doesn't know what they are talking about This isn't about Yarn 1 either - you neither use it, nor recommend it.  I use it often. My employer also uses it often, and I have worked with the people in those teams often. I've also helped advise on how we can better support it to make a seamless experience for Yarn users in multiple products for customers who do use Yarn.This isn't about bugs - you'd otherwise be at arms about some other things we won't list here. I have cited a concern around bugs, which you dismissed by saying ""Yarn 1, as it is, is fine. It has its flaws, and some people would like more features, but, and that's critical, its value comes from its stability."" As an SME, I trust and believe your input, but you seem to invalidate that as if it never happened.  This isn't about community - you certainly didn't talk to Yarn's users, nor even cared about their opinion. Throughout this I've been pinging friends, colleagues, and peers from across the ecosystem who are experienced Yarn users with my feedback before posting it.  If this is your concern, why didn't you bring it up earlier? Further, why are you solving this problem by adding more things rather than proposing that the alleged red flag and liability be removed? How does shipping Yarn meaningfully address this concern? This isn't about package management ecosystem - otherwise you'd listen to those actually writing those package managers (you may have missed it, but I'm not even the only one - another ghost is lurking this thread as well). A former maintainer of npm and a former maintainer of Yarn have commented -1 against this PR. Why are you ignoring their input?  What this is is that you believe that modern npm is as good as Yarn, and you don't see how this proposal would do you any good. To you, Yarn should die a fiery death to give back npm all its glory, and you have many amazing ideas to make that happen. It's fine, we all have opinions. I'm more an Emacs user myself, but whatever. The problem is that this, quite literally, isn't the discussion at hand. Right here, right now, we're talking about the existing state of the ecosystem.  This is an entirely unproductive comment that assumes bad faith on the part of those who've made objections to a proposal. For the better or worse, whether you like it or not, Yarn is used. That's it. It's really not up to you (or me) to decide whether its users are right or wrong. Nobody disagrees with this. Every single person I know, from Node.js TSC members to package maintainers to end-users to engineering managers, are happy that Yarn is used and that there is more than one option. This is also unrelated to Node.js and this PR. Nobody thinks others are ""wrong"" for their package manager choice, and it's frankly weird that that is being asserted here. (not only would Node tolerate a for-profit project, it would actively fight for it, do you still want to laugh at Deno?)  This is a weird remark. Several Node.js TSC members actively engage with Deno and there's a lot of hope that - like yarn did with npm - Deno can help move the ecosystem forward as a whole, no matter which platform end-users choose. Also, Deno is now a for-profit company using the open core model, in case you missed the news. In that news, they actively attacked how Node.js operates as an open governance project, despite our project leadership's eagerness to work with them. Perhaps not the best example.  Frankly, the question really isn't ""should Node add Yarn"" - it's rather ""can Node prone inclusivity by persisting in providing a subpar developer experience to a specific set of people, depending on the tool they use?"". I don't understand what you're saying here? As a side note, out of the 1000 most depended packages on the npm registry, 216 definitely use Yarn, and 291 use npm (the rest being undeterminable). Your point here is that a majority of projects in the top 1000 most depended packages in the npm registry are undeterminable?"
technical,"I am -1 based on the fact that Yarn v1 (a.k.a. anything other than Berry) is frozen. That does not sound like a dependency that we should be taking on imo.  The 1.x line is frozen - features and bugfixes now happen on this. This point is discussed (and answered) there. As the maintainer of both release lines I don't mind discussing it further, but I think this linked thread would be a better opportunity to discuss these topics than an implementation PR - both for ergonomic reasons and because it's already been answered.  Just to be clear, I don't mind waiting for a TSC evaluation. I've only contributed to Node a couple times, and processes / ownerships are still a bit fuzzy to me. I'm happy to wait for the decision of whoever group is in charge of this kind of work. At least the implementation exists and provides a concrete picture of what the outcome would be."
technical,A reminder for the TSC folks that consulting our Technical values and priorities may be helpful in informing your vote on this issue. TSC vote concluded that we would not include yarn directly and agreed on moving towards Corepack as the way forward instead. See this issue for results of the vote. Closing this as we will not land the PR. Please let us know if you think that was not the right thing to do.
technical,"The elephant in the room here is Yarn 2. Yarn 2 should **never** be merged into Node core. For those that aren't aware, Yarn 2 is effectively a soft fork of Node.js. It works by replacing the node cli with its own that runs a ** 10,000 line of code** file before any script you attempt to run. This file monkey patches the module system replacing the Node module resolution logic with its own.  Aside from slowing down startup on scripts, this also results in breakage of packages (as some usage of core modules breaks in Yarn projects). I don't think Node should support an environment where some ecosystem packages don't work.  It would be a major mistake to merge this without one of the following conditions:  1. Agree to never merge Yarn 2 (in which case I don't understand merging Yarn 1 either) 2. Yarn agrees to stop monkey patching the runtime. (Node should probably address their use-case for doing so, but that's another matter).  As (1) and (2) seems unlikely, I don't think this should be merged. Well yes, that's what we've been saying since the beginning. If you read the thread (for example my very last message) you'll also get one of the reasons why it still matters."
technical,"What will the planned future of the Yarn v1 be? Every project, which was once feature freezed, is to be sunset. There is v2 and there may be  v3, etc. The time will come when the user group is very small or 0. Do you have any metrics which shows the trend of the v1 usage?  Node.js has a release calendar that makes changes planable. A lifetime plan for Yarn v1 is needed too, specially in the current case when it can be one of the elements of the Node.js installer.  Honestly, I would like to remove it from the official docker images, too. The time will come when the question is raised: - keep Yarn v1 and add v2 to the docker image - replace Yarn v1 with v2 Sorry for the extra topic I just wanted to highlight this question, too. What do we need to do at this point to get this moving forward? I'd like to see this move forward."
technical,"with respect to your earlier questions, this issue is now open to work on/agree on what a vote should look like Which is exactly why I included this caveat. There were suggestions in that RFC that you expressed support for. I don't believe consensus is impossible. **Are you suggesting that the Node Foundation will be taking on responsibility for updating Yarn1 in the future, if it's included? If that were the case then my concerns would be answered.** Right now the problem is, if you have to patch Yarn1 you're SOL. You have to support floating patches forever and the version users can install independently diverges from the one bundled with Node.  Surely that is true, and also not afaict relevant?"
technical,"Which is a very interesting example, thanks for bringing that up. The RFC you're talking about has received strong objections from both Yarn and pnpm - objections which from what transpire from the issue don't plan to be addressed. In other words, npm being the only package manager is already putting the ecosystem at risk (at least under their current handling of RFCs). Also note that this isn't a unique occurence - in fact, it already happened a few months ago with npm 7, which tried to change how peer dependencies are semantically defined. Just like now, the RFC received early and strong pushback from both Yarn and pnpm (and other projects), but the RFC author decided to ignore them and go with it. The lack of package managers diversity (and thus neutrality) is a problem. The role of the Node TSC is to find a solution. This PR (along with others) is a way to do that. If you have a better one go for it? I don't mind seeing competing ideas. Which peer dep RFC are you referring to? (peer deps have always been semantically required, for example, but i'm not sure if that's what you're referring to)"
technical,"only rational reply in this godforsaken discussion thread. npm is an optional dependency on various linux distros such as Arch, I think it's better idea to push for yarn to be a optional dependency on  distro packages rather than core nodejs. While the decision is up for consideration by the TSC, I'm going to go ahead and lock the conversation here to just collaborators. While discussion on a topic is always a good thing, there's really not much more to be said here that's going to add to the discussion. Please allow the TSC time to review and decide what action will be taken, if any."
technical,"...having a single authority for that  That depends entirely on the point of view and on how accountable that authority is to the community. Even with the RFC process, npm's roadmap is still controlled by a singular commercial interest. Perhaps the better forum for such discussions would be the OpenJS Foundation, where a spec and reference implementation for such things could live while allowing competition on specific implementations. But... that's a separate conversation.  iarna does raise a valid point about ensuring that bugs that need to be fixed in the 1.x stream are fixed in 1.x. As I understand it, there's no intent on shipping yarn2 in core which means we cannot interpret ""fixed in 2"" as being meaningful here. arcanis addresses that concern rather well here. So long as there is a demonstrated commitment to ensuring that security and critical bug fixes are made, and that yarn1 remains functional/compatible with Node.jslatest, then I'm not sure what else can be said there that would be convincing. Specifically, no one has asserted that the yarn versions shipped in node.js would be ""unupdatable"".  I would not expect Node.js to just accept that unquestioningly. I would expect Node.js to hold Yarn accountable to that promise that the issues would be addressed, and if it turns out the issues are not addressed and major bugs persist, then we would always have the option of removing yarn from the distribution.  iarna also said:  ...if that lands in a way that's acceptable and adopted the major package managers still under development, it would be very uncomfortable for Node itself to be shipping with one that didn't support it.  But that's not the case here. Node.js would not just be shipping a single package manager option that did not support the feature, it would continue shipping multiple package manager options. It would be up to users to determine what they want to use -- and whether support for that new feature is important to them or not. The users would give us the necessary feedback to determine which is the right path. That said, it would be rather unfriendly to the ecosystem as a whole for a single package manager run by a singular commercial interest to unilaterally ship a new feature that would qualify as a ""must support"" by others. with respect to your earlier questions, this issue is now open to work on/agree on what a vote should look like"
technical,"Yes, I do, if that's the expectation going in and we message it responsibly.  I feel like we've learned in the past that people don't listen to what we tell them. This has also been a... consistent experience throughout the ecosystem, whether it be at the language level, the platform level, or the tooling level. I'm surprised that there's an assertion that we should decide rely on that now. Without actually weighing in yes/no, I wanted to add some (IMO very important) background info to the conversation that I don't think has been clearly stated anywhere in this thread so far:  Yarn v1 is both a runtime and install-time dependency of Yarn v2. Edit: slightly more accurate: **Yarn v2 installs are meant to be managed by Yarn v1**. (In more details: Yarn v2 is meant to be installed via Yarn v1. Yarn v2 is meant to be installed locally on a per-project basis -- Yarn v2 is not designed to be installable globally. The binary you hit first when running a yarn command is the Yarn v1 binary. Yarn v1 redirects to run Yarn v2 if the project has set up Yarn v2 locally.)  So, the notion of including ""Yarn v2 instead of Yarn v1"" isn't technically doable with the current design of Yarn v1/v2.  I think that's also a big part of the background info needed to explain why including Yarn v1 with Node would make any sense, and why not including Yarn v2 with Node would make any sense.  From the perspective of a potential Yarn v2 user, and given the current designs of Yarn v1 & v2, this proposal is as close as it gets to ""including Yarn v2 with Node"". (And if I understand correctly, this proposal would seamlessly provide the closest thing to ""including Yarn v3 with Node"" as soon as/if/when that Yarn release exists.)  **EDIT/correction?**: Placing the Yarn v2 binary (having downloaded it with Yarn v1) at my usual global package install location as yarn does seem to yield a usable package manager. It's not the way the Yarn v2 docs instruct users to do things, though. So to be strictly accurate, Yarn v2 appears to be usable without Yarn v1. There's no apparent technical barrier stopping this workflow. But it's not supported by Yarn maintainers, I suppose. It appears to be a minified/bundled file, though. So not the usual global package install. The missing piece of the ""user story"" here is there's no easy way to update ""globally installed Yarn v2"" because Yarn v2 isn't officially meant to be globally installed, and it's not on the registries as a unified package, that I know of."
technical,"For those that aren't aware, what you're actually talking about is Yarn PnP, Yarn 2, just like Yarn 1, supports both PnP and node modules. Yarn doesn't replace the node cli, we simply add the require hook, if you're not a fan of that use the node modules linker. Yarn 2 by default does not install to node modules/ and does not work with the the node CLI. I verified this 30 minutes ago using the guides on the website."
technical,"I would expect Node.js to hold Yarn accountable to that promise that the issues would be addressed, and if it turns out the issues are not addressed and major bugs persist, then we would always have the option of removing yarn from the distribution. I can't imagine you reasonably expect us to be able to remove Yarn at some point in the future if it's merged into core? We've seen this time and again - once something gets into core, it's effectively impossible to remove without breaking a massive amount of the ecosystem. I'm curious how this example - especially with the scale of its impact - would be any different?   This  is the root of the reason I think it make zero sense for Yarn v1 to go into Node.js core. Whether it's in a year when we decide for some reason that Yarn v1 isn't a match, or if it's in 7 years when it's gotten zero updates and is one of the Regrets from a current TSC member because we're permanently checking in technical debt, there's no chance we're ever able to remove this without causing massive ecosystem pain. If Yarn v1 were being actively developed and improved, I think this PR would make sense. Thinking about our future selves and our users future selves, I can't find any avenue to understand how one could come to the conclusion that checking in ""frozen"" (I still take issue with the phrasing of this - it's effectively EOL, outside of a commitment by  current maintainers  to provide security fixes  indefinitely ) code is a good choice  if  enhancing the experience of our users is the only motivation,  especially  given that - at least to me - there's zero chance of ever reversing the decision. Yes, I do, if that's the expectation going in and we message it responsibly."
technical,"As someone that has been using Node.js heavily for years, I think adding Yarn 1 to Node.js would be a huge mistake.  - Modern npm is as good as Yarn 1. - Yarn 1 has a lot of critical bugs that will never get fixed. - It would better to focus on npm which is actively maintained. *(Promising to fix critical security bugs does not mean Yarn 1 is actively mainained)* - It will hinder package management evolution, as Yarn 1 will lag behind and block package maintainer from using new features since many users are on Yarn 1. (If it's not in core, package maintainers can use that as an argument, but if it's in core (""blessed""), users will demand support) - It will confuse beginners why there are multiple package managers. - It will add more maintenance work for Node.js core team without any clear benefit. It's easy enough to install Yarn for the ones that want it. Yes, I do, if that's the expectation going in and we message it responsibly.  I feel like we've learned in the past that people don't listen to what we tell them. This has also been a... consistent experience throughout the ecosystem, whether it be at the language level, the platform level, or the tooling level. I'm surprised that there's an assertion that we should decide rely on that now."
technical,"Being able to author a PBR model in Substance Painter or download one from Sketchfab, and have the model appear as the artist designed it, is good for three.js users. I don't think there's any definition of ""best result"" where that sort of consistency can be dismissed. This isn't reinventing any wheels, and it isn't a design flaw. PBR calculations are done in linear space, with every engine I'm aware of. If you pass sRGB data into the renderer and pretend it's linear, the lighting and blending math will come out wrong. The difference is not huge, and so this not a major concern for many three.js users, but nevertheless it is not as good as it could be. For that reason, your ""good"" result example is not actually correct. But as you've said before, backward-compatibility is important, so I'm not here to advocate for changing any three.js defaults. But because glTF is a new format, and because we're trying to get PBR right, we're going to mark sRGB textures as sRGB, even if other loaders are not doing so.  ...textures that are used for color information should have the sRGB flag checked, and textures that are used for masks and numerical calculations in shaders and effects (like Normal maps) should have it unchecked. And if you follow this simple guideline you mostly get the best effect.  This is precisely what we are doing. ...I'm pretty confident that the large majority of three.js users are passing sRGB colors into three.js without converting, despite the fact that renderer lighting calculations assume linear colorspace  You are talking about PBR materials with glTF, but I assume this is just as much a problem with a Phong material?   ...because glTF is a new format, and because we're trying to get PBR right, we're going to mark sRGB textures as sRGB, even if other loaders are not doing so.  should other loaders be doing so? It seems like this inconsistency between loaders is a point of confusion for users, and it would make sense for all of them to treat sRGB textures the same way if possible."
technical,"Thanks for reporting this. Some notes...  **File size** I'm not sure where you're getting these numbers, this is what I see: **Materials** Your model uses Diffuse + Specular. Unfortunately, seems like the specular texture is not being exported. GLTF supports 2 PBR modes: Metalness + Roughness and Specular + Glossiness. You want to export your model using the second mode. GLTFLoader supports both but maybe the Babylon.js doesn't have an option to export in that mode? In that case you may want to do a feature request on their project.  Let us know what you find out. 1. File size. If you download the files (with Firefox) you'll get: Even more: the GLB file does not even contain the equivalent (Metalness) of the Specular texture !  2) Materials As I said before, the original MAX  and FBX files contain a standard PHONG material: Diffuse + Specular + Normal None of the current GLTF exporters and converters is able to generate a correct Physical material. PHONG is way more popular than Physical.  p.s. I'm not interested in the GLTF format (SEA3D is better from any point of view). I just want to help other poeple."
technical,"I have no idea what you are talking about. I've never marked yours or anyone else's comments as disruptive.  However, I would like to say that, much as I have a thick skin and don't care about your opinions re usernames, ad hominem attacks of the kind that you frequently make are not acceptable and should be grounds for being excluded from this community. agreed, that would be a very useful addition."
technical,"1. File size. If you download the files (with Firefox) you'll get: Even more: the GLB file does not even contain the equivalent (Metalness) of the Specular texture !  2) Materials As I said before, the original MAX  and FBX files contain a standard PHONG material: Diffuse + Specular + Normal None of the current GLTF exporters and converters is able to generate a correct Physical material. PHONG is way more popular than Physical.  p.s. I'm not interested in the GLTF format (SEA3D is better from any point of view). I just want to help other poeple. Another userful sample: Here is a NATIVE Physical material in 3DS Max 2018 exported to GLTF with Babylon3D exporter: Now the metalness is present, but the result is still wrong: - it looks emissive (but it's not) - it has a red color bump  However, this GLB file looks better (compared to THREE) in the Babylon sandbox. Again, this is how the Physical material should look in THREE. So let's forget now about PHONG and buggy exporters and converters. We should investigate why the GLB file looks better in the Babylon sandbox. Buggy GLTF Loader in THREE ?"
technical,It's the best place for sure and it shows the current GLTF status. Really? Here is a statement from Trump: Google already failed with UTF8. Many of them blame THREE.js for bad GLTF results. The posted samples prove they are wrong. are you suggesting any particular actions we should take? If not I vote to close this issue.
technical,"PHONG looks great with Diffuse + Specular only. PBR does not look great with BaseColor + MetallicRoughness only. That's the main problem here. The Normal/Bump and Environment textures are irrelevant at this point. On top of that: more texture layers = bigger file size and performances drop That was a wise decision.  I'm not debating the ""PBR only"" bad choice for GLTF. This topic shows GLTFLoader design flaws. We don't reinvent the wheel here, so ""renderer.gammaOutput = true"" is not an option now, when GLTF represents less than 1% of the market. cheers Because you asked me to: The quality drop is obvious. Also, in 3DS Max 2018 the Physical material looks WAY better than the GLTF result."
technical,"None of these issue are specific to glTF (see #11337), but with GLTFLoader we're trying to achieve consistency with other engines and 3D authoring environments,  That's completely wrong Don! Here we're talking about GLTFLoader and THREE.js The users are interested in the best results with THREE. Other engines and 3D authoring environments are irrelevant here.  And you still don't understand the main problem here. For the last time:  1) JSONLoader (or SEA3D loader) + PBR = GOOD results (close to Phong):  2) GLTFLoader + PBR = BAD results p.s. In the first sample you don't even need to get good results !!! Being able to author a PBR model in Substance Painter or download one from Sketchfab, and have the model appear as the artist designed it, is good for three.js users. I don't think there's any definition of ""best result"" where that sort of consistency can be dismissed. This isn't reinventing any wheels, and it isn't a design flaw. PBR calculations are done in linear space, with every engine I'm aware of. If you pass sRGB data into the renderer and pretend it's linear, the lighting and blending math will come out wrong. The difference is not huge, and so this not a major concern for many three.js users, but nevertheless it is not as good as it could be. For that reason, your ""good"" result example is not actually correct. But as you've said before, backward-compatibility is important, so I'm not here to advocate for changing any three.js defaults. But because glTF is a new format, and because we're trying to get PBR right, we're going to mark sRGB textures as sRGB, even if other loaders are not doing so.  ...textures that are used for color information should have the sRGB flag checked, and textures that are used for masks and numerical calculations in shaders and effects (like Normal maps) should have it unchecked. And if you follow this simple guideline you mostly get the best effect.  This is precisely what we are doing."
technical,"Good catch! :thumbsup:   If you set visible to false to the imported AmbientLight the character starts to look less red.  Something better (no wasted resources):  That indicates buggy GLTF Loader (and it has to be fixed). If I use that for other loaders, I get wrong colors for the loaded models. Just think about this scenario: use various loaders for the same scene and one of them is GLTF. It will mess up the entire scene!  Anyway, after removing that ambient light and using this workaround, this is the result: Much better compared to the initial GLTF one, but the material quality is far away from this one: Impressive lighting response and great metalness for bra and bikini.  At this stage I won't use GLTF in any serious project. Can you try adding a envMap cubemap to these examples too? PBR looks the best when a envMap is supplied."
technical,"I've added offline. Of course it looks better, but PHONG with Environment map still looks WAY better (in THREE). Also, the envMap does not fix the GLTF Loader issue(s). Could you update the online samples?"
technical,"Seems like the glb includes a AmbientLight. If you set visible to false to the imported AmbientLight the character starts to look less red.  The last thing to do is setting renderer.gammaOutput = true. (Needed when using GLTF #12766) Good catch! :thumbsup:   If you set visible to false to the imported AmbientLight the character starts to look less red.  Something better (no wasted resources):  That indicates buggy GLTF Loader (and it has to be fixed). If I use that for other loaders, I get wrong colors for the loaded models. Just think about this scenario: use various loaders for the same scene and one of them is GLTF. It will mess up the entire scene!  Anyway, after removing that ambient light and using this workaround, this is the result: Much better compared to the initial GLTF one, but the material quality is far away from this one: Impressive lighting response and great metalness for bra and bikini.  At this stage I won't use GLTF in any serious project."
technical,"That bad choice is yours, but the GLTFLoader is part of THREE, so it's up to Ricardo ( mrdoob ) if they will be removed or not.  Errr.... this is extremely fuzzy. GLTFLoader appears to be a community provided example, that lives in /examples. If you load three.js alone (three.min.js) there wont be any mention of GLTF.  If you use three.js off the shelf you get a scenegraph and various webgl abstractions. In this context, GLTF is just another one of many many examples of how you can translate some generic 3d data / scene file into three.js's structs.  So, at a glance, three.js seems like this generic library, that draws stuff to screen. It doesn't care if you fetch that data from some remote server, and it doesn't care how you parse it (collada, fbx, sea3d, gltf... and 40 others). At the end of the day, you are rendering a THREE.Mesh with THREE.Geometry and THREE.Material. **Absolutely all of the loaders share this feature. All of them result in this data structure.**  However, in practice, this is not the case, and you seem to be in the right.  GLTF is a ""first class citizen"" of three.js. mrdoob wrote that several times. SEA3d is some **random format** written by **some unknown people** while GLTF has the backing of THE khronos group. On top of that, it's probably meant to be the backbone of the whole VR/AR revolution, hence so much backing by other big players.  This is just an unfortunate circumstance that three.js found itself in as the most widely used webgl library. People want to do 3d, which three.js solves really well with it's scene graph and other abstractions (things like Mesh, Geometry, Material, Texture etc.). Unfortunately, people also want to make experiences and expect three.js to be able to do that. This is of course complicated, hence, favoring one particular format and giving it preferential treatment makes sense. It may somewhat hurt the very essence of the library (draw stuff on screen) but it's a trade off.  If you care to read all the guidelines, there's a document called owners. donmccurdy owns that particular loader, so his word carries as much weight as mrdoob's, so good luck there :)  The directive right now is:  So anything that the khronos group comes up with has to be reflected in three.js. If you look at the discussion historically, mrdoob doesn't really follow what's going on and donmccurdy is the authority on all things khronos/gltf related.  So for example, khronos has defined a specification for gltf ""extensions"". Out of infinitely many extensions, one involves texture transformations. It has been ratified by the khronos group and because of that, **three.js absolutely must support it**. This warrants a PR with **11 thousands line of code** and increasing the size of the library by 1/3.  I think you're wasting a lot of effort head butting a wall here, and that this issue should be closed. good point As I said before, Google already failed with UTF8. In the current development status, GLTF (and PBR) follow the same path."
technical,"Coming from Linux, it was confusing to ctrl-v in vim and see my last pasta. Had to google on how to get the standard behavior. I agree with this, and will start to be more vigilant in marking disruptive or otherwise useless comments. In this case, since I was the one being targeted, I didn't think it was appropriate to do so.  However, RemusMar has consistently exhibited what can only be described as disruptive behaviour over a long period of time and I don't think that marking comments is going to bring about any kind of change.  We're consistently having to deal with ad hominem attacks  from this user in the form of name calling or other vaguely aggressive statements, and it's quite frankly tedious and detrimental to the development of the three.js community."
technical," I don't think this repo is the right place for this post. It's neither a feature request, nor a bug. So my question is: What are you trying to accomplish with this issue? Bashing glTF?  If you encounter problems with an exporter or converter, I suggest you open an issue at the respective github repo.   In any case, PBR was a bad choice for GLTF and also, all the current converters are collection of bugs.  I generally reject such Trump-like statements. They have a provocative nature and are not objective at all."
technical,"I think the documents wouldn't hurt by having some guideline on how to author textures. The texture can still be in the wrong space regardless of the gltf format, like under calibrated or over calibrated. Something saying ""when making a texture in photoshop do that, when making it in GIMP do this"". I have no idea what you are talking about. I've never marked yours or anyone else's comments as disruptive.  However, I would like to say that, much as I have a thick skin and don't care about your opinions re usernames, ad hominem attacks of the kind that you frequently make are not acceptable and should be grounds for being excluded from this community."
technical,"Wow, I never say that before. And apparently I ""own"" the FBXLoader  ˜ ˜ I think it's a software engineering term. You ""own""  a code base?"
technical,"That right there is disruptive content... :( You're free to fork the project, the examples and do whatever work you deem fit. If it makes you feel any better, what the fbx loader owner did is not at all different - the loader did exist for a couple of years and was written by other people before he took ownership. Fork's no different than this. I think the documents wouldn't hurt by having some guideline on how to author textures. The texture can still be in the wrong space regardless of the gltf format, like under calibrated or over calibrated. Something saying ""when making a texture in photoshop do that, when making it in GIMP do this""."
technical,"agreed, that would be a very useful addition. I'm the one who was doing this. As collaborators we have to moderate issues and ensure correct language and behavior."
technical,"Can you try adding a envMap cubemap to these examples too? PBR looks the best when a envMap is supplied. I've added offline. Of course it looks better, but PHONG with Environment map still looks WAY better (in THREE). Also, the envMap does not fix the GLTF Loader issue(s)."
technical,"Just removed the FORCED sRGB encoding in the GLTFLoader. The result is better from any point of view: - better lighting and material quality - no need of ""renderer.gammaOutput = true"" anymore (a bad idea anyway) - now you can use the GLTFLoader with other loaders for the same scene (renderer) I've addressed each of those points in this. we will not be removing the sRGB encoding assignment to sRGB textures in GLTFLoader. If you would like to override that, it is easy to change the texture encoding after loading the model.  If there are no other actions to take here, this issue should be closed."
technical,"Another userful sample: Here is a NATIVE Physical material in 3DS Max 2018 exported to GLTF with Babylon3D exporter: Now the metalness is present, but the result is still wrong: - it looks emissive (but it's not) - it has a red color bump  However, this GLB file looks better (compared to THREE) in the Babylon sandbox. Again, this is how the Physical material should look in THREE. So let's forget now about PHONG and buggy exporters and converters. We should investigate why the GLB file looks better in the Babylon sandbox. Buggy GLTF Loader in THREE ? If one could quickly prototype some hacks over the existing phong / standard implementations i bet it would be pretty useful ˜"
technical,"So in your opinion you're free to mess it up, isn't it? In my opinion everyone who owns something is free to mess it up. I'm not sure if there should be place for disagreement with this, in the modern world :). If you have a Ferrari and decide to put a brick on the gas pedal and have it drive off a cliff, i may think it's a waste but it's YOUR Ferrari, can't get simpler than that. Unless it's 1917 and you're in tsarist Russia."
technical,"I don't have enough spare time for ""GitHub activities"". You got the report and the working samples. That's all for now. cheers In that case, I still don't consider this to be a complete or actionable bug report and I continue to vote to close this issue."
technical,"I don't think this repo is the right place for this post. It's neither a feature request, nor a bug. So my question is: What are you trying to accomplish with this issue? Bashing glTF?  If you encounter problems with an exporter or converter, I suggest you open an issue at the respective github repo.   In any case, PBR was a bad choice for GLTF and also, all the current converters are collection of bugs.  I generally reject such Trump-like statements. They have a provocative nature and are not objective at all. It's the best place for sure and it shows the current GLTF status. Really? Here is a statement from Trump: Google already failed with UTF8. Many of them blame THREE.js for bad GLTF results. The posted samples prove they are wrong."
technical,"Please do. That's better than resume the conversation in this closed thread. Just removed the FORCED sRGB encoding in the GLTFLoader. The result is better from any point of view: - better lighting and material quality - no need of ""renderer.gammaOutput = true"" anymore (a bad idea anyway) - now you can use the GLTFLoader with other loaders for the same scene (renderer)"
technical,"Because you asked me to: The quality drop is obvious. Also, in 3DS Max 2018 the Physical material looks WAY better than the GLTF result. None of these issue are specific to glTF (see #11337), but with GLTFLoader we're trying to achieve consistency with other engines and 3D authoring environments,  That's completely wrong Don! Here we're talking about GLTFLoader and THREE.js The users are interested in the best results with THREE. Other engines and 3D authoring environments are irrelevant here.  And you still don't understand the main problem here. For the last time:  1) JSONLoader (or SEA3D loader) + PBR = GOOD results (close to Phong):  2) GLTFLoader + PBR = BAD results p.s. In the first sample you don't even need to get good results !!!"
technical,"The glTF format supports PBR and unlit shaders at this time. Whether the BabylonJS and FBX2GLTF tools do the Phong-to-PBR conversion in a way that preserves Phong specular maps, I don't know ” that would be a question for the repos of those tools. If you are trying to preserve the exact appearance of models using classic Phong shaders, you may have an easier time with other formats.  This is a deliberate decision and not a bug. Base color and emissive textures in glTF (and, typically, diffuse textures in any format...) are in sRGB colorspace. GLTFLoader marks them as such, so that they're converted to linear colorspace for correct PBR lighting calculations. Finally colors should be converted back to sRGB (e.g. renderer.gammaOutput=true).  If you skip all of this, with any format, lighting calculations are incorrect. SEA3DLoader, FBXLoader, and ColladaLoader never touch the .encoding property of any texture, and leave it to the end user to change texture colorspace and renderer colorspace. I'm pretty confident that the large majority of three.js users are passing sRGB colors into three.js without converting, despite the fact that renderer lighting calculations assume linear colorspace, and getting results that are ""good enough"" but inconsistent with other engines and authoring environments. For correct results you should be using renderer.gammaOutput=true, and marking sRGB textures as sRGB.  None of these issue are specific to glTF, but with GLTFLoader we're trying to achieve consistency with other engines and 3D authoring environments, and have chosen to treat all sRGB textures as sRGB for a first step. If you're mixing models from other formats in the scene, then yes it's awkward, and you'd need to either mark the diffuse textures of those formats as sRGB or mark the colorspace on the glTF models to linear (the latter is incorrect for all model formats involved, but may look good enough if you don't need precise colors).  It does not seem like there is anything actionable here, unless there are specific issues we can report to the tools mentioned. RemusMar if you are happy with your SEA3D workflow, that's great ” I'm not interested in debating formats or persuading you to change from something that is already working well for you. PHONG looks great with Diffuse + Specular only. PBR does not look great with BaseColor + MetallicRoughness only. That's the main problem here. The Normal/Bump and Environment textures are irrelevant at this point. On top of that: more texture layers = bigger file size and performances drop That was a wise decision.  I'm not debating the ""PBR only"" bad choice for GLTF. This topic shows GLTFLoader design flaws. We don't reinvent the wheel here, so ""renderer.gammaOutput = true"" is not an option now, when GLTF represents less than 1% of the market. cheers"
technical,"Yeah, it's unfortunate but I agree that it's not worth breaking backwards compatibility over this.  However, as I've been working with larger FBX scenes consisting of multiple models, animated cameras and so on I've found myself wishing that the output of the loader was something more like GLTFLoader's output - that is, it should return an fbx object with properties:  There may be other loaders that would benefit from a similar change.  We should add this to the backburner (and certainly wait on #11337), but if any loaders do have breaking changes made for whatever reason, then we can use that as opportunity to apply this change as well.  Perhaps we should open a new issue to keep track of this? Please do. That's better than resume the conversation in this closed thread."
technical,If one could quickly prototype some hacks over the existing phong / standard implementations i bet it would be pretty useful ˜ Seems like the glb includes a AmbientLight. If you set visible to false to the imported AmbientLight the character starts to look less red.  The last thing to do is setting renderer.gammaOutput = true. (Needed when using GLTF #12766)
technical,"In that case, I still don't consider this to be a complete or actionable bug report and I continue to vote to close this issue. Thanks for reporting this. Some notes...  **File size** I'm not sure where you're getting these numbers, this is what I see: **Materials** Your model uses Diffuse + Specular. Unfortunately, seems like the specular texture is not being exported. GLTF supports 2 PBR modes: Metalness + Roughness and Specular + Glossiness. You want to export your model using the second mode. GLTFLoader supports both but maybe the Babylon.js doesn't have an option to export in that mode? In that case you may want to do a feature request on their project.  Let us know what you find out."
technical,"Unity is treating textures as sRGB automatically rather than leaving it up to the user. This is exactly what the GLTFLoader does, and what we are arguing the other loaders should have been doing from the start. That bad choice is yours, but the GLTFLoader is part of THREE, so it's up to Ricardo ( mrdoob ) if they will be removed or not.  Errr.... this is extremely fuzzy. GLTFLoader appears to be a community provided example, that lives in /examples. If you load three.js alone (three.min.js) there wont be any mention of GLTF.  If you use three.js off the shelf you get a scenegraph and various webgl abstractions. In this context, GLTF is just another one of many many examples of how you can translate some generic 3d data / scene file into three.js's structs.  So, at a glance, three.js seems like this generic library, that draws stuff to screen. It doesn't care if you fetch that data from some remote server, and it doesn't care how you parse it (collada, fbx, sea3d, gltf... and 40 others). At the end of the day, you are rendering a THREE.Mesh with THREE.Geometry and THREE.Material. **Absolutely all of the loaders share this feature. All of them result in this data structure.**  However, in practice, this is not the case, and you seem to be in the right.  GLTF is a ""first class citizen"" of three.js. mrdoob wrote that several times. SEA3d is some **random format** written by **some unknown people** while GLTF has the backing of THE khronos group. On top of that, it's probably meant to be the backbone of the whole VR/AR revolution, hence so much backing by other big players.  This is just an unfortunate circumstance that three.js found itself in as the most widely used webgl library. People want to do 3d, which three.js solves really well with it's scene graph and other abstractions (things like Mesh, Geometry, Material, Texture etc.). Unfortunately, people also want to make experiences and expect three.js to be able to do that. This is of course complicated, hence, favoring one particular format and giving it preferential treatment makes sense. It may somewhat hurt the very essence of the library (draw stuff on screen) but it's a trade off.  If you care to read all the guidelines, there's a document called owners. donmccurdy owns that particular loader, so his word carries as much weight as mrdoob's, so good luck there :)  The directive right now is:  So anything that the khronos group comes up with has to be reflected in three.js. If you look at the discussion historically, mrdoob doesn't really follow what's going on and donmccurdy is the authority on all things khronos/gltf related.  So for example, khronos has defined a specification for gltf ""extensions"". Out of infinitely many extensions, one involves texture transformations. It has been ratified by the khronos group and because of that, **three.js absolutely must support it**. This warrants a PR with **11 thousands line of code** and increasing the size of the library by 1/3.  I think you're wasting a lot of effort head butting a wall here, and that this issue should be closed."
technical,"I've addressed each of those points in this. we will not be removing the sRGB encoding assignment to sRGB textures in GLTFLoader. If you would like to override that, it is easy to change the texture encoding after loading the model.  If there are no other actions to take here, this issue should be closed. That bad choice is yours, but the GLTFLoader is part of THREE, so it's up to Ricardo ( mrdoob ) if they will be removed or not. In any case, you (and looeee ) continue to be on the wrong path. Again: your workflow is not good and is not backwards compatible. You should start learning from the more experienced people:  **In Unity you don't have to do anything**. Maps that are placed in the metallic/smoothness, ambient occlusion are treated as linear by the shader and maps in the albedo are treated as sRGB. What the shader does for the albedo and specular is ""linearize"" the maps. It removes the gamma-encoded values from the map in the shader code by applying an inverse gamma to it of 0.4545. In the Unity workflow, this is done automatically and **you don't need to flag the images as sRGB**."
technical,"In my opinion everyone who owns something is free to mess it up. I'm not sure if there should be place for disagreement with this, in the modern world :). If you have a Ferrari and decide to put a brick on the gas pedal and have it drive off a cliff, i may think it's a waste but it's YOUR Ferrari, can't get simpler than that. Unless it's 1917 and you're in tsarist Russia. That right there is disruptive content... :( You're free to fork the project, the examples and do whatever work you deem fit. If it makes you feel any better, what the fbx loader owner did is not at all different - the loader did exist for a couple of years and was written by other people before he took ownership. Fork's no different than this."
technical,"Could you update the online samples? The glTF format supports PBR and unlit shaders at this time. Whether the BabylonJS and FBX2GLTF tools do the Phong-to-PBR conversion in a way that preserves Phong specular maps, I don't know ” that would be a question for the repos of those tools. If you are trying to preserve the exact appearance of models using classic Phong shaders, you may have an easier time with other formats.  This is a deliberate decision and not a bug. Base color and emissive textures in glTF (and, typically, diffuse textures in any format...) are in sRGB colorspace. GLTFLoader marks them as such, so that they're converted to linear colorspace for correct PBR lighting calculations. Finally colors should be converted back to sRGB (e.g. renderer.gammaOutput=true).  If you skip all of this, with any format, lighting calculations are incorrect. SEA3DLoader, FBXLoader, and ColladaLoader never touch the .encoding property of any texture, and leave it to the end user to change texture colorspace and renderer colorspace. I'm pretty confident that the large majority of three.js users are passing sRGB colors into three.js without converting, despite the fact that renderer lighting calculations assume linear colorspace, and getting results that are ""good enough"" but inconsistent with other engines and authoring environments. For correct results you should be using renderer.gammaOutput=true, and marking sRGB textures as sRGB.  None of these issue are specific to glTF, but with GLTFLoader we're trying to achieve consistency with other engines and 3D authoring environments, and have chosen to treat all sRGB textures as sRGB for a first step. If you're mixing models from other formats in the scene, then yes it's awkward, and you'd need to either mark the diffuse textures of those formats as sRGB or mark the colorspace on the glTF models to linear (the latter is incorrect for all model formats involved, but may look good enough if you don't need precise colors).  It does not seem like there is anything actionable here, unless there are specific issues we can report to the tools mentioned. RemusMar if you are happy with your SEA3D workflow, that's great ” I'm not interested in debating formats or persuading you to change from something that is already working well for you."
technical,Three possible causes for the wrong GLTF results: 1. the Babylon3D exporter is buggy (they say it's not) 2. the FBX2GLTF converter is buggy (they say it's not) 3. the GLTF importer is buggy.  But you want to close the topic because there is no issue and everytbody is happy ... the GLTF importer is buggy  You mean the GLTFLoader? Can you identify what the bug is? It would be especially helpful if you can find a very simple model that demonstrates the bug. If you've made bug reports on the Babylon3D exporter and FBX2GLTF can you link to them here?
technical,"You are talking about PBR materials with glTF, but I assume this is just as much a problem with a Phong material?  Yes, the problem is the same for Phong materials or PBR materials loaded in any other format.   ...should other loaders be doing so? It seems like this inconsistency between loaders is a point of confusion for users, and it would make sense for all of them to treat sRGB textures the same way if possible.  If we had a time machine, yes, the other loaders should also be marking sRGB textures containing color data as sRGB. But making the change now would cause confusion and break backward-compatibility, and the gammaOutput=true setting needed to fix output is not intuitive. I don't think changing other loaders can be justified given those issues.  Let's keep an eye on this. I hope the resolution there will make color workflows more intuitive. With that and NodeMaterial, there may be opportunities to fix some existing issues without breaking anyone's existing applications. They are irrelevant. I get much better results in 3DS Max and that tells me that the GLTFLoader and/or your PBR model are not properly implemented.  Your girlfriend looks bad but you're happy because your boss told you that's normal. OMG ... In fact you should fix your ""correct"" example to look good."
technical,"I agree with this, and will start to be more vigilant in marking disruptive or otherwise useless comments. In this case, since I was the one being targeted, I didn't think it was appropriate to do so.  However, RemusMar has consistently exhibited what can only be described as disruptive behaviour over a long period of time and I don't think that marking comments is going to bring about any kind of change.  We're consistently having to deal with ad hominem attacks  from this user in the form of name calling or other vaguely aggressive statements, and it's quite frankly tedious and detrimental to the development of the three.js community. This is the current (july 2018) GLTF status. Test case: the same skinned mesh exported from 3DS Max 2018. Standard material: Diffuse + Specular + Normal  1) SEA3D exporter + SEA3D importer:SEA file size: 658 KB Result: close to perfect  2) Babylon3D GLTF exporter + GLTF importer:GLB file size: 1,850 KB Result: messed up materials  I've also tested the FBX2GLTF utility (by Facebook): the same wrong results  Important note: there is nothing wrong with THREE.js and PBR materials:  In any case, PBR was a bad choice for GLTF and also, all the current converters are collection of bugs."
technical,are you suggesting any particular actions we should take? If not I vote to close this issue. Three possible causes for the wrong GLTF results: 1. the Babylon3D exporter is buggy (they say it's not) 2. the FBX2GLTF converter is buggy (they say it's not) 3. the GLTF importer is buggy.  But you want to close the topic because there is no issue and everytbody is happy ...
technical,"That bad choice is yours, but the GLTFLoader is part of THREE, so it's up to Ricardo ( mrdoob ) if they will be removed or not. In any case, you (and looeee ) continue to be on the wrong path. Again: your workflow is not good and is not backwards compatible. You should start learning from the more experienced people:  **In Unity you don't have to do anything**. Maps that are placed in the metallic/smoothness, ambient occlusion are treated as linear by the shader and maps in the albedo are treated as sRGB. What the shader does for the albedo and specular is ""linearize"" the maps. It removes the gamma-encoded values from the map in the shader code by applying an inverse gamma to it of 0.4545. In the Unity workflow, this is done automatically and **you don't need to flag the images as sRGB**. Unity is treating textures as sRGB automatically rather than leaving it up to the user. This is exactly what the GLTFLoader does, and what we are arguing the other loaders should have been doing from the start."
technical,"good point As I said before, Google already failed with UTF8. In the current development status, GLTF (and PBR) follow the same path. Unity is treating textures as sRGB automatically rather than leaving it up to the user. This is exactly what the GLTFLoader does, and what we are arguing the other loaders should have been doing from the start.  Unity has an editor which is a foreign concept for three.js. When you ""load"" a texture into unity, you bring it into the editor which can do all kinds of transformations before actually storing it for game's use.  I think that three.js has a different paradigm, since it doesn't care how you create the assets nor how you store them. An analogy would be if three.js came with folder /utils and then /textureTools or something like that, where you could prepare the assets for three.js.  This difference should possibly be considered.  The whole gamma correct workflow is extremely complicated, and if i recall correctly you may lose some precision if you store the textures in one way versus another. With three.js this is still kinda low-levelish so i'd prefer to leave multiple options to the user."
technical,"Unity is treating textures as sRGB automatically rather than leaving it up to the user. This is exactly what the GLTFLoader does, and what we are arguing the other loaders should have been doing from the start.  Unity has an editor which is a foreign concept for three.js. When you ""load"" a texture into unity, you bring it into the editor which can do all kinds of transformations before actually storing it for game's use.  I think that three.js has a different paradigm, since it doesn't care how you create the assets nor how you store them. An analogy would be if three.js came with folder /utils and then /textureTools or something like that, where you could prepare the assets for three.js.  This difference should possibly be considered.  The whole gamma correct workflow is extremely complicated, and if i recall correctly you may lose some precision if you store the textures in one way versus another. With three.js this is still kinda low-levelish so i'd prefer to leave multiple options to the user. Wow, I never say that before. And apparently I ""own"" the FBXLoader  ˜ ˜"
technical,"They are irrelevant. I get much better results in 3DS Max and that tells me that the GLTFLoader and/or your PBR model are not properly implemented.  Your girlfriend looks bad but you're happy because your boss told you that's normal. OMG ... In fact you should fix your ""correct"" example to look good. Yeah, it's unfortunate but I agree that it's not worth breaking backwards compatibility over this.  However, as I've been working with larger FBX scenes consisting of multiple models, animated cameras and so on I've found myself wishing that the output of the loader was something more like GLTFLoader's output - that is, it should return an fbx object with properties:  There may be other loaders that would benefit from a similar change.  We should add this to the backburner (and certainly wait on #11337), but if any loaders do have breaking changes made for whatever reason, then we can use that as opportunity to apply this change as well.  Perhaps we should open a new issue to keep track of this?"
technical,"...I'm pretty confident that the large majority of three.js users are passing sRGB colors into three.js without converting, despite the fact that renderer lighting calculations assume linear colorspace  You are talking about PBR materials with glTF, but I assume this is just as much a problem with a Phong material?   ...because glTF is a new format, and because we're trying to get PBR right, we're going to mark sRGB textures as sRGB, even if other loaders are not doing so.  should other loaders be doing so? It seems like this inconsistency between loaders is a point of confusion for users, and it would make sense for all of them to treat sRGB textures the same way if possible. You are talking about PBR materials with glTF, but I assume this is just as much a problem with a Phong material?  Yes, the problem is the same for Phong materials or PBR materials loaded in any other format.   ...should other loaders be doing so? It seems like this inconsistency between loaders is a point of confusion for users, and it would make sense for all of them to treat sRGB textures the same way if possible.  If we had a time machine, yes, the other loaders should also be marking sRGB textures containing color data as sRGB. But making the change now would cause confusion and break backward-compatibility, and the gammaOutput=true setting needed to fix output is not intuitive. I don't think changing other loaders can be justified given those issues.  Let's keep an eye on this. I hope the resolution there will make color workflows more intuitive. With that and NodeMaterial, there may be opportunities to fix some existing issues without breaking anyone's existing applications."
technical,"the GLTF importer is buggy  You mean the GLTFLoader? Can you identify what the bug is? It would be especially helpful if you can find a very simple model that demonstrates the bug. If you've made bug reports on the Babylon3D exporter and FBX2GLTF can you link to them here? You mean the GLTFLoader?  Yes. Importer = Loader + Parser You have everything you need to study the issue. Any PHONG material (Diffuse + Specular + Normal) exported or converted to GLTF gives wrong results. p.s. PHONG represents 50-60% of the current samples, compared to Physical less than 1%. That's why I said that PBR was a bad choice for GLTF."
technical, .NET 5.0 will ship with IdentityServer 4 in some ASP.NET templates. As the IS folks have stated. We will keep supporting IdentityServer4 until the end of life of .NET Core 3.1 in November 2022.  Planning has begun for .NET 6.0 and we'll make an announcement when ready.
technical,"You don't need something as sophisticated as IS4/5 for simple use cases - there are numerous other libraries that are free. We're using Microsoft.AspNetCore.Identity in our application, which is already built in and matches your requirements exactly. 1. You're not paying anything right now to begin with. The framework and platform is already free to use. And you're likely making money from it. 2. IdentityServer is only tangentially related to Identity management. It's an identity provider which is still a special thing you likely don't want to put into every other app. 3. Identity management itself is built into the framework with ASP.NET Identity. That is completely unrelated to what IdentityServer offers though except that it integrates well into ASP.NET Identity. 4. Other platforms actually often don't have solutions like IdentityServer, especially not built-in. A common alternative that I know of is Keycloak (which you can easily use for your .NET apps as well). 5. There are other alternatives to building your own identity provider. In Azure, you could use Azure AD. On-premise you have ADFS. And you can also use free third-party services like Auth0 or just integrate other identity providers directly. 5. If you absolutely need to ship your own identity provider, realize the complexity this involves (auth is a complex thing to master!) and consider paying for a license. 6. You can also check out OpenIddict if you need an open (free) implementation. But who knows whether that will always stay free to use."
technical,"Maybe before you jump to conclusions or imply things, find out all the details. Again. thanks for all the comments and feedback. This wasn't an easy decision for us, and we are still in the process of finding the right balance. The more data we have, the better. Please contact us directly. I broke down the **exact** sponsorship numbers on my blog."
technical,"Thank you for your clarification! Does ""we"" mean Microsoft, the .NET Foundation or the open source community?"
technical,"Most of full stack framework out there has official package for crucial things like this (say laravel). Coming from other communities like php, node, etc, dev can build and experiment with everything totally free. This makes it harder for newcomers to learn aspnet core without the support from the community or microsoft officially. Edit: and yes its totally up to the creators if that's their decision Here are some alternatives that may have come to pass without this move :  1. They stop developing identity server completely as it's not financially viable for them to continue it. 2. Support and ongoing changes dry up. You have a live issue and there's no one to help you out. Also consider the difference now : The team may have more funds to invest in making the API and docs even better, saving you time and money. In other words, positioning this as a negative move is perhaps a little short sighted. The current situation was probably unsustainable and this was no surprise to me when I saw the news. You have until the end of life of netcore 31 which is next year, to plan for a migration strategy. As I understand it, IS4 will continue to work, it's simply not going to be updated. I suspect that the time saving of paying the money will outweigh the cost of moving but you will undoubtedly know better."
technical,"Most of full stack framework out there has official package for crucial things like this (say laravel). Coming from other communities like php, node, etc, dev can build and experiment with everything totally free. This makes it harder for newcomers to learn aspnet core without the support from the community or microsoft officially. Edit: and yes its totally up to the creators if that's their decision I agree you got to get paid, but I think this has turned out to become an exploit by Duende with Microsoft Complacency. Sad to say it, but with timing it boils down to being opportunistic, adoption was very low till the fantastic/free **Skoruba.Admin GUI kicked in** which is what really made it popular. ### Lacking:  -Full reference implementation samples: In many ways the  lacking Full reference implementation  including the GUI component in the identity management reference implementation/template is what creates a lot of confusion, If you look on Stack Overflow there are many many question handling identity and user/roles/claims management. **So many versions** of evolving membership management, so basic things like user management/retrieval etc is very complicated for the average developer like some have stated here - On uservoice before it was bought out by Microsoft, **it was top 10 most voted** (before it was closed by Microsoft) requests for Identity Server & Management like the earlier web forms version had provided. - Just to show case the pain of the developer And even on **Asp Core Github  1500 issues are around identity,**960 issues on Identity on the archived section, its close 4500 just around identity in the main archives - Identity alone 7500 issues - Combining the search with membership, claims, roles, providers, tokens etc its close to 22,000 questions!!  And here there was very little activity before jskoruba put up his free admin GUI"
technical,"It's definitely not basic, it's incredibly valuable. I think one of the tricky things with the pricing is for startups. An STS might be one of the first bits of infra you set up, and at this point your business is surviving on free trials and any credits you can wrangle from developer evangelists. IS definitely kept me coming back to dotnet even if we weren't planning to use it elsewhere. I wish Microsoft would have sponsored the hell out of it so Brock and Dominick didn't have to worry about any of this. IS has done a lot for the dotnet ecosystem. I don't have a great amount to add to this other than the below.  I absolutely don't want to pay anything extra (yearly!!) for such a **very basic** thing like identity in my framework. Identity is ANYTHING but basic, and **many** get it so incredibly wrong & make it very easy for their applications to be breached. Like many others have said there are other options available"
technical,.NET 5.0 will ship with IdentityServer 4 in some ASP.NET templates. As the IS folks have stated. We will keep supporting IdentityServer4 until the end of life of .NET Core 3.1 in November 2022.  Planning has begun for .NET 6.0 and we'll make an announcement when ready. I don't see why this is a big deal - still fine for ASP.NET Core to ship OSS templates that include IdentityServer4 under this license. If companies want great tools for solving problems as complicated and critical as identity management they should have no problem paying for it.
technical,"A full OpenID Provider has been built in.  Not sure why it's considered necessary to have it now. I get that IdentityServer going non-free sucks for some users but there are alternatives like OpenIDdict. Begging Microsoft to make things like this distracts them from improving core functionality.  Microsoft doesn't have to own everything dotnet. I get it that the creators need money so it is their decision whatever they want to do.  For the .net ecosystem it is a rather disastrous event. As a small company that has chosen IdentityServer because it is the ""official"" framework of choice, both according to Microsoft and the general community, you are now facing a lot of not needed and not wanted problems. Sure you can say, yes 1500 is not much in the business environment. But this is not true for all of them. For us it is a lot of money and also a lot of time that we have already invested. We now have to migrate to something different. In a healthy ecosystem, it should not happen that a pillar on which so many have built, changes the license so negatively.  Yes, open source is not about ""free work"" but you clearly want to have a .net ecosystem with all the basic tools that keep steady and have a very open license. It would have been better if they did the license change back then when they converted IdentityServer to .net core."
technical,"If folks wanted this to be free, perhaps they should petition MS to sponsor the project at a level that would ensure that? I just want to throw my hat in because I keep seeing the 1500 number over and over in this and other threads, but I can't see how this would apply to anything but the absolute most basic IS4 implementation.  I'm putting together a solution that uses IdentityServer4 for a relatively small startup. We don't have a problem paying to support the project, which is what we thought we were doing when we paid for the Enterprise AdminUI license at 8400 / year. This was a stretch for us, but we chose it over similar open source projects like this to financially support the IS4 project. The per-client licensing model of Duende IdentityServer is what's going to make this untenable for us going forward. Let me describe our clients: 1: AdminUI (which alone is 8400 / year) 2: AdminUI Webhooks 3: Delegation gateway 1 4: Delegation gateway 2 (different use case, same idea. Can't be the same client) 5: Worker service 1 6: Worker service 2 7: Worker service 3  As you can see, we've used 7 clients (2 of which are required by the IS4-affiliated product we're already paying thousands a year for) before even getting to the part where a single actual website or mobile app exists.  Worker services (client credentials grants) come online at an alarming rate, and they're generally dead-simple pieces of code, sometimes the entire service is 100 lines of code. I don't know how other shops are handling IS4 clients, but using Duende IdentityServer will immediately put us in the Enterprise tier at 12,000 / year, putting our total cost at over 20k / year. I wanted to point all of this out since people may read these threads thinking ""I can swing 1500 / year"". I'd be shocked to find out that our setup was unique in the number of clients that end up being created. This client based pricing model is going to immediately price out many startups like the one I'm working with. The IS4 authors deserve to be well compensated and I hope Duende IdentityServer is a success for their sake, but I'm also hoping they introduce something like a per-user pricing model."
technical,"If you look on Stack Overflow there are many many question handling identity and user/roles/claims management. Yet, none of that is directly related to IdentityServer. Don't confuse ASP.NET Identity with what IdentityServer does. I think a lot of people are confused and have been for a while. The name IdentityServer is so generic that people thought it was required.  It doesn't help at the official docs saying ""Identity with SPA"" is a how to with IS4.  That's so overblown and doesn't acknowledge the cases of when to use OpenID and when not. Just found this out on a Reddit discussion today."
technical,"I agree with everything you said  this is a pretty core function to the Microsoft Stack, had they done it right in the first place it would have never been issue today. Also he is out of line with personal comments like that. What an idiot, not ok, just because he dont agree with someone else view. Why is this closed?! If folks wanted this to be free, perhaps they should petition MS to sponsor the project at a level that would ensure that?"
technical,"I agree you got to get paid, but I think this has turned out to become an exploit by Duende with Microsoft Complacency. Sad to say it, but with timing it boils down to being opportunistic, adoption was very low till the fantastic/free **Skoruba.Admin GUI kicked in** which is what really made it popular. ### Lacking:  -Full reference implementation samples: In many ways the  lacking Full reference implementation  including the GUI component in the identity management reference implementation/template is what creates a lot of confusion, If you look on Stack Overflow there are many many question handling identity and user/roles/claims management. **So many versions** of evolving membership management, so basic things like user management/retrieval etc is very complicated for the average developer like some have stated here - On uservoice before it was bought out by Microsoft, **it was top 10 most voted** (before it was closed by Microsoft) requests for Identity Server & Management like the earlier web forms version had provided. - Just to show case the pain of the developer And even on **Asp Core Github  1500 issues are around identity,**960 issues on Identity on the archived section, its close 4500 just around identity in the main archives - Identity alone 7500 issues - Combining the search with membership, claims, roles, providers, tokens etc its close to 22,000 questions!!  And here there was very little activity before jskoruba put up his free admin GUI If you look on Stack Overflow there are many many question handling identity and user/roles/claims management. Yet, none of that is directly related to IdentityServer. Don't confuse ASP.NET Identity with what IdentityServer does."
technical,"Having sustainable OSS projects  is  part of a healthy ecosystem. If not 1500, what amount would you pay? If the answer is ""none"" then  you're the problem . If you're hosting on Azure, you might look at its Easy Auth feature for App Services. That's free, too. We use that quite a lot for the ""easy"" scenarios. If your situation is complex, that's when you need something more powerful, and I don't see why it must be free."
technical,"If you're hosting on Azure, you might look at its Easy Auth feature for App Services. That's free, too. We use that quite a lot for the ""easy"" scenarios. If your situation is complex, that's when you need something more powerful, and I don't see why it must be free. In fact you have until end of 2022 to move over. That's two years."
technical,"If you've been using IdentityServer all this time, you've already been depending on a 3rd party company. OSS software isn't about other people doing things for you for free - either roll your own, pick another technology, or pay the bill. If you can't afford 1500 a year for IdentityServer4 to manage something as critical as identity for your business applications, drop them a line and let them know - pricing software products is a complicated business. IS4 will actually remain free and will be supported in line with .NET Core 3.1. It's only IS5 (as far as I'm aware) that will be commercial. But the main point is right - if you don't want to pay for the future version of IS, just use the out of the box solution or roll your own. If you would rather not take on that responsibility and / or it'll be quicker to use a third-party package written by experts, then you can pay for them to do it for you. I don't see the problem."
technical,"If you've been using IdentityServer all this time, you've already been depending on a 3rd party company. OSS software isn't about other people doing things for you for free - either roll your own, pick another technology, or pay the bill. If you can't afford 1500 a year for IdentityServer4 to manage something as critical as identity for your business applications, drop them a line and let them know - pricing software products is a complicated business. IS4 will work until 2022 at that point you won't able to get security fixes, etc.  They went enterprise pricing, there is no hobby pricing or ""starter"".   They went from 0 to 60.  It's ok, it's their choice, but we have a choice too.  Before we migrate to one of the other FOSS systems we're waiting for Microsofts response to this.  Surely this is a pivot moment to solve this internally  for the framework.  MVC1-5 came with auth solutions so I expect they will continue that tradition.  Json parsing used require 3rd party (at least if you wanted performance).  That's internal now and far better.  We'll see what they do."
technical,"Does ""we"" mean Microsoft, the .NET Foundation or the open source community? It used to be core part of the MS Stack back in early webforms/MVC time frames. However MS did not release the comparable component with an admin/UI, in the new ASP stack. So people gravitated towards that as a viable and that it was a free option.  **From a financial point** 1500 in countries where they make less than 3/day is not an option and a pain of the smaller business that you should personally exp. to understand. Many of these sites just have basic features enabled, with a large community. Also with this kind of a mindset other competing products like Wordpress on PHP have gained huge ground on ASP. Some day PHP and its frameworks are going to catch up. Look at the search results, there are several several questions and issues just on this topic which are closed or unanswered with a heavy hammer. for e.g. 57 votes ASP identity. Looking through the issues..  **there are several pertinent questions that are core to the stack, that are simply too hard for a developer to tackle on his own**, and too crucial for any one to monopolize. ### Multi tenancy, Dynamic Roles with claims, Federation, SSO, ASP Identity Core is DB facing more that anything, Open ID integration and more  citizenmatt  please  give us an option that's doesn't burn a hole!"
technical,"what about non-for-profit organizations?, the rationale here is that everyone is using this product for commercial, profit, not all of us work like that. For the dotnet team, could be nice to have some guidance on how to integrate aspnet identity with an openid free/paid provider to offer the idp role. it could be hard to keep with the new licensing model since the new company dont have sales/marketing rep in our country, is an us-europe centric target they have, and duende name wont fly around here. i still have hope than in the next couple of years some higher up in microsoft get it and buy this excelent product, like they did before with dundas, minecraft and a lot of strategic assets. It's definitely not basic, it's incredibly valuable. I think one of the tricky things with the pricing is for startups. An STS might be one of the first bits of infra you set up, and at this point your business is surviving on free trials and any credits you can wrangle from developer evangelists. IS definitely kept me coming back to dotnet even if we weren't planning to use it elsewhere. I wish Microsoft would have sponsored the hell out of it so Brock and Dominick didn't have to worry about any of this. IS has done a lot for the dotnet ecosystem."
technical,"Was this project sponsored by Udelt, Microsoft, Ritter Insurance Marketing, and Knab? Was there not enough sonsorship money? This list goes on Sponsors. Was this more of a thought that they can make more money by overpricing it for even the smallest of projects? Maybe before you jump to conclusions or imply things, find out all the details."
technical,we'll make an announcement when ready Thank you for your clarification!
technical,"I just want to throw my hat in because I keep seeing the 1500 number over and over in this and other threads, but I can't see how this would apply to anything but the absolute most basic IS4 implementation.  I'm putting together a solution that uses IdentityServer4 for a relatively small startup. We don't have a problem paying to support the project, which is what we thought we were doing when we paid for the Enterprise AdminUI license at 8400 / year. This was a stretch for us, but we chose it over similar open source projects like this to financially support the IS4 project. The per-client licensing model of Duende IdentityServer is what's going to make this untenable for us going forward. Let me describe our clients: 1: AdminUI (which alone is 8400 / year) 2: AdminUI Webhooks 3: Delegation gateway 1 4: Delegation gateway 2 (different use case, same idea. Can't be the same client) 5: Worker service 1 6: Worker service 2 7: Worker service 3  As you can see, we've used 7 clients (2 of which are required by the IS4-affiliated product we're already paying thousands a year for) before even getting to the part where a single actual website or mobile app exists.  Worker services (client credentials grants) come online at an alarming rate, and they're generally dead-simple pieces of code, sometimes the entire service is 100 lines of code. I don't know how other shops are handling IS4 clients, but using Duende IdentityServer will immediately put us in the Enterprise tier at 12,000 / year, putting our total cost at over 20k / year. I wanted to point all of this out since people may read these threads thinking ""I can swing 1500 / year"". I'd be shocked to find out that our setup was unique in the number of clients that end up being created. This client based pricing model is going to immediately price out many startups like the one I'm working with. The IS4 authors deserve to be well compensated and I hope Duende IdentityServer is a success for their sake, but I'm also hoping they introduce something like a per-user pricing model. That's great feedback - exactly what the Duente dudes need to hear in order to get their pricing right"
technical,"We're done here and I'm locking the issue. Some of you were hopping over the line, some of you hopped over it, sprinted in the wrong direction and then threw things from 2.7 miles past the line. In any case, this is not the venue to discuss Duende's pricing, what open source means, and whether Microsoft should produce everything and kill off the .NET open source ecosystem in the process. After all, one of the reasons we went with Identity Server in the first place was the community reaction to our suggestion we produced a simple authorization service, and the loud and clear message we shouldn't try to reinvent something that already had a good open source solution.  Once .NET 5 is done and dusted I'm sure we'll have senior msft folks come up with an appropriate venue for a wider discussion over some of the issues. I will remind all of you we have a code of conduct. The current version (IdentityServer4 v4.x) will be the last version we work on as free open source. We will keep supporting IdentityServer4 until the end of life of .NET Core 3.1 in November 2022.  To continue our work, we have formed a new company Duende Software, and IdentityServer4 will be rebranded as Duende IdentityServer. Duende IdentityServer will contain all new feature work and will target .NET Core 3.1 and .NET 5 (and all versions beyond). Currently, some of the ASP.NET Core templates use IdentityServer4. How the above announcement affect? ASP.NET Core 5.0 will be shipped with IdentityServer4?"
technical,"I don't have a great amount to add to this other than the below.  I absolutely don't want to pay anything extra (yearly!!) for such a **very basic** thing like identity in my framework. Identity is ANYTHING but basic, and **many** get it so incredibly wrong & make it very easy for their applications to be breached. Like many others have said there are other options available The per-client licensing model of Duende IdentityServer is what's going to make this untenable for us going forward. For me it comes down to the large step functions in price, and particularly the single vs. unlimited issuers. I need multiple issuers. That puts me immediately at 12k/year. The 10 client limit is also too low in general. I'd fit under it right now but likely won't within a year or two. 12k/year for self-hosted OIDC is also just frankly hard to stomach relative to what we pay for other licensed software components. I'd be much more amenable to pricing that scaled linearly instead of this 7000/year jump between 10 clients and 1 issuer to unlimited. From what I can see, there's no other value add at all to the Enterprise Edition - simply the ability to go beyond 10 clients and 1 issuer. There's pricing we'd be happy to pay to support the project and maintain our integration investment, but this isn't it."
technical,"As he says, pricing a product is hard. If you want to give us feedback on the pricing and explain your situation, this is not the right place to do that. Please contact us directly. thanks! Totally true, identity and access management should be baked into the framework as it is needed in almost any serious app."
technical,"The per-client licensing model of Duende IdentityServer is what's going to make this untenable for us going forward. For me it comes down to the large step functions in price, and particularly the single vs. unlimited issuers. I need multiple issuers. That puts me immediately at 12k/year. The 10 client limit is also too low in general. I'd fit under it right now but likely won't within a year or two. 12k/year for self-hosted OIDC is also just frankly hard to stomach relative to what we pay for other licensed software components. I'd be much more amenable to pricing that scaled linearly instead of this 7000/year jump between 10 clients and 1 issuer to unlimited. From what I can see, there's no other value add at all to the Enterprise Edition - simply the ability to go beyond 10 clients and 1 issuer. There's pricing we'd be happy to pay to support the project and maintain our integration investment, but this isn't it. Was this project sponsored by Udelt, Microsoft, Ritter Insurance Marketing, and Knab? Was there not enough sonsorship money? This list goes on Sponsors. Was this more of a thought that they can make more money by overpricing it for even the smallest of projects?"
technical,"1. You're not paying anything right now to begin with. The framework and platform is already free to use. And you're likely making money from it. 2. IdentityServer is only tangentially related to Identity management. It's an identity provider which is still a special thing you likely don't want to put into every other app. 3. Identity management itself is built into the framework with ASP.NET Identity. That is completely unrelated to what IdentityServer offers though except that it integrates well into ASP.NET Identity. 4. Other platforms actually often don't have solutions like IdentityServer, especially not built-in. A common alternative that I know of is Keycloak (which you can easily use for your .NET apps as well). 5. There are other alternatives to building your own identity provider. In Azure, you could use Azure AD. On-premise you have ADFS. And you can also use free third-party services like Auth0 or just integrate other identity providers directly. 5. If you absolutely need to ship your own identity provider, realize the complexity this involves (auth is a complex thing to master!) and consider paying for a license. 6. You can also check out OpenIddict if you need an open (free) implementation. But who knows whether that will always stay free to use. We have 2020 and those features should be something that should be absolutely solved **in a standardized way** and freely available in a famework: - Access token creation with including custom claims - Refreshtoken creation and presistence - Everything needs to be easily scalable to multiple instances. - Storage of credentials should be easily customizable.  Again, the point here is that we invested time and money in solutions using IS for those things. Because MS said this is the way to go. We are still in a crucial situation with .net and hoping that it gain more ppl using it. Also if Blazor should gain traction something like this just doesn't help."
technical,"Is this really so basic? If it's such a simple thing to do, Brock and Dominick wouldn't be able to create such a product - and you wouldn't be so disappointed at the move to a commercial model, you'd simply roll your own, or fork IS4 and continue from there.  Clearly there's complexity in auth in general and within IS, and let's not pretend otherwise. what about non-for-profit organizations?, the rationale here is that everyone is using this product for commercial, profit, not all of us work like that. For the dotnet team, could be nice to have some guidance on how to integrate aspnet identity with an openid free/paid provider to offer the idp role. it could be hard to keep with the new licensing model since the new company dont have sales/marketing rep in our country, is an us-europe centric target they have, and duende name wont fly around here. i still have hope than in the next couple of years some higher up in microsoft get it and buy this excelent product, like they did before with dundas, minecraft and a lot of strategic assets."
technical,"Sorry, but this is just an arrogant statement by you. I absolutely don't want to pay anything extra (yearly!!) for such a **very basic** thing like identity in my framework. Again, I completely get that the IS devs neeed more money and want to be paid. But I (and a lot of other not so high profile devs like you) invested a lot of time into using this product since MS declared it the defacto standard. I don't expect some - again VERY BASIC - functionality in my tech stack to change the licence 180. This is an absolutely not planned change that cost a lot of time and money. If this was a framework that helps solving some black magic math issues, I wouldn't have such problems. But we absolutely need fully FOSS solutions for the very basic things in every day life. And I don't agree that MS should't fill this gap here. It was their mistake to promote IS in the first place without making sure that it keeps the same license **forever**. You don't need something as sophisticated as IS4/5 for simple use cases - there are numerous other libraries that are free. We're using Microsoft.AspNetCore.Identity in our application, which is already built in and matches your requirements exactly."
technical,"I think a lot of people are confused and have been for a while. The name IdentityServer is so generic that people thought it was required.  It doesn't help at the official docs saying ""Identity with SPA"" is a how to with IS4.  That's so overblown and doesn't acknowledge the cases of when to use OpenID and when not. Just found this out on a Reddit discussion today. you realize that's a commit graph, right? That has everything to do with the amount of code changes being committed and none to do with how often the project is adopted by end-users? Plus those code changes are overwhelmingly committed by the two original maintainers of the project. Then again, I'm not surprised users with blank Github profiles might be confused by how OSS is actually produced. Bringing this back to ASP.NET Core -  Identity  is already built-in. As poke explained very well:  IdentityServer is only tangentially related to Identity management. It's an identity provider which is still a special thing you likely don't want to put into every other app. Identity management itself is built into the framework with ASP.NET Identity. That is completely unrelated to what IdentityServer offers though except that it integrates well into ASP.NET Identity. Other platforms actually often don't have solutions like IdentityServer, especially not built-in. A common alternative that I know of is Keycloak (which you can easily use for your .NET apps as well).  If the issue at-hand here is that now you have to migrate IS to something else, guess what:  1. You can keep using IS4,  supported by the IS team on Github , up until Dec 31 2022, for free. 2. After that...  you can still keep using it , but they're not going to be patching bugs for free anymore. 3. So in the event that you really do run into a critical problem, you can fork IS4 and patch it yourself if needed."
technical,"I think it's rather because there were no other arguments in favor of JSX/DSX other than personal preference.  Not really, many were given before, just read fully both threads. I did mention these 2 things before: (1) No more 'child' & 'children' stuff (2) easy for 3rd party tools to manipulate (parse, analyse and regenerate)  With  (2) you can enhance the markup to do things not possible with just Dart, for example DSX's spread operator or generating many function parameters from a smaller set.  Others have provided lots of good points but I am not digging it up for you ,) (1) as mentioned such things can also be changed/improved in Dart and there were already discussions. This just won't happen before Dart 2 release. Just assuming that DSX allows all kind of new features and Dart doesn't isn't really a fair argument in my opinion. (2) I'm pretty sure this can as well done with Dart and there is of course already a parser for Dart.   Others have provided lots of good points but I am not digging it up for you ,)  There is no need to dig them up for **me**, but this comes up frequently and you might be able to convince others that you actually have valid arguments. I followed the discussion and can't remember good factual arguments and that might be the same for others. If you summarize them you can just post a link to the next that questions that.  As discussed before I can accept personal preference as a valid argument, but if you claim to have lots of factual arguments as well, then I think it's valid to ask to get them pointed out."
technical,"I think it's rather because there were no other arguments in favor of JSX/DSX other than personal preference.  Not really, many were given before, just read fully both threads. I did mention these 2 things before: (1) No more 'child' & 'children' stuff (2) easy for 3rd party tools to manipulate (parse, analyse and regenerate)  With  (2) you can enhance the markup to do things not possible with just Dart, for example DSX's spread operator or generating many function parameters from a smaller set.  Others have provided lots of good points but I am not digging it up for you ,) (1) No more 'child' & 'children' stuff  I don't really understand why that's desireable. ""child"" and ""children"" aren't special. Consider ListTile for example. How would you do that one? Why are ""icon"" in IconButton, or ""home"" in MaterialApp, something you want to give a name for, but not ""child"" in Expanded? All three are just arbitrary arguments that happen to take Widget objects. There's nothing magical about ""child"" vs ""home"".    (2) easy for 3rd party tools to manipulate (parse, analyse and regenerate)  You can parse, analyze, and regenerate Dart code. But I agree we should make that easier. Hopefully in the coming years the Dart team will provide better APIs for this.    (3) notice that the switching between markup and programming is easily detected.  Why is that desireable? I mean, why would any of this count as ""programming""? It's all just expressions.    I mean inside XML you have '{}' to delimit code and in code you have '<Capital' to delimit markup.  I don't really understand the distinction.    Also separate all the 'style' things from the main structure.  You can do this today in Flutter if you really want to, just put the style in a variable like you did in the XML case."
technical,"I think it's rather because there were no other arguments in favor of JSX/DSX other than personal preference.  Not really, many were given before, just read fully both threads. I did mention these 2 things before: (1) No more 'child' & 'children' stuff (2) easy for 3rd party tools to manipulate (parse, analyse and regenerate)  With  (2) you can enhance the markup to do things not possible with just Dart, for example DSX's spread operator or generating many function parameters from a smaller set.  Others have provided lots of good points but I am not digging it up for you ,) (I apologize in advance for the wall of text.)  As someone who hasn't used either React-Native or Flutter long enough to consider myself a definitive source whether raw Dart or JSX/DSX is ""better"", this issue thread has been rather fascinating to read. There are a couple things that I would like to put my 0.02 down on though.  To begin, I find myself agreeing with various people on the nature of what JSX actually is and how it benefits developers. First and foremost, JSX was designed as a form of ""dynamic HTML"" that could be inlined into existing Javascript code. It's indispensable for JS-based web platforms like React in that it enables web developers to cleanly and efficiently interact with the DOM without having to wrestle with the god-awful native way (or the only-slightly-better jQuery way). Also, by its very nature, JSX encourages UI development that can be easily decoupled from the underlying data, promoting a well-organized project structure. In that environment, JSX is a tool to enable greater productivity, and I feel that it would be virtually impossible to argue with that.  How that paragraph relates to React-Native is that, even though it is a mobile-development platform, it is directly descended from React. As such, nearly all of the syntax and paradigms were still originally created with web-development in mind. That is by design - RN's whole shtick is that you can ""create cross-platform mobile apps using React"", so it's  supposed  to feel like web development when using it.  RN apps are also predominantly written in Javascript, so the inclusion of JSX is a natural one. JSX helps RN development for nearly all the same reasons that it helps in web development. (I really think that this is one big reason that, in RN at least, the JSX approach is used so much more frequently than the native approach. RN itself just  feels  like a web platform, so the more web-natural approach is inevitably going to become predominant.)  Flutter, on the other hand, has no such design philosophy. It is intended to be a purely native cross-platform solution, and though it states that it was inspired by React-Native, it writes a lot more like a native desktop or mobile app than a web app. It also runs using Dart and not Javascript, which from the standpoint of integrating something like JSX is a major consideration. For one, while the JS DOM functions can horribly verbose (both because of the function's design and the JS language itself), Dart as a language is much more facilitating of clean UI-declarative code while Flutter for the most part does a good job of keeping UI constructors concise. For another (as they pointed out) Dart is a statically-typed language, so the nature of JSX being designed for a dynamically-typed language like JS is going to run into snags in cases where, for example, a UI constructor requires a Widget of a specific type, the only solution to which just adds to the verbosity. Personally, it feels like a solution that, over time, will inevitably result in a DSL that has gotten bloated and difficult to maintain as it has to account for a growing number of use cases inherent in a system that it wasn't intended to be used for.  As such, I really don't see how JSX/DSX would benefit Flutter development productivity beyond just being a matter of personal preference. Both syntaxes overall are roughly equivalent in verbosity, and where one loses verbosity in specific instances it makes up for it in clarity (the closing XML tags, for example). It largely boils down to if someone is coming to Flutter from a web-oriented background (React/RN, Angular, etc.) or from a native background (Java/Kotlin/Swift, WPF/UWP, etc.) that will determine which approach they would prefer. Even on this thread alone, there are a lot of user stories that say they were extremely skeptical of JSX at first but after using it for a few months changed their opinion to ""can't do without"". (Though the cynical part of me wants to point out that the same thing could very well happen to them for the native Dart approach if they gave it a chance.)  All that being said, I can't really see myself agreeing that DSX should become officially supported by the Flutter team as an alternative to native UI constructors. While it's perfectly fine as a third-party solution (and all the props to  for actually implementing it), it doesn't really fit with the core nature of Flutter as a non-web-technology-based platform. As such, more power to anyone who wants to use DSX in their own projects, but I would side with the mindset that there are plenty of other more important things the Flutter team could and should be spending their time on.  Now with THAT being said, while I don't really agree with DSX being officially supported, I do think that there should be an official UI format of  some  kind. As birkir pointed out, nearly every major native UI platform, be it desktop or mobile, has a UI format in addition to the direct code-based approach (and most of them are pre-processed into the code-based approached anyway). Having separate UI files from logic files has always been the recommended way to embrace the MVVM pattern (which, incidentally, is one thing that has always rubbed me the wrong way about JSX). What I would argue, therefore, is that Flutter have something similar - instead of an inline UI DSL format, it should have a separate UI format that is intended to go into its own file away from the Dart code.  As part of this line of thinking, I've actually done some work this past weekend along that end. If I could be allowed to shill for a moment, I developed a project that I've coined ""FLUI"" that is my attempt to show what such a format might look like. Rather than going with an existing DSL (or a modified version), I developed an entirely new one that takes inspiration from YAML and I've tried my best to keep close to the ""feel"" of the Flutter-constructor-approach layout. Granted, it's a  very  early implementation, so I don't really expect it to not have a massive slew of issues, but I've included the source for the processor script (written in C#/.NET Standard) so that people can play with it if they want to. :)"
technical,"said he hopes proponent of JSX Syntax just to spend some days really working with Flutter before continuing lamenting. This comment shows we really have spent a lot of days working with Flutter, and the request of JSX is really our feeling from our heart. **Group Dart:** ""Dart syntax is way better and JSX/DSX is just not good"" **Group JSX/DSX:** ""JSX/DSX syntax is way better and Dart is just not good""  I can't be the only person to see this? Both sides make valid points in favor and against their position.  I think what is lost here is that  not only had criticism BUT ALSO DID SOMETHING ABOUT IT. Trying to bridge the gap of web devs/react/react-native and flutter in order to benefit Flutter.  And my 2 cents... As a full stack dev I have exp with a wide array of languages and approaches... JSX is one of my favorite ways to write code and I do hope there will be an alternative syntax to Darts... And i'm not saying the current syntax is bad, it's just that I prefer JSX style."
technical,"We all know that the syntax for building UI is a very important part of mobile development experience. For now the syntax is a little verbose, I have to new something just for the purpose of adding a margin: margin: new EdgeInsets.symmetric(horizontal: 4.0), I think there may be an easier way.  Would it be possible to build a DSL like what the Kotlin team did for Android developers? It's called Anko, a DSL for building Android UI. A concise syntax helps to keep the code readable & maintainable, can also make the building work enjoyable, it does matter. Please Flutter team estimate it seriously before you make the decision. We all love to see Flutter make a bigger success in the coming years. **Please dont introduce XML-like syntax to Flutter.**  I programmed in Android Java for more than a year, then I started looking for a cross-platform toolset to try. I started with React Native and then tried React. I didnt really like the JSX syntax because it is not quite javascript and not quite html, so just another thing to learn. When I tried Flutter I found it much easier to get started (probably mainly due to my Java background).  I think some of the reasons why I wouldnt like to see an XML-syntax added to flutter : 1. Another thing to learn - could be spent learn Futures instead ,P 2. Context-switching - you are context switching between XML and code which is just unnecessary cognitive load. 3. There have been days when I have programmed in Java in the morning and Python in the afternoon. With React you may need to understand Javascript, Babel, JSX, HTML, CSS and more in one codebase. 4. The reason why XML is not necessary in Flutter is because dart has named arguments which replace XML attributes pretty well. 5. Dart has the very cool dartfmt which indents the code really well for no effort. #### Comparing to Android  6. You have to learn the programmatic way anyway, why add another way of doing things? 7. The XML layout in android is faster to display changes on the device, but running in Flutter is practically instant anyway so adding XML wouldnt provide that advantage. 8. The Android XML + programmatic combination adds complexity, such as inflating XML snippets and injecting into the XML tree programmatically. 9. Instant run is so fast in Flutter you dont need the XML model to help visualize how it will appear, you can just press a key and see the change immediately. 10. The errors from programmatic layout issues are different from layout issues in XML, so thats two sets of things you need to understand.  I would go one step further and remove pubspec.yaml and replace it with pubspec.dart and have configuration in dart code."
technical,"As mentioned several times before DSX is simply a different syntax and it is a superset of Dart, so everything you can do with Dart you can do it inside DSX. You can also mix and match both syntaxes as you see fit. Obviously you didn't even bother to check out what are some of the DSX features that were done to support Dart #But then the argument of having an easier conversion from react to flutter is invalid. As JSX is radically different from your prototype :  And none of your examples here or from your link actually simplify the code or improves readability  As much as I can relate to your feeling of missing JSX (got the same when starting flutter), after some experience, the native syntax feels pretty good actually  As a side note, there's a much better solution to your separation of concern. That is a template file You could have an xml/yaml file next to your widget. And then use the awesome code generation tooling dart provides.   I'd rather prefer a : which then using a custom code-gen generates the following dart file : The end result is even better than a ""DSX"" for separation of UI/logic. It's better for potential UI generators too.  And it's much easier to implement using built."
technical,"I think you underestimate how willing people are to try things, even when they're not fully polished. For example, someone could easily write a shell script that wraps flutter run to do the preprocessing first, then call flutter run. I have a script myself that wraps hot reload to do something similar (I run the analyzer first, then only if it passes do I send the hot reload signal to the flutter script). Not really (see e.g. statement for why it might be difficult to make such statements), but probably not in the coming weeks. That's quite possible, indeed. I'm just saying that based on these experiments, we may reach any number of conclusions, all the way from ""do nothing"" to ""add radical new features to the language syntax"" and anything in between. The point being, we haven't made any decisions yet, and are very open to learning what needs to be done based on all these discussions and experiments. Add JSX to backlog and let it compete, gladiator-style, with the million-plus-one other urgent requirements?"
technical,"Can we please keep a constructive tone when debating. Adding features because others have it is weak argument. Okay. Why does flutter have hot reloading? Where did that come from? Jesus dude.  How did we fail to provide solid argument for you guys? **Project traction and attracting developers** is our number one reason.  Reason number two, **readability**: Reason number three, **GUI Builders**. I'll quote the first line in the README.   A new mobile app SDK to help developers and **designers** build modern mobile apps for iOS and Android.  I would hate to see Flutter going down the same rabbithole as Polymer before it even reaches beta."
technical,"Really !!! The only thing radical in these discussions has been the reaction of the naysayers.  As it's stated in the title of this ticket, DSX is JSX-like, it is not JSX-identical or else it would had been called JSX, and the additions to it are minor and provides options to developers.  You could write it like:  Hummmm, you seem to be confusing 'separation of concern' with 'separation of technology'. These things are very different, you separating dart code and markup code in different files, is simply 'separation of technology' and provides none of the benefits of 'separation of concerns'. A concern here would a component/widget that encapsulates reusable code cleanly, it doesn't matter that inside that component/widget it is using different technologies.  Also separating technologies as you recommend is highly inferior to JSX/DSX which uses the host language for all of its imperative constructs (for loops, function calls, if statements, etc). After much of code and examples posted here (especially), I come to conclusion, that JSX adds much more value to JS in contrast to DSX and Dart. But one feature is very important from my point of view: Closing tags. Like: reduces a LOT of cognitive complexy in deep structures like here in contrast to:  But well, if you use it like this: there is a small profit."
technical,"I think than if would be possible indent and format the widget in other way would be more similar to jsx and friendly for users who have experience with xml and html (almost all the android devs)...check this code in codelab check this dart to jsx code  and compare with this other format more htmlish  dart  this is a bit more similar and now you only need view from left to right to notice the different widgets similar to html/xml/jsx  the element (widget) attributes has more indentation than the new widgets, so this makes more clear could understand and check the code  would be great if I could have automatic indentation for this format on the different ides, right now I do this by hand.... After reading all of the comments here and privately discussing with my friends (native mobile apps developers,java/kotlin/objective-c/swift guys),My observation:  People are asking for two things, *    Better readability and easier writting  . Deep nesting mixed with some syntax noises(parentheses, semicolon, new, child, children) is annoying in current way of coding. *    Seperate style/design from code  .  Visual seperation is good for reading(Differentiating the style from imperative code by a glance)  and a real seperation is good for tooling(e.g, IDE with Layout Editor) .  There are also mainly two groups with different opinions, * Improving current syntax without introducing another complexity to solve the problem. There have already been  some improvements in this direction.For instance,optional new,const in Dart 2.0 and proposed virtual ""closing tag"" comments feature. * Introducing extra markup languages(JSX like or XML like) to solve the problem.  You can not easily conclude which one is better at present. So just let the community do their experiments first and make the final decision(accept or reject) later."
technical,"Yeah, the third case is quite common in Flutter, so it makes sense to skip the extra { nesting. Ah but I am making most of those things unnecessary by using css-like styles !!! The transpiler expands these css-like styles into appropriate Flutter calls. Now the tagging structure is much cleaner and styles can be easily imported from designer tools like InVision, Figma, Atomic, etc"
technical,Can you send me an email please? Already made this proposal in the old thread but I still think that's an important point  IMHO removing the need to use new/const already helped a lot. I have real difficulties in the way dart format formats the trees. it does not emphasis enough the tree structure IMHO compared to:
technical,"I don't really understand why that's desireable. ""child"" and ""children"" aren't special. Consider ListTile for example. How would you do that one? Why are ""icon"" in IconButton, or ""home"" in MaterialApp, something you want to give a name for, but not ""child"" in Expanded? All three are just arbitrary arguments that happen to take Widget objects. There's nothing magical about ""child"" vs ""home"".  Less boilerplate, you don't need to say it because it is inherited in the structure.   Why is that desireable? I mean, why would any of this count as ""programming""? It's all just expressions.  It's related to (2) because it makes life of toolmakers, specially GUI builders, much easier since they don't need to fully parse Dart, but it also makes reading the code easier.   I don't really understand the distinction.  The format of XML is very simple so when you see '{}' you know it is calculating an expression in dart. Same for the opposite, when reading dart code and you see '<Capital' (a less-than followed by a word that is capitalized. example \<Row) you know that an object hierarchy is being created from XML markup. Also in the final XML processor I would avoid passing objects to attributes of parents and instead create child tags as below:"
technical,That's the longest and pointless Github issue I ever seen. also reported DC
technical,"I just wanted to respond to the readability question raised here, though I understand readability is only one of the many factors we need to consider.   Code readability and simplicity which in turn drives a whole bunch of other benefits.   If this is the main benefit then this is something we can test. We could take a mixture of engineers who are used to writing HTML, React, iOS code, Android code, Flutter, command-line apps, and so on, and present them each with various syntaxes, and ask them to describe what they think the resulting UI would look like. We can then measure which syntax gets the best results. InMatrix is this something we could look at after the animation work wraps up, maybe?  There are certainly ways to empirically study code readability, and we can have a more serious discussion when it's time for Q4 planning. To make such a study useful, we need to define what kinds of reading tasks are important to developers in the context of Flutter programming. In addition to reading a whole build method and picture what the resulting UI might be, readability also affects smaller tasks such as identifying the build method in a dart file, matching braces, reading inline comments, etc.  To support some of those more narrowly-scoped tasks, we can experiment with UI enhancements in the editor first, which is usually cheaper than introducing and maintaining a markup language. The closing label feature in VS code is one of such UI enhancements, and we should understand how well it solves the brace matching problem it sets out to address. There are plenty of other options in this space we haven't tried yet. For example, a different font or background color might be used to display the build method to help the developer mentally separate it from the rest of their code.  Another thing that strikes me as important is how we can encourage people to not write giant build method and take advantage of the composition nature of the framework. If the build method is taller than the height of your screen, it's going to be hard to read regardless it's Dart or XML. Another thing that strikes me as important is how we can encourage people to not write giant build method and take advantage of the composition nature of the framework. If the build method is taller than the height of your screen, it's going to be hard to read regardless it's Dart or XML.  It's not just the build method. It's all other methods that the build method calls to build the widget tree. Very common in React to use smaller methods to build sub-tree pieces and then call those into the larger tree.  Also in WebStorm with JSX, each XML node has +/- that can be used to expand/collapse node and children to make reading structures larger than the height of the screen easier."
technical,"I know this conversation is old and yes it was getting quite hot, But i will like to drop a few lines to what i consider is happening here.  Flutter is NEW. Its a completely NEW way to do things. Yes, it did borrow from the react paradigm. But doesn't mean it needs to follow its same steps. I don't think its the flutter's team objective to attract developers from react native to come to flutter, its only to build something new that developers might find interest in. Google uses it internally before sharing it with the world and they have been productive with it. I share the comments with Hixie that its not any different from JSX to build the UI. Yes its a little more verbose to right pure dart. But it actually makes debugging your code a lot easier.  The reason why im against a markup language or JSX or anything that sits on top of a technology is that it require tons more work from the platform. You can be happy composing Markup language for a UI, but you will have so many developers working on the platform crying and pulling hairs out to make it work for you. Another point of view is that JSX Worked for a Javascript community, Where as always its main objective is to make things easier for the developer and not worry about the tradeoffs. Don't get me wrong React(JSX) for web was a match made in heaven because HTML Is markup anyway. But for React Native look at all the code in the repository they had to do to make it work. Adding JSX to flutter would require tons of work and 2 things to think of when adding new features. And again just to be able to remove the child parameter and the const and new Keywords?. I prefer knowing what is really happening in the code and have control over what is happening than have magical Syntax that all its doing is add overhead.  Well that is my opinion. Don't want to start a new gigantic discussion. Just to mention the fact that JSX Is awesome for a react/javascript community because it worked for them, but for Dart/flutter i  find it a bit overkill to add JSX Just to attract developers from React Native.  Wow, could have written a blog post xD Another thing to learn - could be spent learn Futures instead ,P  The thing to learn makes the current recursive object construction monstrosity simpler and familiar to everyday React Native devs, so my guess is people will prefer to learn that.   Context-switching - you are context switching between XML and code which is just unnecessary cognitive load.  Not an issue, there is no context-switch, it is just part of the environment that makes programming UI/UX cleaner.   Then it would generate Flutter code which is not intended to ever be editted  Why not? Not very useful then."
technical,"the Flutter team has seen this feedback (on requesting a JSX/DSX approach) and they  This bug is still open because we haven't figured out what we want to do here. We're eagerly looking at the experiments (e.g. 's) to see how people use them. We're planning, at some point, to provide a hook in the build system for codegen tools such as this, though this isn't something we're likely to do in the near future. On the long term we hope to use what we learn here to influence Dart's development as  a language. This could mean adding something like E4X/H4X/JSX/DSX to Dart itself. Or maybe we'll learn that nobody really ends up using it so we will do nothing. Or maybe everyone needs something different so codegen hooks and custom packages like 's are the answer. We don't yet know. Apple thinks that cross platform means IPhone, IPad & Mac OS. They are more likely to add turrets on top of the walled garden than build something cross platform :)"
technical,"You keep asking for 'valid arguments' and when they are given you dismiss them as 'future Dart will have this' or 'this is not a valid argument'.  The fact is that right now Dart/Flutter has noisy child/children everywhere when building a widget and XML/DSX doesn't. Right now it is a very valid argument for using DSX to remove this child/children noise. Can you just accept that as a valid argument? (just because you say Dart will have that in the future, it doesn't make the argument invalid).  It is also a fact that parsing XML is much simpler than parsing the full Dart language *and* every language out there has an XML parser, whereas only Dart has a full and complete Dart language parser. Can you see that this is also a valid argument?  There are lots of valid arguments, they are just not valid for you and that's why I stopped arguing about it. If people are interested in what was said, just read the 2 threads on JSX fully. I have no interest in convincing you to use DSX, you are happy with plain Dart so be it, I am not. Arguments for an optional DSX syntax:  1) Onboard and attract more developers coming from React (web and native) 2) Better experience of porting React Native components into Flutter widgets 3) Drives consistency of child/children properties in widgets 4) Code readability (opinionated argument) 5) View logic linting separate from the dart linting 6) Opens up a world for UI building tools 7) Opens up the ecosystem for pre processors"
technical,"I appreciate your work for the community and the people who want to build UIs that way, but I really hope I'm never forced to use anything like that in Flutter :-((( As a newcomer to Flutter but pretty familiar with React, a few things stood out to me:  - The state management model is pretty much the same - The widget/component virtual render tree is pretty much the same - Knowing the state and component model, I basically feel ready to write an app now, excepting some Dart specifics and platform APIs, but... - The styling language is a stumbling block. I'm referring  but it's not easy to figure out what imports are required to make things work (EdgeInset? Color?) or when I should use primitives instead of class instances. - The CSS parser from 's DSX converter is really useful for figuring out layout equivalents in the Flutter model.  Regarding JSX: - I don't think it's necessary to invent new JSX syntax to support Flutter patterns. Some of the syntax quibbles in the this thread could be resolved using some of the newer React patterns like higher-order-components (functions that build component classes) and render props (components that take functions as arguments, the functions return elements). E.g., named ""child slots"" could translate to something like this in JSX:    - The best argument against JSX I saw was that Dart has named arguments. - Why is it important for an element to know whether it has multiple children or one child? Maybe a ""fragment"" construct could disambiguate that API."
technical,"(I apologize in advance for the wall of text.)  As someone who hasn't used either React-Native or Flutter long enough to consider myself a definitive source whether raw Dart or JSX/DSX is ""better"", this issue thread has been rather fascinating to read. There are a couple things that I would like to put my 0.02 down on though.  To begin, I find myself agreeing with various people on the nature of what JSX actually is and how it benefits developers. First and foremost, JSX was designed as a form of ""dynamic HTML"" that could be inlined into existing Javascript code. It's indispensable for JS-based web platforms like React in that it enables web developers to cleanly and efficiently interact with the DOM without having to wrestle with the god-awful native way (or the only-slightly-better jQuery way). Also, by its very nature, JSX encourages UI development that can be easily decoupled from the underlying data, promoting a well-organized project structure. In that environment, JSX is a tool to enable greater productivity, and I feel that it would be virtually impossible to argue with that.  How that paragraph relates to React-Native is that, even though it is a mobile-development platform, it is directly descended from React. As such, nearly all of the syntax and paradigms were still originally created with web-development in mind. That is by design - RN's whole shtick is that you can ""create cross-platform mobile apps using React"", so it's  supposed  to feel like web development when using it.  RN apps are also predominantly written in Javascript, so the inclusion of JSX is a natural one. JSX helps RN development for nearly all the same reasons that it helps in web development. (I really think that this is one big reason that, in RN at least, the JSX approach is used so much more frequently than the native approach. RN itself just  feels  like a web platform, so the more web-natural approach is inevitably going to become predominant.)  Flutter, on the other hand, has no such design philosophy. It is intended to be a purely native cross-platform solution, and though it states that it was inspired by React-Native, it writes a lot more like a native desktop or mobile app than a web app. It also runs using Dart and not Javascript, which from the standpoint of integrating something like JSX is a major consideration. For one, while the JS DOM functions can horribly verbose (both because of the function's design and the JS language itself), Dart as a language is much more facilitating of clean UI-declarative code while Flutter for the most part does a good job of keeping UI constructors concise. For another (as they pointed out) Dart is a statically-typed language, so the nature of JSX being designed for a dynamically-typed language like JS is going to run into snags in cases where, for example, a UI constructor requires a Widget of a specific type, the only solution to which just adds to the verbosity. Personally, it feels like a solution that, over time, will inevitably result in a DSL that has gotten bloated and difficult to maintain as it has to account for a growing number of use cases inherent in a system that it wasn't intended to be used for.  As such, I really don't see how JSX/DSX would benefit Flutter development productivity beyond just being a matter of personal preference. Both syntaxes overall are roughly equivalent in verbosity, and where one loses verbosity in specific instances it makes up for it in clarity (the closing XML tags, for example). It largely boils down to if someone is coming to Flutter from a web-oriented background (React/RN, Angular, etc.) or from a native background (Java/Kotlin/Swift, WPF/UWP, etc.) that will determine which approach they would prefer. Even on this thread alone, there are a lot of user stories that say they were extremely skeptical of JSX at first but after using it for a few months changed their opinion to ""can't do without"". (Though the cynical part of me wants to point out that the same thing could very well happen to them for the native Dart approach if they gave it a chance.)  All that being said, I can't really see myself agreeing that DSX should become officially supported by the Flutter team as an alternative to native UI constructors. While it's perfectly fine as a third-party solution (and all the props to  for actually implementing it), it doesn't really fit with the core nature of Flutter as a non-web-technology-based platform. As such, more power to anyone who wants to use DSX in their own projects, but I would side with the mindset that there are plenty of other more important things the Flutter team could and should be spending their time on.  Now with THAT being said, while I don't really agree with DSX being officially supported, I do think that there should be an official UI format of  some  kind. As birkir pointed out, nearly every major native UI platform, be it desktop or mobile, has a UI format in addition to the direct code-based approach (and most of them are pre-processed into the code-based approached anyway). Having separate UI files from logic files has always been the recommended way to embrace the MVVM pattern (which, incidentally, is one thing that has always rubbed me the wrong way about JSX). What I would argue, therefore, is that Flutter have something similar - instead of an inline UI DSL format, it should have a separate UI format that is intended to go into its own file away from the Dart code.  As part of this line of thinking, I've actually done some work this past weekend along that end. If I could be allowed to shill for a moment, I developed a project that I've coined ""FLUI"" that is my attempt to show what such a format might look like. Rather than going with an existing DSL (or a modified version), I developed an entirely new one that takes inspiration from YAML and I've tried my best to keep close to the ""feel"" of the Flutter-constructor-approach layout. Granted, it's a  very  early implementation, so I don't really expect it to not have a massive slew of issues, but I've included the source for the processor script (written in C#/.NET Standard) so that people can play with it if they want to. :) As both a React/RN and a Flutter user, I heavily disagree with the idea of  ""DSX"".   DSX would bring **nothing**. JSX is used in react because the JS syntax is horrible. But in the case of Flutter, widgets creations is super easy.  it is already out of the box readable, easy to write, type/generic compatible, and without any unneeded duplication.  The only complain you could possibly have with the current syntax is ""It is hard to know where is the closing parenthesis of a widget""  But then again, dart plugin of the officially supported IDEs solves this problem. So that when we open the code from before in say vscode, we'll see As for the ""It's hard to differenciate casual code from UI code"", react rules apply to flutter too :  Widgets should be either dumb or smart. Smart widget don't have UI logic. Dumb widgets have nothing but UI logic.  If you follow this pattern, you should never fall into a situation where you fail to differentiate UI from the rest. This is even more true when following something like BLoC pattern, which heavily enforce separation of business and UI."
technical,"There's a lot of unneeded characters in the default react syntax. Let's compare words repetition and characters count for each syntax (excluding function definition, indentation and 'return')   React without JSX: - 133 characters, including 3 parenthesis, 3 brackets, 3 :, 4 , and 11 spaces - React.createElement written twice  JSX: - 104 characters, with  2 parenthesis, 3 brackets, 1 :, 4 < and 5 spaces - Container and Text written twice  Dart: - 99 characters, with 2 parenthesis, 4 :, 3 , and 4 spaces - No repetition   In term of characters, the obvious winner here is dart syntax.  Now we also have to take other dart specifics into account.   Dart types single child vs multi-child, has const constructors and allows generics and even positioned parameters. JSX supports none of these.  A few examples that would badly convert to JSX : As mentioned several times before DSX is simply a different syntax and it is a superset of Dart, so everything you can do with Dart you can do it inside DSX. You can also mix and match both syntaxes as you see fit. Obviously you didn't even bother to check out what are some of the DSX features that were done to support Dart"
technical,"this PR contains the following merge commits. Please rebase your branch to remove these commits. click here for bot help Babel.js has this neat little website, where you can type JSX and it converts it to plain Javascript I will do an equivalent one for DSX to Dart. Just a proof of concept, let's see how long it takes me..."
technical,"Since this feature request is an alternative to the current way and it doesn't change the current way, there shouldn't be any controversy at all, if you don't like JSX/DSX continue programming as you do today.  The so called controversy only exists because the Flutter team needs to do work to enable the community to do DSX properly. If the Flutter tools (compiler, analyzer, ide support, etc) supported pre-processing with source maps, DSX would had been done long ago, and most likely other language innovations/ideas, from 3rd party developers, would had happened too. Becomes: The argument for and against JSX has been exhausted by discussion, there is plenty of material that you can look up. I'm only sharing my personal opinion.  First, code readability. I can immediately get a crisp clear idea about the structure of the UI in JSX (takes seconds), the mental modal is very visual. Second, usability. I would argue that JSX can be used even without knowing JS/Dart or anything about underlying API. This is very suitable for someone who is just learning programming, or for some one who is a part of my team, designers now can code the UI.  The description of the application is completely declarative (not just expressive), when you work on a large project with hundreds of components, this way of describing UI makes a huge difference (you have to try it to really appreciate it). I didn't like JSX when I first saw it too, once I used it just felt right, this how I want to describe UI from now on, it was a clear breakthrough in the way I approach building interfaces.  It's not about writing less lines of code, it's not about having a ""syntax sugar"", it's about building tools for humans. I'm also against the argument that everyone should use JSX, this is ridiculous. You use the tool that let's you get your work done faster with less confusion, in the case of many (including myself), it would be JSX (or DSX in the case of Flutter), that's all."
technical,"Looks like most of the controversy here is not around JSX at this point, but DSX. I'd suggest splitting DSX discussion into it's own thread and leave this one generic to JSX.  In the end DSX is just one way of getting something closer to JSX so we shouldn't be mixing these two discussions in one thread regardless. Big no for this, i really think that 1 language only is a big gain, jsx syntax will come with more things like separation of xml from js, etc... Not good. thats my opinion."
technical,"I have to disagree with this quote from the **Group JSX/DSX**   Dart is just not good  Dart is very good and robust language, nobody is dissing the language. The discussion is not about Dart, but a synthetic layer on top of it that most UI developers already use today, and we are proposing that Flutter incorporates something like that.  - Android has XML Layouts. - iOS has Storyboard XIB (XML) - GTK+ has XML for Pango etc. - Qt has QML (YAML like) - Xamarin has XAML  Most of these frameworks and languages have UI markup languages that separate the View from the Controller logic. Then React comes along with different approach (that we are proposing here) and I think we have to agree that RN is flying right now in terms of user growth and popularity, and I may be wrong, but mainly because of JSX.  ...  **Is it really so crazy proposal that we have to get this kind of feedback from Flutter team/users?** birkir and all of them bring a host of troubles Flutter does not have \o/ There is no need for another language. You can separate the view in Flutter as well, even with the same language."
technical,"I agree so much with  that they can co-exist...thats it and I thought this had been settled on the old thread Bravo, that's absolutely it.  In React you can do it 4 ways (and I just found out about the other 2 ways today !!!)  (1) You can use JSX (which is what I like) (2) You can use the original way (which is similar to Flutter)  At the end of the link above they even mention 2 promising community projects  (3) Hyperscript syntax for React.js markup (4) Terse syntax for hyperscript.  Having alternatives is a good thing, gone are the days where you could get a Ford in any color you liked as long as it's black :)"
technical,"E4X was an ECMA standard. Mozilla shipped it for a while, but then removed it (a very unusual move for a browser vendor). Other browser vendors never wanted to implement it.  I would say only Adobe fully championed E4X and had a good following with developers. Browser vendors do add and remove stuff from their browsers all the time, didn't Google remove MathML support?   That's one possible theory. People tend to ask for many other things besides, though, and many of the things they ask for get implemented, and there are workarounds for animated GIFs too, so I'm not sure this fully explains the situation.  **Here is the thing about React and JSX. Initially I completely dismissed it**, I was in love with Angular and doing massive work with Ember. I tried desperately to convince the team to go with Angular but that didn't go anywhere as others had their eyes on Aurelia. Someone pointed me to React, I read the docs and evaluated and simply said that there was nothing in there that Angular or others couldn't do it. Then last year I worked on a new project and had the chance to try something new so I gave React a try and **you really don't fully appreaciate what React brings to the table until you develop with it for awhile, then it becomes night and day against all other frameworks. It's a mixture of simplicity and expressiveness brought together by JSX.**   How would a GUI editor handle this markup? I don't really understand how to visualise the UI for this. I would represent the \<List.builder as a rectangle and if its child/children where other widgets I would put rectangles for that inside it. Since the child in this case is a function, you can simply put a rectangle saying 'uneditable/code' to tell users that this widget is created from code or in this case easily deduce that the function is a shallow wrapper to the <Text widget and simply present that, I mean a rectangle that says that the function is a shallow function wrapper and inside it the List item widget rectangle (\<Text in this case). But really, markup is a huge huge investment in time and added complexity tooling to transition it to real program.  All I am asking is to add these simple extensions on the Dart compiler so that if developers want to they can write using this XML structure. The old way would continue to work and the amount of work involved to do this is not huge at all. You can actually see how many lines of code it takes the babel.js compiler to do JSX and I am talking hundreds and not thousands of lines (I just checked it).   And the tooling is always last to come. So in the mean time, while all that manifests into reality, there's two worlds. Two interesting ways of doing everything. One in the default language and one in markup  Sure but React has been like this and that is not an issue at all.   When I write documentation, I have to write it twice, once for Java and once for UiBinder XML Markup.  Not in React because markup lives inside code.   is the added complexity worth the journey!  Absolutely, it's like the argument of whether you should train your developers with the latest techniques and risk them leaving your company. The bigger risk is keeping them around untrained. So you must adopt the latest techniques out there or risk being left behind.  React is leading the journey with the latest techniques to develop UI/UX. It has tremendous traction with developers. Your greatest risk is not meeting the React bar.   JSX I think aims to solve other issues where you want to blend together what your doing with HTML and javascript  JSX is not just for HTML, React Native generates Views (like Flutter Widgets) from the XML markup.   I think the real question is, how can the UI be built faster.  More like how UI/UX can be built better. Better meaning: faster, higher quality (code and UI/UX), smooth designer-developer interaction, etc.  By the way, really nice job done on the developer tools, 'flutter doctor' was awesome !!! I am now cooking with gas and can be dangerously creative ,)"
technical,"The original design of JSX is about familiar way of creating/manipulating tree structures which specially show up when doing UI work, think component hierarchies which show up in web development, native development, any UI development, etc. That's the point, we are not looking to replace the current way, we are looking to add an alternative way that is familiar to React developers.  Your Anko proposal would be familiar to Android Kotlin developers so go ahead and propose a spec that works with current Flutter hierarchy in a separate ticket. Once I see (or try an online version of your spec) I would be able to see if it can generate/interoperate with current Flutter widget hierarchy. Not that I recommend you doing this but it is possible: create an anonymous function and call it. There is no such thing as the best solution, it's all about having choice, having the choice of using something familiar that maps directly to Flutter widgets and adds no overhead. By the way, try the following on my online transpiler at: and you get: DSX is similar to JSX but for Dart & Flutter so it has features of its own which are described on the link above."
technical,"Really !!! ""nobody seems to bother to explain what exactly Flutter would gain... blah blah blah"". Haven't you read this thread fully? Is your attention span greater than your JSX knowledge?  You guys are suffering from NIH syndrome (Not-invented-here). ""Good artists copy, great artists steal"", mediocre artists, well, behave like you.  Just the fact that supporting JSX is relatively simple, and it will tremendously help attract new customers (React Native mobile developers) to the platform, makes it a no-brainer which you guys fail to see. It doesn't bold well for the platform. Can we please keep a constructive tone when debating."
technical,"It's a way to describe UI  in Javascript  (hence the ""JS"" part of the name). And no, since it's an inlined DSL, it is  not  language-agnostic. And even if it was, that still doesn't make it  the ""better choice"", since there are plenty of truly language-agnostic DSLs out there that would be woefully inadequate for UI declarations.  Flow is just like TypeScript: a static type checking system for Javascript. It is not a React tool, nor was React designed to be used with it. React is first and foremost a Javascript library, and JSX was designed to be used with React. Whatever secondary tools and utilities get introduced into React development are ultimately irrelevant to React+JSX interoperability.  I have used JSX, and though I do have personal opinions of it, I have deliberately left those opinions out of this discussion. In fact, had you read my previous comments, you would know that I had praised JSX for revolutionizing UI development in React. Other than some mildly tangential comments I've made about the market penetration of JSX as a whole, my arguments have been specifically about JSX  in Flutter . And on that topic, there is no practical basis to determine the efficacy of DSX, so all we can do is examine how JSX has been implemented in other places, and that examination does not bode overly well.  Unless, of course,  you've been using DSX every day as well and can enlighten us on the practical advantages of using DSX in Flutter?   That is what I'm doing.  DSX is being proposed as a UI solution for people familiar with JSX. There are key design elements in JSX that were intended to be used for an environment completely different from Dart and Flutter.  Those differences need to be addressed for DSX to be successful.  I am not being a  hater . I'm trying to promote constructive discussion and ask the important questions. Yet all the responses I've been getting has amounted to subjective tautology (""JSX is good because it's the future, and it's the future because it's good""), dismissive hand-waving of crucial design points (""DSX does not need to account for differences between JS and Dart because there aren't any""), or just plain hostile (""You obviously don't like JSX so stop talking about DSX"").  You do not make a successful product solely with unadulterated praise. It needs to stand up to and account for criticism as well. People showing up and saying ""OMG yes please, make DSX"", while uplifting, is not helpful. There have been several people throughout this thread that have brought up perfectly valid criticisms of DSX, both with its initial design and with the concept as a whole. And for the most part, many of these criticisms have yet to be directly addressed, with the general attitude being dismissive.  My only fear is that all this unconditional love for JSX is preventing people from viewing DSX objectively. I get why you guys want something like JSX in Flutter, and I can relate - my opinion that Flutter needs a dedicated UI DSL is what lead me to create flui. But if the only people allowed to talk about DSX are the people who have nothing but good things to say about it, then it  will  fail. Can we recenter the discussion on this topic? In fact, I don't see any reason to keep this issue open. Dart team stated that they have other priorities. And the pro JSX side volunteered to make their own DSX implementation  Maybe we should just have a few open source repository proposing different solutions (even barely working). Such as DSX, or templates. And then consider redirecting from Flutter's readme or awesome flutter to these repos. And if there's anything blocking a DSX implementation, create another issue with the specifics.  Then let the community do its job."
technical,"It's a way to describe UI  in Javascript  (hence the ""JS"" part of the name). And no, since it's an inlined DSL, it is  not  language-agnostic. And even if it was, that still doesn't make it  the ""better choice"", since there are plenty of truly language-agnostic DSLs out there that would be woefully inadequate for UI declarations.  Flow is just like TypeScript: a static type checking system for Javascript. It is not a React tool, nor was React designed to be used with it. React is first and foremost a Javascript library, and JSX was designed to be used with React. Whatever secondary tools and utilities get introduced into React development are ultimately irrelevant to React+JSX interoperability.  I have used JSX, and though I do have personal opinions of it, I have deliberately left those opinions out of this discussion. In fact, had you read my previous comments, you would know that I had praised JSX for revolutionizing UI development in React. Other than some mildly tangential comments I've made about the market penetration of JSX as a whole, my arguments have been specifically about JSX  in Flutter . And on that topic, there is no practical basis to determine the efficacy of DSX, so all we can do is examine how JSX has been implemented in other places, and that examination does not bode overly well.  Unless, of course,  you've been using DSX every day as well and can enlighten us on the practical advantages of using DSX in Flutter?   That is what I'm doing.  DSX is being proposed as a UI solution for people familiar with JSX. There are key design elements in JSX that were intended to be used for an environment completely different from Dart and Flutter.  Those differences need to be addressed for DSX to be successful.  I am not being a  hater . I'm trying to promote constructive discussion and ask the important questions. Yet all the responses I've been getting has amounted to subjective tautology (""JSX is good because it's the future, and it's the future because it's good""), dismissive hand-waving of crucial design points (""DSX does not need to account for differences between JS and Dart because there aren't any""), or just plain hostile (""You obviously don't like JSX so stop talking about DSX"").  You do not make a successful product solely with unadulterated praise. It needs to stand up to and account for criticism as well. People showing up and saying ""OMG yes please, make DSX"", while uplifting, is not helpful. There have been several people throughout this thread that have brought up perfectly valid criticisms of DSX, both with its initial design and with the concept as a whole. And for the most part, many of these criticisms have yet to be directly addressed, with the general attitude being dismissive.  My only fear is that all this unconditional love for JSX is preventing people from viewing DSX objectively. I get why you guys want something like JSX in Flutter, and I can relate - my opinion that Flutter needs a dedicated UI DSL is what lead me to create flui. But if the only people allowed to talk about DSX are the people who have nothing but good things to say about it, then it  will  fail. Can you elaborate on why you want this? Maybe show an example of what it would look like compared to today?"
technical,"Coming From React-native, I first supported JSX like implementation, and didn't like the nested objects, but I am beginning to enjoy OOP and see everything as an Object!  For the people coming from React-native, I highly recommend this plugin! Can you expand on your exp with react-native(JSX), Flutter(OOP), and your journey from one to the other?"
technical,"Sorry guy, I didn't mean to hurt your feelings... I was just trying to gently guide you while trying to manage my own frustrations. We are all humans after all. Can you send me an email please?"
technical,"when I see this, I get flashbacks from xml layouts from Android.. I don't think it's a good idea to implement this. Now that you dont even have to write new and const it even looks better. Can you share an example/img/link of, ""xml layouts from Android"" you're referring to?"
technical,"I am a self-taught programmer, and like most self-taught guys, I started with Javascript. Then I started to learn React and React-Native. I think in recent years, specially after ES6, OOP style was added to Javascript.  So people like me aren't used to OOP style of programming. Even though React native Component are classes just like Widgets in Flutter.  JSX kind of hides the pure OOP picture. Basically, it hides what happens under the hood. Note: React was designed for web developers, and web developers are used to html syntax. That is why JSX is so popular, among web developers.  Personally, I think pure OOP makes more sense for big projects. clarktank,  When discussing computer languages you have to be aware of: (1) Syntax - the characters and words that make up the language (2) Semantics - the meaning of those characters and words  For example, function calls in many languages look like the following (i.e. have the following syntax):  Imagine now that another language decides to use square brackets instead of rounded brackets for function calls, it would look like the following: The thing here is that the syntax is different but the semantics are the same. I mean both are basically doing function calls but with a different syntax. Not true at all. JSX/DSX is just a different syntax to exactly the same thing (the semantics are the same). In the case of JSX, XML tags just create React Components (just like you can do it in Pure Javascript). In the case of DSX, XML tags just create Flutter Widgets (just like you can do it in Pure Dart). The syntax is different but it generates exactly the same thing so it is identical under the hood.  JSX is popular because it is a great way to manage component tree hierarchies, whether for web, mobile or any UI development. Notice in the code below you don't know if the dropdown component is for web or mobile for example.  How is that? (considering that using JSX/DSX or Pure Javascript/Dart generates exactly the same thing under the hood)."
technical,"DSX is not XML, it's XML-like so it doesn't need to follow XML semantics, kind of like Angular template language ,) Anyways, I am always open to better alternatives or suggestions and would love to have a discussion here. Coming From React-native, I first supported JSX like implementation, and didn't like the nested objects, but I am beginning to enjoy OOP and see everything as an Object!  For the people coming from React-native, I highly recommend this plugin!"
technical,"Ah but I am making most of those things unnecessary by using css-like styles !!! The transpiler expands these css-like styles into appropriate Flutter calls. Now the tagging structure is much cleaner and styles can be easily imported from designer tools like InVision, Figma, Atomic, etc Cool, I use many widgets with closures like FutureBuilder. I hope your transpiler can generate something like"
technical,"Personally I don't believe the two ideas can coexist in the current state of Flutter - we really shouldn't be encouraging a split of this nature this early - but at this point it looks like the only compromise. Could you be more specific as to why they can't coexist ? (given the fact that DSX is just syntatic sugar, or as emalamela says ""just a wrapper of the current way"").  Also why is it too early to provide a different syntax to exactly the same thing? I am basically asking why is there a need to delay this, what would be different in the future that is not there yet right now?  Personally I rather not put any limits on what people can do with Dart/Flutter. Let the market/community decide. Basically if what is created doesn't add value, people won't use it and it will die. If it becomes popular, it is because the community found it useful and valued it. No need to pick winners and losers right now."
technical,"Kotlin is nice, I am a fan *but* it doesn't run on iOS..... actually it does but it hasn't been released yet (pre-release stage right now). For UI/UX development I still prefer JSX instead of Anko DSL. I like the fact that I can visually separate declarative markup from imperative code and be able to mix and match components together quite easily. Dart, Kotlin and Swift all have similarities  I like that : 1. Dart is more familiar if you come from Java. 2. You can take your Dart skills and use them to build web pages - which is valuable when creating apps, you can build features on the web (and show them in a WebView) where it makes more sense (maybe quick admin pages or product listings which need to be google indexed). 3. Dart was built from the start to compile to javascript which I assume is not easy to add to a language later.  These are basically the reasons I chose Dart over Kotlin / Swift / React.   Although the decision to support Dart and Swift in googles new Fuchsia OS is confusing to me."
technical,"That looks amazing! Can I try it somewhere? Don't worry pretty soon you will be able to try my version and it is even better than jayjun.  Quick update: 2 weekends ago I got the UI working great, Last weekend I got the parser fully working and half of the transpiler working. This coming weekend I hope to finish it off if I avoid the Superbowl ,)  I have thick skin and mule like stubbornness so I don't even notice these down-votes, thanks for pointing them out though."
technical,"That looks amazing! Can I try it somewhere? DSX +1  Would've loved to write a bunch of pro/cons, but by having read all these comments, I feel like I'm just gonna repeat everything over and over. Stop being so naive and ignorant, nobody says you'll be FORCED to write UIs using DSX, it's simply an option (better alternative). There's a reason you can write JS in 101203103 different ways."
technical,"hi, thank you for your response.  I'm wondering about your solution. Are we misusing the xml namespace as described in w3shool-XML Namespaces?  As it states that namespace is primarily used to solve the naming conflict in XML document.  So when someone reads the above XML, they might think that you are declaring a Container tag under the namespace 'children' of the namespace 'actions', not you are enforcing every nested children must be a Container. It does confuse me when I first read your propose syntax without reading the above explanation.  Could we have something better? DSX is not XML, it's XML-like so it doesn't need to follow XML semantics, kind of like Angular template language ,) Anyways, I am always open to better alternatives or suggestions and would love to have a discussion here."
technical,"hi, thank you for your response.  I'm wondering about your solution. Are we misusing the xml namespace as described in w3shool-XML Namespaces?  As it states that namespace is primarily used to solve the naming conflict in XML document.  So when someone reads the above XML, they might think that you are declaring a Container tag under the namespace 'children' of the namespace 'actions', not you are enforcing every nested children must be a Container. It does confuse me when I first read your propose syntax without reading the above explanation.  Could we have something better? Dude, it's basic etiquette. Any new message on this thread emails everyone subscribed to it, so it's rude to post comment that doesn't contribute to the discussion because it annoys people for no reason. Basic reactions like ""+1"" and ""Thanks"" can be conveyed with a simple thumbs up reaction, so just do that.  That being said, if this thread has really devolved into arguing over whether someone should or shouldn't post a ""+1"" message, that's a big red flag that all constructive discussion has officially died, and it really should be closed (perhaps permanently this time)."
technical,"This may be a counter argument to this request, and/or maybe some insight's to keep in mind if you want markup. I have strong feelings that adding markup creating some challenges with GWT I'd hate to see another API go through.  I've seen a couple other frameworks go through this transition regarding with UI building. Markup like this works great for tooling, in so far it's heavenly for the IDE developers. It's easier to separate the responsibilities of who does what. Although I think it can be done better.  GWT started out this way, building UI's with Java. Then came along UiBinder, where you could build the UI in xml markup, with a specification. Then the tooling, Eclipse Plugin, was able to generate UI's in xml markup. Android is doing it too, no need to belabor the point. So what I saw happen, markup works great for UI IDE developers. But really, markup is a huge huge investment in time and added complexity tooling to transition it to real program. And the tooling is always last to come. So in the mean time, while all that manifests into reality, there are two worlds. Two interesting ways of doing everything. One in the default language and one in markup. So I support GWT today. When I write documentation, I have to write it twice, once for Java and once for UiBinder XML Markup. So the real question, if you want to go down the markup road, I think the question should be asked, is the added complexity worth the journey!  JSX I think aims to solve other issues where you want to blend together what your doing with HTML and javascript. It really doesn't feel like the added complexity of markup specification suits the needs of writing UI with markup. Especially when you don't really have a document markup language as the target. At least not for the everyday user.  On a positive note. I like to work on tooling. So I can see a markup language being quite useful. It's much easier to write and modify AST when your using markup.  But then again, if you have enough minds on the job, it doesn't really matter what you do. At the end of the day, if the developer can write his application faster with your api, your going to get traction. But at what cost to the engineering team.  I think the real question is, how can the UI be built faster. I think tooling could write the dart, skip any middle man markup. And my aim isn't really to say it's not worth it, but really count the cost's on all the fronts if the road is taken! E4X was an ECMA standard. Mozilla shipped it for a while, but then removed it (a very unusual move for a browser vendor). Other browser vendors never wanted to implement it.  I would say only Adobe fully championed E4X and had a good following with developers. Browser vendors do add and remove stuff from their browsers all the time, didn't Google remove MathML support?   That's one possible theory. People tend to ask for many other things besides, though, and many of the things they ask for get implemented, and there are workarounds for animated GIFs too, so I'm not sure this fully explains the situation.  **Here is the thing about React and JSX. Initially I completely dismissed it**, I was in love with Angular and doing massive work with Ember. I tried desperately to convince the team to go with Angular but that didn't go anywhere as others had their eyes on Aurelia. Someone pointed me to React, I read the docs and evaluated and simply said that there was nothing in there that Angular or others couldn't do it. Then last year I worked on a new project and had the chance to try something new so I gave React a try and **you really don't fully appreaciate what React brings to the table until you develop with it for awhile, then it becomes night and day against all other frameworks. It's a mixture of simplicity and expressiveness brought together by JSX.**   How would a GUI editor handle this markup? I don't really understand how to visualise the UI for this. I would represent the \<List.builder as a rectangle and if its child/children where other widgets I would put rectangles for that inside it. Since the child in this case is a function, you can simply put a rectangle saying 'uneditable/code' to tell users that this widget is created from code or in this case easily deduce that the function is a shallow wrapper to the <Text widget and simply present that, I mean a rectangle that says that the function is a shallow function wrapper and inside it the List item widget rectangle (\<Text in this case)."
technical,"I used Adobe Flex Builder extensibly...   Developers tended to view it as an ActionScript tool.  Yes, but I often switched from design view to code view and vice-versa. Starting a screen I would go to design view and use drag/drop to layout widgets and generate first static screen. Then I would add in code and some static data to fill in screen so I could show people something running that looked like production ready stuff. Productivity was incredible. As development progressed, I connected the front end to the back end and the amount of ActionScript code grew and yes it dominated the code overall but even at close to release time, I often used the design view to tweak the UI without having to dig into code.   However my overall impression is that defining UIs with a combination of markup and code is a mixed bag at best.  Not in today's world. Imperative languages have evolved in the philosophy of Python and are great for development. Declarative techniques with embedded markup (XML) became mainstream with the advent of React, and JSON became the preferred text based data format. E4X was only implemented in ActionScript  E4X was an ECMA standard. Mozilla shipped it for a while, but then removed it (a very unusual move for a browser vendor). Other browser vendors never wanted to implement it. (They've implemented other new ECMA features, though.) With E4H, the browser vendors were never interested in implementing it (though again, they've implemented plenty of other things I've helped invent).   Well, if I asked you for 2 things and you didn't do either in 3 months and there is an alternative to the first thing, I would also only ask you for what is totally impossible to do given your responsiveness and previous delivery performance.  That's one possible theory. People tend to ask for many other things besides, though, and many of the things they ask for get implemented, and there are workarounds for animated GIFs too, so I'm not sure this fully explains the situation.   Kind of funny but it's like putting the XML closing tag that you mentioned before was too verbose.  Indeed. This is an optional IDE feature, and one that you don't have to write into the code, which makes a big difference to whether the verbosity is an issue, though.  How would a GUI editor handle this markup? I don't really understand how to visualise the UI for this."
technical,"your second demo example has 466 chars compared to dart's 391 (both without spaces). To me it looks visually bloated. I would have to learn it's semantics, which I don't have to with normal Dart code. And I have no idea how to use general purpose programming paradigms with it (if, forEach, etc.).  If I had the choice, I would stick with the present model. Everyone is just giving its own subjective opinion on comparing both syntaxes, but we have to agree on one fact: it's a very controversial feature. There is love and hate, but most opinion diverge on the usefulness of JSX over plain Dart.  In any case I think it's totally unreasonable to ask the Flutter team to commit to support this feature before the 1.0 release. While the current way of building UI may not seem great for everyone - it works (and it works great in some opinions).  If people really want a JSX-like syntax now, a community driven effort seems like the way to go. And it would help make the case (or not) when the Flutter team considers it in post 1.0 release.  Personally I strongly agree with the following argument: it's not because JSX works for React (where you are already building the UI using a markup language) that it automatically works for Flutter."
technical,"supporting pre-processing via source maps is a generic mechanism that does not depend on any specific transpiler. It is functionality that is meant to support any future imaginable language. The Chrome browser/debugger supports it quite nicely and I can debug any language that transpiles to JS.  For testing you can come up with any simple kind of transpiler to show how to use source maps. For example write a trivial transpiler that generates Dart/Flutter code with a blank line between every line on the original file. (.d2 = .dart, .d2 is Dart/Flutter file, out .dart file will contain a blank line between every line in the original file).  Yes, I can work on generating source map for a testfile. Flutter is currently reluctant to try to please NativeScript, ReactNative, Android, Web, and other developers who accustomed to similar XML layouts. They have more important things to do, so let's disband and go to sleep."
technical,"Maybe it is only my issue. I really feel that I have a hard time to read the UI code, even it is in Dart 2 syntax. flutter was inspired from React Native I don't see how that says ""everything is the same except the language is called Flutter instead of JS"".  I really feel that I have a hard time to read the UI code. and you claim the only imaginable solution to that is DSX? Have you considered that Flutter is not even 1.0 and that IDE support can improve over time? There were only the first tiny steps taken recently to get better Flutter IDE support in the code editor like quick fixes (""wrap xxx"") and closing labels. There are unlimited possibilities to improve developer experience and most in this discussion is like ""Flutter is not yet perfect and therefore we need DSX"" usually without concrete arguments about how or why DSX would solve or improve that."
technical,"Dude, it's basic etiquette. Any new message on this thread emails everyone subscribed to it, so it's rude to post comment that doesn't contribute to the discussion because it annoys people for no reason. Basic reactions like ""+1"" and ""Thanks"" can be conveyed with a simple thumbs up reaction, so just do that.  That being said, if this thread has really devolved into arguing over whether someone should or shouldn't post a ""+1"" message, that's a big red flag that all constructive discussion has officially died, and it really should be closed (perhaps permanently this time). Got it but while you are at it perhaps you should also consider basic etiquette while blasting this thread with your novels (long & fictional writing).  Please stop spreading FUD (fear uncertainty & doubt) with your spray-and-pray barrage of senseless questions (you know, throw as much crap on the wall and see if anything sticks) and demands.  After all your writings you haven't added any value to DSX so I have no interest having a discussion with you on this subject. Your motive is obvious though, promote FLUI while blasting DSX.  Tell me something, do you have answers to your own questions when they are applied to FLUI? Let's discuss FLUI for a bit shall we?"
technical,"plenty were given  not really. Just general claims which can be true or not. No additional details were provided that would allow others to verify.   Not really, my work is almost ready  Great. Then there is no need for the Flutter team to do anything and the issue can be closed ,p Does this include support autocompletion and syntax checks in all IDEs supported by Flutter? Great. Then there is no need for the Flutter team to do anything and the issue can be closed   Does this include support autocompletion and syntax checks in all IDEs supported by Flutter?  Most IDEs already support XML and JSX so it wouldn't be hard for them to add my minor additions."
technical,"Well, there's always the option writing an analyzer plugin that parses DSX and converts them into regular Dart function calls, so that the compiler doesn't have to do any extra work.  There is currently no way in the dart language to implement this without any hacks (think race conditions, recursive imports and stuff). This needs to be integrated at level where everything will work as expected, hot reloading, static analysis, etc.   If you ask me, DSX should only be opt-in, ideally through some sort of plug-in. I think it's extremely unnecessary to add it to the language itself, as then server-side and Web users of Dart have to adapt to the changes, not just Flutter users. The vast majority of code written in Dart doesn't even remotely need any XML syntax, so enforcing it on everybody is a bad decision.  If you read the thread, that has been the idea from day one. We just want the support from flutter/dart to make an transpiler.   TLDR, if you want DSX that bad, write an analyzer plugin and bring it to Dart yourself. The Internet will love you, you'll get thousands of Github stars, and you'll feel just like it's React. Win-win.  Read the thread, this has already been done by   (analyzer plugin isn't gonna cut it)  P.S. I'll even race you to do it  Great! Just even a theory of how this would work would be interesting to see. Guess we're all waiting on some sort of pre-processing support, then."
technical,"The only thing that is stopping me from giving Flutter a try is the fact that they chose not to use JSX. IMHO JSX is the best choice to express component hierarchy Heard about Flutter, wanted to give it a try, immediately stopped even considering that the minute I saw the syntax, which is a shame because chances are that this a great piece of tech. to build products.  I've been doing React/React Native development for the past 2.5 years, sacrificing the productivity leverage the JSX syntax offers when it comes to describing UI is a lot to ask. I would seriously consider spending the time to learn all about Flutter and study whether it's worth switching to when such a feature is supported out of the box.  I can't even imagine the numbers of potential adopters/users/devs Flutter is losing because of this, will revisit in the next months."
technical,"Babel.js has this neat little website, where you can type JSX and it converts it to plain Javascript I will do an equivalent one for DSX to Dart. Just a proof of concept, let's see how long it takes me... Here's Hixie's latest example in DSX, using Airbnb's style guide and assuming all child elements can automagically map to child properties. If the intention is to improve readability, I think future Dart wins in spades.  You have to check documentation to know what can turn into tags (presumably Widgets only), or how many child elements are required/allowed.  If your goal is to output HTML from JavaScript, JSX makes a ton of sense as a middle ground. Because when you want tags at the end of the day, React.createElement('div', null, 'foo') is far worse than <divfoo</div.  If your goal is to output a tree of Dart objects from... Dart, and a nicely formatted tree of Dart constructors is perfectly (arguably more) readable, I don't see a point in detouring through XML. And I'm not missing those XMLs coming from Android."
technical,Would this help with the issue of losing track of closing tags ? Idea: Auto generated closing tags Here's some proposed markup syntax for Hixie 's example
technical,"I wished Amy proponent of JSX Syntax just to spend some days really working with Flutter before continuing lamenting. I came from an Xaml based systembut quickly got used to it. Just give it a real try. Hey. Do you think that I have not spent a lot of time learning flutter? In the layout, at the end of ""Packing widgets"" section, the code that the tutorial gave does not work. **I mentioned the issue under the flutter issue block, but no one wants to care about this.**  - Dart code: main.dart - Pubspec: pubspec.yaml"
technical,"Very good catch !!! I bowl my virtual hat to you. My initial prototype has had some holes in it from the beginning that I was aware of and just waiting to people to find them and come forward. I was just hoping people were interested in discussing the technology instead of fighting over it.  The solution I have to this is to provide the array type as another parameter on the namespace. As the namespace is getting large, we can set the short form for 'children' be '*'.  In Example 2, if actions were an array of 'Container' instead of default 'Widget', it would look like the following alternatives: hi, thank you for your response.  I'm wondering about your solution. Are we misusing the xml namespace as described in w3shool-XML Namespaces?  As it states that namespace is primarily used to solve the naming conflict in XML document.  So when someone reads the above XML, they might think that you are declaring a Container tag under the namespace 'children' of the namespace 'actions', not you are enforcing every nested children must be a Container. It does confuse me when I first read your propose syntax without reading the above explanation.  Could we have something better?"
technical,"After reading all of the comments here and privately discussing with my friends (native mobile apps developers,java/kotlin/objective-c/swift guys),My observation:  People are asking for two things, *    Better readability and easier writting  . Deep nesting mixed with some syntax noises(parentheses, semicolon, new, child, children) is annoying in current way of coding. *    Seperate style/design from code  .  Visual seperation is good for reading(Differentiating the style from imperative code by a glance)  and a real seperation is good for tooling(e.g, IDE with Layout Editor) .  There are also mainly two groups with different opinions, * Improving current syntax without introducing another complexity to solve the problem. There have already been  some improvements in this direction.For instance,optional new,const in Dart 2.0 and proposed virtual ""closing tag"" comments feature. * Introducing extra markup languages(JSX like or XML like) to solve the problem.  You can not easily conclude which one is better at present. So just let the community do their experiments first and make the final decision(accept or reject) later. hooluupog virtual closing tag comments already work in IntelliJ, AS, VSCode since a while"
technical,"I have yet to see an argument in favor of JSX. Above discussions only contains arguments like ""it's better"", ""it increases productivity"", or similar but no argument **why** or **how** it would actually be better than pure Dart code, especially considering planned language changes that would reduce the difference between JSX-like and pure Dart code even more.   You can program iOS with Objective-C and Swift (2 languages)  You can program Android with Java and Kotlin (2 languages)  How is that related to the JSX discussion? How can that be considered an advantage? You can use Swift on iOS because it's the successor of Objectiv-C. You can use Kotlin on Android because it's considered a slick improvement to Java. That sounds like you argue that JSX should replace Dart. That doesn't make much sense to me. How is that related to the JSX discussion? How can that be considered an advantage? You can use Swift on iOS because it's the successor of Objectiv-C. You can use Kotlin on Android because it's considered a slick improvement to Java. That sounds like you argue that JSX should replace Dart. That doesn't make much sense to me.  You actually said it (JSX is a slick improvement over the current way) but came up to the wrong conclusion (JSX to replace Dart)."
technical,"I am not sure Dart is more familiar if you come from Java. I came from Java and have no problems with Kotlin or Swift, basically the type declaration is reversed, nothing really new as it was used in Pascal and ActionScript.  Yes, you can build web pages with Dart but I don't see major take up with that. The only other language that is doing well on the web is TypeScript, since it integrates well with the top 3 most popular web frameworks.  Take a look at the different syntax available for React on the Web: Both Dart versions have no chance against JSX !!!  TypeScript was designed to compile to Javascript and it is better than Dart for that. It also supports JSX. Dart is getting squeezed from all sides. Swift has momentum so it is wise to support it on Fuchsia OS along with your baby. How long until the prototype? I'd love to use it!  I used React for a while, and JSX increased my productivity ten-fold. This isn't a controversial issue: other teams decided, correctly, that this would be better, so why not Flutter? (Rhetorical: I read the thread... (facepalm))"
technical,"I'm not aware of any obstacles preventing someone outside Google's Flutter/Dart teams from writing a JSX-like thing for Dart/Flutter.  I would *love* to see bugs on those if someone is trying.  (It's also possible that I've missed them in the comments above, since I admit not to having read every word of this very long bug.) :)  Thanks!  Glad to see such enthusiasm! How would Intellij and/or VS Code (with Dart-Code) be able to set breakpoints and step through code from a .dsx file ? (I mean this is not a .dart file). How about auto-complete functionality from the .dsx file  (as with a .dart file) ?  You have invested a lot on the tooling but the tooling does not support pre-processing (with source maps) of a new language (like DSX) that transpiles into Dart/Flutter seamlessly.  A transpiler is available but there is no easy way to fully integrate it. P.S. This ticket is only half of the comments !!!"
technical,"I'm not aware of any obstacles preventing someone outside Google's Flutter/Dart teams from writing a JSX-like thing for Dart/Flutter.  I would *love* to see bugs on those if someone is trying.  (It's also possible that I've missed them in the comments above, since I admit not to having read every word of this very long bug.) :)  Thanks!  Glad to see such enthusiasm! Huumm, a little improvement but not so good... Here are the things that gets accomplished by using XML: (1) No more 'child' & 'children' stuff (2) easy for 3rd party tools to manipulate (parse, analyse and regenerate) (3) notice that the switching between markup and programming is easily detected. I mean inside XML you have '{}' to delimit code and in code you have '<Capital' to delimit markup. Also separate all the 'style' things from the main structure. I know this is basically fully endorsing React's way but you are half way there anyways ,)"
technical,"Already made this proposal in the old thread but I still think that's an important point  IMHO removing the need to use new/const already helped a lot. I have real difficulties in the way dart format formats the trees. it does not emphasis enough the tree structure IMHO compared to: I agree for the purpose to make flutter simple that don't want to use JSX-like syntax, and I also support the concept of split into multiple widgets, but now it let me feels like the MVC era is back in jquery. Such a scenario that there was a simple widget, with padding,border and center layout for it later, many  ""} "" symbol seriously affect the readability.However, it is a integral widget, I don't want to split it, any useful solution? Though my english is poor,I try hard to express my thoughts."
technical,"they can have their DSX. If the Flutter team doesn't implement it, it can be an open source initiative. That doesn't mean the pure-Dart approach like it is now won't work anymore. Both worlds can co-exist. I agree so much with  that they can co-exist...thats it and I thought this had been settled on the old thread"
technical,"I don't miss JSX a bit in Flutter. This would only bloat the framework and tools for a few small gains here and there. I am 100% with you on this issue. Lack of JSX, which is a perfect fit for Flutter, makes Flutter look old and rusty, feels like 1990s technology. Actually it seems like everyone, in one way or another, is adopting JSX, the latest one being the popular Vue.js framework."
technical,"vote is vote, it sure means.  I respect your feeling, maybe it's true. But even with a reverse ratio (1 to 2), that still means 33% user base. Can you say 33% is a small share? Yeah, some people are watching. This also means  lack of JSX-like is one of the reasons prevent people from choosing flutter .  Flutter aims at more developers, not only ones read all issues. I am a self-taught programmer, and like most self-taught guys, I started with Javascript. Then I started to learn React and React-Native. I think in recent years, specially after ES6, OOP style was added to Javascript.  So people like me aren't used to OOP style of programming. Even though React native Component are classes just like Widgets in Flutter.  JSX kind of hides the pure OOP picture. Basically, it hides what happens under the hood. Note: React was designed for web developers, and web developers are used to html syntax. That is why JSX is so popular, among web developers.  Personally, I think pure OOP makes more sense for big projects."
technical,"Dart, Kotlin and Swift all have similarities  I like that : 1. Dart is more familiar if you come from Java. 2. You can take your Dart skills and use them to build web pages - which is valuable when creating apps, you can build features on the web (and show them in a WebView) where it makes more sense (maybe quick admin pages or product listings which need to be google indexed). 3. Dart was built from the start to compile to javascript which I assume is not easy to add to a language later.  These are basically the reasons I chose Dart over Kotlin / Swift / React.   Although the decision to support Dart and Swift in googles new Fuchsia OS is confusing to me. I am not sure Dart is more familiar if you come from Java. I came from Java and have no problems with Kotlin or Swift, basically the type declaration is reversed, nothing really new as it was used in Pascal and ActionScript.  Yes, you can build web pages with Dart but I don't see major take up with that. The only other language that is doing well on the web is TypeScript, since it integrates well with the top 3 most popular web frameworks.  Take a look at the different syntax available for React on the Web: Both Dart versions have no chance against JSX !!!  TypeScript was designed to compile to Javascript and it is better than Dart for that. It also supports JSX. Dart is getting squeezed from all sides. Swift has momentum so it is wise to support it on Fuchsia OS along with your baby."
technical,"How long until the prototype? I'd love to use it!  I used React for a while, and JSX increased my productivity ten-fold. This isn't a controversial issue: other teams decided, correctly, that this would be better, so why not Flutter? (Rhetorical: I read the thread... (facepalm)) I am working on it this weekend but I keep increasing the scope with new ideas so hopefully I can have something out this weekend.  Some of the interesting things I am experimenting with: 1) using xml namespace on child tags so they get inserted with correct Dart named argument in the parent. 2) spread operator to better organize style like properties so they can be defined/grouped outside xml in a map and then brought in and spread on a tag like typical React stuff. 3) styleCSS property that generates flutter styles from CSS.  (1) & (2) are generic so works for all Dart Flutter code. (3) is specialized per Flutter Widget (Container, Text, etc) and I am only doing those 2 for now."
technical,"Nice job with the transpiler One thing I would say is easier to follow with JSX is the closing tags. The idea which I mentioned before : Idea: Auto generated closing tags In your first example, would give flutter code of (removed the optional new in future Dart) : I appreciate your work for the community and the people who want to build UIs that way, but I really hope I'm never forced to use anything like that in Flutter :-((("
technical,"I get the arguments for JSX, but I think there are just as many folks (self included) who hate the thought of jamming XML into a programming language. It just feels wrong  (but I *totally* get that others don't feel the same way).  Given that it is next to impossible to take features away once implemented, I'd suggest the pre-cautionary principle.  Let's see how some of newer Dart 2 syntax features play out before committing to a substantial change to the way Flutter apps are built. I can understand you. I used to be against JSX and mixing js with xml/html.... then I tried it. After a few months spent with react, I fell in love with JSX. Two killer advantages are: 1. no new syntax and utilities 2. no more passing of variables, functions etc."
technical,"Once again Plain Dart or DSX is simply a matter of style, no point arguing which is better. It is just a personal preference and people have their reasons for picking one or the other. What has happened in the React World is that once you go with JSX, you simply don't go back. As others have stated above you get hooked on JSX after using for awhile. JSX has been adopted by others outside React's World (Typescript, Vue) and it is a great fit for Flutter. What the community needs is generic pre-processing capabilities with source map support built in the Dart tools (compiler, analyser, IDE, etc) so that DSX or anything better can be developed by the community and fully integrated into Flutter (debugger, auto-complete, etc). Sure, thing is this ticket is not about a generic 'improving the developer experience' thing. It is very specific and it is about getting JSX support in Flutter. The community wants DSX as an alternative to the plain Dart way. I cannot see myself using this (but I'm willing to give it a try). I dont think this is a silver bullet, but there is so much enthusiasm that its worthwhile doing.  I think google should remove the language obstacles to allow the community to build what they want."
technical,"That's not a given, who would had thought that Google would remove MathML from Chrome? It is not a change to the way Flutter apps are build at all, it's just an alternative way that doesn't change the current way, and most important it is simply a different syntax to the class library. A simple transpiler does the mapping without needing any information from Flutter classes so it works just fine for anybody's code, as well as Flutter now and in the future.  Yes, you don't know React until you spend a few months working with it and then you realise how phenomenal a library it is for dealing with component hierarchies. I could say exactly the same thing about Flutter. Spend some weeks using it and you won't miss JSX."
technical,"No, I didn't miss the comment at all, it just makes no sense you telling me that people coming from JSX will find DSX too complex.  The same thing applies to the current dart implementation.         Anyway, I don't think we can convince each other here. So I'll just list a few more reasons as to why I dislike JSX in flutter and then ""wait and see"".   **1. Widget creation is different from React**  In react, it's the library that handles components creation. JSX is fine because it says ""Don't bother about how things work. We do things for you"".  In flutter that is not the case. We manually instantiate a new widget every on build call. **This is really important to understand, and JSX would make it less clear.**  And, in the continuation of that logic :  **2. Peoples may think <Foo / does something special that new Foo() don't**  <Foo /  in a method feels special. It seems like it does something extraneous built-in in the framework. Which is true in react, where  components are wrapped into a React.Element. This translates in react into <Foo / != new Foo() and don't have direct access to Foo.  This doesn't apply in flutter and may cause confusion:  In react, if Foo never uses its child then Bar is never instantiated. And Foo is instantiated after the render method returns. While in flutter this is the opposite. Both Bar and Foo are created immediately  This would potentially lead in React developers making un-optimized flutter code.  **3. In general Dart/flutter is not JS/react**  If we add JSX in dart, I can already see the issues about type union or be able to do return foo && <Component / or the upcoming async rendering in react. Justified with a ""we already have JSX so we can have that too !""  I'd prefer a proprietary syntax or no syntax at all, for the sake of not having to implement the latest JSX/react feature in dart  **4. JSX makes some dart specifics unclear**  A small example, Scaffold requires for appbar a PrefferedSizeWidget. If we created Scaffold using JSX, peoples would expect that you can replace any given JSX with another. Which is not true. I mean, it makes it very unclear why we can do. I disagree with a lot of what you said but I appreciate your effort since you are taking the time to think deeply into the issue.   1. Widget creation is different from React  To me it doesn't matter because this is just an implementation detail, conceptually once you see some XML, in React it is a Component, in Flutter it is a Widget.   2. Peoples may think <Foo / does something special that new Foo() don't  I think people will learn pretty quickly that Dart/DSX is not Javascript/JSX.   3. In general Dart/flutter is not JS/react  Yes, we finally agree on something but even though they are different the common thread is that they are declarative UI frameworks and I think declarative tree structures are handled nicely with JSX/DSX. It keeps you on the declarative programming way of thinking. We are not adding JSX in Dart, we are adding DSX, it is different but has similarities to JSX and familiarity is a huge thing. So with that reasoning why are you using Dart? it looks pretty similar to Java and yet it is different than Java, the heck with it, let's scrap all of these Java keywords and concepts and come up with something vaguely similar to Erland that you can only program with one hand while doing a pretzel yoga move on top of Mount Everest ,)   4. JSX makes some dart specifics unclear  Not really, if you connect incomparable Widgets the Dart compiler will spit error messages just like if you did it in plain Dart. I can not emphasize enough that DSX is simply thin syntactic sugar, there is no magic, just a difference syntax to the same thing."
technical,"It's crazy to see this issue getting buried. In my opinion it's going to be a make or break for Flutter to implement JSX-like syntax for composing widgets.  I simply don't understand the target audience, many ios and android devs are moving to react native, it would seem to be the perfect opportunity to harvest market share.  I encourage people involved to give react native a spin and see what we are talking about. I don't miss JSX a bit in Flutter. This would only bloat the framework and tools for a few small gains here and there."
technical,"(1) No more 'child' & 'children' stuff  I don't really understand why that's desireable. ""child"" and ""children"" aren't special. Consider ListTile for example. How would you do that one? Why are ""icon"" in IconButton, or ""home"" in MaterialApp, something you want to give a name for, but not ""child"" in Expanded? All three are just arbitrary arguments that happen to take Widget objects. There's nothing magical about ""child"" vs ""home"".    (2) easy for 3rd party tools to manipulate (parse, analyse and regenerate)  You can parse, analyze, and regenerate Dart code. But I agree we should make that easier. Hopefully in the coming years the Dart team will provide better APIs for this.    (3) notice that the switching between markup and programming is easily detected.  Why is that desireable? I mean, why would any of this count as ""programming""? It's all just expressions.    I mean inside XML you have '{}' to delimit code and in code you have '<Capital' to delimit markup.  I don't really understand the distinction.    Also separate all the 'style' things from the main structure.  You can do this today in Flutter if you really want to, just put the style in a variable like you did in the XML case. I don't really understand why that's desireable. ""child"" and ""children"" aren't special. Consider ListTile for example. How would you do that one? Why are ""icon"" in IconButton, or ""home"" in MaterialApp, something you want to give a name for, but not ""child"" in Expanded? All three are just arbitrary arguments that happen to take Widget objects. There's nothing magical about ""child"" vs ""home"".  Less boilerplate, you don't need to say it because it is inherited in the structure.   Why is that desireable? I mean, why would any of this count as ""programming""? It's all just expressions.  It's related to (2) because it makes life of toolmakers, specially GUI builders, much easier since they don't need to fully parse Dart, but it also makes reading the code easier.   I don't really understand the distinction.  The format of XML is very simple so when you see '{}' you know it is calculating an expression in dart. Same for the opposite, when reading dart code and you see '<Capital' (a less-than followed by a word that is capitalized. example \<Row) you know that an object hierarchy is being created from XML markup."
technical,"I would like to know if people who disagree with it have used React with and without JSX. I encourage everyone to try it, so this discussion would be less theoretical. After the initial shallow learning curve, it becomes very natural (and it's not all about the number of characters, because the closing tags allow you to read it easier). It's not different from learning anything else that's new, like Flutter, which in the long run boosts productivity. And I agree that it's not for everyone, which is why it would be optional. I think it's important to keep in mind that we have one another's best interests in mind - making it easier to build stuff. I think this transpiler would help, and it would be easier to adopt if it had official support.  Also, Thank you I don't think anybody here doubts that JSX React is an improvement over classic React. But like I said, that solution was created because of problems related to properties of the underlying platform. Flutter doesn't have these properties.  The question is not 'is JSX helpful for React?' the question is 'is something like JSX helpful for Flutter?'.  I think the answer is no."
technical,"Another thing to learn - could be spent learn Futures instead ,P  The thing to learn makes the current recursive object construction monstrosity simpler and familiar to everyday React Native devs, so my guess is people will prefer to learn that.   Context-switching - you are context switching between XML and code which is just unnecessary cognitive load.  Not an issue, there is no context-switch, it is just part of the environment that makes programming UI/UX cleaner.   Then it would generate Flutter code which is not intended to ever be editted  Why not? Not very useful then. I don't think its the flutter's team objective to attract developers from react native to come to flutter  Really !!! React Native is dominating and given that the total number of mobile devs using cross platform tools is limited, do you really think Flutter can become a home run hit without attracting React Native people?   Yes its a little more verbose to right pure dart. But it actually makes debugging your code a lot easier.  That's not a correct statement. Debugging JSX code, which is just javascript code is not any easier or harder, it is the same.   The reason why im against a markup language or JSX or anything that sits on top of a technology is that it require tons more work from the platform.  Who cares how much work was placed in the platform? Devs just want the latest techniques that improve code readability and productivity. No dev wants to use old and obsolete techniques when newer stuff provides better alternative.    I prefer knowing what is really happening in the code and have control over what is happening than have magical Syntax that all its doing is add overhead.  This is non-sense, I know exactly what goes on with JSX, it is a tiny little layer that transforms almost one to one but provides lots of benefits.  Negligible compile time overhead if you ask me.   Just to mention the fact that JSX Is awesome for a react/javascript community because it worked for them, but for Dart/flutter i find it a bit overkill to add JSX Just to attract developers from React Native.  JSX should also work great for Dart/Flutter. It is not an overkill by any means. Is there any good reason why JSX would not work for Dart/Flutter? If I were to code it up and released it, why would it not be a good fit for Dart/Flutter development?"
technical,"part/part of is an existing dart feature. It somehow joins two files into one. Usually used for code generation.  There are a few real cases scenarios that use such technique.  Like json serializable which generates a toJSON method and a fromJSON factory for classes based on their properties.  part/part of don't really do anything on its own. The interesting part is when you combine it to something like source gen. I don't think putting code on a repo would solve my problem because the current issue is about properly integrating DSX with Flutter tools as to provide a great developer experience with debugger, auto-complete, etc. working on .dsx file.  Telling users that they can use DSX but can't use debugger or enjoy auto-complete is a non starter for me. If anybody wants to help, what I need is to figure out a way to add full preprocessing support (with source map) to Dart Tools and VS Code Dart plug in. Once the tools support that DSX or any other transpiling language (any language that is a superset of Dart but compiles everything down to Dart) would just work. I don't need to justify anything, I am very confident that DSX will be a hit and there are almost 100 people interested in it on this ticket alone.  The DSX transpiler is blindingly fast and can transform .dsx files into .dart files faster than you can blink, so speed is not a problem, just trying to get feature parity as to make it a no-brainer for people to use DSX.   If that's the case, why is React the only framework with market presence that is using it? Both native and cross-platform mobile app frameworks have stuck with their storyboards, their XML files, their XAML files, and other such UI definition DSTs.  Just make a timeline and you will see the UI development evolution. Android and iOS development via their current ways started over 10 years ago so it uses 10 years old techniques (totally imperative techniques). The Reactive UI development (declarative) techniques started to appear for the web around 8 years ago. React appeared 5 years ago and it was the first Reactive framework to combine technologies seamlessly with JSX. Vue is now the latest Reactive framework that supports the older 'separation of technologies' techniques but it also supports JSX. On mobile Flutter is the latest and it uses Reactive framework techniques as React and it could take advantage of DSX just as Vue takes advantage of JSX. Vue is killing Angular because it is providing choice to developers and not being overly opinionated.  The problem with separate template files is that the imperative programming constructs there (if, for loop, etc) are so weak compared to what's available on the programming language used for the business logic. Combining the 2 in the way JSX does, to me, is clearly the future. It is !!!  Vue uses JSX"
technical,"no the DSX transpiler code has not been released because it is way too early for that. You can simply use 2 hand written equivalent files (.dsx and .dart) to test out pre-processing capabilities like breakpoints, stepping, auto-complete, etc.  Once the tools support pre-processing with some sort of source maps, I can add that to the transpiler and be unblocked. This would also enable others to experiment to their hearts desires. I don't work on Flutter, but it seems sort of unreasonable to demand certain tooling to benefit others, but then also refuse to release any early source code or examples of what you want the tooling to integrate with. For what it's worth, I don't know of any language or toolkit that provides pre-processing hooks of any kind - probably because it's just plain not easy, there are a lot of corner cases and assumptions around the language and syntax. If you generated source maps, I imagine getting at least some basic support in an IDE is pretty trivial, independent of Dart/Flutter. But again, this is all conjecture without knowing what your tooling does and how you'd expect it to work."
technical,"thank you.  thank you. Yes, it's just another option for people who love JSX to use. If that's not your thing cool continue doing as you are doing, nothing changes there. yes you can use render props for the named parameters but there is nothing you can do about the positional parameters. The key was that I didn't want to hard code anything in the transpiler so that it would work for everything.  great use that then, obviously DSX is not for everyone. this is not a replacement, it's an option for people that like JSX.  DSX is for React Native people that love JSX, if you don't see its benefits, don't use it. It's not DSX versus plain Dart. It's DSX vs. JSX, the closer the 2 are the better. thank you for listening !!! You hit the nail on the head why such huge resistance to something that will make React Native people really happy. Once React Native fans move to Flutter there will be tonnes of people to help out on these other needed things. Priority 1 is to steal mind share from React Native and make it as simple as possible for React Native people to move over.  How is DSX 'less dynamic' and 'less expressive' than plain Dart? DSX is Dart didn't you try the transpiler.  thank you.  off course it is. it is not helpful for current Flutter devs but it is very helpful for designers that can output CSS from their tools, it is very helpful for people vested in the React Native platform. very good observations. I get that... It wouldn't affect me in that way, but as I said before, there are missing features right now that I need to create a flutter app.   Once React Native fans move to Flutter there will be tonnes of people to help out on these other needed things. Priority 1 is to steal mind share from React Native and make it as simple as possible for React Native people to move over.  So you propose the flutter team should put features on hold for something that is questionable and may or may not attract a fraction of a specific subset of web developers? Every DSL is by definition less expressive than a GPL. With DSX how do I conditionally hide a widget? How do I iterate over a collection and map every element to a widget? How do I use a switch? You now have to make up syntax and semantics for constructs that you already have in your GPL. So why invent the DSL in the first place?  That wasn't the question... What problems would a DSL solve that you can't solve right now? You keep saying it's better, why is it better? I have no doubt DSX would attract some JSX people. I know people don't like things that are different. And familiarity seems to be the only argument. So why not use CSS? Why not use JavaScript? More people know how to use these than Dart.  If you design your system based on some other thing for the sole purpose of being familiar, you're not really innovating."
technical,"thank you.  thank you. Yes, it's just another option for people who love JSX to use. If that's not your thing cool continue doing as you are doing, nothing changes there. yes you can use render props for the named parameters but there is nothing you can do about the positional parameters. The key was that I didn't want to hard code anything in the transpiler so that it would work for everything.  great use that then, obviously DSX is not for everyone. this is not a replacement, it's an option for people that like JSX.  DSX is for React Native people that love JSX, if you don't see its benefits, don't use it. It's not DSX versus plain Dart. It's DSX vs. JSX, the closer the 2 are the better. thank you for listening !!! You hit the nail on the head why such huge resistance to something that will make React Native people really happy. Once React Native fans move to Flutter there will be tonnes of people to help out on these other needed things. Priority 1 is to steal mind share from React Native and make it as simple as possible for React Native people to move over.  How is DSX 'less dynamic' and 'less expressive' than plain Dart? DSX is Dart didn't you try the transpiler.  thank you.  off course it is. it is not helpful for current Flutter devs but it is very helpful for designers that can output CSS from their tools, it is very helpful for people vested in the React Native platform. very good observations. I get the arguments for JSX, but I think there are just as many folks (self included) who hate the thought of jamming XML into a programming language. It just feels wrong  (but I *totally* get that others don't feel the same way).  Given that it is next to impossible to take features away once implemented, I'd suggest the pre-cautionary principle.  Let's see how some of newer Dart 2 syntax features play out before committing to a substantial change to the way Flutter apps are built."
technical,"Yes, very fair statement to say but I have spent a lot of time with Flutter and I can certainly see the value DSX would add to it. As leedstyh has noticed, DSX fans lead the race by almost 2-to-1 which is pretty amazing considering people in this forum are Flutter people. I have a question:  When using DSX syntax, we implicitly assume that the nested child/children is of type Widget. What if we want to explicitly state that we want nested child/children be a specific sub-type of Widget?  for example, when I want children of my custom widget only accept a list of Container, I can annotate the children with List<Container and IDE will give an error as soon as I put anything different from Container. As I'm aware of there is no way to enforce type safe like this when using dsx. Maybe we can have some error when the app compiles though but I think the error raise when I'm typing is still a better experience.  I think we should give everyone some time to try out and get familiar with flutter way to declare UI, at least after v1 release. Then we could have a better look at this feature."
technical,"**Group Dart:** ""Dart syntax is way better and JSX/DSX is just not good"" **Group JSX/DSX:** ""JSX/DSX syntax is way better and Dart is just not good""  I can't be the only person to see this? Both sides make valid points in favor and against their position.  I think what is lost here is that  not only had criticism BUT ALSO DID SOMETHING ABOUT IT. Trying to bridge the gap of web devs/react/react-native and flutter in order to benefit Flutter.  And my 2 cents... As a full stack dev I have exp with a wide array of languages and approaches... JSX is one of my favorite ways to write code and I do hope there will be an alternative syntax to Darts... And i'm not saying the current syntax is bad, it's just that I prefer JSX style. I have to disagree with this quote from the **Group JSX/DSX**   Dart is just not good  Dart is very good and robust language, nobody is dissing the language. The discussion is not about Dart, but a synthetic layer on top of it that most UI developers already use today, and we are proposing that Flutter incorporates something like that.  - Android has XML Layouts. - iOS has Storyboard XIB (XML) - GTK+ has XML for Pango etc. - Qt has QML (YAML like) - Xamarin has XAML  Most of these frameworks and languages have UI markup languages that separate the View from the Controller logic. Then React comes along with different approach (that we are proposing here) and I think we have to agree that RN is flying right now in terms of user growth and popularity, and I may be wrong, but mainly because of JSX.  ...  **Is it really so crazy proposal that we have to get this kind of feedback from Flutter team/users?**"
technical,"just because something worked for JS doesn't mean it's a good idea for Dart.  JSX is an incredible idea for UI/UX development in a reactive framework regardless of language. If I had more free time I would put out LSX which is JSX for Logo ,)   I don't see why 2 languages would be better than one.  You can program iOS with Objective-C and Swift (2 languages) You can program Android with Java and Kotlin (2 languages) I have yet to see an argument in favor of JSX. Above discussions only contains arguments like ""it's better"", ""it increases productivity"", or similar but no argument **why** or **how** it would actually be better than pure Dart code, especially considering planned language changes that would reduce the difference between JSX-like and pure Dart code even more.   You can program iOS with Objective-C and Swift (2 languages)  You can program Android with Java and Kotlin (2 languages)  How is that related to the JSX discussion? How can that be considered an advantage? You can use Swift on iOS because it's the successor of Objectiv-C. You can use Kotlin on Android because it's considered a slick improvement to Java. That sounds like you argue that JSX should replace Dart. That doesn't make much sense to me."
technical,"But really, markup is a huge huge investment in time and added complexity tooling to transition it to real program.  All I am asking is to add these simple extensions on the Dart compiler so that if developers want to they can write using this XML structure. The old way would continue to work and the amount of work involved to do this is not huge at all. You can actually see how many lines of code it takes the babel.js compiler to do JSX and I am talking hundreds and not thousands of lines (I just checked it).   And the tooling is always last to come. So in the mean time, while all that manifests into reality, there's two worlds. Two interesting ways of doing everything. One in the default language and one in markup  Sure but React has been like this and that is not an issue at all.   When I write documentation, I have to write it twice, once for Java and once for UiBinder XML Markup.  Not in React because markup lives inside code.   is the added complexity worth the journey!  Absolutely, it's like the argument of whether you should train your developers with the latest techniques and risk them leaving your company. The bigger risk is keeping them around untrained. So you must adopt the latest techniques out there or risk being left behind.  React is leading the journey with the latest techniques to develop UI/UX. It has tremendous traction with developers. Your greatest risk is not meeting the React bar.   JSX I think aims to solve other issues where you want to blend together what your doing with HTML and javascript  JSX is not just for HTML, React Native generates Views (like Flutter Widgets) from the XML markup.   I think the real question is, how can the UI be built faster.  More like how UI/UX can be built better. Better meaning: faster, higher quality (code and UI/UX), smooth designer-developer interaction, etc.  By the way, really nice job done on the developer tools, 'flutter doctor' was awesome !!! I am now cooking with gas and can be dangerously creative ,) I just wanted to respond to the readability question raised here, though I understand readability is only one of the many factors we need to consider.   Code readability and simplicity which in turn drives a whole bunch of other benefits.   If this is the main benefit then this is something we can test. We could take a mixture of engineers who are used to writing HTML, React, iOS code, Android code, Flutter, command-line apps, and so on, and present them each with various syntaxes, and ask them to describe what they think the resulting UI would look like. We can then measure which syntax gets the best results. InMatrix is this something we could look at after the animation work wraps up, maybe?  There are certainly ways to empirically study code readability, and we can have a more serious discussion when it's time for Q4 planning. To make such a study useful, we need to define what kinds of reading tasks are important to developers in the context of Flutter programming. In addition to reading a whole build method and picture what the resulting UI might be, readability also affects smaller tasks such as identifying the build method in a dart file, matching braces, reading inline comments, etc.  To support some of those more narrowly-scoped tasks, we can experiment with UI enhancements in the editor first, which is usually cheaper than introducing and maintaining a markup language. The closing label feature in VS code is one of such UI enhancements, and we should understand how well it solves the brace matching problem it sets out to address. There are plenty of other options in this space we haven't tried yet. For example, a different font or background color might be used to display the build method to help the developer mentally separate it from the rest of their code.  Another thing that strikes me as important is how we can encourage people to not write giant build method and take advantage of the composition nature of the framework. If the build method is taller than the height of your screen, it's going to be hard to read regardless it's Dart or XML."
technical,"Guess we're all waiting on some sort of pre-processing support, then. I prefer builder syntax than passing parameters through constructor like in this."
technical,"I disagree with a lot of what you said but I appreciate your effort since you are taking the time to think deeply into the issue.   1. Widget creation is different from React  To me it doesn't matter because this is just an implementation detail, conceptually once you see some XML, in React it is a Component, in Flutter it is a Widget.   2. Peoples may think <Foo / does something special that new Foo() don't  I think people will learn pretty quickly that Dart/DSX is not Javascript/JSX.   3. In general Dart/flutter is not JS/react  Yes, we finally agree on something but even though they are different the common thread is that they are declarative UI frameworks and I think declarative tree structures are handled nicely with JSX/DSX. It keeps you on the declarative programming way of thinking. We are not adding JSX in Dart, we are adding DSX, it is different but has similarities to JSX and familiarity is a huge thing. So with that reasoning why are you using Dart? it looks pretty similar to Java and yet it is different than Java, the heck with it, let's scrap all of these Java keywords and concepts and come up with something vaguely similar to Erland that you can only program with one hand while doing a pretzel yoga move on top of Mount Everest ,)   4. JSX makes some dart specifics unclear  Not really, if you connect incomparable Widgets the Dart compiler will spit error messages just like if you did it in plain Dart. I can not emphasize enough that DSX is simply thin syntactic sugar, there is no magic, just a difference syntax to the same thing. I spent hours reading your posts and I really appreciate your effort on this issue. But I think it is (kind of) easy to end the argument. Remember that flux was the official state management solution for react but now everyone is using redux? And how many navigation libs are there for react-native? Just make a DSX repo and let react devs jump in."
technical,"As a newcomer to Flutter but pretty familiar with React, a few things stood out to me:  - The state management model is pretty much the same - The widget/component virtual render tree is pretty much the same - Knowing the state and component model, I basically feel ready to write an app now, excepting some Dart specifics and platform APIs, but... - The styling language is a stumbling block. I'm referring  but it's not easy to figure out what imports are required to make things work (EdgeInset? Color?) or when I should use primitives instead of class instances. - The CSS parser from 's DSX converter is really useful for figuring out layout equivalents in the Flutter model.  Regarding JSX: - I don't think it's necessary to invent new JSX syntax to support Flutter patterns. Some of the syntax quibbles in the this thread could be resolved using some of the newer React patterns like higher-order-components (functions that build component classes) and render props (components that take functions as arguments, the functions return elements). E.g., named ""child slots"" could translate to something like this in JSX:    - The best argument against JSX I saw was that Dart has named arguments. - Why is it important for an element to know whether it has multiple children or one child? Maybe a ""fragment"" construct could disambiguate that API. I still think, as a newcomer to Flutter with some experience with Angular, React, etc., that the regular Dart way, and more in the Dart 2.0, is better than this DSX. It just makes more sense than some XML and CSS-ish parameters. '‚"
technical,"Can you expand on your exp with react-native(JSX), Flutter(OOP), and your journey from one to the other? I think angular template syntax does follow xml semantics and as I'm aware of there is no conflict use case between angular syntax and xml document.  In typescript, we have support for generic component. So I think we could have something like this: But again, the generic component is used for type checking the property input. I don't know if the above syntax has a right semantic meaning in this use case.  I really feel that Flutter team have created the current API specifically for the new way to declare UI they supposed that's better than JSX and any effort we are trying to bind JSX syntax to the current API just make it look unnatural/uncomfortable to use."
technical,"I think it's rather because there were no other arguments in favor of JSX/DSX other than personal preference. That's fine of course as discussed already above, but please don't imply people are against JSX because they don't understand it, when there is no list of good factual arguments that show where JSX is better than plain Dart. I think it's rather because there were no other arguments in favor of JSX/DSX other than personal preference.  Not really, many were given before, just read fully both threads. I did mention these 2 things before: (1) No more 'child' & 'children' stuff (2) easy for 3rd party tools to manipulate (parse, analyse and regenerate)  With  (2) you can enhance the markup to do things not possible with just Dart, for example DSX's spread operator or generating many function parameters from a smaller set.  Others have provided lots of good points but I am not digging it up for you ,)"
technical,"No. If ""JSX didn't become popular because it's the best solution for writing UIs, but because it's the best solution for writing HTML"", why does React still uses JSX for building a mobile app and desktop app? Even your Github desktop application, Walmart cross-platform mobile app, and Tesla app, and Skype are also built by RN.   React does not put a for loop in the JSX tag because the concept of React is about the component. The upper part is the logic and the below part is the JSX, and It is always like this. Everything is separated into many components and connected together into a large component.   **In fact, most of the people here who against JSX  can only guess  JSX is some kind of HTML, XML, or a less verbose solution. People should understand why people love to build an app either with React and  why JSX is so important on this.** I think it's rather because there were no other arguments in favor of JSX/DSX other than personal preference. That's fine of course as discussed already above, but please don't imply people are against JSX because they don't understand it, when there is no list of good factual arguments that show where JSX is better than plain Dart."
technical,"Apple thinks that cross platform means IPhone, IPad & Mac OS. They are more likely to add turrets on top of the walled garden than build something cross platform :) I think than if would be possible indent and format the widget in other way would be more similar to jsx and friendly for users who have experience with xml and html (almost all the android devs)...check this code in codelab check this dart to jsx code  and compare with this other format more htmlish  dart  this is a bit more similar and now you only need view from left to right to notice the different widgets similar to html/xml/jsx  the element (widget) attributes has more indentation than the new widgets, so this makes more clear could understand and check the code  would be great if I could have automatic indentation for this format on the different ides, right now I do this by hand...."
technical,"Whether it died isn't the issue, it's why it died. Did it die because it didn't provide a solution that people wanted? Did it die because of implementation issues? Did it die because of patent issues? Was it too early? Too late? I think it's important to learn lessons from past experiences. Why did E4X and E4H fail where JSX has succeeded?  It died because E4X was only implemented in ActionScript which was only used inside Adobe Flash/Flex and Adobe killed the project. Instead Adobe changed direction towards open standards where there is no single source provider with possibility of lock-in and ecosystem implosion.   What I find interesting is that people who are new to Flutter often ask for two things: a markup language, and animated GIFs. Then three months in, they are still asking for animated GIFs, but not for a markup language. It could be that this is because the markup language isn't actually needed once you're used to writing build methods in Dart. It could be that they still want a markup language but have worked around the omission and so don't think to ask anymore. It's worth figuring out which because otherwise we risk spending effort on something that is the wrong choice (in either direction).  Well, if I asked you for 2 things and you didn't do either in 3 months and there is an alternative to the first thing, I would also only ask you for what is totally impossible to do given your responsiveness and previous delivery performance.   (We're also experimenting with IDEs that put in virtual ""closing tag"" comments for you so you can see the structure without having to actually write it.)  Kind of funny but it's like putting the XML closing tag that you mentioned before was too verbose.   If this is the main benefit then this is something we can test. We could take a mixture of engineers who are used to writing HTML, React, iOS code, Android code, Flutter, command-line apps, and so on, and present them each with various syntaxes, and ask them to describe what they think the resulting UI would look like. We can then measure which syntax gets the best results. InMatrix is this something we could look at after the animation work wraps up, maybe?  Sure you can do that go ahead, I am sure you will find out that ""Once you do React(with JSX) you simply don't go back"". Survey experienced React developers and ask them if they think JSX is bad and it should never had been done. Show your way and ask them if they want to replace JSX with what you have. Before you do that, close the doors and lock up the place because these developers are just going to grab their stuff and sprint for the closest exit.   The thing is, you can put exactly the same ""any valid dart code"" in the XML structure as in the Dart expression.  Sure, but for the GUI builders that's just a block of bytes that doesn't need to be touched and can be easily skipped. Making it design/code interchangeability practically possible instead of in principle. I used Adobe Flex Builder extensibly...   Developers tended to view it as an ActionScript tool.  Yes, but I often switched from design view to code view and vice-versa. Starting a screen I would go to design view and use drag/drop to layout widgets and generate first static screen. Then I would add in code and some static data to fill in screen so I could show people something running that looked like production ready stuff. Productivity was incredible. As development progressed, I connected the front end to the back end and the amount of ActionScript code grew and yes it dominated the code overall but even at close to release time, I often used the design view to tweak the UI without having to dig into code.   However my overall impression is that defining UIs with a combination of markup and code is a mixed bag at best.  Not in today's world. Imperative languages have evolved in the philosophy of Python and are great for development. Declarative techniques with embedded markup (XML) became mainstream with the advent of React, and JSON became the preferred text based data format."
technical,"clarktank,  When discussing computer languages you have to be aware of: (1) Syntax - the characters and words that make up the language (2) Semantics - the meaning of those characters and words  For example, function calls in many languages look like the following (i.e. have the following syntax):  Imagine now that another language decides to use square brackets instead of rounded brackets for function calls, it would look like the following: The thing here is that the syntax is different but the semantics are the same. I mean both are basically doing function calls but with a different syntax. Not true at all. JSX/DSX is just a different syntax to exactly the same thing (the semantics are the same). In the case of JSX, XML tags just create React Components (just like you can do it in Pure Javascript). In the case of DSX, XML tags just create Flutter Widgets (just like you can do it in Pure Dart). The syntax is different but it generates exactly the same thing so it is identical under the hood.  JSX is popular because it is a great way to manage component tree hierarchies, whether for web, mobile or any UI development. Notice in the code below you don't know if the dropdown component is for web or mobile for example.  How is that? (considering that using JSX/DSX or Pure Javascript/Dart generates exactly the same thing under the hood). I used react-native for almost one year and had no idea JSX elements are Objects that are being instantiated, until I started with Flutter/Dart. From my perspective, it does hide the OOP picture, even though as you said it semantically does the same thing under the hood!  In large application, JSX could also get as ugly as heavily nested objects. So syntactically, I would rather stay consistent than introducing another style which could be as confusing."
technical,"Flutter is currently reluctant to try to please NativeScript, ReactNative, Android, Web, and other developers who accustomed to similar XML layouts. They have more important things to do, so let's disband and go to sleep. I wished Amy proponent of JSX Syntax just to spend some days really working with Flutter before continuing lamenting. I came from an Xaml based systembut quickly got used to it. Just give it a real try."
technical,"Great. Then there is no need for the Flutter team to do anything and the issue can be closed   Does this include support autocompletion and syntax checks in all IDEs supported by Flutter?  Most IDEs already support XML and JSX so it wouldn't be hard for them to add my minor additions. I wonder if the JSX syntax might be more appreciated in angular Dart ? It seems like its more of a what-you-are-used-to situation rather than ""better"". The JSX syntax probably feels more natural for web developers.  I know that programming feels awkward for me even just using different code highlighting colors for a day."
technical,"Whether E4X died or not is irrelevant because nothing lives forever.  Whether it died isn't the issue, it's why it died. Did it die because it didn't provide a solution that people wanted? Did it die because of implementation issues? Did it die because of patent issues? Was it too early? Too late? I think it's important to learn lessons from past experiences. Why did E4X and E4H fail where JSX has succeeded?  What I find interesting is that people who are new to Flutter often ask for two things: a markup language, and animated GIFs. Then three months in, they are still asking for animated GIFs, but not for a markup language. It could be that this is because the markup language isn't actually needed once you're used to writing build methods in Dart. It could be that they still want a markup language but have worked around the omission and so don't think to ask anymore. It's worth figuring out which because otherwise we risk spending effort on something that is the wrong choice (in either direction).   On your giant nested expressions can you easily see structure?  Yes, at least as easily as with XML. Personally, I find XML to be very verbose and it obfuscates the structure. But I think this is more about what you're used to.  (We're also experimenting with IDEs that put in virtual ""closing tag"" comments for you so you can see the structure without having to actually write it.)   Great so you know that parsing a markup language is trivial compared to parsing an imperative programming language.  My experience is that declarative vs imperative is not the distinction that matters when it comes to determining how easy a language is to parse. (For example, HTML is ""declarative"" but it may be among the most complicated languages to parse that I've ever dealt with.)   Code readability and simplicity which in turn drives a whole bunch of other benefits.  If this is the main benefit then this is something we can test. We could take a mixture of engineers who are used to writing HTML, React, iOS code, Android code, Flutter, command-line apps, and so on, and present them each with various syntaxes, and ask them to describe what they think the resulting UI would look like. We can then measure which syntax gets the best results. InMatrix is this something we could look at after the animation work wraps up, maybe?   can this structure be easily manipulated by other tools like GUI builders interchangebly ?  Yes, in principle at least. It should be relatively straight-forward to mechanically convert Dart expressions to XML or JSON or whatever other structured language one might use. It's just a matter of converting the syntax, the actual information is the same. Personally I wouldn't convert it to a different syntax if I was making a GUI editor, I would just parse it into a data structure in memory straight from Dart.   You can't do that easily when the programmer goes inside your ""giant nested expressions"" and writes any valid dart code that doesn't follow the structure that the GUI editor is expecting. With a fixed XML structure that is not a problem.  The thing is, you can put exactly the same ""any valid dart code"" in the XML structure as in the Dart expression. They are literally mechanically interchangeable. So I don't really see how using XML helps with this particularly. For example how would you turn this into XML?:  It handles it just fine, you just don't know how to do it.  I meant specifically JSX. I agree that your proposed syntax would be a perfectly valid way to approach the problem. I worked on Adobe's Flex SDK, which combined XML markup and ActionScript, for the last couple years that the product existed at Adobe.  I understand the appeal of markup for defining UIs however I can also remember some drawbacks:  - Flex application visuals could be defined in terms of markup and code. As I remember it, code tended to dominate in large real-world apps.  Readability isn't necessarily a benefit for apps defined as a complex hybrids of markup and code. - The ""Flex Builder"" IDE had to support apps that were defined by  markup and code. This made the IDE difficult to build and maintain. Developers tended to view it as an ActionScript tool. - Evolving and maintaining the XML ""compiler"" was a significant burden that kept a team of engineers busy full-time.  Keeping the compiler and toolkit in lockstep slowed down the evolution of the overall sdk.  It's been years and I can no longer recall all the details. However my overall impression is that defining UIs with a combination of markup and code is a mixed bag at best."
technical,"The great news is that even is this was adopted by the Flutter team (which isn't likely based on this thread) no one would be FORCED to use it. It's an OPTION. I don't understand why it's so emotionally charged that some people might like writing code with a syntax you don't like...  I don't care what other people like or don't like. But I care about features that help me progress with my application. Resources are limited, and I would rather have a proper video/stream player, native child views, and some vector graphics format.   For example, right now in React and React Native you can write your whole application without JSX, it's just an option. Here's what React looks like without JSX: It's a lot messier than the JSX version, and almost no one uses it. That's probably why some of us who are coming from React would appreciate the possibility to avoid having to do it this style again.  That's because the web was not designed for a system like that. If you have a static markup language like HTML and want to ""dynamise"" it, you have to invent a system that needs to work ontop of that. What you will end up with is some construct that is constrained by the underlying platform. Flutter on the other hand has no markup language. I have yet to see a reason to invest resources into something that is less dynamic and less expressive. I would like to know if people who disagree with it have used React with and without JSX. I encourage everyone to try it, so this discussion would be less theoretical. After the initial shallow learning curve, it becomes very natural (and it's not all about the number of characters, because the closing tags allow you to read it easier). It's not different from learning anything else that's new, like Flutter, which in the long run boosts productivity. And I agree that it's not for everyone, which is why it would be optional. I think it's important to keep in mind that we have one another's best interests in mind - making it easier to build stuff. I think this transpiler would help, and it would be easier to adopt if it had official support.  Also, Thank you"
technical,"Why do you think it does matter? JSX is a way to describe interface. It's language agnostic on it's own. Look here. It's not JavaScript. But well, why it can't be done with JSX? (Beside there is no implementation of this (yet))  And.. you know... flow come from fb too: Please, stop selling arguments for and against extensions you've never use. I use JSX every day and I'm happy with it ATM, although I was very skeptical about it. I can imagine, that JSX evolves in other patterns, like it was with AngularJS.  And maybe this topic helps to find better pattern for Dart? DSX is just one proposal. Look at builder pattern example above or other language tweaks presented here. And, well, maybe your flui is a better way? I don't know. **But let's find it out and help to each other to improve their suggestions instead of discussing about bad things in someones else proposal.** I would like to propose close this topic and open new umbrella topic with limited conversation. All proposals to improve the way, how flutter may be used, discuss in own topics objectively with love and without hate."
technical,"You can tell JSX (or DSX in this case) is a big deal just by looking for the amount of comments in this issue.  JSX is something that you don't know you love until you try it. It's weird at first to have markup in your code but then it becomes awesome.  It seems unnecessary for guys that already know flutter/dart, I get it. But for someone that never touched dart (like myself), JSX is way easier to start with and that's the point. If you guys want more people to come by and start building a lot more awesome stuff, the tooling needs to be easier.  And, as eseidel Google told the other guy ""Flutter is a way to write to iOS and Android in a single codebase. We try to make that **fast and easy** Flutter talks all the way down to the hardware. We have a very layered approach, so you can take as little or as much as you want. We have a bunch of investment in **tooling**.""  Implement JSX, pretty please. I'm a fan of Dart and also a heavy user of react. I always find Dart terser and more maintainable than JSX. So, I think JSX/DSX could and maybe should implemented in a community way."
technical,"Becomes: The argument for and against JSX has been exhausted by discussion, there is plenty of material that you can look up. I'm only sharing my personal opinion.  First, code readability. I can immediately get a crisp clear idea about the structure of the UI in JSX (takes seconds), the mental modal is very visual. Second, usability. I would argue that JSX can be used even without knowing JS/Dart or anything about underlying API. This is very suitable for someone who is just learning programming, or for some one who is a part of my team, designers now can code the UI.  The description of the application is completely declarative (not just expressive), when you work on a large project with hundreds of components, this way of describing UI makes a huge difference (you have to try it to really appreciate it). I didn't like JSX when I first saw it too, once I used it just felt right, this how I want to describe UI from now on, it was a clear breakthrough in the way I approach building interfaces.  It's not about writing less lines of code, it's not about having a ""syntax sugar"", it's about building tools for humans. I'm also against the argument that everyone should use JSX, this is ridiculous. You use the tool that let's you get your work done faster with less confusion, in the case of many (including myself), it would be JSX (or DSX in the case of Flutter), that's all. I'm coming from React, and first time trying out Flutter. It's really odd that flutter team doesn't have JSX syntax, I mean seriously! In JS world, JSX is extremely prevalent in every React-alternatives now a days! JSX is super friendly for developers. Please implement this ASAP, so you can help grow the community."
technical,"Sorry dude I don't have the time to argue endlessly and repeat what I said before over and over, we won't end up in agreement anyways so best of luck with your FLUI.  The online DSX transpiler has been live since Feb 2018 and anybody can use it so there is no need to take my word for anything. Press 'Compile' and it compiles what its written on the left panel and places results on right panel. Open debugger and you will see the AST written out.  It makes no major difference at all, like the OOP (Object Oriented Programming) concept and syntax for 'classes'. It's almost identical in typeless Javascript or typed Dart, same can be said for 'if' statement, 'for' statement, etc   it still needs to bring something to the table that is going to convince people to use it.  Apparently it already does for 100 people in this ticket, and that's 100 times larger than just me using it, good enough for me. I'm not arguing with you just for the sake of argument or because of some deep-seated anti-JSX bias. I'm trying to get you to answer questions that need to be answered. You are developing a tool that you presumably intend for other people to use, yet you still haven't offered a compelling reason  why  they should use it beyond the vague and subjective benefits of ""familiarity"" and ""because it's better"". The former, as I said before, is not necessarily a good thing, and the latter is as of yet a claim made without any tangible support.  If you want your tool to be a success, it needs to be set in stone what you are doing and why you are doing it, and you need to do so in a way that it can be easily conveyed to others. That's not to say that you can't make a product until it is liked by  everyone , but clear and concise objectives are crucial to shaping design and implementation. Otherwise you are just going to end up with a direction-less utility that will be a niche product at best and will be extremely lucky if it ends up in production code of any scale.  I didn't even see that that link was a working example. I've never used herokuapp before and it just looked like a gist or something, so that's on me. :P  (Though I will point out that tinkering with an online sandbox is not the same as testing the transpiler in a more practical environment.)  You already had to deal with one such difference in child strong-typing. What about attribute strong-typing? What about widgets in different libraries with the same name? What happens if someone makes a widget with more than one nameless positional argument? What happens if we import two libraries that have widgets with the same name? What happens in some scenario that I haven't thought of pops up to further showcase the inherent difference between systems like Javascript and Dart? I have to say, you being so flippant on this discussion point makes me worry about DSX's longevity in a real-world setting.  Again, that's 100 people who upvoted the issue on the basis of ""Consider JSX-like syntax inside dart code"". They upvoted because they want  JSX , and as you've been so keen to point out, DSX is not JSX. So why else would they want to use DSX? Because inline XML-like UI declaration is ""the future""? Again, I just don't see it.  We've already covered JSX in Vue not getting any traction, but there's also the two React alternatives mentioned in the Web Components article you linked: Inferno and Preact. As far as I can tell, they have both failed to make any kind of waves at all in the JS-based web-app development world, despite also natively supporting JSX-like syntax. I really think that people need to have a long and hard look about  exactly why  people like JSX in React, because by all accounts it just doesn't seem to be because of JSX itself. If  that  question can be answered, then we can move forward toward ""future"" innovations rather than just frankensteining that one feature from that one library we liked into everywhere else we personally think it should be."
technical,"I cannot see myself using this (but I'm willing to give it a try). I dont think this is a silver bullet, but there is so much enthusiasm that its worthwhile doing.  I think google should remove the language obstacles to allow the community to build what they want. I'm not aware of any obstacles preventing someone outside Google's Flutter/Dart teams from writing a JSX-like thing for Dart/Flutter.  I would *love* to see bugs on those if someone is trying.  (It's also possible that I've missed them in the comments above, since I admit not to having read every word of this very long bug.) :)  Thanks!  Glad to see such enthusiasm!"
technical,"I don't think putting code on a repo would solve my problem because the current issue is about properly integrating DSX with Flutter tools as to provide a great developer experience with debugger, auto-complete, etc. working on .dsx file.  Telling users that they can use DSX but can't use debugger or enjoy auto-complete is a non starter for me. If anybody wants to help, what I need is to figure out a way to add full preprocessing support (with source map) to Dart Tools and VS Code Dart plug in. Once the tools support that DSX or any other transpiling language (any language that is a superset of Dart but compiles everything down to Dart) would just work. I don't need to justify anything, I am very confident that DSX will be a hit and there are almost 100 people interested in it on this ticket alone.  The DSX transpiler is blindingly fast and can transform .dsx files into .dart files faster than you can blink, so speed is not a problem, just trying to get feature parity as to make it a no-brainer for people to use DSX.   If that's the case, why is React the only framework with market presence that is using it? Both native and cross-platform mobile app frameworks have stuck with their storyboards, their XML files, their XAML files, and other such UI definition DSTs.  Just make a timeline and you will see the UI development evolution. Android and iOS development via their current ways started over 10 years ago so it uses 10 years old techniques (totally imperative techniques). The Reactive UI development (declarative) techniques started to appear for the web around 8 years ago. React appeared 5 years ago and it was the first Reactive framework to combine technologies seamlessly with JSX. Vue is now the latest Reactive framework that supports the older 'separation of technologies' techniques but it also supports JSX. On mobile Flutter is the latest and it uses Reactive framework techniques as React and it could take advantage of DSX just as Vue takes advantage of JSX. Vue is killing Angular because it is providing choice to developers and not being overly opinionated.  The problem with separate template files is that the imperative programming constructs there (if, for loop, etc) are so weak compared to what's available on the programming language used for the business logic. Combining the 2 in the way JSX does, to me, is clearly the future. It is !!!  Vue uses JSX I'm not saying you  do  need to justify anything. Back when you were insisting that the Flutter team pick up this proposal and implement it themselves, yeah, I would've said you had a fair amount of justifying to do. Now that you are trying to do it yourself, you can do whatever you want for what ever justification you think is sufficient, and more power to you. I'm merely stating the reasons I see that it might not be as easy or as popular as you seem to think it will be, and I am putting the ball in your court to defy them.  I assume that, at this point, you've tested it so far on UIs and Apps that are trivial in size. What about non-trivial ones? What about ones that fall within edge cases? Also, the actual time that the process takes is not the only relevant part - just the fact that the developer has to go through another checklist of manual actions before they can build is enough of a turn off for many people.  You also have yet to actually release the source code of the project, so no one's been able to go through your process, double-check your findings, and suggest improvements. At this point, all anyone can really do is take you at your word that it is both convenient and performant. I've been using Vue for coming close to a year now, and in that time I have gone through a good number of open source project repos to see how different things are done. While I don't consider myself a Vue master by any means, what I will say is that in not a single one of them have I ever seen JSX actually utilized - people seem to massively prefer the .vue approach (template-script-styling) over the render+JSX approach. I didn't ven know myself that Vue even supported JSX (via a babel plugin at least) until after your reply I did some digging through the Vue documentation and discovered a tiny snippet of information on it in the render function section.  But this is irrelevant to my overall point. Vue is still a Javascript framework. Flutter is most assuredly not. As such, there are a lot of reasons that make JSX the newest greatest thing in a Javascript-centric environment that won't translate to Dart+Flutter, many of which have already been covered in this thread. Until I see it catch on in a non-Javascript development environment, I shall respectfully disagree."
technical,"Sure but JSX doesn't use namespace so it is not a XML feature of interest.  Since you split the 'children' out of action into it's own tag, kind of reminds me of the new Fragment tag of React.  It's a balance between verbosity and conciseness that's for sure.   I really feel that Flutter team have created the current API specifically for the new way to declare UI they supposed that's better than JSX and any effort we are trying to bind JSX syntax to the current API just make it look unnatural/uncomfortable to use.  There is nothing new in the way Flutter declares UI in code, Perhaps using DSX is unnatural/uncomfortable to you but to JSX developers it is not. JSX/DSX is perfect for Flutter, it fits like a glove and if you don't like gloves go bare handed ,) it sure does !!! you can argue with 'feeling', 'thinking', 'suspect', 'imho', 'opinion' but this is data, a concrete data point. If the data is useless it shouldn't be collected. I guess the data is useless because it doesn't paint your picture. I'm not sure but I have read some of your comments above and I think you have mentioned JSX/XML node interchangeably. Anyway, personally I think using namespace as a solution is not ideal.  Just compare"
technical,"Sorry, but this isn't a problem. Furthermore, this doesn't make any sense at all. Beside of that, we use JSX with TypeScript. absolutely! I'm with you on this one.  The 100."
technical,"I spent hours reading your posts and I really appreciate your effort on this issue. But I think it is (kind of) easy to end the argument. Remember that flux was the official state management solution for react but now everyone is using redux? And how many navigation libs are there for react-native? Just make a DSX repo and let react devs jump in. I've never seen the part/part of syntax in Dart before, and I'm having trouble finding any documentation for it. Is it something that Dart/Flutter officially supports? I'd love to use something like that in FLUI.  You keep going around in circles with the DSX justification. DSX is not JSX. DSX is JSX-like. DSX is meant to be a familiar syntax for React developers. DSX is merely think syntactic sugar for Dart. People will learn that DSX is not JSX. (And so on.)  While I get the point you are trying to make with all those explanations, I think the fact that you have to keep making them reveals a major issue regarding the nature of DSX in general, and it's a point that **rrouselGit** brought up as well. Even as you keep saying that DSX is  not  JSX, people who find it are going to think that it is, and that's a problem. JSX, and the people who use it, come from an ecosystem that is so conceptually different on fundamental levels from Dart/Flutter. As such, developing a feature for ""familiarity"" is not necessarily a good thing. One of the more apparent reasons for such is, as was pointed out, people are going to try something like this:  Because they come from Javascript/JSX, they expect that syntax to work in DSX. When it doesn't, it becomes a point of cognitive dissonance which may actually  hurt  their interest in not just DSX but in Flutter as a whole. Familiarity is beneficial when it's used as a means to ease people into something new, but that can be a double-edged sword - when 90% of the features are the same, the remaining 10% can serve to just frustrate and annoy.  Another issue with DSX is that it is not likely to be a seamlessly integrated feature any time soon, regardless of if it's a third-party plugin or if it gets officially adopted by Flutter. As you've said yourself, in order for it to truly work with the Flutter debugging and deployment process, it needs official support from not only the Flutter team but also the Dart team. Failing that, without pre-processing and tooling support, the only way DSX would work is with external manual conversion tools, which is just another (potentially lengthy) step that developers will have to go through.  While typing this up, there's another thing that occurred to me. There have been several pro-JSX people in this thread that have praised JSX, saying that the ""separation of concerns"" approach to UI design is really the only way they will ever consider developing UIs again. If that's the case, why is React the only framework with market presence that is using it? Both native and cross-platform mobile app frameworks have stuck with their storyboards, their XML files, their XAML files, and other such UI definition DSTs. Even other popular JS frameworks like Angular and the up-and-coming Vue still do the ""separation of technologies"" approach. React developers talk like JSX is the way of the future, but I've yet to see it pop up anywhere other than in React in a framework that has gotten any real traction."
technical,"**Please dont introduce XML-like syntax to Flutter.**  I programmed in Android Java for more than a year, then I started looking for a cross-platform toolset to try. I started with React Native and then tried React. I didnt really like the JSX syntax because it is not quite javascript and not quite html, so just another thing to learn. When I tried Flutter I found it much easier to get started (probably mainly due to my Java background).  I think some of the reasons why I wouldnt like to see an XML-syntax added to flutter : 1. Another thing to learn - could be spent learn Futures instead ,P 2. Context-switching - you are context switching between XML and code which is just unnecessary cognitive load. 3. There have been days when I have programmed in Java in the morning and Python in the afternoon. With React you may need to understand Javascript, Babel, JSX, HTML, CSS and more in one codebase. 4. The reason why XML is not necessary in Flutter is because dart has named arguments which replace XML attributes pretty well. 5. Dart has the very cool dartfmt which indents the code really well for no effort. #### Comparing to Android  6. You have to learn the programmatic way anyway, why add another way of doing things? 7. The XML layout in android is faster to display changes on the device, but running in Flutter is practically instant anyway so adding XML wouldnt provide that advantage. 8. The Android XML + programmatic combination adds complexity, such as inflating XML snippets and injecting into the XML tree programmatically. 9. Instant run is so fast in Flutter you dont need the XML model to help visualize how it will appear, you can just press a key and see the change immediately. 10. The errors from programmatic layout issues are different from layout issues in XML, so thats two sets of things you need to understand.  I would go one step further and remove pubspec.yaml and replace it with pubspec.dart and have configuration in dart code. If Devs are complaining about the difficulty of laying out the pages visually - an idea would be to create a layout designer which lets you set your themes and design pages visually by drag-and-drop. Then it would generate Flutter code which is not intended to ever be editted - but rather creates classes which can be used to compose your app.  It wouldnt need to be 2-way editing (XML/layout) like Android XML, but you just save your layout for later. If you need to change it you can regenerate the code and (hopefully) just change some arguments."
technical,"How would Intellij and/or VS Code (with Dart-Code) be able to set breakpoints and step through code from a .dsx file ? (I mean this is not a .dart file). How about auto-complete functionality from the .dsx file  (as with a .dart file) ?  You have invested a lot on the tooling but the tooling does not support pre-processing (with source maps) of a new language (like DSX) that transpiles into Dart/Flutter seamlessly.  A transpiler is available but there is no easy way to fully integrate it. P.S. This ticket is only half of the comments !!! is there a repo with that compiler somewhere? It would be great to share the code and get the community involved on hacking on it, even if it isn't perfect."
technical,"Hey. Do you think that I have not spent a lot of time learning flutter? In the layout, at the end of ""Packing widgets"" section, the code that the tutorial gave does not work. **I mentioned the issue under the flutter issue block, but no one wants to care about this.**  - Dart code: main.dart - Pubspec: pubspec.yaml is there any connection from your comment to the topic of this issue?"
technical,"Yeap, the amount of hate here is epic, just consider this: There are 3587 open tickets, if you sort them by ""thumbs down"" you get 1 (this one) with 57 ""thumbs down"" 3586 (other tickets) with 1 or less ""thumbs down"" It's a way to describe UI  in Javascript  (hence the ""JS"" part of the name). And no, since it's an inlined DSL, it is  not  language-agnostic. And even if it was, that still doesn't make it  the ""better choice"", since there are plenty of truly language-agnostic DSLs out there that would be woefully inadequate for UI declarations.  Flow is just like TypeScript: a static type checking system for Javascript. It is not a React tool, nor was React designed to be used with it. React is first and foremost a Javascript library, and JSX was designed to be used with React. Whatever secondary tools and utilities get introduced into React development are ultimately irrelevant to React+JSX interoperability.  I have used JSX, and though I do have personal opinions of it, I have deliberately left those opinions out of this discussion. In fact, had you read my previous comments, you would know that I had praised JSX for revolutionizing UI development in React. Other than some mildly tangential comments I've made about the market penetration of JSX as a whole, my arguments have been specifically about JSX  in Flutter . And on that topic, there is no practical basis to determine the efficacy of DSX, so all we can do is examine how JSX has been implemented in other places, and that examination does not bode overly well.  Unless, of course,  you've been using DSX every day as well and can enlighten us on the practical advantages of using DSX in Flutter?   That is what I'm doing.  DSX is being proposed as a UI solution for people familiar with JSX. There are key design elements in JSX that were intended to be used for an environment completely different from Dart and Flutter.  Those differences need to be addressed for DSX to be successful.  I am not being a  hater . I'm trying to promote constructive discussion and ask the important questions. Yet all the responses I've been getting has amounted to subjective tautology (""JSX is good because it's the future, and it's the future because it's good""), dismissive hand-waving of crucial design points (""DSX does not need to account for differences between JS and Dart because there aren't any""), or just plain hostile (""You obviously don't like JSX so stop talking about DSX"").  You do not make a successful product solely with unadulterated praise. It needs to stand up to and account for criticism as well. People showing up and saying ""OMG yes please, make DSX"", while uplifting, is not helpful. There have been several people throughout this thread that have brought up perfectly valid criticisms of DSX, both with its initial design and with the concept as a whole. And for the most part, many of these criticisms have yet to be directly addressed, with the general attitude being dismissive.  My only fear is that all this unconditional love for JSX is preventing people from viewing DSX objectively. I get why you guys want something like JSX in Flutter, and I can relate - my opinion that Flutter needs a dedicated UI DSL is what lead me to create flui. But if the only people allowed to talk about DSX are the people who have nothing but good things to say about it, then it  will  fail."
technical,"we try to encourage people to break down their build methods into smaller reusable widgets.  Does it really become a reusable widget or simply a wrapper/composite widget? I mean to be reusable you should have at least 2 usage instances.  The AppBar is so unique that you can't really call it an reusable component, it's a wrapper/composite component and in these cases why not just use a local method to build that part of the UI? I guess if it did more things it would make sense to place it in a wrapper/composite component.   One thing we've found in Flutter is that big build methods are not great for performance  Since you mentioned performance, having the animation loop drive the build method will cause performance problems for smooth animation. You don't want your build method called 60 times a second or more, specially considering that the build method is user code (for example, it could have a super long loop that takes forever and it would cause animations to skip). Being a Flutter beginner perhaps I got that wrong. It's also relatively small, so that's ok.  Regarding performance, this is somewhat off-topic for this issue so please file a new issue if you want to discuss it (or e-mail flutter-dev or post on stack overflow, whatever you prefer)."
technical,"we try to encourage people to break down their build methods into smaller reusable widgets.  Does it really become a reusable widget or simply a wrapper/composite widget? I mean to be reusable you should have at least 2 usage instances.  The AppBar is so unique that you can't really call it an reusable component, it's a wrapper/composite component and in these cases why not just use a local method to build that part of the UI? I guess if it did more things it would make sense to place it in a wrapper/composite component.   One thing we've found in Flutter is that big build methods are not great for performance  Since you mentioned performance, having the animation loop drive the build method will cause performance problems for smooth animation. You don't want your build method called 60 times a second or more, specially considering that the build method is user code (for example, it could have a super long loop that takes forever and it would cause animations to skip). Being a Flutter beginner perhaps I got that wrong. It's crazy to see this issue getting buried. In my opinion it's going to be a make or break for Flutter to implement JSX-like syntax for composing widgets.  I simply don't understand the target audience, many ios and android devs are moving to react native, it would seem to be the perfect opportunity to harvest market share.  I encourage people involved to give react native a spin and see what we are talking about."
technical,"Here's Hixie's latest example in DSX, using Airbnb's style guide and assuming all child elements can automagically map to child properties. If the intention is to improve readability, I think future Dart wins in spades.  You have to check documentation to know what can turn into tags (presumably Widgets only), or how many child elements are required/allowed.  If your goal is to output HTML from JavaScript, JSX makes a ton of sense as a middle ground. Because when you want tags at the end of the day, React.createElement('div', null, 'foo') is far worse than <divfoo</div.  If your goal is to output a tree of Dart objects from... Dart, and a nicely formatted tree of Dart constructors is perfectly (arguably more) readable, I don't see a point in detouring through XML. And I'm not missing those XMLs coming from Android. It's what using XML enables... Look it's been 5 months with just talk, now I am doing something about it so give me some time (I do have a full time job and can only spare about 4 hours on weekends)."
technical,"I'm a fan of Dart and also a heavy user of react. I always find Dart terser and more maintainable than JSX. So, I think JSX/DSX could and maybe should implemented in a community way. Just because Flutter is promoted as reactive style framework doesn't mean it's just a Flavor of ReactJS. From most comments I get the impression people just have troubles accepting that something new isn't the same as what they already know.  You could try to work with Flutter for a while how it currently is and then provide proper arguments why you think JSX/DSX would be better then plain Dart **in a factual way** instead of just ""because of personal preference"" or ""because it's better""."
technical,"yuriy-manifold just because something worked for JS doesn't mean it's a good idea for Dart. If you had read above discussions, you'd see that in fact it **is** controversial. I don't see why 2 languages would be better than one. just because something worked for JS doesn't mean it's a good idea for Dart.  JSX is an incredible idea for UI/UX development in a reactive framework regardless of language. If I had more free time I would put out LSX which is JSX for Logo ,)   I don't see why 2 languages would be better than one.  You can program iOS with Objective-C and Swift (2 languages) You can program Android with Java and Kotlin (2 languages)"
technical,"It's what using XML enables... Look it's been 5 months with just talk, now I am doing something about it so give me some time (I do have a full time job and can only spare about 4 hours on weekends). Just found this thread, interesting on both sides. As a React developer interested in Flutter there are a couple other arguments that I haven't seen mentioned (though I did only briefly skim all the comments.)  1. Closing tag declaration increases understanding of children vs properties. UI can unfortunately be deeply nested, and having a clear </item tag vs an unknown ) helps to clarify and parse children. It also allows me to move code around between components and very declaratively see when something it out of place (a closing </list tag before a </listitem for example.) This is trickier with multiple nested )'s.  2. My initial gut reaction to seeing multiple nested components inside constructors gives me flashbacks to ""callback hell"". It was a very painful era of JS that is starting to get better, going back to it feels a bit like a step backwards.  3. Related to #2, the unfortunate fact is we have to convince people to switch (or at least try it out). Many people are using React/React Native and even more using HTML/JS. Creating a simple and mostly syntactically familiar tutorial/guide that specifically targets some of React's pain points might be extremely compelling to those who are a tad bit fatigued."
technical,"Would it be possible to build a DSL like what the Kotlin team did for Android developers?  Seems like a lot of people would like to use Kotlin with Flutter. Honestly, I don't understand, why did the developers decide to reinvent the wheels in Dart?  In my humble opinion, Flutter doesn't need JSX. Flutter should have chosen Kotlin instead of Dart. Kotlin allows us to write complex ui logic with beautiful syntax out of the box, has huge community, production-ready tools, battle-tested in Android development...  Just saying. Kotlin is nice, I am a fan *but* it doesn't run on iOS..... actually it does but it hasn't been released yet (pre-release stage right now). For UI/UX development I still prefer JSX instead of Anko DSL. I like the fact that I can visually separate declarative markup from imperative code and be able to mix and match components together quite easily."
technical,"Can we recenter the discussion on this topic? In fact, I don't see any reason to keep this issue open. Dart team stated that they have other priorities. And the pro JSX side volunteered to make their own DSX implementation  Maybe we should just have a few open source repository proposing different solutions (even barely working). Such as DSX, or templates. And then consider redirecting from Flutter's readme or awesome flutter to these repos. And if there's anything blocking a DSX implementation, create another issue with the specifics.  Then let the community do its job. Leave it opened as is because people will continue to open new tickets asking for JSX (as it has already happened twice before)."
technical,"I don't think its the flutter's team objective to attract developers from react native to come to flutter  Really !!! React Native is dominating and given that the total number of mobile devs using cross platform tools is limited, do you really think Flutter can become a home run hit without attracting React Native people?   Yes its a little more verbose to right pure dart. But it actually makes debugging your code a lot easier.  That's not a correct statement. Debugging JSX code, which is just javascript code is not any easier or harder, it is the same.   The reason why im against a markup language or JSX or anything that sits on top of a technology is that it require tons more work from the platform.  Who cares how much work was placed in the platform? Devs just want the latest techniques that improve code readability and productivity. No dev wants to use old and obsolete techniques when newer stuff provides better alternative.    I prefer knowing what is really happening in the code and have control over what is happening than have magical Syntax that all its doing is add overhead.  This is non-sense, I know exactly what goes on with JSX, it is a tiny little layer that transforms almost one to one but provides lots of benefits.  Negligible compile time overhead if you ask me.   Just to mention the fact that JSX Is awesome for a react/javascript community because it worked for them, but for Dart/flutter i find it a bit overkill to add JSX Just to attract developers from React Native.  JSX should also work great for Dart/Flutter. It is not an overkill by any means. Is there any good reason why JSX would not work for Dart/Flutter? If I were to code it up and released it, why would it not be a good fit for Dart/Flutter development? Let's take the concrete example:  For now the syntax is a little verbose, I have to new something just for the purpose of adding a margin: margin: new EdgeInsets.symmetric(horizontal: 4.0), I think there may be an easier way.  If you want to add a margin on the left and right, how would you like to express that? Specifically, let us take a simple example, and see what it would look like in various syntaxes.  Flutter as written today Flutter as written later this year once we remove ""new"" and ""const"" keywords How would you recommend we express these  exact  semantics?"
technical,"One place for comments and votes and it is here. The request for JSX-like functionality is not going away and the ticket is opened because it needs Flutter tools support (compiler & VS Code IDE) and I have updated the ticket request with this info (first comment). If an enormous number of people starts asking for this, it will give the Flutter team incentive to put resources into it. Looks like most of the controversy here is not around JSX at this point, but DSX. I'd suggest splitting DSX discussion into it's own thread and leave this one generic to JSX.  In the end DSX is just one way of getting something closer to JSX so we shouldn't be mixing these two discussions in one thread regardless."
technical,"I agree for the purpose to make flutter simple that don't want to use JSX-like syntax, and I also support the concept of split into multiple widgets, but now it let me feels like the MVC era is back in jquery. Such a scenario that there was a simple widget, with padding,border and center layout for it later, many  ""} "" symbol seriously affect the readability.However, it is a integral widget, I don't want to split it, any useful solution? Though my english is poor,I try hard to express my thoughts. Lord help us all.  Ok, this is going nowhere. It doesn't look like anyone's about to switch sides anytime soon. We definitely need to reach some sort of compromise here. Of course ""pro-DSX"" vs ""anti-DSX"" doesn't really have a remotely satisfying compromise, which is a frustrating realization. Could we possibly reframe our positions so that they could be more compatible?"
technical,"Just because Flutter is promoted as reactive style framework doesn't mean it's just a Flavor of ReactJS. From most comments I get the impression people just have troubles accepting that something new isn't the same as what they already know.  You could try to work with Flutter for a while how it currently is and then provide proper arguments why you think JSX/DSX would be better then plain Dart **in a factual way** instead of just ""because of personal preference"" or ""because it's better"". Maybe it is only my issue. I really feel that I have a hard time to read the UI code, even it is in Dart 2 syntax."
technical,"Time has come... It's with great pleasure that I announce the online DSX transpiler Nice job with the transpiler One thing I would say is easier to follow with JSX is the closing tags. The idea which I mentioned before : Idea: Auto generated closing tags In your first example, would give flutter code of (removed the optional new in future Dart) :"
technical,"is there a repo with that compiler somewhere? It would be great to share the code and get the community involved on hacking on it, even if it isn't perfect. no the DSX transpiler code has not been released because it is way too early for that. You can simply use 2 hand written equivalent files (.dsx and .dart) to test out pre-processing capabilities like breakpoints, stepping, auto-complete, etc.  Once the tools support pre-processing with some sort of source maps, I can add that to the transpiler and be unblocked. This would also enable others to experiment to their hearts desires."
technical,"No, I didn't miss the comment at all, it just makes no sense you telling me that people coming from JSX will find DSX too complex.  There are many reasons to choose DSX but with alternatives people will pick what they prefer for whatever reason they have. Totally opposite, DSX only implements 2 xml transforms and gains everything else for free from the hosting language. Imagine the effort trying to re-implement the power of Dart inside your new templating language. No thanks, I'll take Dart. No, I didn't miss the comment at all, it just makes no sense you telling me that people coming from JSX will find DSX too complex.  The same thing applies to the current dart implementation.         Anyway, I don't think we can convince each other here. So I'll just list a few more reasons as to why I dislike JSX in flutter and then ""wait and see"".   **1. Widget creation is different from React**  In react, it's the library that handles components creation. JSX is fine because it says ""Don't bother about how things work. We do things for you"".  In flutter that is not the case. We manually instantiate a new widget every on build call. **This is really important to understand, and JSX would make it less clear.**  And, in the continuation of that logic :  **2. Peoples may think <Foo / does something special that new Foo() don't**  <Foo /  in a method feels special. It seems like it does something extraneous built-in in the framework. Which is true in react, where  components are wrapped into a React.Element. This translates in react into <Foo / != new Foo() and don't have direct access to Foo.  This doesn't apply in flutter and may cause confusion:  In react, if Foo never uses its child then Bar is never instantiated. And Foo is instantiated after the render method returns. While in flutter this is the opposite. Both Bar and Foo are created immediately  This would potentially lead in React developers making un-optimized flutter code.  **3. In general Dart/flutter is not JS/react**  If we add JSX in dart, I can already see the issues about type union or be able to do return foo && <Component / or the upcoming async rendering in react. Justified with a ""we already have JSX so we can have that too !""  I'd prefer a proprietary syntax or no syntax at all, for the sake of not having to implement the latest JSX/react feature in dart  **4. JSX makes some dart specifics unclear**  A small example, Scaffold requires for appbar a PrefferedSizeWidget. If we created Scaffold using JSX, peoples would expect that you can replace any given JSX with another. Which is not true. I mean, it makes it very unclear why we can do."
technical,"Yeah, I've seen that in cited comment. But it's not the same. This introduce an align shift and disturb reading flow. And this doesn't help me in other IDEs and text processors. IMHO templates suffers from NIH syndrom. I don't say that approach to mix PHP and HTML is the right way to do that. But react shows with JSX how it can be done better. No, I didn't miss the comment at all, it just makes no sense you telling me that people coming from JSX will find DSX too complex.  There are many reasons to choose DSX but with alternatives people will pick what they prefer for whatever reason they have. Totally opposite, DSX only implements 2 xml transforms and gains everything else for free from the hosting language. Imagine the effort trying to re-implement the power of Dart inside your new templating language. No thanks, I'll take Dart."
technical,"Can you share an example/img/link of, ""xml layouts from Android"" you're referring to? No. If ""JSX didn't become popular because it's the best solution for writing UIs, but because it's the best solution for writing HTML"", why does React still uses JSX for building a mobile app and desktop app? Even your Github desktop application, Walmart cross-platform mobile app, and Tesla app, and Skype are also built by RN.   React does not put a for loop in the JSX tag because the concept of React is about the component. The upper part is the logic and the below part is the JSX, and It is always like this. Everything is separated into many components and connected together into a large component.   **In fact, most of the people here who against JSX  can only guess  JSX is some kind of HTML, XML, or a less verbose solution. People should understand why people love to build an app either with React and  why JSX is so important on this.**"
technical,"You apparently didn't pick up on the sarcasm inherent in his comment. (He literally did everything I said not to do.) While a fun bit of trolling that I can appreciate, it's not appropriate here. And if, by chance, he was being sincere, then I apologize for my snideness, but my point still stands - that kind of comment is  still  not appropriate here. Did my disclaimer in my very first comment on this thread perhaps tip you off?  You equate me not having much experience with JSX and me not knowing how it works at all? While I've never worked on any serious React project, I've done my fair share of tinkering. I understand perfectly well how it  works . And that makes sense in the case where the markup language and the host language are distinct. With JSX, the markup language is designed as an  extension  of the host language. As such, JSX was designed as an extension of JS, and that's why it works as well as it does. DSX is an implementation of JSX for Dart.  You don't see the problem there? A markup language designed as an extension of one language being jerry-rigged into a foundationally different language. It is  inevitable  for there to be a load of issues, edge cases, and considerations. First, the whole idea behind separating markup and programming is that, if you are doing it correctly, there is a clear separation between the two which results in  no  duplication.  Secondly, if you are doing anything much more complex than ifs and fors in your UI code (which are constructs that can easily be handled in many markup solutions), then I would argue that it's a sign there is something wrong in your design anyway. As per MVC/MVVM design principles, if you are incorporating complex logic in your UI constructs, that's a potential sign of smelly code and you should seriously consider a redesign anyway.  (That's not to say that you  can't  write declarative UI in the MVVM fashion using JSX, but it's just something that invites trouble for little objective gain. Why use something in which you  can  write code that is standards-compliant when you can use something that makes it difficult to write code that  isn't ?) You haven't ""proved"" anything. You've given a bunch of subjective claims that have yet to be backed up with any substantiating logic. (Though to your credit, this latest post is a big improvement.)  You are still asking for them to do non-trivial stuff, so the onus is on you to convince them and the rest of us why they should and why it's so important that they should push down other things on their to-do list.  It is in the nature of JS itself that such a thing is easily doable (relatively speaking), so much so that Dart is far from the only language that has a transpiler to JS. As I have pointed out many times, Dart is not JS. It is static and strongly typed, meaning a lot of things that would be easy to do in JS are hugely complex in Dart.  By that logic, I should make a UI solution in which you define constructs using hex-encoded braille. I mean, if all that really matters is personal preference, all I need to say to defend its existence is that ""some people prefer it"", right?  You are developing a tool that you intend for other people to use, so you need reasoning beyond ""some people might like it"" to make yourself convincing. If you can't put it into words  why  they should use it over something else, then what makes you believe that they will? And what's there to incentivize you to  ensure  that they will when designing DSX's feature set?  First, those keywords (aside from async/await) became common programming lexicon because of the immense popularity of languages like C and BASIC over the course of several decades. As I've mentioned before, JSX is far from proven in its longevity - it's been around for 5 years and is yet to see any significant use outside of React despite the option being available.  Second, there's a big difference between familiarity and convention. if, while, for, struct, class, enum, try/catch/finally, async/await... those are all great ways to verbally represent a concept. There are reasons to defend using those keywords beyond them just being what people are familiar with - they make conceptual sense. (Of course, that doesn't mean that they are constants. Some languages do if ... then. Some do if ... elif while others do if ... else if and still others do if...endif. Some do foreach, others do from. And so on, and so forth.)  Meanwhile, the argument to use JSX because it is ""familiar"" doesn't fit the same category. JSX is one way to represent declarative UI, but it is neither the only one nor the most popular one. Furthermore, it was designed to be used in one type of environment, so using it in another environment can turn familiarity into a bad thing - familiarity leads them to expect it to work more or less exactly the same way it works elsewhere, so if it doesn't then that leads to a mental disconnect which is something that you want to  avoid .  They ask for it anyway, and the issue gets redirected here all the same. I don't see how the thread being closed would change that.  Read any book about product design. Chapter one is always about creating a statement, a manifesto, a slogan, anything tangible and expressible in plain English that describes what the product is and why people should care about it. There's a reason that the most common form of advice given to amateur entrepreneurs is to make an ""elevator pitch"", something that clearly and concisely communicates the product and the draw in 30 seconds or less. If you can't put it succinctly why people should use your product, it's a sign that it's suffering from an identity crisis. If the person designing the product can't adequately respond to criticism, then that gives the impression of a lack of faith in their own product. Both of those things are big red flags to investors.  In this situation, the product is DSX, and the investors are developers considering using it. The only people you have backing you are people who apparently would unconditionally cheer on anything with ""JSX"" in the description. Every other person in this thread I have seen question what you are doing has walked away seemingly unconvinced following your answer.  You are currently sitting at or near a 0% conversion rate, and  that's  where I'm coming from when I say that you have yet to adequately respond to criticism. Maybe you don't care, but if you seriously intend for DSX to be a UI declaration markup plugin usable and used in real-world projects, you might want to start.  But then again, maybe you are an exception. Ok this conversation has gone well beyond the kinds of discussions that we consider acceptable in the Flutter community, and so I'm going to lock this thread and close the bug. Please consider reading for more details on how we behave here.  The next step if anyone wants to contribute code to address this issue would be to come up with a design for how to do build system integration such that we can have code generation work with Gradle, Xcode, hot reload, and integration with existing apps. If anyone is interested in working on this, please don't hesitate to reach out to me. Otherwise I expect this is something we'll work on early next year. Once we have a mechanism to do that, solutions like DSX will be easy to integrate into the Flutter ecosystem. We may even find solutions that should be integrated by default. In the meantime we're also working on improvements to Dart's syntax to see if we can make UI expressions even easier to write.  I would like to ask that people do not open new bugs on this topic unless there is something very constructive and new that is worth bringing up."
technical,"Can you elaborate on why you want this? Maybe show an example of what it would look like compared to today? Ok, so the ""Basic widgets"" example would look like the following:"
technical,"flutter was inspired from React Native I don't see how that says ""everything is the same except the language is called Flutter instead of JS"".  I really feel that I have a hard time to read the UI code. and you claim the only imaginable solution to that is DSX? Have you considered that Flutter is not even 1.0 and that IDE support can improve over time? There were only the first tiny steps taken recently to get better Flutter IDE support in the code editor like quick fixes (""wrap xxx"") and closing labels. There are unlimited possibilities to improve developer experience and most in this discussion is like ""Flutter is not yet perfect and therefore we need DSX"" usually without concrete arguments about how or why DSX would solve or improve that. Once again Plain Dart or DSX is simply a matter of style, no point arguing which is better. It is just a personal preference and people have their reasons for picking one or the other. What has happened in the React World is that once you go with JSX, you simply don't go back. As others have stated above you get hooked on JSX after using for awhile. JSX has been adopted by others outside React's World (Typescript, Vue) and it is a great fit for Flutter. What the community needs is generic pre-processing capabilities with source map support built in the Dart tools (compiler, analyser, IDE, etc) so that DSX or anything better can be developed by the community and fully integrated into Flutter (debugger, auto-complete, etc). Sure, thing is this ticket is not about a generic 'improving the developer experience' thing. It is very specific and it is about getting JSX support in Flutter. The community wants DSX as an alternative to the plain Dart way."
technical,"Just found this thread, interesting on both sides. As a React developer interested in Flutter there are a couple other arguments that I haven't seen mentioned (though I did only briefly skim all the comments.)  1. Closing tag declaration increases understanding of children vs properties. UI can unfortunately be deeply nested, and having a clear </item tag vs an unknown ) helps to clarify and parse children. It also allows me to move code around between components and very declaratively see when something it out of place (a closing </list tag before a </listitem for example.) This is trickier with multiple nested )'s.  2. My initial gut reaction to seeing multiple nested components inside constructors gives me flashbacks to ""callback hell"". It was a very painful era of JS that is starting to get better, going back to it feels a bit like a step backwards.  3. Related to #2, the unfortunate fact is we have to convince people to switch (or at least try it out). Many people are using React/React Native and even more using HTML/JS. Creating a simple and mostly syntactically familiar tutorial/guide that specifically targets some of React's pain points might be extremely compelling to those who are a tad bit fatigued. One of the main reasons why React became popular in the Web Developers community was the support of JSX.  It's truly disappointing to see ""down votes"" on such a nice feature request. It improves readability and expedites adoption.  I think this is a major difference between Open JavaScript and Dart. JavaScript is truly open source whereas a request on Dart gets into conversation like this and ultimately you're demotivated with down-votes.  Make more room for new ideas!"
technical,"The difference here is that we would now be able to answer with the following : ""We don't plan on implementing this in dart/flutter for now. But you can take a look at community alternatives [here] and [there] or read [this issue]"" and close the issue as duplicate. One place for comments and votes and it is here. The request for JSX-like functionality is not going away and the ticket is opened because it needs Flutter tools support (compiler & VS Code IDE) and I have updated the ticket request with this info (first comment). If an enormous number of people starts asking for this, it will give the Flutter team incentive to put resources into it."
technical,"Another thing that strikes me as important is how we can encourage people to not write giant build method and take advantage of the composition nature of the framework. If the build method is taller than the height of your screen, it's going to be hard to read regardless it's Dart or XML.  It's not just the build method. It's all other methods that the build method calls to build the widget tree. Very common in React to use smaller methods to build sub-tree pieces and then call those into the larger tree.  Also in WebStorm with JSX, each XML node has +/- that can be used to expand/collapse node and children to make reading structures larger than the height of the screen easier. One thing we've found in Flutter is that big build methods are not great for performance, and we try to encourage people to break down their build methods into smaller reusable widgets. In particular, in Flutter having a build method that's built out of the results of other methods is somewhat of an antipattern that we'd rather discourage rather than make easier. (This is somewhat of a recent realisation so a lot of our examples and widgets don't do this well yet.)"
technical,"I've never seen the part/part of syntax in Dart before, and I'm having trouble finding any documentation for it. Is it something that Dart/Flutter officially supports? I'd love to use something like that in FLUI.  You keep going around in circles with the DSX justification. DSX is not JSX. DSX is JSX-like. DSX is meant to be a familiar syntax for React developers. DSX is merely think syntactic sugar for Dart. People will learn that DSX is not JSX. (And so on.)  While I get the point you are trying to make with all those explanations, I think the fact that you have to keep making them reveals a major issue regarding the nature of DSX in general, and it's a point that **rrouselGit** brought up as well. Even as you keep saying that DSX is  not  JSX, people who find it are going to think that it is, and that's a problem. JSX, and the people who use it, come from an ecosystem that is so conceptually different on fundamental levels from Dart/Flutter. As such, developing a feature for ""familiarity"" is not necessarily a good thing. One of the more apparent reasons for such is, as was pointed out, people are going to try something like this:  Because they come from Javascript/JSX, they expect that syntax to work in DSX. When it doesn't, it becomes a point of cognitive dissonance which may actually  hurt  their interest in not just DSX but in Flutter as a whole. Familiarity is beneficial when it's used as a means to ease people into something new, but that can be a double-edged sword - when 90% of the features are the same, the remaining 10% can serve to just frustrate and annoy.  Another issue with DSX is that it is not likely to be a seamlessly integrated feature any time soon, regardless of if it's a third-party plugin or if it gets officially adopted by Flutter. As you've said yourself, in order for it to truly work with the Flutter debugging and deployment process, it needs official support from not only the Flutter team but also the Dart team. Failing that, without pre-processing and tooling support, the only way DSX would work is with external manual conversion tools, which is just another (potentially lengthy) step that developers will have to go through.  While typing this up, there's another thing that occurred to me. There have been several pro-JSX people in this thread that have praised JSX, saying that the ""separation of concerns"" approach to UI design is really the only way they will ever consider developing UIs again. If that's the case, why is React the only framework with market presence that is using it? Both native and cross-platform mobile app frameworks have stuck with their storyboards, their XML files, their XAML files, and other such UI definition DSTs. Even other popular JS frameworks like Angular and the up-and-coming Vue still do the ""separation of technologies"" approach. React developers talk like JSX is the way of the future, but I've yet to see it pop up anywhere other than in React in a framework that has gotten any real traction. part/part of is an existing dart feature. It somehow joins two files into one. Usually used for code generation.  There are a few real cases scenarios that use such technique.  Like json serializable which generates a toJSON method and a fromJSON factory for classes based on their properties.  part/part of don't really do anything on its own. The interesting part is when you combine it to something like source gen."
technical,"hooluupog virtual closing tag comments already work in IntelliJ, AS, VSCode since a while People can't use my experiments because it can't be seamlessly embedded right now into Flutter, so it remains an outside/online experiment where people can only see the potential. Can you be more specific timewise for when can we expect some movement on the build system changes to support preprocessing? Are we talking a month, a quarter, six month, a year, 2 years, a decade, a jubilee, etc Please read the top paragraphs of the JSX spec to see that there is no need to change the Dart language since JSX/DSX has no semantics, and it is not meant to be implemented by engines or incorporated into languages. It is intended to be used in a preprocessor (transpiler). DSX could be used with Flutter & on Web to make React-Dart exactly like React.js but with the Dart language. How can people use something that is not available for them to use in the first place and then conclude that nothing needs to be done because people are not using it ? This reminds me so much of Melissa McCarthy's Sean Spicer impersonation on SNL about the 'travel ban'... 'circular using of the word' Preprocessing capabilities are badly needed to enable experimentation. Without it nothing moves forward and you won't learn anything new."
technical,"they can have their DSX.  If the Flutter team doesn't implement it, it can be an open source initiative.  That doesn't mean the pure-Dart approach like it is now won't work anymore.  Both worlds can co-exist.  I fully agree with this. Though i think it should be a pluggable rather than an out-of-the-box solution. Having one standard that works but being able to customize the experience with extra things is a great paradigm. That way the Flutter team can keep its focus on (what i believe are) more relevant issues, the community can experiment with different tools/solutions and then we can have a discussion with more data and experience with DSX or any other alternatives to the current meta.  True that. However, i think we can all agree we don't want Dart/Flutter to become another JS/Web Frontend ecosystem. Flutter isn't even out of beta yet and we're already want it to have 2 standards for something kinda subjective.  Most are community driven though React refers to them. Good midway. Now, only 2 of them are officially supported: the React.createElement and the JSX way, which is a wrapper over the other one. The value of JSX is notorious in that context but it is not that clear over here. With this in mind, can we meet midway by having only one official standard and the docs referencing DSX whenever relevant?  I believe the Flutter team should be focusing on the lacking features that truly prevent developers from building apps. I can't recommend Flutter to my managers if we can't even add Maps support and developing a Camera features takes 2 weeks.  Bear in mind, I'm not closing the door on DSX forever. Maybe it will become the solution for UI building but we need the community experimenting with it to make that decision. Personally I don't believe the two ideas can coexist in the current state of Flutter - we really shouldn't be encouraging a split of this nature this early - but at this point it looks like the only compromise."
technical,"Thank you! Please avoid this kind of comment, be it ""+1"" or ""Thanks"". This sends an email to all subscribers for nothing interesting."
technical,"sounds like a broken record asking for 'good' arguments, plenty were given before but you just didn't get it, which is OK, 'to each their own'.   Adding something like JSX to Dart causes an awful lot of work and complexity in the Flutter tools and the IDE. You need to provide proper arguments for others to even consider looking at it.  Not really, my work is almost ready and it took me very little time considering I only worked on it on weekends.  Again, DSX is just a slick improvement that people can choose to use or not, because it doesn't change the current way, it just provides an alternative that others (React developers) will be instantly familiar with. plenty were given  not really. Just general claims which can be true or not. No additional details were provided that would allow others to verify.   Not really, my work is almost ready  Great. Then there is no need for the Flutter team to do anything and the issue can be closed ,p Does this include support autocompletion and syntax checks in all IDEs supported by Flutter?"
technical,"Adding features because others have it is weak argument. Okay. Why does flutter have hot reloading? Where did that come from? Jesus dude.  How did we fail to provide solid argument for you guys? **Project traction and attracting developers** is our number one reason.  Reason number two, **readability**: Reason number three, **GUI Builders**. I'll quote the first line in the README.   A new mobile app SDK to help developers and **designers** build modern mobile apps for iOS and Android.  I would hate to see Flutter going down the same rabbithole as Polymer before it even reaches beta. Project traction and attracting developers  The relation is still unclear.   Reason number two, readability:  Making Dart code more readable seems a better goal to me   Reason number three, GUI Builders. I'll quote the first line in the README.  As far as I remember it was mentioned above already that there is no reason why using Dart code would prevent that."
technical,"Add JSX to backlog and let it compete, gladiator-style, with the million-plus-one other urgent requirements? React is written for the web and thus needs a simple solution for writing HTML. JSX didn't become popular because it's the best solution for writing UIs, but because it's the best solution for writing HTML.  Flutter doesn't have that constraint, so we shouldn't settle with a mediocre, verbose solution for the wrong reasons.  If we want to reduce verbosity I'd rather take inspiration e.g. from Anko. There you can define new local variables anywhere and use normal for-loops to dynamically construct the list of children, which can make the code easier to follow. Also, the LayoutBuilder would become easier on the eye since each nesting level is already a lambda function anyway (no need for passing an extra builder lambda). Anyway, this is just for inspiration and I don't think Flutter should copy that 1:1."
technical,"React Native is not web development nor it uses HTML. Ask experienced React developers (or fully read this and the other JSX thread) and you will see that JSX is considered by many React developers as the best way for writing UIs. This statement clearly demonstrates that you don't know JSX. JSX (as in DSX) uses all programming constructs (for-loops, etc) from the hosting language (Javascript/Dart).  This ticket is only interested in JSX-like functionality, for other approaches (like Anko) please create your own ticket for discussion there. React Native is not web development nor it uses HTML. Ask experienced React developers (or fully read this and the other JSX thread) and you will see that JSX is considered by many React developers as the best way for writing UIs.  React came out long before React Native. The original design of JSX is based on Rendering HTML, not native UIs. Nobody has been able to show a single convincing argument for what JSX does better. By comparing  you haven't done an up-to-date comparison and you're simply putting more stuff one a single line. You must compare it to:  Notice how all the ugly and closing tags are gone. Only expressions are allowed. So you must use conditional operators (c ? x : y, which makes else if very ugly) and Array.map etc. (which can also be very ugly) or move parts of your code to the top of the render function or into a separate helper function. It's the same with Flutter, of course. Anko doesn't have this limitation and makes writing (some) UI code more natural.  I think in a discussion about introducing JSX it's quite valid and necessary to ask if that's actually the best solution or if we can find something better. Otherwise we'll waste resources on the wrong tasks."
technical,"React is written for the web and thus needs a simple solution for writing HTML. JSX didn't become popular because it's the best solution for writing UIs, but because it's the best solution for writing HTML.  Flutter doesn't have that constraint, so we shouldn't settle with a mediocre, verbose solution for the wrong reasons.  If we want to reduce verbosity I'd rather take inspiration e.g. from Anko. There you can define new local variables anywhere and use normal for-loops to dynamically construct the list of children, which can make the code easier to follow. Also, the LayoutBuilder would become easier on the eye since each nesting level is already a lambda function anyway (no need for passing an extra builder lambda). Anyway, this is just for inspiration and I don't think Flutter should copy that 1:1. React Native is not web development nor it uses HTML. Ask experienced React developers (or fully read this and the other JSX thread) and you will see that JSX is considered by many React developers as the best way for writing UIs. This statement clearly demonstrates that you don't know JSX. JSX (as in DSX) uses all programming constructs (for-loops, etc) from the hosting language (Javascript/Dart).  This ticket is only interested in JSX-like functionality, for other approaches (like Anko) please create your own ticket for discussion there."
technical,"I'm with you on this one.  The 100. React wasn't designed for TypeScript. It was designed for Javascript. All the widget definitions, attributes, properties, and everything else was designed to be used in the dynamic environment of JavaScript, so the type safety of TypeScript doesn't introduce any new factors with how JSX interacts with React. This is yet another example of how JSX was designed for a completely different setting than what Flutter is."
technical,"#But then the argument of having an easier conversion from react to flutter is invalid. As JSX is radically different from your prototype :  And none of your examples here or from your link actually simplify the code or improves readability  As much as I can relate to your feeling of missing JSX (got the same when starting flutter), after some experience, the native syntax feels pretty good actually  As a side note, there's a much better solution to your separation of concern. That is a template file You could have an xml/yaml file next to your widget. And then use the awesome code generation tooling dart provides.   I'd rather prefer a : which then using a custom code-gen generates the following dart file : The end result is even better than a ""DSX"" for separation of UI/logic. It's better for potential UI generators too.  And it's much easier to implement using built. Really !!! The only thing radical in these discussions has been the reaction of the naysayers.  As it's stated in the title of this ticket, DSX is JSX-like, it is not JSX-identical or else it would had been called JSX, and the additions to it are minor and provides options to developers.  You could write it like:  Hummmm, you seem to be confusing 'separation of concern' with 'separation of technology'. These things are very different, you separating dart code and markup code in different files, is simply 'separation of technology' and provides none of the benefits of 'separation of concerns'. A concern here would a component/widget that encapsulates reusable code cleanly, it doesn't matter that inside that component/widget it is using different technologies.  Also separating technologies as you recommend is highly inferior to JSX/DSX which uses the host language for all of its imperative constructs (for loops, function calls, if statements, etc)."
technical,"is there any connection from your comment to the topic of this issue? said he hopes proponent of JSX Syntax just to spend some days really working with Flutter before continuing lamenting. This comment shows we really have spent a lot of days working with Flutter, and the request of JSX is really our feeling from our heart."
technical,There are also plans to improve Dart to make writing Flutter UI code less verbose  Nice to see acknowledgement that the current way could use some improvements. Please do provide details on such proposal. Your best bet is to interact with the React Native community for feedback. Several Dart language feature requests could make code more short/readable:  - Optional new and const for Dart 2 - Meta-issue for discussion of the proposal to allow named arguments everywhere - - this could allow widget with child argument to skip named argument. - Variable Number of Arguments) - this could allow widgets with children argument (like Row and Column) to skip this named argument and its list declaration.  With those changes code could looks like:  Moreover depending on your IDE you can optionnaly have synthetic comments at the end of parenthesis and you could see something like this in your IDE:
technical,"Since you mentioned productivity, can you provide an example that clearly showcases the productivity loss of using Flutter's Pattern instead of JSX? The example should be built with the basis of knowing enough JS **and** Dart to be able to code said example. I believe that if we don't take that into account then we are also comparing programming languages, which is not the same discussion.  It is also the most downvoted one. It's quite controversial. Since this feature request is an alternative to the current way and it doesn't change the current way, there shouldn't be any controversy at all, if you don't like JSX/DSX continue programming as you do today.  The so called controversy only exists because the Flutter team needs to do work to enable the community to do DSX properly. If the Flutter tools (compiler, analyzer, ide support, etc) supported pre-processing with source maps, DSX would had been done long ago, and most likely other language innovations/ideas, from 3rd party developers, would had happened too."
technical,"I don't want to argue against the feature but comments like ""if you don't do x then horribly y will happen"" are just ridiculous. Since you mentioned productivity, can you provide an example that clearly showcases the productivity loss of using Flutter's Pattern instead of JSX? The example should be built with the basis of knowing enough JS **and** Dart to be able to code said example. I believe that if we don't take that into account then we are also comparing programming languages, which is not the same discussion.  It is also the most downvoted one. It's quite controversial."
technical,"to be clear, JSX compiles to function calls (in React calls to createElement(ComponentClass), in Dart it should be constructors).  - How do I use a switch? As far as I know in Dart as in JS, you can't use a switch inline as it is not an expression. If you are outside of the widget tree you can just use a switch normally. So you propose the flutter team should put features on hold for something that is questionable and may or may not attract a fraction of a specific subset of web developers?  React Native is not web development, it's cross platform mobile development that is the main competitor to Flutter, and yes JSX is like a light source, it will attract lots of React Native devs. I asked for this feature in Aug 2017, too much talking too little action.   Every DSL is by definition less expressive than a GPL. With DSX how do I conditionally hide a widget? How do I iterate over a collection and map every element to a widget? How do I use a switch? You now have to make up syntax and semantics for constructs that you already have in your GPL. So why invent the DSL in the first place?  You are completely wrong. The good news is that I was once wrong too. Most (but not all) DSLs try to recreate programming constructs in markup (and that is not very powerful), JSX brings markup into programming (taking full advantage of the host language). The difference is transformational. The answer to all your questions is basically use the way you do it in Dart because it is Dart. Everything in between '{}' is Dart code with the exception of the spread operator. It's all dart expressions but you can also use anonymous functions that returns an expression. As you see in the transpiler a tag (\<Text) is just a new on a widget (new Text()).   What problems would a DSL solve that you can't solve right now?  Why use Dart when you can use zeros and ones, isn't that all that is necessary to solve any computing problem?   You keep saying it's better, why is it better?  I gave my reasons before and won't repeat them here. JSX people will agree with me but 'to each their own'.   I have no doubt DSX would attract some JSX people. I know people don't like things that are different. And familiarity seems to be the only argument. So why not use CSS? Why not use JavaScript? More people know how to use these than Dart.  Yes, using CSS makes sense to ease the designer-developer workflow. DSX supports that. The advantage of Dart over Javascript is execution speed (performance).   If you design your system based on some other thing for the sole purpose of being familiar, you're not really innovating.  You are so full of wrong biases that are most likely preventing you from reaching your full potential. Open your mind, try different things."
technical,"You also said that you needed preprocessing support from the Flutter/Dart team in order to do so. Am I incorrect in that?  JSX was developed by Facebook for React, put through a rigorous proposal/design/implementation/iteration process, and then released into the world years before you got your hands on it. It's been rigorously tested and proven time and time again in real world environments. It's a mature technology. There's no reason to demand to see a spec sheet for something like that.  DSX, on the other hand, is being developed today by you and a handful of people. You've waxed eloquently about what it can and will be able to do, but all we've actually  seen  is a small handful of purpose-built code snippets and your word that they were generated by the transpiler. People who have even want to try it out and suggest possible changes or improvements aren't able to do so, so they have no reason to support your efforts beyond ""Yay JSX!"".  I'm not accusing you of lying or anything, I'm just saying that JSX has earned a level of faith that DSX has not, so how are you going to turn some heads if you don't let people tinker with it? JSX has been in Vue for almost 2 years now. And unlike Vue itself, JSX is a pre-existing technology that needs no introduction, especially for people familiar to React. If JSX was going to take the Vue.js world by storm, I can't help but feel it would've done so by now. (Particularly if it's any indication that as many people are clamoring for JSX in Flutter as you claim.)\  JSX and DSX are the same syntactic concept. The problem is that, where JSX was built on a weakly-typed dynamic language like JavaScript, DSX is being built on a strongly-typed static language like Dart. That means there are lots of problems DSX will have to account for that JSX didn't have to if it's going to be anything other than a niche ""JSX for Flutter"" implementation, and it's going to take some  ingenious  modifications to make DSX really work without making it too bloated to justify claiming that it's more visually concise.  And to address the ""DSX is just Dart, if DSX can't do something, just use Dart"" rebuttal, then my counter-rebuttal would be if I have to keep falling back to Dart whenever I run into a scenario that DSX doesn't handle, then why shouldn't I just use Dart all the time?  And to address the rebuttal for *that* reading ""you can if you want to, DSX is just an option"", then you are really selling yourself short. Even if it really is just going to be ""an option"", it still needs to bring something to the table that is going to convince people to use it. You yourself said that DSX is not JSX, so people who want just JSX aren't going to get what they want. **That** means there needs to be some tangible reasons beyond the ""JSX-like appeal"" for people to want to use it.  If you're just building a tool that you yourself want to use, then all this is moot and you can go nuts. But if you are actually building something you intend for other people to use, then you need to put it in a solid form why you think they should.  Somewhat off-topic, but I would like to point out that web components really are a promising look at the future, even if support for them is getting added slower than tar. Think about it this way: React does what it does because it essentially implements the idea of web components in Javascript-only. Imagine how much better it would be if those features were supported by the browser and benefitted from non-sandboxed performance and not having to operate through DOM manipulation? (Granted it might be another 20 years before we find out, but still...) Sorry dude I don't have the time to argue endlessly and repeat what I said before over and over, we won't end up in agreement anyways so best of luck with your FLUI.  The online DSX transpiler has been live since Feb 2018 and anybody can use it so there is no need to take my word for anything. Press 'Compile' and it compiles what its written on the left panel and places results on right panel. Open debugger and you will see the AST written out.  It makes no major difference at all, like the OOP (Object Oriented Programming) concept and syntax for 'classes'. It's almost identical in typeless Javascript or typed Dart, same can be said for 'if' statement, 'for' statement, etc   it still needs to bring something to the table that is going to convince people to use it.  Apparently it already does for 100 people in this ticket, and that's 100 times larger than just me using it, good enough for me."
technical,"You are so full of wrong biases that are most likely preventing you from reaching your full potential. Open your mind, try different things. I will unsubscribe from this issue now. Please never mention me here or anywhere again, thx. Sorry guy, I didn't mean to hurt your feelings... I was just trying to gently guide you while trying to manage my own frustrations. We are all humans after all."
technical,"Thinking on the amount of energy that was invested just in this discussion and what could have been done good to improve the current framework instead makes me sad. Sorry, but this isn't a problem. Furthermore, this doesn't make any sense at all. Beside of that, we use JSX with TypeScript. absolutely!"
technical,"that's just more of the same  what is the actual advantage over plain Dart?  ""JSX is a slick improvement"" is just not convincing in any way and isn't an argument. It's just a personal opinion without (again) any argument to back it up, similar with other pro-JSX arguments above.  You can't expect anyone to consider your suggestions if you're not willing to provide good arguments.  Adding something like JSX to Dart causes an awful lot of work and complexity in the Flutter tools and the IDE. You need to provide proper arguments for others to even consider looking at it. sounds like a broken record asking for 'good' arguments, plenty were given before but you just didn't get it, which is OK, 'to each their own'.   Adding something like JSX to Dart causes an awful lot of work and complexity in the Flutter tools and the IDE. You need to provide proper arguments for others to even consider looking at it.  Not really, my work is almost ready and it took me very little time considering I only worked on it on weekends.  Again, DSX is just a slick improvement that people can choose to use or not, because it doesn't change the current way, it just provides an alternative that others (React developers) will be instantly familiar with."
technical,"I don't work on Flutter, but it seems sort of unreasonable to demand certain tooling to benefit others, but then also refuse to release any early source code or examples of what you want the tooling to integrate with. For what it's worth, I don't know of any language or toolkit that provides pre-processing hooks of any kind - probably because it's just plain not easy, there are a lot of corner cases and assumptions around the language and syntax. If you generated source maps, I imagine getting at least some basic support in an IDE is pretty trivial, independent of Dart/Flutter. But again, this is all conjecture without knowing what your tooling does and how you'd expect it to work. supporting pre-processing via source maps is a generic mechanism that does not depend on any specific transpiler. It is functionality that is meant to support any future imaginable language. The Chrome browser/debugger supports it quite nicely and I can debug any language that transpiles to JS.  For testing you can come up with any simple kind of transpiler to show how to use source maps. For example write a trivial transpiler that generates Dart/Flutter code with a blank line between every line on the original file. (.d2 = .dart, .d2 is Dart/Flutter file, out .dart file will contain a blank line between every line in the original file).  Yes, I can work on generating source map for a testfile."
technical,"The reaction tells us everything.  As now, +1 nearly double times -1  This doesn't mean anything.  Except those who are watching all Flutter issues double the one that land on this issue because they are already used to JSX (and are explicitaly looking for it). So this rather means  the people that want a JSX-like experience double the people watching all issues that have voted -1 . (imho a part of the people voting +1 haven't even really tried flutter) Sure but JSX doesn't use namespace so it is not a XML feature of interest.  Since you split the 'children' out of action into it's own tag, kind of reminds me of the new Fragment tag of React.  It's a balance between verbosity and conciseness that's for sure.   I really feel that Flutter team have created the current API specifically for the new way to declare UI they supposed that's better than JSX and any effort we are trying to bind JSX syntax to the current API just make it look unnatural/uncomfortable to use.  There is nothing new in the way Flutter declares UI in code, Perhaps using DSX is unnatural/uncomfortable to you but to JSX developers it is not. JSX/DSX is perfect for Flutter, it fits like a glove and if you don't like gloves go bare handed ,) it sure does !!! you can argue with 'feeling', 'thinking', 'suspect', 'imho', 'opinion' but this is data, a concrete data point. If the data is useless it shouldn't be collected. I guess the data is useless because it doesn't paint your picture."
technical,"So you propose the flutter team should put features on hold for something that is questionable and may or may not attract a fraction of a specific subset of web developers?  React Native is not web development, it's cross platform mobile development that is the main competitor to Flutter, and yes JSX is like a light source, it will attract lots of React Native devs. I asked for this feature in Aug 2017, too much talking too little action.   Every DSL is by definition less expressive than a GPL. With DSX how do I conditionally hide a widget? How do I iterate over a collection and map every element to a widget? How do I use a switch? You now have to make up syntax and semantics for constructs that you already have in your GPL. So why invent the DSL in the first place?  You are completely wrong. The good news is that I was once wrong too. Most (but not all) DSLs try to recreate programming constructs in markup (and that is not very powerful), JSX brings markup into programming (taking full advantage of the host language). The difference is transformational. The answer to all your questions is basically use the way you do it in Dart because it is Dart. Everything in between '{}' is Dart code with the exception of the spread operator. It's all dart expressions but you can also use anonymous functions that returns an expression. As you see in the transpiler a tag (\<Text) is just a new on a widget (new Text()).   What problems would a DSL solve that you can't solve right now?  Why use Dart when you can use zeros and ones, isn't that all that is necessary to solve any computing problem?   You keep saying it's better, why is it better?  I gave my reasons before and won't repeat them here. JSX people will agree with me but 'to each their own'.   I have no doubt DSX would attract some JSX people. I know people don't like things that are different. And familiarity seems to be the only argument. So why not use CSS? Why not use JavaScript? More people know how to use these than Dart.  Yes, using CSS makes sense to ease the designer-developer workflow. DSX supports that. The advantage of Dart over Javascript is execution speed (performance).   If you design your system based on some other thing for the sole purpose of being familiar, you're not really innovating.  You are so full of wrong biases that are most likely preventing you from reaching your full potential. Open your mind, try different things. thank you for the details."
technical,also reported DC Thank you!
technical,"There are a few other things to consider:  - Separating layout specification from app code can make code more future-proof, e.g. this Dart 2 syntax proposal would not affect build methods if they were transformed according to an upgradeable pragma into Dart code from an agnostic format like JSX. - Defining the layout as markup makes it possible to separate the render engine from the (stateful/virtualized) widget engine ala React's relationship with ReactDOM and React Native, potentially making it easier to port widgets to new platforms. - Defining the markup format makes it easier to port existing layouts to Flutter, for example from Sketch or other design tools thank you.  thank you. Yes, it's just another option for people who love JSX to use. If that's not your thing cool continue doing as you are doing, nothing changes there. yes you can use render props for the named parameters but there is nothing you can do about the positional parameters. The key was that I didn't want to hard code anything in the transpiler so that it would work for everything.  great use that then, obviously DSX is not for everyone. this is not a replacement, it's an option for people that like JSX.  DSX is for React Native people that love JSX, if you don't see its benefits, don't use it. It's not DSX versus plain Dart. It's DSX vs. JSX, the closer the 2 are the better. thank you for listening !!! You hit the nail on the head why such huge resistance to something that will make React Native people really happy. Once React Native fans move to Flutter there will be tonnes of people to help out on these other needed things. Priority 1 is to steal mind share from React Native and make it as simple as possible for React Native people to move over.  How is DSX 'less dynamic' and 'less expressive' than plain Dart? DSX is Dart didn't you try the transpiler.  thank you.  off course it is. it is not helpful for current Flutter devs but it is very helpful for designers that can output CSS from their tools, it is very helpful for people vested in the React Native platform. very good observations."
technical,"One of the main reasons why React became popular in the Web Developers community was the support of JSX.  It's truly disappointing to see ""down votes"" on such a nice feature request. It improves readability and expedites adoption.  I think this is a major difference between Open JavaScript and Dart. JavaScript is truly open source whereas a request on Dart gets into conversation like this and ultimately you're demotivated with down-votes.  Make more room for new ideas! That looks amazing! Can I try it somewhere?"
technical,"How is that related to the JSX discussion? How can that be considered an advantage? You can use Swift on iOS because it's the successor of Objectiv-C. You can use Kotlin on Android because it's considered a slick improvement to Java. That sounds like you argue that JSX should replace Dart. That doesn't make much sense to me.  You actually said it (JSX is a slick improvement over the current way) but came up to the wrong conclusion (JSX to replace Dart). that's just more of the same  what is the actual advantage over plain Dart?  ""JSX is a slick improvement"" is just not convincing in any way and isn't an argument. It's just a personal opinion without (again) any argument to back it up, similar with other pro-JSX arguments above.  You can't expect anyone to consider your suggestions if you're not willing to provide good arguments.  Adding something like JSX to Dart causes an awful lot of work and complexity in the Flutter tools and the IDE. You need to provide proper arguments for others to even consider looking at it."
technical,"I can understand you. I used to be against JSX and mixing js with xml/html.... then I tried it. After a few months spent with react, I fell in love with JSX. Two killer advantages are: 1. no new syntax and utilities 2. no more passing of variables, functions etc. That's not a given, who would had thought that Google would remove MathML from Chrome? It is not a change to the way Flutter apps are build at all, it's just an alternative way that doesn't change the current way, and most important it is simply a different syntax to the class library. A simple transpiler does the mapping without needing any information from Flutter classes so it works just fine for anybody's code, as well as Flutter now and in the future.  Yes, you don't know React until you spend a few months working with it and then you realise how phenomenal a library it is for dealing with component hierarchies."
technical,"Big no for this, i really think that 1 language only is a big gain, jsx syntax will come with more things like separation of xml from js, etc... Not good. thats my opinion. That's the longest and pointless Github issue I ever seen."
technical,"Leave it opened as is because people will continue to open new tickets asking for JSX (as it has already happened twice before). The difference here is that we would now be able to answer with the following : ""We don't plan on implementing this in dart/flutter for now. But you can take a look at community alternatives [here] and [there] or read [this issue]"" and close the issue as duplicate."
technical,"Got it but while you are at it perhaps you should also consider basic etiquette while blasting this thread with your novels (long & fictional writing).  Please stop spreading FUD (fear uncertainty & doubt) with your spray-and-pray barrage of senseless questions (you know, throw as much crap on the wall and see if anything sticks) and demands.  After all your writings you haven't added any value to DSX so I have no interest having a discussion with you on this subject. Your motive is obvious though, promote FLUI while blasting DSX.  Tell me something, do you have answers to your own questions when they are applied to FLUI? Let's discuss FLUI for a bit shall we? The fact that you refer to my responses that I put a lot of time and effort into being as well-thought-out and unbiased as I can make them as ""long & fictional writing"" illustrates a lot about your character  and how you are approaching this discussion. I'm trying to promote discussion regarding very real issues surrounding any implementation of JSX in Flutter, while you lambast anyone with any form of contrary opinion. Which of us is the bigger offender of basic etiquette? The only thing I am ""demanding"" is that you address the numerous issues brought up by many people regarding DSX with more than either a hand-wave or open hostility. For someone who is proposing a significant change/addition to Flutter's feature set, I feel like that's not an unreasonable expectation. I'm asking  you  to defend your position. You've repeatedly said that JSX/DSX is the best/future, but have yet to explain how or why. Several people have expressed valid concerns about DSX, but instead of addressing them, you wave it off by hiding behind the counterargument ""if you don't like it, don't use it"". My ""motive"" is to get you to answer questions that need to be asked regarding  any  technical project, first and foremost being why people should ever use it over the alternatives. (And as I have explained before, familiarity is not a good enough reason.)  As far as FLUI is concerned, all I am doing is proposing an alternative solution to the overall problem (UI declarative syntax for Flutter) while requesting that people do the same to it that I am doing to DSX - offer sincere and constructive criticism. I am not saying that FLUI is objectively better than DSX - I'm proposing an alternative formed from a different approach to UI development and asking people to form their own opinions.  (I'd also like to point out that, other than my initial mention where I was proposing a possible alternative approach to GUI representation, the only times I've ever even talked about FLUI is when you brought it up. So how does it makes sense that my ulterior motive is to promote it when you are talking about it more than I am?)  FLUI is not DSX - it doesn't have to answer  every  question that I posed to you regarding DSX because many of them are specific to DSX's design. That's not to say that it doesn't have its own set of questions that need to be answered, though, and no, I don't have all those answers. That's  why  I value critical discussion - FLUI/DSX aren't going to stand up to the court of public opinion unless they can survive getting raked across the coals a couple times. This is not the appropriate place to discuss FLUI though. If you want to discuss FLUI at length, the project has its own issue board, so feel free to post there.  Instead of responding to criticism, you've instead been defensive and evasive, so much so that you are directly responsible for the two separate occasions (approaching three) where this thread had to be temporarily closed due to things getting too heated. So I'm going to break from ""etiquette"" and say this once: shelf your ego, stop interpreting criticisms as personal attacks, and answer the important questions. Either that, or make peace with DSX never getting off the ground."
technical,"this thread is 100% about Dart and syntax.   The discussion is not about Dart, but a synthetic layer on top of it that most UI developers already use today, and we are proposing that Flutter incorporates something like that.  So it's not about Dart... but Flutter needs to use something besides Dart for layouts? Seems like your saying Dart isn't good enough while also claiming this has nothing to do with Dart?  I don't think it matters at this point, the Flutter team has seen this feedback (on requesting a JSX/DSX approach) and they want to continue down their original path. I do think it could be handled better but it doesn't seem like they're opposed to the community creating solutions.  I'm happy there is another cross platform option... will Apple be the next to offer something up? And maybe they will see what so many of us like about react/react-native? IDK if they have anything cooking. the Flutter team has seen this feedback (on requesting a JSX/DSX approach) and they  This bug is still open because we haven't figured out what we want to do here. We're eagerly looking at the experiments (e.g. 's) to see how people use them. We're planning, at some point, to provide a hook in the build system for codegen tools such as this, though this isn't something we're likely to do in the near future. On the long term we hope to use what we learn here to influence Dart's development as  a language. This could mean adding something like E4X/H4X/JSX/DSX to Dart itself. Or maybe we'll learn that nobody really ends up using it so we will do nothing. Or maybe everyone needs something different so codegen hooks and custom packages like 's are the answer. We don't yet know."
technical,"Everyone is just giving its own subjective opinion on comparing both syntaxes, but we have to agree on one fact: it's a very controversial feature. There is love and hate, but most opinion diverge on the usefulness of JSX over plain Dart.  In any case I think it's totally unreasonable to ask the Flutter team to commit to support this feature before the 1.0 release. While the current way of building UI may not seem great for everyone - it works (and it works great in some opinions).  If people really want a JSX-like syntax now, a community driven effort seems like the way to go. And it would help make the case (or not) when the Flutter team considers it in post 1.0 release.  Personally I strongly agree with the following argument: it's not because JSX works for React (where you are already building the UI using a markup language) that it automatically works for Flutter. The great news is that even is this was adopted by the Flutter team (which isn't likely based on this thread) no one would be FORCED to use it. It's an OPTION. I don't understand why it's so emotionally charged that some people might like writing code with a syntax you don't like...  I don't care what other people like or don't like. But I care about features that help me progress with my application. Resources are limited, and I would rather have a proper video/stream player, native child views, and some vector graphics format.   For example, right now in React and React Native you can write your whole application without JSX, it's just an option. Here's what React looks like without JSX: It's a lot messier than the JSX version, and almost no one uses it. That's probably why some of us who are coming from React would appreciate the possibility to avoid having to do it this style again.  That's because the web was not designed for a system like that. If you have a static markup language like HTML and want to ""dynamise"" it, you have to invent a system that needs to work ontop of that. What you will end up with is some construct that is constrained by the underlying platform. Flutter on the other hand has no markup language. I have yet to see a reason to invest resources into something that is less dynamic and less expressive."
technical,Here's some proposed markup syntax for Hixie 's example the interesting thing you did was add a 3rd attrib possibility that simplifies the look of expressions when they are another tag. JSX has.
technical,"Could you be more specific as to why they can't coexist ? (given the fact that DSX is just syntatic sugar, or as emalamela says ""just a wrapper of the current way"").  Also why is it too early to provide a different syntax to exactly the same thing? I am basically asking why is there a need to delay this, what would be different in the future that is not there yet right now?  Personally I rather not put any limits on what people can do with Dart/Flutter. Let the market/community decide. Basically if what is created doesn't add value, people won't use it and it will die. If it becomes popular, it is because the community found it useful and valued it. No need to pick winners and losers right now. The only thing that is stopping me from giving Flutter a try is the fact that they chose not to use JSX. IMHO JSX is the best choice to express component hierarchy"
technical,"React Native is not web development nor it uses HTML. Ask experienced React developers (or fully read this and the other JSX thread) and you will see that JSX is considered by many React developers as the best way for writing UIs.  React came out long before React Native. The original design of JSX is based on Rendering HTML, not native UIs. Nobody has been able to show a single convincing argument for what JSX does better. By comparing  you haven't done an up-to-date comparison and you're simply putting more stuff one a single line. You must compare it to:  Notice how all the ugly and closing tags are gone. Only expressions are allowed. So you must use conditional operators (c ? x : y, which makes else if very ugly) and Array.map etc. (which can also be very ugly) or move parts of your code to the top of the render function or into a separate helper function. It's the same with Flutter, of course. Anko doesn't have this limitation and makes writing (some) UI code more natural.  I think in a discussion about introducing JSX it's quite valid and necessary to ask if that's actually the best solution or if we can find something better. Otherwise we'll waste resources on the wrong tasks. The original design of JSX is about familiar way of creating/manipulating tree structures which specially show up when doing UI work, think component hierarchies which show up in web development, native development, any UI development, etc. That's the point, we are not looking to replace the current way, we are looking to add an alternative way that is familiar to React developers.  Your Anko proposal would be familiar to Android Kotlin developers so go ahead and propose a spec that works with current Flutter hierarchy in a separate ticket. Once I see (or try an online version of your spec) I would be able to see if it can generate/interoperate with current Flutter widget hierarchy. Not that I recommend you doing this but it is possible: create an anonymous function and call it. There is no such thing as the best solution, it's all about having choice, having the choice of using something familiar that maps directly to Flutter widgets and adds no overhead."
technical,"I could say exactly the same thing about Flutter. Spend some weeks using it and you won't miss JSX. The reaction tells us everything.  As now, +1 nearly double times  -1"
technical,"I think angular template syntax does follow xml semantics and as I'm aware of there is no conflict use case between angular syntax and xml document.  In typescript, we have support for generic component. So I think we could have something like this: But again, the generic component is used for type checking the property input. I don't know if the above syntax has a right semantic meaning in this use case.  I really feel that Flutter team have created the current API specifically for the new way to declare UI they supposed that's better than JSX and any effort we are trying to bind JSX syntax to the current API just make it look unnatural/uncomfortable to use. The reaction tells us everything.  As now, +1 nearly double times -1  This doesn't mean anything.  Except those who are watching all Flutter issues double the one that land on this issue because they are already used to JSX (and are explicitaly looking for it). So this rather means  the people that want a JSX-like experience double the people watching all issues that have voted -1 . (imho a part of the people voting +1 haven't even really tried flutter)"
technical,"I don't think anybody here doubts that JSX React is an improvement over classic React. But like I said, that solution was created because of problems related to properties of the underlying platform. Flutter doesn't have these properties.  The question is not 'is JSX helpful for React?' the question is 'is something like JSX helpful for Flutter?'.  I think the answer is no. There are a few other things to consider:  - Separating layout specification from app code can make code more future-proof, e.g. this Dart 2 syntax proposal would not affect build methods if they were transformed according to an upgradeable pragma into Dart code from an agnostic format like JSX. - Defining the layout as markup makes it possible to separate the render engine from the (stateful/virtualized) widget engine ala React's relationship with ReactDOM and React Native, potentially making it easier to port widgets to new platforms. - Defining the markup format makes it easier to port existing layouts to Flutter, for example from Sketch or other design tools"
technical,"Your counter arguments can't really dismiss the idea, can they?  1. Relation is pretty clear. Unless it's not a goal for the project to be popular? 2. Great! Will it be close to JSX readability? What is the current proposal for such a thing? 3. It was stated that it **could be done**. Adapting JSX support for currently available GUI builders will be far much simpler. There are also plans to improve Dart to make writing Flutter UI code less verbose  Nice to see acknowledgement that the current way could use some improvements. Please do provide details on such proposal. Your best bet is to interact with the React Native community for feedback."
technical,"Very opinionated statement and simply not true There's a lot of unneeded characters in the default react syntax. Let's compare words repetition and characters count for each syntax (excluding function definition, indentation and 'return')   React without JSX: - 133 characters, including 3 parenthesis, 3 brackets, 3 :, 4 , and 11 spaces - React.createElement written twice  JSX: - 104 characters, with  2 parenthesis, 3 brackets, 1 :, 4 < and 5 spaces - Container and Text written twice  Dart: - 99 characters, with 2 parenthesis, 4 :, 3 , and 4 spaces - No repetition   In term of characters, the obvious winner here is dart syntax.  Now we also have to take other dart specifics into account.   Dart types single child vs multi-child, has const constructors and allows generics and even positioned parameters. JSX supports none of these.  A few examples that would badly convert to JSX :"
technical,"why is it an issue for you if there is the current approach **and** DSX? (that's what I derive from your downvotes) they can have their DSX.  If the Flutter team doesn't implement it, it can be an open source initiative.  That doesn't mean the pure-Dart approach like it is now won't work anymore.  Both worlds can co-exist.  I fully agree with this. Though i think it should be a pluggable rather than an out-of-the-box solution. Having one standard that works but being able to customize the experience with extra things is a great paradigm. That way the Flutter team can keep its focus on (what i believe are) more relevant issues, the community can experiment with different tools/solutions and then we can have a discussion with more data and experience with DSX or any other alternatives to the current meta.  True that. However, i think we can all agree we don't want Dart/Flutter to become another JS/Web Frontend ecosystem. Flutter isn't even out of beta yet and we're already want it to have 2 standards for something kinda subjective.  Most are community driven though React refers to them. Good midway. Now, only 2 of them are officially supported: the React.createElement and the JSX way, which is a wrapper over the other one. The value of JSX is notorious in that context but it is not that clear over here. With this in mind, can we meet midway by having only one official standard and the docs referencing DSX whenever relevant?  I believe the Flutter team should be focusing on the lacking features that truly prevent developers from building apps. I can't recommend Flutter to my managers if we can't even add Maps support and developing a Camera features takes 2 weeks.  Bear in mind, I'm not closing the door on DSX forever. Maybe it will become the solution for UI building but we need the community experimenting with it to make that decision."
technical,"Lord help us all.  Ok, this is going nowhere. It doesn't look like anyone's about to switch sides anytime soon. We definitely need to reach some sort of compromise here. Of course ""pro-DSX"" vs ""anti-DSX"" doesn't really have a remotely satisfying compromise, which is a frustrating realization. Could we possibly reframe our positions so that they could be more compatible? they can have their DSX. If the Flutter team doesn't implement it, it can be an open source initiative. That doesn't mean the pure-Dart approach like it is now won't work anymore. Both worlds can co-exist."
technical,"I'm not arguing with you just for the sake of argument or because of some deep-seated anti-JSX bias. I'm trying to get you to answer questions that need to be answered. You are developing a tool that you presumably intend for other people to use, yet you still haven't offered a compelling reason  why  they should use it beyond the vague and subjective benefits of ""familiarity"" and ""because it's better"". The former, as I said before, is not necessarily a good thing, and the latter is as of yet a claim made without any tangible support.  If you want your tool to be a success, it needs to be set in stone what you are doing and why you are doing it, and you need to do so in a way that it can be easily conveyed to others. That's not to say that you can't make a product until it is liked by  everyone , but clear and concise objectives are crucial to shaping design and implementation. Otherwise you are just going to end up with a direction-less utility that will be a niche product at best and will be extremely lucky if it ends up in production code of any scale.  I didn't even see that that link was a working example. I've never used herokuapp before and it just looked like a gist or something, so that's on me. :P  (Though I will point out that tinkering with an online sandbox is not the same as testing the transpiler in a more practical environment.)  You already had to deal with one such difference in child strong-typing. What about attribute strong-typing? What about widgets in different libraries with the same name? What happens if someone makes a widget with more than one nameless positional argument? What happens if we import two libraries that have widgets with the same name? What happens in some scenario that I haven't thought of pops up to further showcase the inherent difference between systems like Javascript and Dart? I have to say, you being so flippant on this discussion point makes me worry about DSX's longevity in a real-world setting.  Again, that's 100 people who upvoted the issue on the basis of ""Consider JSX-like syntax inside dart code"". They upvoted because they want  JSX , and as you've been so keen to point out, DSX is not JSX. So why else would they want to use DSX? Because inline XML-like UI declaration is ""the future""? Again, I just don't see it.  We've already covered JSX in Vue not getting any traction, but there's also the two React alternatives mentioned in the Web Components article you linked: Inferno and Preact. As far as I can tell, they have both failed to make any kind of waves at all in the JS-based web-app development world, despite also natively supporting JSX-like syntax. I really think that people need to have a long and hard look about  exactly why  people like JSX in React, because by all accounts it just doesn't seem to be because of JSX itself. If  that  question can be answered, then we can move forward toward ""future"" innovations rather than just frankensteining that one feature from that one library we liked into everywhere else we personally think it should be. Thinking on the amount of energy that was invested just in this discussion and what could have been done good to improve the current framework instead makes me sad."
technical,Several Dart language feature requests could make code more short/readable:  - Optional new and const for Dart 2 - Meta-issue for discussion of the proposal to allow named arguments everywhere - - this could allow widget with child argument to skip named argument. - Variable Number of Arguments) - this could allow widgets with children argument (like Row and Column) to skip this named argument and its list declaration.  With those changes code could looks like:  Moreover depending on your IDE you can optionnaly have synthetic comments at the end of parenthesis and you could see something like this in your IDE: This conversation is getting heated. I would like to encourage everyone to read our code of conduct: I'm going to close this conversation for a few days while everyone considers how they can respectfully and productively contribute.
technical,"E4X was only implemented in ActionScript  E4X was an ECMA standard. Mozilla shipped it for a while, but then removed it (a very unusual move for a browser vendor). Other browser vendors never wanted to implement it. (They've implemented other new ECMA features, though.) With E4H, the browser vendors were never interested in implementing it (though again, they've implemented plenty of other things I've helped invent).   Well, if I asked you for 2 things and you didn't do either in 3 months and there is an alternative to the first thing, I would also only ask you for what is totally impossible to do given your responsiveness and previous delivery performance.  That's one possible theory. People tend to ask for many other things besides, though, and many of the things they ask for get implemented, and there are workarounds for animated GIFs too, so I'm not sure this fully explains the situation.   Kind of funny but it's like putting the XML closing tag that you mentioned before was too verbose.  Indeed. This is an optional IDE feature, and one that you don't have to write into the code, which makes a big difference to whether the verbosity is an issue, though.  How would a GUI editor handle this markup? I don't really understand how to visualise the UI for this. This may be a counter argument to this request, and/or maybe some insight's to keep in mind if you want markup. I have strong feelings that adding markup creating some challenges with GWT I'd hate to see another API go through.  I've seen a couple other frameworks go through this transition regarding with UI building. Markup like this works great for tooling, in so far it's heavenly for the IDE developers. It's easier to separate the responsibilities of who does what. Although I think it can be done better.  GWT started out this way, building UI's with Java. Then came along UiBinder, where you could build the UI in xml markup, with a specification. Then the tooling, Eclipse Plugin, was able to generate UI's in xml markup. Android is doing it too, no need to belabor the point. So what I saw happen, markup works great for UI IDE developers. But really, markup is a huge huge investment in time and added complexity tooling to transition it to real program. And the tooling is always last to come. So in the mean time, while all that manifests into reality, there are two worlds. Two interesting ways of doing everything. One in the default language and one in markup. So I support GWT today. When I write documentation, I have to write it twice, once for Java and once for UiBinder XML Markup. So the real question, if you want to go down the markup road, I think the question should be asked, is the added complexity worth the journey!  JSX I think aims to solve other issues where you want to blend together what your doing with HTML and javascript. It really doesn't feel like the added complexity of markup specification suits the needs of writing UI with markup. Especially when you don't really have a document markup language as the target. At least not for the everyday user.  On a positive note. I like to work on tooling. So I can see a markup language being quite useful. It's much easier to write and modify AST when your using markup.  But then again, if you have enough minds on the job, it doesn't really matter what you do. At the end of the day, if the developer can write his application faster with your api, your going to get traction. But at what cost to the engineering team.  I think the real question is, how can the UI be built faster. I think tooling could write the dart, skip any middle man markup. And my aim isn't really to say it's not worth it, but really count the cost's on all the fronts if the road is taken!"
technical,"Let's take the concrete example:  For now the syntax is a little verbose, I have to new something just for the purpose of adding a margin: margin: new EdgeInsets.symmetric(horizontal: 4.0), I think there may be an easier way.  If you want to add a margin on the left and right, how would you like to express that? Specifically, let us take a simple example, and see what it would look like in various syntaxes.  Flutter as written today Flutter as written later this year once we remove ""new"" and ""const"" keywords How would you recommend we express these  exact  semantics? this PR contains the following merge commits. Please rebase your branch to remove these commits. click here for bot help"
technical,"birkir and all of them bring a host of troubles Flutter does not have \o/ There is no need for another language. You can separate the view in Flutter as well, even with the same language. this thread is 100% about Dart and syntax.   The discussion is not about Dart, but a synthetic layer on top of it that most UI developers already use today, and we are proposing that Flutter incorporates something like that.  So it's not about Dart... but Flutter needs to use something besides Dart for layouts? Seems like your saying Dart isn't good enough while also claiming this has nothing to do with Dart?  I don't think it matters at this point, the Flutter team has seen this feedback (on requesting a JSX/DSX approach) and they want to continue down their original path. I do think it could be handled better but it doesn't seem like they're opposed to the community creating solutions.  I'm happy there is another cross platform option... will Apple be the next to offer something up? And maybe they will see what so many of us like about react/react-native? IDK if they have anything cooking."
technical,"Yes, Angular people will be very comfortable with JSX but so will React Native devs and this is for mobile development. JSX will certainly not be taken by current Flutter devs but this second alternative will attract new devs to Flutter and that's for sure.  The article above got it so wrong, React without JSX is basically non-existent and all reactive web frameworks do allow mixing of markup and programming on their DSL. Time has come... It's with great pleasure that I announce the online DSX transpiler"
technical,"I get that... It wouldn't affect me in that way, but as I said before, there are missing features right now that I need to create a flutter app.   Once React Native fans move to Flutter there will be tonnes of people to help out on these other needed things. Priority 1 is to steal mind share from React Native and make it as simple as possible for React Native people to move over.  So you propose the flutter team should put features on hold for something that is questionable and may or may not attract a fraction of a specific subset of web developers? Every DSL is by definition less expressive than a GPL. With DSX how do I conditionally hide a widget? How do I iterate over a collection and map every element to a widget? How do I use a switch? You now have to make up syntax and semantics for constructs that you already have in your GPL. So why invent the DSL in the first place?  That wasn't the question... What problems would a DSL solve that you can't solve right now? You keep saying it's better, why is it better? I have no doubt DSX would attract some JSX people. I know people don't like things that are different. And familiarity seems to be the only argument. So why not use CSS? Why not use JavaScript? More people know how to use these than Dart.  If you design your system based on some other thing for the sole purpose of being familiar, you're not really innovating. to be clear, JSX compiles to function calls (in React calls to createElement(ComponentClass), in Dart it should be constructors).  - How do I use a switch? As far as I know in Dart as in JS, you can't use a switch inline as it is not an expression. If you are outside of the widget tree you can just use a switch normally."
technical,"I have a question:  When using DSX syntax, we implicitly assume that the nested child/children is of type Widget. What if we want to explicitly state that we want nested child/children be a specific sub-type of Widget?  for example, when I want children of my custom widget only accept a list of Container, I can annotate the children with List<Container and IDE will give an error as soon as I put anything different from Container. As I'm aware of there is no way to enforce type safe like this when using dsx. Maybe we can have some error when the app compiles though but I think the error raise when I'm typing is still a better experience.  I think we should give everyone some time to try out and get familiar with flutter way to declare UI, at least after v1 release. Then we could have a better look at this feature. Very good catch !!! I bowl my virtual hat to you. My initial prototype has had some holes in it from the beginning that I was aware of and just waiting to people to find them and come forward. I was just hoping people were interested in discussing the technology instead of fighting over it.  The solution I have to this is to provide the array type as another parameter on the namespace. As the namespace is getting large, we can set the short form for 'children' be '*'.  In Example 2, if actions were an array of 'Container' instead of default 'Widget', it would look like the following alternatives:"
technical,"As both a React/RN and a Flutter user, I heavily disagree with the idea of  ""DSX"".   DSX would bring **nothing**. JSX is used in react because the JS syntax is horrible. But in the case of Flutter, widgets creations is super easy.  it is already out of the box readable, easy to write, type/generic compatible, and without any unneeded duplication.  The only complain you could possibly have with the current syntax is ""It is hard to know where is the closing parenthesis of a widget""  But then again, dart plugin of the officially supported IDEs solves this problem. So that when we open the code from before in say vscode, we'll see As for the ""It's hard to differenciate casual code from UI code"", react rules apply to flutter too :  Widgets should be either dumb or smart. Smart widget don't have UI logic. Dumb widgets have nothing but UI logic.  If you follow this pattern, you should never fall into a situation where you fail to differentiate UI from the rest. This is even more true when following something like BLoC pattern, which heavily enforce separation of business and UI. Very opinionated statement and simply not true"
technical,"Yes, the tagging syntax is more verbose for this case and that's why I mentioned the short form for children being '*'. Anyways, this case is the exception and not the rule. Most of the time you don't need to even specify 'children', let alone 'Container', but the functionality needs to be there to cover all possible use cases. vote is vote, it sure means.  I respect your feeling, maybe it's true. But even with a reverse ratio (1 to 2), that still means 33% user base. Can you say 33% is a small share? Yeah, some people are watching. This also means  lack of JSX-like is one of the reasons prevent people from choosing flutter .  Flutter aims at more developers, not only ones read all issues."
technical,"I'm not saying you  do  need to justify anything. Back when you were insisting that the Flutter team pick up this proposal and implement it themselves, yeah, I would've said you had a fair amount of justifying to do. Now that you are trying to do it yourself, you can do whatever you want for what ever justification you think is sufficient, and more power to you. I'm merely stating the reasons I see that it might not be as easy or as popular as you seem to think it will be, and I am putting the ball in your court to defy them.  I assume that, at this point, you've tested it so far on UIs and Apps that are trivial in size. What about non-trivial ones? What about ones that fall within edge cases? Also, the actual time that the process takes is not the only relevant part - just the fact that the developer has to go through another checklist of manual actions before they can build is enough of a turn off for many people.  You also have yet to actually release the source code of the project, so no one's been able to go through your process, double-check your findings, and suggest improvements. At this point, all anyone can really do is take you at your word that it is both convenient and performant. I've been using Vue for coming close to a year now, and in that time I have gone through a good number of open source project repos to see how different things are done. While I don't consider myself a Vue master by any means, what I will say is that in not a single one of them have I ever seen JSX actually utilized - people seem to massively prefer the .vue approach (template-script-styling) over the render+JSX approach. I didn't ven know myself that Vue even supported JSX (via a babel plugin at least) until after your reply I did some digging through the Vue documentation and discovered a tiny snippet of information on it in the render function section.  But this is irrelevant to my overall point. Vue is still a Javascript framework. Flutter is most assuredly not. As such, there are a lot of reasons that make JSX the newest greatest thing in a Javascript-centric environment that won't translate to Dart+Flutter, many of which have already been covered in this thread. Until I see it catch on in a non-Javascript development environment, I shall respectfully disagree. Vue specify has a broad variety of usages. JSX is just ""there"". But it's not the dominant syntax You could plug JSX to Angular if you wanted to.  Although nobody does.  A big candidate for the future is web-components. And they are used directly in html similar to what you'd find in Angular or the most common form of Vue"
technical,"This conversation is getting heated. I would like to encourage everyone to read our code of conduct: I'm going to close this conversation for a few days while everyone considers how they can respectfully and productively contribute. We all know that the syntax for building UI is a very important part of mobile development experience. For now the syntax is a little verbose, I have to new something just for the purpose of adding a margin: margin: new EdgeInsets.symmetric(horizontal: 4.0), I think there may be an easier way.  Would it be possible to build a DSL like what the Kotlin team did for Android developers? It's called Anko, a DSL for building Android UI. A concise syntax helps to keep the code readable & maintainable, can also make the building work enjoyable, it does matter. Please Flutter team estimate it seriously before you make the decision. We all love to see Flutter make a bigger success in the coming years."
technical,"One thing we've found in Flutter is that big build methods are not great for performance, and we try to encourage people to break down their build methods into smaller reusable widgets. In particular, in Flutter having a build method that's built out of the results of other methods is somewhat of an antipattern that we'd rather discourage rather than make easier. (This is somewhat of a recent realisation so a lot of our examples and widgets don't do this well yet.) we try to encourage people to break down their build methods into smaller reusable widgets.  Does it really become a reusable widget or simply a wrapper/composite widget? I mean to be reusable you should have at least 2 usage instances.  The AppBar is so unique that you can't really call it an reusable component, it's a wrapper/composite component and in these cases why not just use a local method to build that part of the UI? I guess if it did more things it would make sense to place it in a wrapper/composite component.   One thing we've found in Flutter is that big build methods are not great for performance  Since you mentioned performance, having the animation loop drive the build method will cause performance problems for smooth animation. You don't want your build method called 60 times a second or more, specially considering that the build method is user code (for example, it could have a super long loop that takes forever and it would cause animations to skip). Being a Flutter beginner perhaps I got that wrong."
technical,"DSX +1  Would've loved to write a bunch of pro/cons, but by having read all these comments, I feel like I'm just gonna repeat everything over and over. Stop being so naive and ignorant, nobody says you'll be FORCED to write UIs using DSX, it's simply an option (better alternative). There's a reason you can write JS in 101203103 different ways. Well, there's always the option writing an analyzer plugin that parses DSX and converts them into regular Dart function calls, so that the compiler doesn't have to do any extra work.  The real question is whether analyzer plugins apply within the context of the compiler.  If you ask me, DSX should only be opt-in, ideally through some sort of plug-in. I think it's extremely unnecessary to add it to the language itself, as then server-side and Web users of Dart have to adapt to the changes, not just Flutter users. The vast majority of code written in Dart doesn't even remotely need any XML syntax, so enforcing it on everybody is a bad decision.  TLDR, if you want DSX that bad, write an analyzer plugin and bring it to Dart yourself. The Internet will love you, you'll get thousands of Github stars, and you'll feel just like it's React. Win-win.  P.S. I'll even race you to do it"
technical,"Well, there's always the option writing an analyzer plugin that parses DSX and converts them into regular Dart function calls, so that the compiler doesn't have to do any extra work.  The real question is whether analyzer plugins apply within the context of the compiler.  If you ask me, DSX should only be opt-in, ideally through some sort of plug-in. I think it's extremely unnecessary to add it to the language itself, as then server-side and Web users of Dart have to adapt to the changes, not just Flutter users. The vast majority of code written in Dart doesn't even remotely need any XML syntax, so enforcing it on everybody is a bad decision.  TLDR, if you want DSX that bad, write an analyzer plugin and bring it to Dart yourself. The Internet will love you, you'll get thousands of Github stars, and you'll feel just like it's React. Win-win.  P.S. I'll even race you to do it Well, there's always the option writing an analyzer plugin that parses DSX and converts them into regular Dart function calls, so that the compiler doesn't have to do any extra work.  There is currently no way in the dart language to implement this without any hacks (think race conditions, recursive imports and stuff). This needs to be integrated at level where everything will work as expected, hot reloading, static analysis, etc.   If you ask me, DSX should only be opt-in, ideally through some sort of plug-in. I think it's extremely unnecessary to add it to the language itself, as then server-side and Web users of Dart have to adapt to the changes, not just Flutter users. The vast majority of code written in Dart doesn't even remotely need any XML syntax, so enforcing it on everybody is a bad decision.  If you read the thread, that has been the idea from day one. We just want the support from flutter/dart to make an transpiler.   TLDR, if you want DSX that bad, write an analyzer plugin and bring it to Dart yourself. The Internet will love you, you'll get thousands of Github stars, and you'll feel just like it's React. Win-win.  Read the thread, this has already been done by   (analyzer plugin isn't gonna cut it)  P.S. I'll even race you to do it  Great! Just even a theory of how this would work would be interesting to see."
technical,"Well, there's always the option writing an analyzer plugin that parses DSX and converts them into regular Dart function calls, so that the compiler doesn't have to do any extra work.  The real question is whether analyzer plugins apply within the context of the compiler.  If you ask me, DSX should only be opt-in, ideally through some sort of plug-in. I think it's extremely unnecessary to add it to the language itself, as then server-side and Web users of Dart have to adapt to the changes, not just Flutter users. The vast majority of code written in Dart doesn't even remotely need any XML syntax, so enforcing it on everybody is a bad decision.  TLDR, if you want DSX that bad, write an analyzer plugin and bring it to Dart yourself. The Internet will love you, you'll get thousands of Github stars, and you'll feel just like it's React. Win-win.  P.S. I'll even race you to do it What is your previous experience with JSX, have you really used it or just looked at it? I think you guys are underestimating JSX by saying it will give small gains here and there. If you don't have any users, you don't have a product."
technical,"By the way, try the following on my online transpiler at: and you get: DSX is similar to JSX but for Dart & Flutter so it has features of its own which are described on the link above. when I see this, I get flashbacks from xml layouts from Android.. I don't think it's a good idea to implement this. Now that you dont even have to write new and const it even looks better."
technical,"Yeah, found it even important enough to upvote :D When sorted by 'upvote', this feature request is listed 7th on the list of 3131 opened tickets though."
technical,"What I'm trying to understand is whether the same reasons that JSX is ""hot"" in React apply to Flutter.  Yes, the exact same thing applies here. The current way looks good to you because that's the only option you have. Give people a second option and the same will happen.  Whether E4X died or not is irrelevant because nothing lives forever. I have used ActionScript with E4X a lot and thought that Adobe did an excellent job there. In a way Flutter is just a newer version of Adobe Flash with Flex apps.   (I've been heavily involved in writing the specs for parsing HTML and been involved in similar work for XML, and I've implemented a parser for Dart, so I have a pretty good idea of how difficult parsing markup languages vs programming languages actually is.)  Great so you know that parsing a markup language is trivial compared to parsing an imperative programming language.   But why is it beneficial to be able to do that?  Code readability and simplicity which in turn drives a whole bunch of other benefits.   It's pretty obvious when scrolling through Flutter apps where the build functions are (they're the giant nested expressions). What is it about ""declarative markup"" that is important to separate from ""code""?  On your giant nested expressions can you easily see structure? can this structure be easily manipulated by other tools like GUI builders interchangebly ? I mean like Adobe Flex Builder use to do, drag and drop widgets around, wire them on UI and then switch to code view and just edit anything you want and then switch back to gui mode and continue to manipulate the widgets. You can't do that easily when the programmer goes inside your ""giant nested expressions"" and writes any valid dart code that doesn't follow the structure that the GUI editor is expecting. With a fixed XML structure that is not a problem.   As far as I can tell, JSX doesn't handle the things I was asking about. For example, React doesn't have the concept of child slots  It handles it just fine, you just don't know how to do it. So going forward just put the example in question here and I will provide you with what I think the JSX version should be.  It looks longer than the dart version but I could had placed everything in the same number of lines. Thing is an IDE/Editor can provide '+' & '-' to expand and collapse these XML nodes anyways.  Make Flutter look familiar to React developers and you have a chance of attracting a bunch of new users to Flutter. Whether E4X died or not is irrelevant because nothing lives forever.  Whether it died isn't the issue, it's why it died. Did it die because it didn't provide a solution that people wanted? Did it die because of implementation issues? Did it die because of patent issues? Was it too early? Too late? I think it's important to learn lessons from past experiences. Why did E4X and E4H fail where JSX has succeeded?  What I find interesting is that people who are new to Flutter often ask for two things: a markup language, and animated GIFs. Then three months in, they are still asking for animated GIFs, but not for a markup language. It could be that this is because the markup language isn't actually needed once you're used to writing build methods in Dart. It could be that they still want a markup language but have worked around the omission and so don't think to ask anymore. It's worth figuring out which because otherwise we risk spending effort on something that is the wrong choice (in either direction).   On your giant nested expressions can you easily see structure?  Yes, at least as easily as with XML. Personally, I find XML to be very verbose and it obfuscates the structure. But I think this is more about what you're used to.  (We're also experimenting with IDEs that put in virtual ""closing tag"" comments for you so you can see the structure without having to actually write it.)   Great so you know that parsing a markup language is trivial compared to parsing an imperative programming language.  My experience is that declarative vs imperative is not the distinction that matters when it comes to determining how easy a language is to parse. (For example, HTML is ""declarative"" but it may be among the most complicated languages to parse that I've ever dealt with.)   Code readability and simplicity which in turn drives a whole bunch of other benefits.  If this is the main benefit then this is something we can test. We could take a mixture of engineers who are used to writing HTML, React, iOS code, Android code, Flutter, command-line apps, and so on, and present them each with various syntaxes, and ask them to describe what they think the resulting UI would look like. We can then measure which syntax gets the best results. InMatrix is this something we could look at after the animation work wraps up, maybe?   can this structure be easily manipulated by other tools like GUI builders interchangebly ?  Yes, in principle at least. It should be relatively straight-forward to mechanically convert Dart expressions to XML or JSON or whatever other structured language one might use. It's just a matter of converting the syntax, the actual information is the same. Personally I wouldn't convert it to a different syntax if I was making a GUI editor, I would just parse it into a data structure in memory straight from Dart.   You can't do that easily when the programmer goes inside your ""giant nested expressions"" and writes any valid dart code that doesn't follow the structure that the GUI editor is expecting. With a fixed XML structure that is not a problem.  The thing is, you can put exactly the same ""any valid dart code"" in the XML structure as in the Dart expression. They are literally mechanically interchangeable. So I don't really see how using XML helps with this particularly. For example how would you turn this into XML?:  It handles it just fine, you just don't know how to do it.  I meant specifically JSX. I agree that your proposed syntax would be a perfectly valid way to approach the problem."
technical,"I worked on Adobe's Flex SDK, which combined XML markup and ActionScript, for the last couple years that the product existed at Adobe.  I understand the appeal of markup for defining UIs however I can also remember some drawbacks:  - Flex application visuals could be defined in terms of markup and code. As I remember it, code tended to dominate in large real-world apps.  Readability isn't necessarily a benefit for apps defined as a complex hybrids of markup and code. - The ""Flex Builder"" IDE had to support apps that were defined by  markup and code. This made the IDE difficult to build and maintain. Developers tended to view it as an ActionScript tool. - Evolving and maintaining the XML ""compiler"" was a significant burden that kept a team of engineers busy full-time.  Keeping the compiler and toolkit in lockstep slowed down the evolution of the overall sdk.  It's been years and I can no longer recall all the details. However my overall impression is that defining UIs with a combination of markup and code is a mixed bag at best. Whether it died isn't the issue, it's why it died. Did it die because it didn't provide a solution that people wanted? Did it die because of implementation issues? Did it die because of patent issues? Was it too early? Too late? I think it's important to learn lessons from past experiences. Why did E4X and E4H fail where JSX has succeeded?  It died because E4X was only implemented in ActionScript which was only used inside Adobe Flash/Flex and Adobe killed the project. Instead Adobe changed direction towards open standards where there is no single source provider with possibility of lock-in and ecosystem implosion.   What I find interesting is that people who are new to Flutter often ask for two things: a markup language, and animated GIFs. Then three months in, they are still asking for animated GIFs, but not for a markup language. It could be that this is because the markup language isn't actually needed once you're used to writing build methods in Dart. It could be that they still want a markup language but have worked around the omission and so don't think to ask anymore. It's worth figuring out which because otherwise we risk spending effort on something that is the wrong choice (in either direction).  Well, if I asked you for 2 things and you didn't do either in 3 months and there is an alternative to the first thing, I would also only ask you for what is totally impossible to do given your responsiveness and previous delivery performance.   (We're also experimenting with IDEs that put in virtual ""closing tag"" comments for you so you can see the structure without having to actually write it.)  Kind of funny but it's like putting the XML closing tag that you mentioned before was too verbose.   If this is the main benefit then this is something we can test. We could take a mixture of engineers who are used to writing HTML, React, iOS code, Android code, Flutter, command-line apps, and so on, and present them each with various syntaxes, and ask them to describe what they think the resulting UI would look like. We can then measure which syntax gets the best results. InMatrix is this something we could look at after the animation work wraps up, maybe?  Sure you can do that go ahead, I am sure you will find out that ""Once you do React(with JSX) you simply don't go back"". Survey experienced React developers and ask them if they think JSX is bad and it should never had been done. Show your way and ask them if they want to replace JSX with what you have. Before you do that, close the doors and lock up the place because these developers are just going to grab their stuff and sprint for the closest exit.   The thing is, you can put exactly the same ""any valid dart code"" in the XML structure as in the Dart expression.  Sure, but for the GUI builders that's just a block of bytes that doesn't need to be touched and can be easily skipped. Making it design/code interchangeability practically possible instead of in principle."
technical,"Vue specify has a broad variety of usages. JSX is just ""there"". But it's not the dominant syntax You could plug JSX to Angular if you wanted to.  Although nobody does.  A big candidate for the future is web-components. And they are used directly in html similar to what you'd find in Angular or the most common form of Vue Who said anything about manual actions? Didn't I make myself clear that I am trying to get complete seamless IDE integration (best possible user experience for developers). How does that have anything to do with people using DSX? I've used JSX for over 2 years and couldn't care less for its source code. Do you need to look at the source code of the Dart compiler to be able to program in Dart?   what I will say is that in not a single one of them have I ever seen JSX actually utilized - people seem to massively prefer the .vue approach (template-script-styling) over the render+JSX approach.  JSX is a new addition so it will take time to spread but the important point to get is that Vue accepts other approaches without forcing developers to use 'the correct way and only way' that things should be done in Vue. Riiiiight, so instead of JSX you use DSX with Flutter.  Web components are zoobies, dead but still walking, they are as widespread as kangaroos in Canada. I could go on for days but to avoid digressing..."
technical,"React wasn't designed for TypeScript. It was designed for Javascript. All the widget definitions, attributes, properties, and everything else was designed to be used in the dynamic environment of JavaScript, so the type safety of TypeScript doesn't introduce any new factors with how JSX interacts with React. This is yet another example of how JSX was designed for a completely different setting than what Flutter is. Why do you think it does matter? JSX is a way to describe interface. It's language agnostic on it's own. Look here. It's not JavaScript. But well, why it can't be done with JSX? (Beside there is no implementation of this (yet))  And.. you know... flow come from fb too: Please, stop selling arguments for and against extensions you've never use. I use JSX every day and I'm happy with it ATM, although I was very skeptical about it. I can imagine, that JSX evolves in other patterns, like it was with AngularJS.  And maybe this topic helps to find better pattern for Dart? DSX is just one proposal. Look at builder pattern example above or other language tweaks presented here. And, well, maybe your flui is a better way? I don't know. **But let's find it out and help to each other to improve their suggestions instead of discussing about bad things in someones else proposal.**"
technical,"Bravo, that's absolutely it.  In React you can do it 4 ways (and I just found out about the other 2 ways today !!!)  (1) You can use JSX (which is what I like) (2) You can use the original way (which is similar to Flutter)  At the end of the link above they even mention 2 promising community projects  (3) Hyperscript syntax for React.js markup (4) Terse syntax for hyperscript.  Having alternatives is a good thing, gone are the days where you could get a Ford in any color you liked as long as it's black :) why is it an issue for you if there is the current approach **and** DSX? (that's what I derive from your downvotes)"
technical,"Yes, that's not a problem. The transpiler is a simple XML processor and when it comes to parsing expressions (everything inside {}), it just becomes a text blob so it writes it out verbatim. Would it be possible to build a DSL like what the Kotlin team did for Android developers?  Seems like a lot of people would like to use Kotlin with Flutter. Honestly, I don't understand, why did the developers decide to reinvent the wheels in Dart?  In my humble opinion, Flutter doesn't need JSX. Flutter should have chosen Kotlin instead of Dart. Kotlin allows us to write complex ui logic with beautiful syntax out of the box, has huge community, production-ready tools, battle-tested in Android development...  Just saying."
technical,"Don't worry pretty soon you will be able to try my version and it is even better than jayjun.  Quick update: 2 weekends ago I got the UI working great, Last weekend I got the parser fully working and half of the transpiler working. This coming weekend I hope to finish it off if I avoid the Superbowl ,)  I have thick skin and mule like stubbornness so I don't even notice these down-votes, thanks for pointing them out though. Would this help with the issue of losing track of closing tags ? Idea: Auto generated closing tags"
technical,"Heard about Flutter, wanted to give it a try, immediately stopped even considering that the minute I saw the syntax, which is a shame because chances are that this a great piece of tech. to build products.  I've been doing React/React Native development for the past 2.5 years, sacrificing the productivity leverage the JSX syntax offers when it comes to describing UI is a lot to ask. I would seriously consider spending the time to learn all about Flutter and study whether it's worth switching to when such a feature is supported out of the box.  I can't even imagine the numbers of potential adopters/users/devs Flutter is losing because of this, will revisit in the next months. Yeah, found it even important enough to upvote :D"
technical,"You missed my ""But then the argument of having an easier conversion from react to flutter is invalid.""  Half of the arguments for DSX is ""JSX is popular in react, we need this here too"". But you're proposing something different (and more complex) from JSX. The other half is about separating UI from code, which a template file can do too. Not true. You could do if and stuff inside your template file. Look at cshtml or angular templates.   The thing is a template file, as long as you already have a parser for it, could be implemented for flutter in less than a week fully working. Be it yaml, xml, cshtml, or html with directives.   While a DSX would require a lot of work.  Bessonov They recently added virtual comments on supported IDEs to mock closing tag.  So in your vscode you'll see the following : The benefits of closing tags. Without having to type them Yeah, I've seen that in cited comment. But it's not the same. This introduce an align shift and disturb reading flow. And this doesn't help me in other IDEs and text processors. IMHO templates suffers from NIH syndrom. I don't say that approach to mix PHP and HTML is the right way to do that. But react shows with JSX how it can be done better."
technical,"the interesting thing you did was add a 3rd attrib possibility that simplifies the look of expressions when they are another tag. JSX has. Yeah, the third case is quite common in Flutter, so it makes sense to skip the extra { nesting."
technical,"I would like to propose close this topic and open new umbrella topic with limited conversation. All proposals to improve the way, how flutter may be used, discuss in own topics objectively with love and without hate. Yeap, the amount of hate here is epic, just consider this: There are 3587 open tickets, if you sort them by ""thumbs down"" you get 1 (this one) with 57 ""thumbs down"" 3586 (other tickets) with 1 or less ""thumbs down"""
technical,"I wonder if the JSX syntax might be more appreciated in angular Dart ? It seems like its more of a what-you-are-used-to situation rather than ""better"". The JSX syntax probably feels more natural for web developers.  I know that programming feels awkward for me even just using different code highlighting colors for a day. Yes, Angular people will be very comfortable with JSX but so will React Native devs and this is for mobile development. JSX will certainly not be taken by current Flutter devs but this second alternative will attract new devs to Flutter and that's for sure.  The article above got it so wrong, React without JSX is basically non-existent and all reactive web frameworks do allow mixing of markup and programming on their DSL."
technical,"Cool, I use many widgets with closures like FutureBuilder. I hope your transpiler can generate something like Yes, that's not a problem. The transpiler is a simple XML processor and when it comes to parsing expressions (everything inside {}), it just becomes a text blob so it writes it out verbatim."
technical,"I'm not sure but I have read some of your comments above and I think you have mentioned JSX/XML node interchangeably. Anyway, personally I think using namespace as a solution is not ideal.  Just compare Yes, the tagging syntax is more verbose for this case and that's why I mentioned the short form for children being '*'. Anyways, this case is the exception and not the rule. Most of the time you don't need to even specify 'children', let alone 'Container', but the functionality needs to be there to cover all possible use cases."
technical,"The reaction tells us everything.  As now, +1 nearly double times  -1 Yes, very fair statement to say but I have spent a lot of time with Flutter and I can certainly see the value DSX would add to it. As leedstyh has noticed, DSX fans lead the race by almost 2-to-1 which is pretty amazing considering people in this forum are Flutter people."
technical,"Who said anything about manual actions? Didn't I make myself clear that I am trying to get complete seamless IDE integration (best possible user experience for developers). How does that have anything to do with people using DSX? I've used JSX for over 2 years and couldn't care less for its source code. Do you need to look at the source code of the Dart compiler to be able to program in Dart?   what I will say is that in not a single one of them have I ever seen JSX actually utilized - people seem to massively prefer the .vue approach (template-script-styling) over the render+JSX approach.  JSX is a new addition so it will take time to spread but the important point to get is that Vue accepts other approaches without forcing developers to use 'the correct way and only way' that things should be done in Vue. Riiiiight, so instead of JSX you use DSX with Flutter.  Web components are zoobies, dead but still walking, they are as widespread as kangaroos in Canada. I could go on for days but to avoid digressing... You also said that you needed preprocessing support from the Flutter/Dart team in order to do so. Am I incorrect in that?  JSX was developed by Facebook for React, put through a rigorous proposal/design/implementation/iteration process, and then released into the world years before you got your hands on it. It's been rigorously tested and proven time and time again in real world environments. It's a mature technology. There's no reason to demand to see a spec sheet for something like that.  DSX, on the other hand, is being developed today by you and a handful of people. You've waxed eloquently about what it can and will be able to do, but all we've actually  seen  is a small handful of purpose-built code snippets and your word that they were generated by the transpiler. People who have even want to try it out and suggest possible changes or improvements aren't able to do so, so they have no reason to support your efforts beyond ""Yay JSX!"".  I'm not accusing you of lying or anything, I'm just saying that JSX has earned a level of faith that DSX has not, so how are you going to turn some heads if you don't let people tinker with it? JSX has been in Vue for almost 2 years now. And unlike Vue itself, JSX is a pre-existing technology that needs no introduction, especially for people familiar to React. If JSX was going to take the Vue.js world by storm, I can't help but feel it would've done so by now. (Particularly if it's any indication that as many people are clamoring for JSX in Flutter as you claim.)\  JSX and DSX are the same syntactic concept. The problem is that, where JSX was built on a weakly-typed dynamic language like JavaScript, DSX is being built on a strongly-typed static language like Dart. That means there are lots of problems DSX will have to account for that JSX didn't have to if it's going to be anything other than a niche ""JSX for Flutter"" implementation, and it's going to take some  ingenious  modifications to make DSX really work without making it too bloated to justify claiming that it's more visually concise.  And to address the ""DSX is just Dart, if DSX can't do something, just use Dart"" rebuttal, then my counter-rebuttal would be if I have to keep falling back to Dart whenever I run into a scenario that DSX doesn't handle, then why shouldn't I just use Dart all the time?  And to address the rebuttal for *that* reading ""you can if you want to, DSX is just an option"", then you are really selling yourself short. Even if it really is just going to be ""an option"", it still needs to bring something to the table that is going to convince people to use it. You yourself said that DSX is not JSX, so people who want just JSX aren't going to get what they want. **That** means there needs to be some tangible reasons beyond the ""JSX-like appeal"" for people to want to use it.  If you're just building a tool that you yourself want to use, then all this is moot and you can go nuts. But if you are actually building something you intend for other people to use, then you need to put it in a solid form why you think they should.  Somewhat off-topic, but I would like to point out that web components really are a promising look at the future, even if support for them is getting added slower than tar. Think about it this way: React does what it does because it essentially implements the idea of web components in Javascript-only. Imagine how much better it would be if those features were supported by the browser and benefitted from non-sandboxed performance and not having to operate through DOM manipulation? (Granted it might be another 20 years before we find out, but still...)"
technical,"thank you for the details. You are so full of wrong biases that are most likely preventing you from reaching your full potential. Open your mind, try different things. I will unsubscribe from this issue now. Please never mention me here or anywhere again, thx."
technical,"I'm coming from React, and first time trying out Flutter. It's really odd that flutter team doesn't have JSX syntax, I mean seriously! In JS world, JSX is extremely prevalent in every React-alternatives now a days! JSX is super friendly for developers. Please implement this ASAP, so you can help grow the community. You can tell JSX (or DSX in this case) is a big deal just by looking for the amount of comments in this issue.  JSX is something that you don't know you love until you try it. It's weird at first to have markup in your code but then it becomes awesome.  It seems unnecessary for guys that already know flutter/dart, I get it. But for someone that never touched dart (like myself), JSX is way easier to start with and that's the point. If you guys want more people to come by and start building a lot more awesome stuff, the tooling needs to be easier.  And, as eseidel Google told the other guy ""Flutter is a way to write to iOS and Android in a single codebase. We try to make that **fast and easy** Flutter talks all the way down to the hardware. We have a very layered approach, so you can take as little or as much as you want. We have a bunch of investment in **tooling**.""  Implement JSX, pretty please."
technical,"(1) as mentioned such things can also be changed/improved in Dart and there were already discussions. This just won't happen before Dart 2 release. Just assuming that DSX allows all kind of new features and Dart doesn't isn't really a fair argument in my opinion. (2) I'm pretty sure this can as well done with Dart and there is of course already a parser for Dart.   Others have provided lots of good points but I am not digging it up for you ,)  There is no need to dig them up for **me**, but this comes up frequently and you might be able to convince others that you actually have valid arguments. I followed the discussion and can't remember good factual arguments and that might be the same for others. If you summarize them you can just post a link to the next that questions that.  As discussed before I can accept personal preference as a valid argument, but if you claim to have lots of factual arguments as well, then I think it's valid to ask to get them pointed out. You keep asking for 'valid arguments' and when they are given you dismiss them as 'future Dart will have this' or 'this is not a valid argument'.  The fact is that right now Dart/Flutter has noisy child/children everywhere when building a widget and XML/DSX doesn't. Right now it is a very valid argument for using DSX to remove this child/children noise. Can you just accept that as a valid argument? (just because you say Dart will have that in the future, it doesn't make the argument invalid).  It is also a fact that parsing XML is much simpler than parsing the full Dart language *and* every language out there has an XML parser, whereas only Dart has a full and complete Dart language parser. Can you see that this is also a valid argument?  There are lots of valid arguments, they are just not valid for you and that's why I stopped arguing about it. If people are interested in what was said, just read the 2 threads on JSX fully. I have no interest in convincing you to use DSX, you are happy with plain Dart so be it, I am not."
technical,"After much of code and examples posted here (especially), I come to conclusion, that JSX adds much more value to JS in contrast to DSX and Dart. But one feature is very important from my point of view: Closing tags. Like: reduces a LOT of cognitive complexy in deep structures like here in contrast to:  But well, if you use it like this: there is a small profit. You missed my ""But then the argument of having an easier conversion from react to flutter is invalid.""  Half of the arguments for DSX is ""JSX is popular in react, we need this here too"". But you're proposing something different (and more complex) from JSX. The other half is about separating UI from code, which a template file can do too. Not true. You could do if and stuff inside your template file. Look at cshtml or angular templates.   The thing is a template file, as long as you already have a parser for it, could be implemented for flutter in less than a week fully working. Be it yaml, xml, cshtml, or html with directives.   While a DSX would require a lot of work.  Bessonov They recently added virtual comments on supported IDEs to mock closing tag.  So in your vscode you'll see the following : The benefits of closing tags. Without having to type them"
technical,"Project traction and attracting developers  The relation is still unclear.   Reason number two, readability:  Making Dart code more readable seems a better goal to me   Reason number three, GUI Builders. I'll quote the first line in the README.  As far as I remember it was mentioned above already that there is no reason why using Dart code would prevent that. Your counter arguments can't really dismiss the idea, can they?  1. Relation is pretty clear. Unless it's not a goal for the project to be popular? 2. Great! Will it be close to JSX readability? What is the current proposal for such a thing? 3. It was stated that it **could be done**. Adapting JSX support for currently available GUI builders will be far much simpler."
technical,"I still think, as a newcomer to Flutter with some experience with Angular, React, etc., that the regular Dart way, and more in the Dart 2.0, is better than this DSX. It just makes more sense than some XML and CSS-ish parameters. '‚ your second demo example has 466 chars compared to dart's 391 (both without spaces). To me it looks visually bloated. I would have to learn it's semantics, which I don't have to with normal Dart code. And I have no idea how to use general purpose programming paradigms with it (if, forEach, etc.).  If I had the choice, I would stick with the present model."
technical,"I am working on it this weekend but I keep increasing the scope with new ideas so hopefully I can have something out this weekend.  Some of the interesting things I am experimenting with: 1) using xml namespace on child tags so they get inserted with correct Dart named argument in the parent. 2) spread operator to better organize style like properties so they can be defined/grouped outside xml in a map and then brought in and spread on a tag like typical React stuff. 3) styleCSS property that generates flutter styles from CSS.  (1) & (2) are generic so works for all Dart Flutter code. (3) is specialized per Flutter Widget (Container, Text, etc) and I am only doing those 2 for now. yuriy-manifold just because something worked for JS doesn't mean it's a good idea for Dart. If you had read above discussions, you'd see that in fact it **is** controversial. I don't see why 2 languages would be better than one."
technical," (Experimental duplicate detection) Thanks for submitting this issue. Please also check if it is already covered by an existing one, like: - Failed to install VS Code update. Please download and reinstall VS Code. os error 145 (#47778)  score: 0.748 -- - Failed to install VS Code update. Please download and resinstall VS Code. (#47494) score: 0.591"
technical,"I had a bit of an epiphany on Friday. I had installed the Vivaldi browser on both my work and home computers. By default, the Vivaldi browser also gets installed into the folder. On my work PC, I have no problems updating the browser. But on my home PC, the updates  always  fail due to can you guess?locked files! Locked by the SYSTEM process. The same exact reason that updates to the user installations of VS Code and VS Code Insiders fail. One thing that's worth checking out is what installer technology is being used by Vivaldi. Is the installer technology the same as that being used by VS Code? If so, then maybe it's something specific to the installer. Ruling out any installer issues with respect to the installation location, it would seem most likely that this issue is machine-specific, I'm sorry to report, because for the life of me I'm unable to determine why this particular machine is behaving in such a manner. 1.27.2 is the second version I haven't been able to update and had to install from downloaded installer. Same errors as everyone else. Tried disabling some startup items and restarting...didn't help."
technical,"FYI - the last couple of Insider updates have had this problem for me.  I could not delete the Code-Insiders.exe file until I rebooted my PC. A new one just came out. Let me know how that goes. It could still fail, but it shouldn't brick your installation."
technical,"Fulg Yeah it's a tough issue for me to track down, since its hard to reproduce. Yeah, I just haven't figured out why that is... Do you think it's just a matter of increasing the retry timeouts? All of this discussion doesn't solve the problem or right explanation of the problem: That is with some installers, which I don't understand cause problem, but many problem comes from electron packer, that when making try to uninstall not fully exited or already doesn't seen in task manager app occasionally making the  <app.exe access denied though in delivered security problem (unble to display current user), the solution only reboot or trying to wait with install after few days (doesn't tried), but maybe problem from firefox updates some files with extension moz-backup cannot be deleted in some circumstences making firefox/nightly  browser unusable till reboot, many apps that use electron have that issue"
technical,"FindZombieHandles reports nothing. I'm at a loss. And can you manually delete the executable file yourself, in the file explorer?"
technical,I can't reproduce this at all any more. The last 4 updates have applied with no issue anyone who solved this problem?
technical,"I started with version 1.27.0-insider.  VS Code prompted for an update.  The update eventually failed. Screenshots and log file should be attached. attempted to auto-update. Microsoft Defender is my anti-virus. I downloaded the update from here and Installed without uninstalling, I had no issue"
technical,"I'll do you one better. I'm attaching the output of that before running the update, while running the update, and at the point when I get the error message. It certainly doesn't look like Code is running. That doesn't necessarily mean that some other process isn't preventing its deletion, though. Awesome, it's great that you can reliably reproduce it. Any thoughts on who might be preventing that deletion? Just to confirm, the setup runs elevated, correct? Does the inno-updater.exe process also run elevated?"
technical,Same access error - encountered many times before.  Download/Reinstall works but in place update should also work as offered.  Why it's it fixed already? Because it seems to be pretty hard to figure out the root cause.
technical,Same access error - encountered many times before.  Download/Reinstall works but in place update should also work as offered.  Why it's it fixed already? But I don't think vivaldi's the cause though (I don't have it). Also had this issue before 1809 so it's not because of the anti-ransomware feature.
technical,"same issue with me with the exact same description from other members. Edit It's really bad for as i can't even uninstall vscode or reinstall it because both inno updater and code are being used by System, even after restarting the machine. Can we change the name of this issue? This isn't about Windows Update."
technical,"Occurring to me today, fails to update via the internal ""Close and Update"" button, as well as manually X-ing the window and trying to allow the update to proceed. After a restart, the files unlocked, I uninstalled manually and downloaded from the site fresh. Not ideal, but... Can you clarify ""broken""?  The update didn't work, but VS Code - Insiders still functions.  It just continues to notify me to restart to update. I can't find 'Code - Insiders.exe' with Process Hacker."
technical,"Except, all I have is Defender and I did not turn on the ransomware protection and I still get this on VSCode and Twitch (Starting to think Discord as well now unless they are truly pushing daily updates with no changelog) Cool! It's just one more thing to potentially rule out. Thanks!"
technical,"I haven't set up anything other than the standard stuff for domain joined computers. This includes ""Windows Defender Advanced Threat Protection"" antivirus, dbaeumer and sbatten would know more about if that could be the culprit here. I don't have any OneDrive set up. Defender and Defender ATP are both enabled as these are corp machines. I don't currently have OneDrive configured on this machine. I just got this again today on my user install."
technical,"Well that didn't happen exactly it was supposed to. I simply landed at the last comment. In fact, even now, on ios safari, it takes you one comment above your intended comment. But even assuming technology worked the way it's supposed to, it'd take a lot to connect the dots. Install failed, some page opened and why am I looking at some random github issue? The developer in me admires this marvel idea, the end user in me is horrified at the new direction this UX is going!! Disabling Malwarebytes while running installer worked for me"
technical,"Well that didn't happen exactly it was supposed to. I simply landed at the last comment. In fact, even now, on ios safari, it takes you one comment above your intended comment. But even assuming technology worked the way it's supposed to, it'd take a lot to connect the dots. Install failed, some page opened and why am I looking at some random github issue? The developer in me admires this marvel idea, the end user in me is horrified at the new direction this UX is going!! Do you have another kind of anti-virus/malware?"
technical,"Well that didn't happen exactly it was supposed to. I simply landed at the last comment. In fact, even now, on ios safari, it takes you one comment above your intended comment. But even assuming technology worked the way it's supposed to, it'd take a lot to connect the dots. Install failed, some page opened and why am I looking at some random github issue? The developer in me admires this marvel idea, the end user in me is horrified at the new direction this UX is going!! Do you have any antivirus set up? How about OneDrive?"
technical,"I don't have Vivaldi installed and am also having this issue Ever since I switched to the user-based installation this is now happening to me ever since. vscode is incapable of updating itself now, always claiming it cannot access Code.exe. What I have noticed though (whenever I remember to check) is that the System process (pid 4) has a lock on Code.exe which remains locked even when vscode and the udpater exit. I can't even delete the file by hand at this point. Occasionally new locks are added within System when the update process attempts to run again. Needless to say when that happens not even the manually downloaded installer can do anything until I restart the machine."
technical,"Well, it may still be a ""ransomware feature"". As I mentioned earlier, I use Trend Micro Maximum Security which has had a ""ransomware folder protection"" feature since late 2017 (so, since around Windows 1803, possibly even 1709) and I've been using it on my Documents, Pictures and some other folders. So, it's not so much whether you're using  the  Windows ransomware folder protection feature, but whether or not you're using  a  ransomware folder protection feature of some software security suite which   may  be interfering (even though, in my case particularly, I'm not protecting).  And as far as Vivaldi, for the 3rd time, I'm not saying it's  causing  anything, but that it  also  exhibits the same update failure behavior as VS Code (SYSTEM process holding a lock on critical files causing updates to fail), which is leading me to believe that this issue may be more specific to a machine configuration than an actual problem with VS Code itself (or Vivaldi, for that matter). Except, all I have is Defender and I did not turn on the ransomware protection and I still get this on VSCode and Twitch (Starting to think Discord as well now unless they are truly pushing daily updates with no changelog)"
technical,"Just windows defender Faced this issue while updating VS Code today. Symantec Endpoint protection is installed. Turned it off, still unable getting same issue."
technical,OK. I just need you to use the task manager and find those zombie processes and what is their full command line. FindZombieHandles reports nothing. I'm at a loss.
technical,"I would look into adding Handle (from the ever resourceful Mark Russinovitch) to the VSCode package and use it in the updater in case of failure to create a log that explains  why  the file is still locked, and who is locking it. This might give a clue and point to the real culprit. I am happy to try this locally since I can readily reproduce the problem, however I haven't looked into what is required to make a custom VSCode build from source. Fulg That's a great idea. I'd be very interested if you used that locally and figure something out. I'll look into embedding that into our updater and call it whenever things cramp up."
technical,"Running into this also with OneDrive + Defender Fulg Yeah it's a tough issue for me to track down, since its hard to reproduce. Yeah, I just haven't figured out why that is... Do you think it's just a matter of increasing the retry timeouts?"
technical,"there is a zombie process that I cannot kill. FWIW. I got tired of dealing with this issue and started updating by manually uninstalling the old version then downloading and installing the new version, and this time  I still got an access denied  error when updating Code.exe from the full installer. As usual this was a random occurrence, simply clicking Retry from the installer solved the problem. It is as if Code.exe stays locked for (say) 30 seconds after the process exited. So this is not strictly a bug in the updater, the same sequence of events can be reproduced outside the VSCode self-update mechanism if you are quick enough."
technical,"Guys, I'm taking a different approach to updating Code in the background especially wrt deleting the old version. The current implementation tries to delete the old version, and if it gets stuck in a file/folder it keeps retrying until it succeeds. If it never succeeds, due to an antivirus program or any other reason, it just throws its hands up and gives up. This leaves a broken installation behind. Not great. The new implementation will first attempt to get exclusive handles on all the files to be deleted. Once all handles are acquired, it promptly deletes the files, this should succeed since the handles are exclusive. Once that's done, all folders get deleted. Then, the update is applied. This will not prevent the background update from not working due to antivirus, etc... but (1) it should reduce the chances of that happening and (2) it prevents broken installations, which seems crucial by now. I'll keep the issue open since I don't have a reproducible case. We'll close it eventually if issues stop coming in and/or if people confirm that this helps their setup. FYI - the last couple of Insider updates have had this problem for me.  I could not delete the Code-Insiders.exe file until I rebooted my PC."
technical,"Guys, I'm taking a different approach to updating Code in the background especially wrt deleting the old version. The current implementation tries to delete the old version, and if it gets stuck in a file/folder it keeps retrying until it succeeds. If it never succeeds, due to an antivirus program or any other reason, it just throws its hands up and gives up. This leaves a broken installation behind. Not great. The new implementation will first attempt to get exclusive handles on all the files to be deleted. Once all handles are acquired, it promptly deletes the files, this should succeed since the handles are exclusive. Once that's done, all folders get deleted. Then, the update is applied. This will not prevent the background update from not working due to antivirus, etc... but (1) it should reduce the chances of that happening and (2) it prevents broken installations, which seems crucial by now. I'll keep the issue open since I don't have a reproducible case. We'll close it eventually if issues stop coming in and/or if people confirm that this helps their setup. Great details. It would be very interesting to find out whether we could detect such a scenario... why would access be denied in your machine? Also, when the error happens, does C:Program FilesMicrosoft VS CodeCode.exe exist?"
technical,"Because it seems to be pretty hard to figure out the root cause. Guys, I'm taking a different approach to updating Code in the background especially wrt deleting the old version. The current implementation tries to delete the old version, and if it gets stuck in a file/folder it keeps retrying until it succeeds. If it never succeeds, due to an antivirus program or any other reason, it just throws its hands up and gives up. This leaves a broken installation behind. Not great. The new implementation will first attempt to get exclusive handles on all the files to be deleted. Once all handles are acquired, it promptly deletes the files, this should succeed since the handles are exclusive. Once that's done, all folders get deleted. Then, the update is applied. This will not prevent the background update from not working due to antivirus, etc... but (1) it should reduce the chances of that happening and (2) it prevents broken installations, which seems crucial by now. I'll keep the issue open since I don't have a reproducible case. We'll close it eventually if issues stop coming in and/or if people confirm that this helps their setup."
technical,"I've had this problem since 1.22.2 (trying to upgrade to 1.23). I'm in a corporate environment with a bunch of security stuff installed and have had the same issues with. As an additional data point, I tried setting update.enableWindowsBackgroundUpdates to false, which launched the installer when updating from within the client. However this also failed to update Code.exe with an access denied error. Manually downloading the installer (1.24.1) and launching it worked. Having the same issue since 1.24.0, both on my work PC  and  my home PC. Getting the same **Access Denied** errors for Code.exe as the original poster. Manually downloading the update and applying it worked for 1.24.0, but I am reporting this now instead of ignoring it like last time, so I did not try to manually update to 1.24.1. I  can  run Process Hacker however when the bug occurs, and following the procedure from this commen, I can see there are no Code.exe processes running when the error dialog is shown. In case the command lines are important, here they are, in order. Let me know if you need anything else, I can reproduce this at will."
technical,"Mine is not Hello, i'm sorry but that PC is property of my company and the technic support guy say thats not possible to do that procedure."
technical,"It does not seem to be highly correlated to domain-joined computers. My domain-joined work computer has no issues updating VS Code (regular or Insiders) nor Vivaldi (which also installs into this). My home computer, on the other hand, which is  NOT  domain-joined exhibits this behavior for both applications.  Therefore, as I stated previously, this issue is not a VS Code-specific issue, but is probably a machine-specific issue. (I'm leaning towards ruling out the installer because it's not the installer which has a problem, rather, it's the SYSTEM account holding a lock on files which need to be replaced causing the installer to fail.) The top culprit is the Anti-Virus program in use and/or it's configuration. I haven't been able to resolve the issue by changing my anti-virus configuration, however. In my case, on my work computer they use Sophos Anti-Virus--again, this computer  does not  have an issue updating VS Code. On my home computer, I'm running **Trend Micro Maximum Security**, the latest version. It may be helpful if others also list their anti-virus program in use to see if there are any program and/or configuration commonalities that could help determine why the SYSTEM process is holding locks on these files. Here's my setup: - Latest VS Code (non-insider) User installed - Windows 10 1809 (2H18) - Windows Defender for anti-virus - NOT domain joined  Both my laptop and my desktop are set up the same as above and exhibit the same problem. I just uninstalled VS Code User and installed VS Code System. Seems to have fixed the problem."
technical,"Sorry, I'm a little late getting back to you.  Yes, my machine is a company machine, and we are also running CrowdStrike Falcon. Hi - we've been having the same problem on various machines. Machines are a mix of corporate managed and personally managed machines, and they are running Windows 10 Pro or Enterprise with MSFT security tools (nothing else added).  The frequent error is due to a refusal to proceed because the ""resources"" directory is not empty due to something behind by an extension. Extension (all folders in path starting from and including ""resources"" only contain the sub-folder indicated in the path - the last folder is empty). We love the tool. Thx."
technical,"I have the same issue for the latest release. Failed to create file handle: The process cannot access the file because it is being used by another process. Hitting this also. Fresh, Stock machine, no fancy antivirus."
technical,"Same issue that was originally reported here with the most recent update as well.  So, had this issue on 1.22 and again now on 1.23.  Be kind of annoying if I have to uninstall and download to get the latest features.  Definitely a work related only issue though, can update on my personal easily. Probably good old McAfee locking everything down in the registries with some unique rule. Hope you're still here and facing the issue! I have one more ask for you. Can you reproduce it and upon seeing the error message popup, can you use Process Hacker and figure out whether inno updater.exe is indeed elevated?"
technical,"Same issue that was originally reported here with the most recent update as well.  So, had this issue on 1.22 and again now on 1.23.  Be kind of annoying if I have to uninstall and download to get the latest features.  Definitely a work related only issue though, can update on my personal easily. Probably good old McAfee locking everything down in the registries with some unique rule. I already got the latest update. Indeed, based on the comments above, I needed to download the exe installer and reinstall. However, with my colleague, he did not experience this issue on his laptop (same as mine)"
technical,"this exists when the error occurs.  I ran the update with Process Monitor opened, figuring it might give another clue to the failure. I have saved the entire log (filtered by the strings ""code"" and ""updater"" in the process names), you can get it here. I am happy to try other things if you want.  On a related note: the dialog box for this error is shown  under  the progress window, but maybe that should be reported separately. It is not always apparent that the error occurred since only the progress window is topmost (the error dialog is not). I am getting the same issue. It has happened so far for the last 2 updates for Insiders. I have had to reinstall it twice now due to it erroring me out. I get the same ""Access is denied. (os error 5)"" error code at the bottom of the log. Getting the same error log as him."
technical,"Disabling Malwarebytes while running installer worked for me I attempted to use the his insiders User setup method, which didn't work (lost during update). After that I tried disabling MWB ransomware protection as others have mentioned - that worked instantly! Note - it seems you don't have to completely disable MWB, only the ransomware module."
technical,Disabling Malwarebytes while running installer worked for me I can't reproduce this at all any more. The last 4 updates have applied with no issue
technical,"Same issue for latest update (1.25~). Is this an issue only for windows users? I can't say anything for Mac OS but on Linux the internal updater is not really used. Most applications are centrally managed using a package manager and at least on ArchLinux it is used for VSCode, too."
technical,"Same error for me while running vscode with user priviliges (while my user is local administrator). Closed vscode and restarted with 'run as administrator'  no error showing up. Closing vcscode again  setup starts I did forget to mention that updating also failed when running VS Code (regular and Insiders) with elevated privileges and without, as well as running the actual installer itself with/without elevated privileges. TBH, I completely expected that given the file locks opened by SYSTEM."
technical,"Fulg That's a great idea. I'd be very interested if you used that locally and figure something out. I'll look into embedding that into our updater and call it whenever things cramp up. I discovered that if you ignore the updater's prompt for a restart, and instead close Code using the  at the top right, the update then proceeds correctly."
technical,Will give Vivaldi a try...  is this happening to you on every update? I don't have Vivaldi installed and am also having this issue
technical,Will give Vivaldi a try...  is this happening to you on every update? I faced this issue multiple times. Running a windows 10 enterprise installation.
technical,"Something I noticed that can help anyone while this is being fixed, and also may help with diagnosing this is that once the VS Code application has finished installing the update, and the little popup comes up in the bottom right telling you to 'Restart to apply Updates', just closing the window manually and restart applies the update no problem~~  Nope, just failed with the update that I just needed to install, exe was deleted and had to reinstall *again*. Any progress on this? I got something else for you to try out today. I just pushed **user installable Windows setup packages** to our build pipeline. This should let you install VS Code without administrator privileges, in your LOCALAPPDATA folder while keeping those nice background updates.  So, I ask you this:  1. Install the Insiders User Setup 2. Make sure it runs fine. 3. Similarly to the suggestion above, open this and change the commit field to outdated. 4. Restart Code, an update should happen  Does the update succeed in this setup? If so, please give it a few more retries just to make sure it succeeds consistently. If not, hit me up with the log file so I can take a look.  Thanks, looking forward for the experiment results “"
technical,"This is how I got this problem and how I fixed it. After the version 1.26 came out I downloaded manually the User Installer and it gave me an access denied error because I tried to install it to Program Files folder  so I opened the installer with run as administrator option. Running Code started to prompt this access denied error all the time (after version 1.27 came out, I suppose) so I did some googling and ended up here. Then I googled some more and I decided reinstall it to the default folder (AppData) so I don't have to use run as administrator option to run the installer. Now it's working fine. I had a bit of an epiphany on Friday. I had installed the Vivaldi browser on both my work and home computers. By default, the Vivaldi browser also gets installed into the folder. On my work PC, I have no problems updating the browser. But on my home PC, the updates  always  fail due to can you guess?locked files! Locked by the SYSTEM process. The same exact reason that updates to the user installations of VS Code and VS Code Insiders fail. One thing that's worth checking out is what installer technology is being used by Vivaldi. Is the installer technology the same as that being used by VS Code? If so, then maybe it's something specific to the installer. Ruling out any installer issues with respect to the installation location, it would seem most likely that this issue is machine-specific, I'm sorry to report, because for the life of me I'm unable to determine why this particular machine is behaving in such a manner."
technical,"I can't say anything for Mac OS but on Linux the internal updater is not really used. Most applications are centrally managed using a package manager and at least on ArchLinux it is used for VSCode, too. I had a problem to update to the latest version just now. Also had the same problem with the previous update. I also tried to install the insider version, still had the update problem. I have malwarebytes installed as some others said above. edit: There's a new update on MalwareBytes just today that is supposed to fix the issue with ransomware protection preventing update on programs."
technical,"anyone who solved this problem? I had the same problem and disabled MalwareBytes as a user suggested earlier. This solved it for me. After some testing around I found MalwareBytes Ransomware protection to be the cause of the issue for me. Tested with 1.25.0 Insiders but maybe it could help on other versions, too. Hopefully, this is the same issue as I have not been able to delete or modify the VSCode executable itself after running the update."
technical,"These are great insights! I had this issue just now on Windows 10 64 Bit, error code os error 5. Was not able to try the update again because file Code.exe did not exist."
technical,"Still happening with VS Code Insiders v1.27.0 (trying to update to 1.28.0).  Interestingly, I noticed that after the update to 1.28.0 failed, the Add/Remove programs dialog indicates that 1.28.0 is installed (it isn't). I have the same issue for the latest release. Failed to create file handle: The process cannot access the file because it is being used by another process."
technical,"Do you have any antivirus set up? How about OneDrive? I haven't set up anything other than the standard stuff for domain joined computers. This includes ""Windows Defender Advanced Threat Protection"" antivirus, dbaeumer and sbatten would know more about if that could be the culprit here. I don't have any OneDrive set up."
technical,"I discovered that if you ignore the updater's prompt for a restart, and instead close Code using the  at the top right, the update then proceeds correctly. I hit this issue today. While following the instructions in (url), the update proceeded as expected, but I received  many  instances of the error shown in the image with different filename listed. Hitting Retry on each seemed to work as I did not notice any repeated files, but it was annoying. After a couple dozen Retrys, I Aborted. Will probably uninstall and reinstall."
technical,I tested the new update with a locked file and it behaved nicely and the installation was kept intact. However a locked directory makes the installation still fail. You can lock a directory easily by opening a Windows Command prompt (cmd.exe) on it. I started with version 1.27.0-insider.  VS Code prompted for an update.  The update eventually failed. Screenshots and log file should be attached.
technical,"A new one just came out. Let me know how that goes. It could still fail, but it shouldn't brick your installation. I tested the new update with a locked file and it behaved nicely and the installation was kept intact. However a locked directory makes the installation still fail. You can lock a directory easily by opening a Windows Command prompt (cmd.exe) on it."
technical,"I had the same problem and disabled MalwareBytes as a user suggested earlier. This solved it for me. After some testing around I found MalwareBytes Ransomware protection to be the cause of the issue for me. Tested with 1.25.0 Insiders but maybe it could help on other versions, too. Hopefully, this is the same issue as I have not been able to delete or modify the VSCode executable itself after running the update. I tried with the insider build (1.26) but the update failed. However, I too have MalwareBytes installed and, according to their forum, their ransomware protection appears to cause this kind of problems for many users with various applications.  As I have recently experienced problems with updating other programs in addition to VS Code, I believe that my problems are not due to any flaw in VS Code."
technical,"I already got the latest update. Indeed, based on the comments above, I needed to download the exe installer and reinstall. However, with my colleague, he did not experience this issue on his laptop (same as mine) I uninstalled Visual Studio Code and checked for Malwarebytes updates. I was already up to date. I reinstalled VSC and I'm back in business.  Everything appears in order. Also here's the error message I received."
technical,"Oh man, sorry about that. It's in resources\app. I've updated the original post. I updated the json file and reopened code, but the update flow didn't kick in automatically.  I kicked off an update flow from the gear in the lower left which went through successfully.  I am currently running 1.23.1. I will keep an eye on this ticket and, if the problem rears its head again, return to the steps you listed."
technical,"That's great to know, looking forward to it! I was able to repro this again. Guess it working was a fluke.  - inno updater.exe *is* elevated - There are no Code.exe (or Code-Insiders.exe) processes.  I can't check 'Integrity' as I'm not able to run Process Hacker due to the security suite my company has running on my machine. This info is all from the bog-standard Task Manager."
technical,"Hi - we've been having the same problem on various machines. Machines are a mix of corporate managed and personally managed machines, and they are running Windows 10 Pro or Enterprise with MSFT security tools (nothing else added).  The frequent error is due to a refusal to proceed because the ""resources"" directory is not empty due to something behind by an extension. Extension (all folders in path starting from and including ""resources"" only contain the sub-folder indicated in the path - the last folder is empty). We love the tool. Thx. I was getting the same error. When i was trying to update vs code insiders , but after reading the this whole thread I disabled the malwarebytes protection software on my PC completely , and **Voila** , the update completed super smooth. I think these protection softwares are the root cause behind this update error."
technical,"The updater, as admin, can't. Yet you can easily delete it. ”  Did you get an elevation prompt to delete the file? I was not asked to elevate. I just get the usual 'are you sure you want to delete this' prompt."
technical,"Oh, And deleting such files only shows disappeared F5 brings them back (until reboot) after reboot thay may disappear or can be deleted successfully, none application of unlocking doesn't successed with them, some sophisticated progs shows that exist handle on system that cannot not be unlocked I would look into adding Handle (from the ever resourceful Mark Russinovitch) to the VSCode package and use it in the updater in case of failure to create a log that explains  why  the file is still locked, and who is locking it. This might give a clue and point to the real culprit. I am happy to try this locally since I can readily reproduce the problem, however I haven't looked into what is required to make a custom VSCode build from source."
technical,"This is a puzzle to me, because we know Code isn't running... so why can't the updater remove Code... unless it is really running?  Can you run this little app I've created, while Code is running? It will output all running process numbers and names. Something like this. Does Code appear in that list, for you? Can you show me the output of the app in your system? I'll do you one better. I'm attaching the output of that before running the update, while running the update, and at the point when I get the error message. It certainly doesn't look like Code is running. That doesn't necessarily mean that some other process isn't preventing its deletion, though."
technical,"Ever since I switched to the user-based installation this is now happening to me ever since. vscode is incapable of updating itself now, always claiming it cannot access Code.exe. What I have noticed though (whenever I remember to check) is that the System process (pid 4) has a lock on Code.exe which remains locked even when vscode and the udpater exit. I can't even delete the file by hand at this point. Occasionally new locks are added within System when the update process attempts to run again. Needless to say when that happens not even the manually downloaded installer can do anything until I restart the machine. I'm also having this exact same issue. I need to download the new installer from the website, reboot, and install it first thing for it to succeed. This was the issue I posted weeks ago."
technical,Microsoftee brilliance at work yet again. F* this I'm having the same problem with the most recent update - all other updates went seamlessly as far as I am aware. I am not using MalwareBytes but I am using BitDefender (free edition) - disabling its Protection Shield did not help.
technical,"Too bad... a lot of other users are hitting it too (see duplicate issues linked), I was hoping we could narrow down further why this happens. My current thinking is that inno updater.exe actually wouldn't run as elevated, even though it would be spawned from an elevated process. Which is something I clearly can't repro. I'm pinging all of you since you all reported the same error Access is denied. I want to ask you to try to reproduce it, so we get more info from it.  In order to reproduce it, close Code, open this and change the commit field to outdated. Start Code, you should get an update flow. If the issue reproduces and you get the dialog which asks you to send us the log file, please do the following:  1. Open Process Hacker 2. Enable the Elevation, Integrity and Command line columns in Process Hacker 3. Search for any Code.exe processes running. If they are running, copy the full command line of each and show me. 4. Check what is the elevation and integrity of the inno updater.exe, does it match its parent?  Thanks for your help here, looking forward to learn more about this and fixing it!"
technical,"I faced this issue multiple times. Running a windows 10 enterprise installation. I've been getting this too just as I switch to the user install. I used information I learned by having the same issue with the Twitch program and checked the System process (PID4) and sure enough, there it was, open file handle for Code.exe. Just have no idea WHY this happens. (Yes yes, something about system taking over the handle for another program when 'something' happens. Just no idea what program and why it needs to have an open file handle on the exe.) ....never managed to get past this stage with Twitch either."
technical,"Like I also have ESET, though I can't disable it. The 'user installable Windows setup packages' build seemed to work, though. I've had the same issue with 1.24.1 and now 1.25. Same error that others have described, with the installer failing after being unable to delete code.exe. I have an employer managed machine as well. As a work around, I've found the following to solve the issue on both machines I've tried it on ... After the installer opens and shows the green progress bar, once it cycles ~3 times I cut and paste code.exe out of the folder. I had to allow the move by clicking through an elevation prompt, but otherwise the update worked right away both times. My idea was giving it enough time to recognize code.exe exists, but have code.exe disappear before it gives up trying to delete it itself. Maybe that will help some of you out!?"
technical,"I was getting the same error. When i was trying to update vs code insiders , but after reading the this whole thread I disabled the malwarebytes protection software on my PC completely , and **Voila** , the update completed super smooth. I think these protection softwares are the root cause behind this update error. I've had this problem since 1.22.2 (trying to upgrade to 1.23). I'm in a corporate environment with a bunch of security stuff installed and have had the same issues with. As an additional data point, I tried setting update.enableWindowsBackgroundUpdates to false, which launched the installer when updating from within the client. However this also failed to update Code.exe with an access denied error. Manually downloading the installer (1.24.1) and launching it worked."
technical,"The same here. What I think strange is that I did the update as administrator, and there the VS Code is 1.24.1, but for my normal user, it's still showing the old version 1.24.0. My laptop is corporate managed with Windows 7 64bit. Interesting detail: as I tried to test other cases, the 1.24.1 update  actually succeeded , so it seems this is a somewhat random problem. I wanted to check if another process was somehow keeping a handle on the old Code.exe (antivirus or whatever). I tried to force another update by changing the commit field in this but that did not help, the update succeeded again. Hopefully the information I posted before will be helpful, as it seems I cannot reproduce the issue anymore."
technical,"My laptop does connect to a domain when I'm at work, though I've experienced the problem even when I'm at home and not connected to a domain. It also happens on my home computer, which is not in a domain."
technical,"My laptop does connect to a domain when I'm at work, though I've experienced the problem even when I'm at home and not connected to a domain. It does not seem to be highly correlated to domain-joined computers. My domain-joined work computer has no issues updating VS Code (regular or Insiders) nor Vivaldi (which also installs into this). My home computer, on the other hand, which is  NOT  domain-joined exhibits this behavior for both applications.  Therefore, as I stated previously, this issue is not a VS Code-specific issue, but is probably a machine-specific issue. (I'm leaning towards ruling out the installer because it's not the installer which has a problem, rather, it's the SYSTEM account holding a lock on files which need to be replaced causing the installer to fail.) The top culprit is the Anti-Virus program in use and/or it's configuration. I haven't been able to resolve the issue by changing my anti-virus configuration, however. In my case, on my work computer they use Sophos Anti-Virus--again, this computer  does not  have an issue updating VS Code. On my home computer, I'm running **Trend Micro Maximum Security**, the latest version. It may be helpful if others also list their anti-virus program in use to see if there are any program and/or configuration commonalities that could help determine why the SYSTEM process is holding locks on these files."
technical,"My laptop does connect to a domain when I'm at work, though I've experienced the problem even when I'm at home and not connected to a domain. Just got it, I have all the standard internal bloatware installed. Failing to rename ""Resources"". I believe I was on this."
technical,"I had a problem to update to the latest version just now. Also had the same problem with the previous update. I also tried to install the insider version, still had the update problem. I have malwarebytes installed as some others said above. edit: There's a new update on MalwareBytes just today that is supposed to fix the issue with ransomware protection preventing update on programs. Just uninstall it after updating Malware bytes, then re-install VS Code, works perfectly."
technical,"You can still use the old update mechanism with the following setting. Though it would be pretty cool if we could make the background updates work on your machines... I would need some more feedback, like what I mentioned above in Just updated to 1.24.0 without issue this time with the in-client updater. My PC is not managed by a corporation. Just as a note though, which could be unrelated, I'm not sure, I actually had seemingly the exact same issue as before with a Git for Windows update recently."
technical,Do you have another kind of anti-virus/malware? Just windows defender
technical,"Something strange going on here..... Both my servers (HPE Blade) running (repspectively) Win 2016 Stable and Win 2019 Preview. Updating 1.24 - 1.25 failed on both. Whats worth noting is that update on Win10 Desktop (Lenovo laptop) went without an issue. Like I also have ESET, though I can't disable it. The 'user installable Windows setup packages' build seemed to work, though."
technical,"Same here, no additional AV is installed, pretty clean system, same problem Microsoftee brilliance at work yet again. F* this"
technical,"Same here, no additional AV is installed, pretty clean system, same problem Mine as well, I managed to solve this by manually running the installer."
technical,"Mine as well, I managed to solve this by manually running the installer. Mine is not"
technical,"Same problem here. Stock Windows 10 Pro on a Surface Pro 4, joined to a work domain. No fancy antivirus. SYSTEM has a handle on the file, and I can't stop that. Rebooting doesn't fix. This seems to have a tenuous association with work computers. Does that suggest being joined to a domain relates to the problem? My laptop does connect to a domain when I'm at work, though I've experienced the problem even when I'm at home and not connected to a domain."
technical,"But I don't think vivaldi's the cause though (I don't have it). Also had this issue before 1809 so it's not because of the anti-ransomware feature. No no, I'm not saying Vivaldi is the cause, I'm saying that Vivaldi installs into a similar location as VS Code and that it, too, fails to update in the same way as VS Code. So there's an element of commonality in the behavior of updating both of these applications."
technical,But I don't think vivaldi's the cause though (I don't have it). Also had this issue before 1809 so it's not because of the anti-ransomware feature. NOTE: This is also happening with regular VS Code (e.g. not Insiders edition).
technical,"Thanks for experimenting. Two asks for you: 1. Did the installation get broken? It should not have broken this time. 2. If you use Process Hacker, you can find the runnaway Code - Insiders.exe process  and see which other process has an open handle to it. Which one is it? Occurring to me today, fails to update via the internal ""Close and Update"" button, as well as manually X-ing the window and trying to allow the update to proceed. After a restart, the files unlocked, I uninstalled manually and downloaded from the site fresh. Not ideal, but..."
technical,"same story with a fresh install.  1. I went through ""Add or Remove Programs"", found ""Microsoft Visual Studio Code"" and uninstalled it. 2. I then went here and downloaded the Stable version for windows. 3. I followed all installation steps, then went to the path you mentioned. Here is what I see. Oh man, sorry about that. It's in resources\app. I've updated the original post."
technical,"All of this discussion doesn't solve the problem or right explanation of the problem: That is with some installers, which I don't understand cause problem, but many problem comes from electron packer, that when making try to uninstall not fully exited or already doesn't seen in task manager app occasionally making the  <app.exe access denied though in delivered security problem (unble to display current user), the solution only reboot or trying to wait with install after few days (doesn't tried), but maybe problem from firefox updates some files with extension moz-backup cannot be deleted in some circumstences making firefox/nightly  browser unusable till reboot, many apps that use electron have that issue Oh, And deleting such files only shows disappeared F5 brings them back (until reboot) after reboot thay may disappear or can be deleted successfully, none application of unlocking doesn't successed with them, some sophisticated progs shows that exist handle on system that cannot not be unlocked"
technical,"NOTE: This is also happening with regular VS Code (e.g. not Insiders edition). OK, so I disabled the Trend Micro Solution Platform service and rebooted. I opened Visual Studio Code - Insiders and checked for updates and then clicked Restart to install updates. No dice. ˜ I got the following error dialog: Looking at the log file, even though I performed the update process starting from within VS Code Insiders, it appears that the failure was as a result of being unable to obtain a lock on Code - Insiders.exe (again, because, SYSTEM is holding open a handle on this file). So, my ""two scenarios"" listed above are a little incorrect in that it's not based on where/how an update is performed, but only on which files are trying to be updated. In most cases (90%), Code - Insiders.exe is trying to be replaced. In some smaller amount of cases, it's inno updater.exe which is trying to be replaced. I would venture that we should forget about inno updater.exe for the moment and focus only on Code - Insiders.exe. Also, it's likely that determining the reason for one would also determine the reason for the other.  I'll post my log file (only changing my username in the logfile), but again, I don't think it'll be all that helpful. The most reliable method to updating VS Code User installation is the 4 step method I posted above: Uninstall, Reboot, Delete leftover files, Install new version. Ugh. What a pain. I think for now I'll go back to a machine-wide installation. I never had issues with that. Hopefully, you guys will be able to figure this out, it sounds like it's been a tricky bugger to get to the bottom of. Let me know if there's anything else I can provide to aid in your efforts, as the User install does seem to be a better installation option for this type of program."
technical,"Well, there are at least two pieces of security software running on my machine: ESET Security and CrowdStrike Falcon (though CrowdStrike is a recent addition, and I think this issue was present before it was installed), neither of which I have any control over. There are also various other controls in place as this is a managed Enterprise installation of Windows. I have co-workers that use Code without issue though so I doubt it's those. My guess was more along the lines of zombie processes spawned by Code that were still holding some resource. A quick check in the task manager shows that all three update processes (inno updater.exe, CodeSetup-insider-guid.exe, and CodeSetup-insider-guid.tmp) are elevated. OK. I just need you to use the task manager and find those zombie processes and what is their full command line."
technical,"I'm pinging all of you since you all reported the same error Access is denied. I want to ask you to try to reproduce it, so we get more info from it.  In order to reproduce it, close Code, open this and change the commit field to outdated. Start Code, you should get an update flow. If the issue reproduces and you get the dialog which asks you to send us the log file, please do the following:  1. Open Process Hacker 2. Enable the Elevation, Integrity and Command line columns in Process Hacker 3. Search for any Code.exe processes running. If they are running, copy the full command line of each and show me. 4. Check what is the elevation and integrity of the inno updater.exe, does it match its parent?  Thanks for your help here, looking forward to learn more about this and fixing it! on my machine, the C:\Program Files\Microsoft VS Code\app folder does not exist...  This is after a couple more failed upgrades if that provides any insight."
technical,"on my machine, the C:\Program Files\Microsoft VS Code\app folder does not exist...  This is after a couple more failed upgrades if that provides any insight. Please completely remove VS Code, reinstall it and give the above steps a try. Thanks!"
technical,"I uninstalled Visual Studio Code and checked for Malwarebytes updates. I was already up to date. I reinstalled VSC and I'm back in business.  Everything appears in order. Also here's the error message I received. Redirecting here is good to raise awareness. But ultimately, it forces everyone to scroll through the comments to figure out why they landed here.  A better UX would be to:  - allow VSCode to update in background - this means 1) do not nag me on every instance, every time whether I want to update or not. 2) do not require admin permissions to do so. - upgrade vscode the next time it restarts silently (maybe make it configurable)  When in doubt, follow Chrome's auto-update behavior. Please do not let VSCode become the next Adobe Flash. :pray:"
technical,"FWIW. I got tired of dealing with this issue and started updating by manually uninstalling the old version then downloading and installing the new version, and this time  I still got an access denied  error when updating Code.exe from the full installer. As usual this was a random occurrence, simply clicking Retry from the installer solved the problem. It is as if Code.exe stays locked for (say) 30 seconds after the process exited. So this is not strictly a bug in the updater, the same sequence of events can be reproduced outside the VSCode self-update mechanism if you are quick enough. Running into this also with OneDrive + Defender"
technical,"I hit this issue today. While following the instructions in (url), the update proceeded as expected, but I received  many  instances of the error shown in the image with different filename listed. Hitting Retry on each seemed to work as I did not notice any repeated files, but it was annoying. After a couple dozen Retrys, I Aborted. Will probably uninstall and reinstall. Same access error - encountered many times before.  Download/Reinstall works but in place update should also work as offered.  Why it's it fixed already?"
technical,"Hitting this also. Fresh, Stock machine, no fancy antivirus. Same error for me while running vscode with user priviliges (while my user is local administrator). Closed vscode and restarted with 'run as administrator'  no error showing up. Closing vcscode again  setup starts"
technical,"Faced this issue while updating VS Code today. Symantec Endpoint protection is installed. Turned it off, still unable getting same issue. Same here, no additional AV is installed, pretty clean system, same problem"
technical,"I've had the same issue with 1.24.1 and now 1.25. Same error that others have described, with the installer failing after being unable to delete code.exe. I have an employer managed machine as well. As a work around, I've found the following to solve the issue on both machines I've tried it on ... After the installer opens and shows the green progress bar, once it cycles ~3 times I cut and paste code.exe out of the folder. I had to allow the move by clicking through an elevation prompt, but otherwise the update worked right away both times. My idea was giving it enough time to recognize code.exe exists, but have code.exe disappear before it gives up trying to delete it itself. Maybe that will help some of you out!? Same issue for latest update (1.25~). Is this an issue only for windows users?"
technical,"I was not asked to elevate. I just get the usual 'are you sure you want to delete this' prompt. Same issue that was originally reported here with the most recent update as well.  So, had this issue on 1.22 and again now on 1.23.  Be kind of annoying if I have to uninstall and download to get the latest features.  Definitely a work related only issue though, can update on my personal easily. Probably good old McAfee locking everything down in the registries with some unique rule."
technical,"Cool! It's just one more thing to potentially rule out. Thanks! same issue with me with the exact same description from other members. Edit It's really bad for as i can't even uninstall vscode or reinstall it because both inno updater and code are being used by System, even after restarting the machine."
technical,"1.27.2 is the second version I haven't been able to update and had to install from downloaded installer. Same errors as everyone else. Tried disabling some startup items and restarting...didn't help. Same problem here. Stock Windows 10 Pro on a Surface Pro 4, joined to a work domain. No fancy antivirus. SYSTEM has a handle on the file, and I can't stop that. Rebooting doesn't fix. This seems to have a tenuous association with work computers. Does that suggest being joined to a domain relates to the problem?"
technical,"Please completely remove VS Code, reinstall it and give the above steps a try. Thanks! same story with a fresh install.  1. I went through ""Add or Remove Programs"", found ""Microsoft Visual Studio Code"" and uninstalled it. 2. I then went here and downloaded the Stable version for windows. 3. I followed all installation steps, then went to the path you mentioned. Here is what I see."
technical,"I tried with the insider build (1.26) but the update failed. However, I too have MalwareBytes installed and, according to their forum, their ransomware protection appears to cause this kind of problems for many users with various applications.  As I have recently experienced problems with updating other programs in addition to VS Code, I believe that my problems are not due to any flaw in VS Code. Similar problem, similar solution -- in case it helps someone. Neither in-Code updates nor exe download worked for me under normal or elevated UAC. ESET turned out to be the culprit. Installs without issue when real-time protection is fully disabled."
technical,"I did forget to mention that updating also failed when running VS Code (regular and Insiders) with elevated privileges and without, as well as running the actual installer itself with/without elevated privileges. TBH, I completely expected that given the file locks opened by SYSTEM. So this just happened to me today on my home machine (the one I described above in earlier comments) with the non user-only installation of VS Code Insiders edition. Additionally, I do not experience this problem on my work computer, updates on that computer have occurred flawelessly. My work computer is an HP zBook."
technical,"No no, I'm not saying Vivaldi is the cause, I'm saying that Vivaldi installs into a similar location as VS Code and that it, too, fails to update in the same way as VS Code. So there's an element of commonality in the behavior of updating both of these applications. So, this worked on my Dell XPS 13 but still fails on my Asus Transformer Pro"
technical,"I had this issue just now on Windows 10 64 Bit, error code os error 5. Was not able to try the update again because file Code.exe did not exist. Something I noticed that can help anyone while this is being fixed, and also may help with diagnosing this is that once the VS Code application has finished installing the update, and the little popup comes up in the bottom right telling you to 'Restart to apply Updates', just closing the window manually and restart applies the update no problem~~  Nope, just failed with the update that I just needed to install, exe was deleted and had to reinstall *again*. Any progress on this?"
technical,"Similar problem, similar solution -- in case it helps someone. Neither in-Code updates nor exe download worked for me under normal or elevated UAC. ESET turned out to be the culprit. Installs without issue when real-time protection is fully disabled. Something strange going on here..... Both my servers (HPE Blade) running (repspectively) Win 2016 Stable and Win 2019 Preview. Updating 1.24 - 1.25 failed on both. Whats worth noting is that update on Win10 Desktop (Lenovo laptop) went without an issue."
technical,"Sorry, but I'm confused now. Is this a random problem or is it related with people who has their laptop managed by the employer? A second question is: why the user VS Code isn't updated when I do the update as administrator? Is this another issue? I'm using Windows 7 Pro 64-bit. sorry if my comments confused you. What I I meant is that I had the error constantly (I could not update from 1.24.0 to 1.24.1), and in the process of investigating possible causes, the problem just  stopped occurring  despite no configuration changes on my part. My PC is still managed by my organization, even though we have Admin-level accounts.  It seems that if you keep retrying, eventually you will succeed. This probably means it is a subtle timing problem in the upgrade process, which occurs more often for some people than others. I managed to reproduce the problem again while writing this comment (I think this proves the bug is indeed random!), this time adding more bits to the Process Monitor log. You can find it here. Interestingly, I see now there are multiple attempts at deleting the file (is this new from 1.24.1?), all of them fail, but there are also several BUFFER OVERFLOW errors. Hope this helps."
technical,"Interesting detail: as I tried to test other cases, the 1.24.1 update  actually succeeded , so it seems this is a somewhat random problem. I wanted to check if another process was somehow keeping a handle on the old Code.exe (antivirus or whatever). I tried to force another update by changing the commit field in this but that did not help, the update succeeded again. Hopefully the information I posted before will be helpful, as it seems I cannot reproduce the issue anymore. Sorry, but I'm confused now. Is this a random problem or is it related with people who has their laptop managed by the employer? A second question is: why the user VS Code isn't updated when I do the update as administrator? Is this another issue? I'm using Windows 7 Pro 64-bit."
technical,"Just updated to 1.24.0 without issue this time with the in-client updater. My PC is not managed by a corporation. Just as a note though, which could be unrelated, I'm not sure, I actually had seemingly the exact same issue as before with a Git for Windows update recently. Sorry, I'm a little late getting back to you.  Yes, my machine is a company machine, and we are also running CrowdStrike Falcon."
technical,"OK, so I disabled the Trend Micro Solution Platform service and rebooted. I opened Visual Studio Code - Insiders and checked for updates and then clicked Restart to install updates. No dice. ˜ I got the following error dialog: Looking at the log file, even though I performed the update process starting from within VS Code Insiders, it appears that the failure was as a result of being unable to obtain a lock on Code - Insiders.exe (again, because, SYSTEM is holding open a handle on this file). So, my ""two scenarios"" listed above are a little incorrect in that it's not based on where/how an update is performed, but only on which files are trying to be updated. In most cases (90%), Code - Insiders.exe is trying to be replaced. In some smaller amount of cases, it's inno updater.exe which is trying to be replaced. I would venture that we should forget about inno updater.exe for the moment and focus only on Code - Insiders.exe. Also, it's likely that determining the reason for one would also determine the reason for the other.  I'll post my log file (only changing my username in the logfile), but again, I don't think it'll be all that helpful. The most reliable method to updating VS Code User installation is the 4 step method I posted above: Uninstall, Reboot, Delete leftover files, Install new version. Ugh. What a pain. I think for now I'll go back to a machine-wide installation. I never had issues with that. Hopefully, you guys will be able to figure this out, it sounds like it's been a tricky bugger to get to the bottom of. Let me know if there's anything else I can provide to aid in your efforts, as the User install does seem to be a better installation option for this type of program. Still happening with VS Code Insiders v1.27.0 (trying to update to 1.28.0).  Interestingly, I noticed that after the update to 1.28.0 failed, the Add/Remove programs dialog indicates that 1.28.0 is installed (it isn't)."
technical,"Can we change the name of this issue? This isn't about Windows Update. Thanks for all the info guys, will continue investigating this. Meanwhile, I'll lock this thread so everyone sees this: If you're having this issue constantly, disable background updates with this."
technical,"attempted to auto-update. Microsoft Defender is my anti-virus. I downloaded the update from here and Installed without uninstalling, I had no issue Thanks for experimenting. Two asks for you: 1. Did the installation get broken? It should not have broken this time. 2. If you use Process Hacker, you can find the runnaway Code - Insiders.exe process  and see which other process has an open handle to it. Which one is it?"
technical,"Here's my setup: - Latest VS Code (non-insider) User installed - Windows 10 1809 (2H18) - Windows Defender for anti-virus - NOT domain joined  Both my laptop and my desktop are set up the same as above and exhibit the same problem. I just uninstalled VS Code User and installed VS Code System. Seems to have fixed the problem. Thanks for the info. Yes, the system-wide installation does not exhibit this problem (to my knowledge) because it's installed to %Program Files%. I, too, have resorted to using the system-wide installation on my home computer due to the update issues.  One question: since you're running Windows 10 1809 and Windows Defender, did you enable the new anti-ransomware folder protection feature? Trend Micro has a similar feature that's been around for a while now and I am using it on some folders (though, not the folder). If so, is it possible for you to list the locations you've enabled it for (just use the well-known folder names such as Documents, Pictures, etc., not the full path to those folders. And also note if you've used the feature on other not well-known folders, e.g. C:\some-other-folder&mdash,I don't want to know the actual path, just whether or not you're using the feature outside of well-known folders as well)."
technical,"I got something else for you to try out today. I just pushed **user installable Windows setup packages** to our build pipeline. This should let you install VS Code without administrator privileges, in your LOCALAPPDATA folder while keeping those nice background updates.  So, I ask you this:  1. Install the Insiders User Setup 2. Make sure it runs fine. 3. Similarly to the suggestion above, open this and change the commit field to outdated. 4. Restart Code, an update should happen  Does the update succeed in this setup? If so, please give it a few more retries just to make sure it succeeds consistently. If not, hit me up with the log file so I can take a look.  Thanks, looking forward for the experiment results “ that new Insiders build succeeds for me when installing forced updates 5 out of 5 times. The old 1.24.1 on the same machine succeeded only once out of 5 (new) attempts.  Not a fan of installing apps in localappdata, but if you must...  BTW, I should have mentioned this earlier: on my machine UAC is partially gimped and set to Never Notify (it is still active however). My account has Admin privileges on the local PC but I don't run VS Code elevated."
technical,"I updated the json file and reopened code, but the update flow didn't kick in automatically.  I kicked off an update flow from the gear in the lower left which went through successfully.  I am currently running 1.23.1. I will keep an eye on this ticket and, if the problem rears its head again, return to the steps you listed. That's great to know, looking forward to it!"
technical,"I was able to repro this again. Guess it working was a fluke.  - inno updater.exe *is* elevated - There are no Code.exe (or Code-Insiders.exe) processes.  I can't check 'Integrity' as I'm not able to run Process Hacker due to the security suite my company has running on my machine. This info is all from the bog-standard Task Manager. The problem could very well be related to this. Guys/gals, are your machines also managed by a corporation?"
technical,"I am getting the same issue. It has happened so far for the last 2 updates for Insiders. I have had to reinstall it twice now due to it erroring me out. I get the same ""Access is denied. (os error 5)"" error code at the bottom of the log. Getting the same error log as him. The same here. What I think strange is that I did the update as administrator, and there the VS Code is 1.24.1, but for my normal user, it's still showing the old version 1.24.0. My laptop is corporate managed with Windows 7 64bit."
technical,"I can delete it manually. Even attempted to do so during the update, though it still failed with a complaint that the file *didn't* exist. Make up your mind, updater... ˜ The updater, as admin, can't. Yet you can easily delete it. ”  Did you get an elevation prompt to delete the file?"
technical,Defender and Defender ATP are both enabled as these are corp machines. I don't currently have OneDrive configured on this machine. I just got this again today on my user install. there is a zombie process that I cannot kill.
technical,"sorry if my comments confused you. What I I meant is that I had the error constantly (I could not update from 1.24.0 to 1.24.1), and in the process of investigating possible causes, the problem just  stopped occurring  despite no configuration changes on my part. My PC is still managed by my organization, even though we have Admin-level accounts.  It seems that if you keep retrying, eventually you will succeed. This probably means it is a subtle timing problem in the upgrade process, which occurs more often for some people than others. I managed to reproduce the problem again while writing this comment (I think this proves the bug is indeed random!), this time adding more bits to the Process Monitor log. You can find it here. Interestingly, I see now there are multiple attempts at deleting the file (is this new from 1.24.1?), all of them fail, but there are also several BUFFER OVERFLOW errors. Hope this helps. These are great insights!"
technical,"Great details. It would be very interesting to find out whether we could detect such a scenario... why would access be denied in your machine? Also, when the error happens, does C:Program FilesMicrosoft VS CodeCode.exe exist? this exists when the error occurs.  I ran the update with Process Monitor opened, figuring it might give another clue to the failure. I have saved the entire log (filtered by the strings ""code"" and ""updater"" in the process names), you can get it here. I am happy to try other things if you want.  On a related note: the dialog box for this error is shown  under  the progress window, but maybe that should be reported separately. It is not always apparent that the error occurred since only the progress window is topmost (the error dialog is not)."
technical,"(Experimental duplicate detection) Thanks for submitting this issue. Please also check if it is already covered by an existing one, like: - Failed to install VS Code update. Please download and reinstall VS Code. os error 145 (#47778)  score: 0.748 -- - Failed to install VS Code update. Please download and resinstall VS Code. (#47494) score: 0.591 This is a puzzle to me, because we know Code isn't running... so why can't the updater remove Code... unless it is really running?  Can you run this little app I've created, while Code is running? It will output all running process numbers and names. Something like this. Does Code appear in that list, for you? Can you show me the output of the app in your system?"
technical,"Redirecting here is good to raise awareness. But ultimately, it forces everyone to scroll through the comments to figure out why they landed here.  A better UX would be to:  - allow VSCode to update in background - this means 1) do not nag me on every instance, every time whether I want to update or not. 2) do not require admin permissions to do so. - upgrade vscode the next time it restarts silently (maybe make it configurable)  When in doubt, follow Chrome's auto-update behavior. Please do not let VSCode become the next Adobe Flash. :pray: This is exactly what the link you just followed pointed to. That comment suggests you use the user installable package which will not ask for permission to update, nor credentials, it just updates in the background."
technical,"Update: restarting VS Code Insiders (non user-only instalation) and re-attempting the update was successful. This is how I got this problem and how I fixed it. After the version 1.26 came out I downloaded manually the User Installer and it gave me an access denied error because I tried to install it to Program Files folder  so I opened the installer with run as administrator option. Running Code started to prompt this access denied error all the time (after version 1.27 came out, I suppose) so I did some googling and ended up here. Then I googled some more and I decided reinstall it to the default folder (AppData) so I don't have to use run as administrator option to run the installer. Now it's working fine."
technical,"Hope you're still here and facing the issue! I have one more ask for you. Can you reproduce it and upon seeing the error message popup, can you use Process Hacker and figure out whether inno updater.exe is indeed elevated? Though I have no idea what changed, I'm actually no longer able to reproduce this. If it shows back up I'll let you know, but I'm not using Insiders on my work machine anymore - mostly because of this bug (I didn't want to get as many update notifications that I couldn't do anything about). I installed Insiders 1.23.0 and successfully updated to 1.24.0."
technical,"Though I have no idea what changed, I'm actually no longer able to reproduce this. If it shows back up I'll let you know, but I'm not using Insiders on my work machine anymore - mostly because of this bug (I didn't want to get as many update notifications that I couldn't do anything about). I installed Insiders 1.23.0 and successfully updated to 1.24.0. Too bad... a lot of other users are hitting it too (see duplicate issues linked), I was hoping we could narrow down further why this happens. My current thinking is that inno updater.exe actually wouldn't run as elevated, even though it would be spawned from an elevated process. Which is something I clearly can't repro."
technical,"So this just happened to me today on my home machine (the one I described above in earlier comments) with the non user-only installation of VS Code Insiders edition. Additionally, I do not experience this problem on my work computer, updates on that computer have occurred flawelessly. My work computer is an HP zBook. Update: restarting VS Code Insiders (non user-only instalation) and re-attempting the update was successful."
technical,"This is exactly what the link you just followed pointed to. That comment suggests you use the user installable package which will not ask for permission to update, nor credentials, it just updates in the background. Updating MailwareBytes did the trick."
technical,"Updating MailwareBytes did the trick. Well that didn't happen exactly it was supposed to. I simply landed at the last comment. In fact, even now, on ios safari, it takes you one comment above your intended comment. But even assuming technology worked the way it's supposed to, it'd take a lot to connect the dots. Install failed, some page opened and why am I looking at some random github issue? The developer in me admires this marvel idea, the end user in me is horrified at the new direction this UX is going!!"
technical,"I'm also having this exact same issue. I need to download the new installer from the website, reboot, and install it first thing for it to succeed. This was the issue I posted weeks ago. Well, it may still be a ""ransomware feature"". As I mentioned earlier, I use Trend Micro Maximum Security which has had a ""ransomware folder protection"" feature since late 2017 (so, since around Windows 1803, possibly even 1709) and I've been using it on my Documents, Pictures and some other folders. So, it's not so much whether you're using  the  Windows ransomware folder protection feature, but whether or not you're using  a  ransomware folder protection feature of some software security suite which   may  be interfering (even though, in my case particularly, I'm not protecting).  And as far as Vivaldi, for the 3rd time, I'm not saying it's  causing  anything, but that it  also  exhibits the same update failure behavior as VS Code (SYSTEM process holding a lock on critical files causing updates to fail), which is leading me to believe that this issue may be more specific to a machine configuration than an actual problem with VS Code itself (or Vivaldi, for that matter)."
technical,"Awesome, it's great that you can reliably reproduce it. Any thoughts on who might be preventing that deletion? Just to confirm, the setup runs elevated, correct? Does the inno-updater.exe process also run elevated? Well, there are at least two pieces of security software running on my machine: ESET Security and CrowdStrike Falcon (though CrowdStrike is a recent addition, and I think this issue was present before it was installed), neither of which I have any control over. There are also various other controls in place as this is a managed Enterprise installation of Windows. I have co-workers that use Code without issue though so I doubt it's those. My guess was more along the lines of zombie processes spawned by Code that were still holding some resource. A quick check in the task manager shows that all three update processes (inno updater.exe, CodeSetup-insider-guid.exe, and CodeSetup-insider-guid.tmp) are elevated."
technical,"So, this worked on my Dell XPS 13 but still fails on my Asus Transformer Pro Will give Vivaldi a try...  is this happening to you on every update?"
technical,"Hello, i'm sorry but that PC is property of my company and the technic support guy say thats not possible to do that procedure. You can still use the old update mechanism with the following setting. Though it would be pretty cool if we could make the background updates work on your machines... I would need some more feedback, like what I mentioned above in"
technical,"These two statements confuse me a bit as they seem a bit contradictory: Does this mean that the module Main has sometimes a soft scope (say at the REPL prompt) and sometimes a hard scope (say when julia -L script.jl)?  Would it not make sense to say that Main always has soft scope? And a module can opt-in to soft scope by using SoftGlobalScope? (I guess) scoping rules cannot be changed in scripts because it would be backwards incompatible, i.e. would break the promise that any code written for 1.0 will run on any 1.* version. You are correct though that the same problem with scoping for the REPL also applies to scripts (naive user at a complete loss why his/her code does not work properly when run as a script). A way to solve/alleviate this problem without major incompatibilty would be to add an option to the julia cmdline to use  softscope (or alternative) , e.g. julia -f programfile, and show this option in any description/tutorial that a beginner is likely to come across. I also see a potential alternative for the softscope that may have some advantages (though i am probably overlooking disadvantages): What if a file (a called script) would always introduce its own local scope: scoping rules would be in complete consistency with those in functions, and with the expectations of a lot of users. It would also remove a lot of the performance liabilities with new users: No more unneeded globals (globals would have to be explicitly defined), and code might be compiled (How many times have you had to say to put everything in a function, and to avoid using globals?)"
technical,"Frankly, I'm pretty annoyed that people are only giving this feedback now. This has change has been on master for ten months. People haven't been using master for interactive use or for teaching, they've been using it to upgrade packages, which are only minimally affected by this and are mostly written by experienced programmers. (I was one of the few people who did give feedback in #19324, though, where I argued for the old behavior.) A non-breaking way out of this would be to change back to the old behavior (ideally not by inserting implicit let blocks or anything ” just restore the old code in julia-syntax.scm as an option) in the REPL.  Or rather, to make it available in environments like IJulia that might want it, add a soft global scope=false flag to include, include string, and Core.eval to restore the old behavior. (I was one of the few people who did give feedback in #19324, though, where I argued for the old behavior.)  Yes, and I greatly appreciate it. It doesn't much matter now since we made the choice, let it bake for ten months and have now released it with a long-term commitment to stability. So the only thing to do now is to focus on what to do going forward.  Having an option to choose between the old behavior and the new one is interesting but it feels very hacky. That means we not only sometimes have a scoping behavior that everyone apparently found incredibly confusing, but we don't always have it and whether we have it or not depends on a global flag. That feels pretty unsatisfactory, I'm afraid."
technical,"A short answer is that I think it's easier if the scope of a variable corresponds to some block construct (e.g. the body of a function or loop). With your suggestion, the scope of a variable would be some subset of a block, which I think is ultimately more complex and confusing --- you can't point to a syntactic form that corresponds to the scope of the variable.  Yes, I can believe this is a mismatch for some people's intuition. But you can only optimize for the first ten minutes of using a language up to a point. The real question is, how hard is it to teach/learn how it works, and which design will save time in the long run (by making the language simpler, making it easier to develop tooling, etc.)? (in agreement with much of the above about modifying the behavior of the REPL) I'd like to see the REPL be in a way that does not lead to this stackoverflow question and sooner would be best as many new eyes are looking at Julia"
technical," (Per, this is the relevant change)"
technical,"what would be the plan to implement the semantics you suggested in this discourse thread, initially only in the REPL and opt-in elsewhere?  It sounds like you are planning to put that directly into the lowering code (julia-syntax.scm), rather than as a syntax rewriting ala SoftScope.jl?  Or would you rather have it as syntax rewriting first (modifying SoftScope to the proposed rule and converting it to a stdlib), and defer putting it into the lowering code for a later Julia release? ### Example 1 This came up with a student who upgraded from 0.6 to 1.0 directly, so never even got a chance to see a deprecation warning, let alone find an explanation for new behavior:  ### Example 2 This crushs the students willingness to learn  I ""get"" why this happens in the sense that I think I can explain, with sufficient reference to the arcana in the manual about what introduces scopes and what doesn't, but I think that this is problematic for interactive use.  In example one, you get a silent failure. In example two, you get an error message that is very there-is-no-spoon. Thats roughly comparable to some Python code I wrote in a notebook at work today. I'm not sure what the rules are in Python, but I do know that generally you can't assign to things at the global scope without invoking global. But at the REPL it does work, presumably because at the REPL the rules are different or the same logic as if they were all are in the scope of function is applied.  I can't language-lawyer the rules enough to propose the concrete change I would like, and based on Slack this isn't even necessarily perceived as an issue by some people, so I don't know where to go with this except to flag it."
technical,"I've just hit this and was completely boggled to be honest, having never seen it before in any other language. I'm planning on introducing an optional Julia course for advanced R users in my uni later this year once things have settled down, and my students will hit this on day 0 when they start randomly typing things in the REPL. And the fact that for loops behave differently from if statements just rubs salt in the wound, however logical this may be in terms of scoping. Scope inside functions is sufficiently hard to get biology students to grasp, the idea of having to explain  albeit perceived  glaring inconsistencies in it in the REPL / in a script / in a for loop / in an if statement (because that's what we're talking about here) in a way that is different from every other language on earth makes me very sad.  I understand the backward compatibility promise that was made, but having this work  as expected by every non-cs person on the planet (and most cs people I suspect)  seems like a bugfix rather than a backward compatibility issue - we're not saying that every bug will be reproduced for ever are we? The REPL fix is obviously essential, so it's great that you're proposing this, but then having to explain you can't copy a script into the REPL and expect the same behaviour seems as bad as or worse than the original problem.  Please, please, please think about treating this as a bugfix and pushing it out with scripts as well as the REPL - even if there's an switch to go to the ""old"" behaviour - and doing it as soon as possible in 1.0.1. A colleague that I was trying to get to learn julia also just ran into this. Having to explain the whole global vs. local variable thing at the first steps is not ideal..."
technical,"we are both on long-awaited vacations (I should not be reading this). We can figure out what to do in a week or so. while the feedback is appreciated, scoping rules are not up for a broader debate or revision. The only thing on the table is special-casing interactive eval. The SoftGlobalScope package is already an excellent experimental implementation and it may just be a matter of making that part of Base and using it in the REPL. A short answer is that I think it's easier if the scope of a variable corresponds to some block construct (e.g. the body of a function or loop). With your suggestion, the scope of a variable would be some subset of a block, which I think is ultimately more complex and confusing --- you can't point to a syntactic form that corresponds to the scope of the variable.  Yes, I can believe this is a mismatch for some people's intuition. But you can only optimize for the first ten minutes of using a language up to a point. The real question is, how hard is it to teach/learn how it works, and which design will save time in the long run (by making the language simpler, making it easier to develop tooling, etc.)?"
technical,"How many mailing-list complaints and github issues have been filed about this by upset users?  Zero, by my count.   Why?  Probably because this behavior is **fundamentally unsurprising** to people ” if you work in global scope, you depend on global state. I think this is a false equivalence ” there is a **vast disparity in the level of potential confusion** here.  In Julia 0.6, I could explain your example to a student in seconds: ""Oh, see this loop depends on a, which you changed here.""   In Julia 1.0, I'm honestly worried about what I will do if I'm in the middle of a linear-algebra lecture and have to mysteriously type a global keyword in front of students who have never heard the word ""scope"" in the CS sense. Absolutely not. Do you seriously want to go back to the pre-v0.2 world (see #1571 and #330) of loop scope?  We have actually never fully supported copying and pasting code from a function line-by-line into the REPL. So we can view this as an opportunity to make that work. Specifically, while it ""worked"" for for loops, it did not work for inner functions:  Inside a function, f will mutate the x from the first line. In the REPL it won't. But with a transformation like that in SoftGlobalScope.jl it could work. Of course, we probably wouldn't want that on by default since then pasting stand-alone function definitions wouldn't work. The first thing that comes to mind is a REPL mode for line-by-line function debugging."
technical,"Unfortunately for those of us who can not handle living on the edge, its brand-new from our perspective. And for those of us who have been encouraged to stay off the development branches, ""it's brand-new from our perspective."""
technical,"Some thoughts I wrote down last night while trying to wrap my head around this issue (yet again) to try to figure out what the best course of action might be. No conclusion, but I think this lays out the problem quite clearly. After having thought about this issue for some years, I don't think there is any ""ideal solution"" this may be one of those problems where there are only suboptimal choices.  People naively view global scope as a funny kind enclosing local scope. This is why global scopes worked the way they did in Julia 0.6 and prior:  - If an outer local scope creates a local variable and an inner local scope assigns to it, then that assignment updates the outer local variable. - If an outer global scope creates a global variable and an inner local scope assigns to it, then that assignment previously updated the outer global variable.  The main difference, however, is:  - Whether an outer local variable exists, by design, does not depend on the order of appearance or execution of the expressions in the outer local scope. - Whether a global variable exists, however, cannot be independent of order, since one evaluates expressions in global scope, one at a time.  Moreover, since global scopes are often quite lengthy”not infrequently spread across multiple files”having the meaning of an expression depend upon other expressions an arbitrary distance from it, is a spooky action at a distance effect, and as such, quite undesirable.  This last observation shows why having the two different versions of a for loop at global scope behave differently is problematic. Also note that the contents of file1.jl and file3.jl are identical and we could simplify the example by including the same file twice with a different meaning and behavior each time. Another problematic case is a long-running REPL session. Try an example from somewhere online? It fails because you happened to have a global variable by the same name that the example uses for a local variable in a for loop or similar construct. So the notion that the new behavior is the only one that can cause confusion is definitely not accurate. I agree that the new behavior is a usability issue in the REPL but I just want to temper the conversation and present the other side clearly here."
technical,"suggested here that one possibility to solve this issue is automatic wrapping of REPL entries in let blocks But wouldn't that be confusing in that you couldn't do and use a after that? Unless global is inserted for all the toplevel assignments, I guess?"
technical,"Okay, I've figured out how to implement a globalize include string function that preserves line-number information, and have added it to my gist.  A possible (non-breaking) way forward, if people like this approach: 1. Release a SoftGlobalScope.jl package with the globalize etc. functions. 2. Use SoftGlobalScope in IJulia (and possibly Juno, vscode, and OhMyREPL). 3. Fold the SoftGlobalScope functions into a future release of the REPL stdlib package and use it in the REPL.  Or is it practical to roll it into REPL.jl immediately?  I'm not completely clear on how stdlib updates work in 1.0. Please take a look at my implementation, in case I'm missing something that will cause it to be fragile. Can't we have it as a non-default feature of the REPL in 1.1?"
technical,"In my ""spare time"", I've been adding scope, scoping, and global-variables tags to the SE questions.  I only stop because of lack of time, not because there aren't more. Conclusion after much discussion including triage: we're going to include something along the lines of SoftGlobalScope in Base and use it in the REPL and all other interactive evaluation contexts. JeffBezanson has pointed out that the way this is implemented is actually essentially the same as how soft scope was previously implemented, so to some extent we're coming around full circle. The difference is that now there is no scope behavior in modules or scripts, only in REPL-like contexts. I also think that  explaining  soft scope as a source rewrite is clearer than trying to distinguish between hard and soft scopes (which we never how Jeff explained it, I might point out)."
technical,"The number of complaints I saw about that in practice (zero) are certain to be dwarfed by the number of complaints and confusion you will see (and are already seeing) about the current behavior. Do you mean that in the below code, a changes between the first and second for loops? In my mind, that's expected behavior, not a bug."
technical,"That.  The other one to seriously consider is Juno.  Remember that people will <shift-enter through their code to do interactive development(especially when working with the regression tests) and then later expect to be able to run the same file.   Should it matter if the code is in a testset or not (which I think might introduce a scope)?  It would be very confusing to the user if the same text changes when in a testset vs. not when using Atom's integration, and is inconsistent with doing ] test as well. It sure sounds to me like the best solution is that the hard-scope is simply an opt-in thing, where if every other usage (including include within scripts) uses softscope unless you say otherwise. Do you want to write var x = 0 to introduce every variable? That would also ""fix"" this, and be more like other languages.That is not how this works. You can't get any change to the language you want just by calling the current behavior a bug.  I reeeally don't think there should be a command line option for this. Then every piece of julia code will have to come with a comment or something telling you which option to use. Some kind of parser directive in a source file would be a bit better, but even better still would be to have a fixed rule. For example, hard scope inside modules only might make sense. Let me try again to provide an explanation of this that might be useful for avoiding the mania, hysteria, and carnage people are seeing in the classroom: ""Julia has two kinds of variables: local and global. Variables you introduce in the REPL or at the top level, outside of anything else, are global. Variables introduced inside functions and loops are local. Updating global variables in a program is generally bad, so if you're inside a loop or function and want to update a global, you have to be explicit about it by writing the global declaration again."" Perhaps that can be improved, suggestions welcome. I know, you'd rather not need any sort of explanation at all. I get that. But it doesn't seem so bad to me."
technical,"Can't we have it as a non-default feature of the REPL in 1.1? Duplicate of #28523 and #28750. To those saying they don't want to teach people about global variables, I suggest teaching functions first, before for loops. Functions are more fundamental anyway, and this will help set the expectation that code should be written in functions. While I understand the inconvenience, this scoping behavior can be turned into a pedagogical advantage: ""In fact, global variables are such a bad idea, particularly using them in loops, that the language makes you bend over backwards to use them.""  Adding a non-default feature to the REPL for this seems ok to me though."
technical,"SoftGlobalScope.jl is now a registered package.  My intention is to enable it by default (opt-out) for IJulia, at least this semester. even your ""counter-example"" is about someone confused by *hard* scope, not by soft scope.  Making more scopes ""hard"" in 0.7 is certain to create more of this sort of confusion."
technical,"Pop quiz: in julia 0.6, is x global or local: The answer is that there's no way to know, because it depends on whether a global x has been defined before. Now, you can say for sure that it is local. Folks, this discussion is verging on no longer being productive. Jeff knows very well that the old behavior was nice in the REPL. Who do you think designed and implemented it in the first place? We have already committed to changing the interactive behavior. A decision still needs to be made about whether a ""script"" is interactive or not. It sounds interactive when you call it ""a script"" but it sounds far less interactive when you call it ""a program""”yet they are exactly the same thing. Please keep the replies short and constructive and focused on the things which still must be decided. If there's comments that deviate from this, they may be hidden and the thread may be locked."
technical,"remember that many of us would like to use Julia as a substitute for Matlab etcetera in technical courses like linear algebra and statistics. These are not programming courses and the students often have no programming background. We never do structured programming ” it's almost all interactive with short snippets and global variables.  Having taught courses using Julia to students with prior exposure to Matlab/R/..., I sympathize with this concern. But at the same time, I don't think that using Julia just as a Matlab etc substitute is a viable approach: as demonstrated countless times by questions on Discourse and StackOverflow, this can lead to performance pitfalls that are difficult to fix and understand, possibly entailing an even larger cost than investing in understanding how Julia is different from these other languages (cf posts with topics ""I translated this code from Matlab and it is 10x slower"").  I think that the key issue is the *silent* failure, the issue *per se* is easy to understand and fix. I would suggest keeping the new behavior, but giving a warning in Main (by default, it should be possible to disable it). For me, the larger issue is the perceived inconsistency. That is, I'm ok with Julia doing things differently, but: - Why should code pasted from a function not work in a REPL? - No other language I've ever used has this behavior, and it's another barrier to adoption - Why do for blocks behave differently from begin and if blocks? (if I sort of understand, but a block is [should be] a block.).  Regarding bullet 2, I think this is a bigger deal than we who have been using Julia for a while (and are committed to the language) might understand. I can tell you I'm currently 0 for 7 in convincing my group to write code in Julia, two of those were due to this for loop problem which I couldn't explain because I hadn't been exposed to it before. The remainder I guess we can chalk up to my lack of charisma.  My preference would be to ensure that code pasted from a function into a REPL behaves identically to the function, and that for loops do the expected thing when using them to analyze data interactively, that is, specifically, that they mutate external / global variables when directed without any special keywords."
technical,"We'll have to consider the script mode choice. For that matter, it's not crazy to me to default to --softscope=yes for any ""script"", i.e. for julia foo.jl, and only turn on the ""hard"" scoping rules for modules and include (at which point you should really be putting most code into functions)."
technical,"I for one would like to see the experiments with let blocking. This would keep the ""you didn't really want so many globals"" aspect of it, along with the simplified scoping explanation, while at the same time make REPL code behave like function interiors (which is seemingly what we've always wanted). If we're going for ""REPL is the same as the inside of a function"" we should also think about outer. Frankly, I'm pretty annoyed that people are only giving this feedback now. This has change has been on master for ten months. People haven't been using master for interactive use or for teaching, they've been using it to upgrade packages, which are only minimally affected by this and are mostly written by experienced programmers. (I was one of the few people who did give feedback in #19324, though, where I argued for the old behavior.) A non-breaking way out of this would be to change back to the old behavior (ideally not by inserting implicit let blocks or anything ” just restore the old code in julia-syntax.scm as an option) in the REPL.  Or rather, to make it available in environments like IJulia that might want it, add a soft global scope=false flag to include, include string, and Core.eval to restore the old behavior."
technical,"I think as long as we all agree --- as we seem to --- that this is mostly or entirely an issue for interaction, then we have a way forward. If we special-case this in the REPL (as is being done for IJulia), the only bad case is developing something in the REPL and then moving it to top-level script code. Arguably that's the point where you should introduce functions, so I don't think it's so bad. Copy-pasting code between the REPL and function bodies will work (mostly), which is probably good enough. Then we also have the option of further justifying/clarifying the distinction by making REPL variables somehow local to the REPL --- i.e. not normal global variables, not available as Main.x. This is very similar to what StefanKarpinski just proposed above, but shared among all input blocks/cells. From a practical point of view, getting this ""fixed"" in the REPL is not only important for teaching/non-programmer users. This behaviour also makes interactive debugging via the REPL (by copy-pasting parts) very unpractical. This mode of debugging can sometimes be preferable (even to a good debugger and) even for experienced programmers (and is often one of the reasons to prefer a dynamic language). Of course for the experienced programmers, being optional shouldn't be a problem, For novice users it would be preferably the default. As a naive programmer, I don't really see what is so wrong in viewing the global scope as a funny kind enclosing local scope, especially in dynamic languages. I do understand that from a compiler point of view it is not necessarily correct (in Julia), but it is a nice, easy and useful model for a (naive) programmer. (I also suspect it might be actually implemented that way in some languages). Julia also seems to presents it that way to the programmer: The following function function will give the error ""a not defined"", which it will not do if a=1 is put before the for loop.  which, unless I complete misunderstood, seems at odds with ""Whether an outer local variable exists, by design, does not depend on the order of appearance or execution of the expressions in the outer local scope"".  I very much agree with avoiding ""spooky action at a distance"", and much prefer explicit definition for using globals at the function/call stack level and would personally also like having something like loading from a file in its own scope, and requiring explicit definition for using global variables. At the level of loops is going a bit to far for me though, as the definitions/context is usually quite near. The 3 files example is a bit contrived (and fails with the expected ""a not defined"" error): You would normally put the initial definition in the same file. There is actual spooky danger in this (and I have been bitten by it in other languages) in that includes are run in the global scope, so you are inadvertently defining a global variable that may interfere with other code. However, having to use global in the loop is not a solution to this problem.  wrt to the long-running REPL session: The current behaviour replaces a very rare and easy to spot failure mode for running an online example in the REPL (you miss copy/pasting the initial definition of the variable before the loop, and already have the same variable defined globally from something previous) with not being able to run an online example correctly if it is part of a function (without adding global everywhere), and not solving the problem if it is not (if global is already there in the online code, you will still use the wrong value in the already existing global variable)"
technical,"I'd rather focus this discussion on things that we can do *before* Julia 2.0.   I agree that having interactive mode behave differently is only a stopgap, though, and we should seriously contemplate changing the scoping rules in a few years. Good point, this also needs import Future.scope. (I think this module/namespace/behavioral effect already is reserved/exists). As a reminder here, the change was to ensure that code behaves the same in all global scope environments, regardless of what else had previously been evaluated in that module. Before this change, you could get completely different answers (resulting from different scope assignment) simply by running the same code twice or by moving it around in a file."
technical,"(I was one of the few people who did give feedback in #19324, though, where I argued for the old behavior.)  Yes, and I greatly appreciate it. It doesn't much matter now since we made the choice, let it bake for ten months and have now released it with a long-term commitment to stability. So the only thing to do now is to focus on what to do going forward.  Having an option to choose between the old behavior and the new one is interesting but it feels very hacky. That means we not only sometimes have a scoping behavior that everyone apparently found incredibly confusing, but we don't always have it and whether we have it or not depends on a global flag. That feels pretty unsatisfactory, I'm afraid. Having an option to choose between the old behavior and the new one is interesting but it feels very hacky. If someone implements an ""unbreak me"" soft-scope AST transformation, it will be very tempting to use it in IJulia, OhMyREPL, etcetera, at which point you get the even more problematic situation in which the default REPL is seen as broken."
technical,"That could be done as this, that transforms an expression by automatically annotating any globals which exist in the module as global if they are assigned inside of any top-level non-function scope.  I actually started looking into implementing something like this a few minutes ago.  However, it looks like it would be *much* easier to implement as an option in julia-syntax.jl:  * Writing an external AST transformation is possible, but it seems like there are lots of tricky corner cases ” you basically have to re-implement the scoping rules ” whereas we already had the code to get it right in julia-syntax.scm. * It's even more tricky for something like IJulia that currently uses include string to evaluate a whole block of code and get the value of the last expression.  Not only would we have to switch to parsing expression by expression, but some hackery may be needed in order to preserve the original line numbers (for error messages etcetera).  (Though I found a hack for ChangePrecision.jl for this sort of thing that may work here also.) * Not to mention of the case of people that include external files, which would not be caught by your AST transformation.  However, it seems easier to explain in terms of the new simpler scoping rules + a transformation that takes REPL-style input and transforms it before evaluating it. I seriously doubt this would be easier to explain to new users than just saying that the rules are less picky for interactive use or for include with a certain flag. Here is a rough draft of a globalize."
technical,"My small suggestion, that does not deal with the repl problem, but would be useful for didactic purposes when teaching the language not-interactively, at least: define a main block named ""program"", like can be done in fortran (it is the same as the ""let...end"" above, just with a more natural notation): one could teach the language without going into the scope details and only eventually discuss that point. How many mailing-list complaints and github issues have been filed about this by upset users?  Zero, by my count.   Why?  Probably because this behavior is **fundamentally unsurprising** to people ” if you work in global scope, you depend on global state. I think this is a false equivalence ” there is a **vast disparity in the level of potential confusion** here.  In Julia 0.6, I could explain your example to a student in seconds: ""Oh, see this loop depends on a, which you changed here.""   In Julia 1.0, I'm honestly worried about what I will do if I'm in the middle of a linear-algebra lecture and have to mysteriously type a global keyword in front of students who have never heard the word ""scope"" in the CS sense."
technical,"I very much appreciate this sentiment and for my use cases it would really help. From my perspective it is really about making the REPL as useful as possible rather than changing the scoping rules of the language directly. That said, the more I think about this problem the more I see the conflicting views I (personally) hold as to what the REPL should do. To be concrete, I'd very much like it if the REPL matched the scoping rules of a function body, i.e., variables are local rather than global and you can just copy-and-paste code directly from a function and know that it will work. I imagine a naive implementation would be something like let-block wrapping (as has been mention previously) of the form to be transformed into this.  Done properly (i.e., by someone who knows what they are doing), I imagine that this would have a number of benefits over the existing REPL. 1. previous workflows with interactive data analysis/computation just work. 2. far fewer posts on Discourse where the basic response is ""stop benchmarking with global variables"" - everything would be local and so hopefully fast! :) 3. copy-and-paste to/from a function body works as expected. 4. a workspace() like function is trivial if the backing store is some sort of Dict, just clear it out. 5. globals become explicit - things are local unless you specifically ask for them to be global, this is a big advantage from my perspective, I don't like implicitly creating globals. A very minor final point (and I hestiate to add this!), this would match the behaviour of Matlab making it easier for people transitioning - at the Matlab REPL all variables seem to be local unless explicitly annotated as global.  Until a few hours ago this story sounded great to me. But after Jeff's comment about functions I thought about pasting in stand-alone function definitions and how this approach would basically prevent that since function definitions should go in the global scope (at least, that is probably what is intended), but then what if they *were* intended to go into the local scope (an inner function)? There is no information to disambiguate the two possibilities. It would seem that two REPL modes are needed, one with local scope and one global scope. On one hand that could be very confusing (imagine the Discourse posts...) but on the other it could be extremely useful. (Having both REPL modes would also be non-breaking since you are just introducing new functionality :) )  Going for the halfway house of SoftGlobalScope.jl might end up being the least confusing compromise but my worry is that it's just another set of rules to remember (which things work in the REPL but not in my function body/global scope and vice-versa). Apologies for the long post but I think this is important for usability (and it helped me think it through!). How many mailing-list complaints and github issues have been filed about this by upset users? Zero, by my count. Why? Probably because this behavior is fundamentally unsurprising to people ” if you work in global scope, you depend on global state.  Hmm, did you really make a systematic study of this? I must have missed that. Nevertheless, this does not mean that this behavior is not a source of bugs or unexpected results, just that after the user has figured it out, it was recognized as correct behavior and thus did not prompt an issue/complaint. I sympathize with this problem. When I taught into some simple programming to econ students necessary for a course, I usually suggested that they go back and forth between wrapping code in functions, and simply commenting out function and end and running things in the global scope, so they could inspect what is happening. This pretty much made up for the lack of debugging infrastructure at that time in Julia.  It appears this approach is no longer feasible. But I wonder if it was really the right way to do it anyway, and in the meantime various things have improved a lot (#265 was fixed, Revise.jl and recently Rebugger.j.  It seems that this issue does not bother experienced users very much, the main concern is confusion in a pedagogical setting. I have not experimented with this myself yet, but I wonder if we could adapt our approaches to teaching instead, eg introduce functions before loops, avoid loops in global scope. These are elements of good style anyway and would benefit students."
technical,"Sorry, but this argument is ridiculous to me.   I'm not talking about classes where I'm teaching programming.  There's a place for simple interactive computations, and in non-CS classes it's common to be introduced to programming languages as a ""glorified calculator"" to start with.  Teaching performance computing in Julia is an entirely different process but it doesn't hurt if they've already been using Julia as their ""calculator.""  If you start by introducing students to Matlab as their ""calculator,"" it's much harder to make the transition to ""real"" programming, because their first instinct is to do as much as possible with Matlab before jumping ship, at which point their bad habits are ingrained and they are reluctant to learn a new language.  In contrast, if you start with Julia as your glorified calculator, when it comes time to do more serious programming you have a much wider array of options available.  You don't have to train them to cram everything into ""vector"" operations or force them to do things badly before they do it right.  Are you saying I shouldn't use Julia in my linear-algebra course?   Or that I should only use it if I'm prepared to teach computer science as well as linear algebra? I agree with stevengj both on the problem (teaching to non programmers becomes much harder) and on the solution (make things work in the REPL and the various IDEs). Including a script would still have the Julia 1.0 scoping rules but that's less of a concern, one just has to be careful to have the ""we can put our for loop in a function and then call the function"" class before the ""we can put our for loop in a file and include the file"" class.  This sounds like a good compromise as interactive debugging at the REPL doesn't become more painful than it needs to be (or more confusing to new users), while normal code in scripts has to follow strict scoping rules and is safe from bugs overwriting some variables accidentally."
technical,"(in agreement with much of the above about modifying the behavior of the REPL) I'd like to see the REPL be in a way that does not lead to this stackoverflow question and sooner would be best as many new eyes are looking at Julia I agree... And also think that the scoping rules shouldn't necessarily change, just all of the interactive interfaces (i.e. the REPL, Jupyter, and Juno control enter). This is not just about beginners learning a new rule. If you can't copy and paste fragments of code into the REPL, jupyter etc and also into functions, it is a major annoyance for intermediate programmers as well. Of course, I also agree with the other posters... with beginners they going to take code fragments they see within functions, copy I into scripts, and be completely confused when it doesn't have the same behaviour when copied inside of a function, in juno, the repl, and jupyter. There will be 100 stack exchange questions which come down to the same issue. Intermediate programmers are going to have all sorts of homegrown solutions with wrapping in let blocks, etc which will confuse things further"
technical,"A colleague that I was trying to get to learn julia also just ran into this. Having to explain the whole global vs. local variable thing at the first steps is not ideal... I don't think treating this as a ""bugfix"" is in the cards, because it would break the 1.0 stability contract.   However, it seems reasonable to me to use softscope for scripts run with julia -i (i.e. ""interactive"" mode). (That is, there would be a flag --softscope={yes|no} and it would default to the value of isinteractive.)"
technical,"I'm not so sure I like the idea that the REPL should behave like a function interior.  It clearly isn't, so I expect it to behave like global scope.  To me the REPL not behaving like global scope would be potentially even more confusing than the discrepency that causes this issue. Regardless, at the very least I think that the documentation should be somewhat more explicit about this issue.  Casually reading the docs I would have assumed that you would need to use the local keyword to get the behavior occurs in global scope by default. I for one would like to see the experiments with let blocking. This would keep the ""you didn't really want so many globals"" aspect of it, along with the simplified scoping explanation, while at the same time make REPL code behave like function interiors (which is seemingly what we've always wanted). If we're going for ""REPL is the same as the inside of a function"" we should also think about outer."
technical,"No, I want to go back to the 0.6 world. I guess I was responding more to: One other fix not mentioned here is to simply stop making ˜for' define a scope-block"
technical,"I'm specifically referring to people complaining that a global loop depends on global state.  I don't recall anyone complaining that this was bad behavior, nor can I find any examples of this. I agree that people have been confused about when and where assignment defines new variables, but it has usually been in the other direction ” they wanted local scopes to act more global (rather than vice versa), or to not have a distinction between begin and let.   IIRC, the complaint was never that assigning to a global variable in a global loop had the surprising side effect of modifying a global. The whole issue of scoping is confusing to new users, and it will continue to be so.  But the confusing part was not cases where assigning to a global variable name affected the global state.  The current behavior makes this worse, not better. I have the feeling that previously, the confusion with soft/hard scope was more of a theoretical nature (of people reading the manual) rather than of a practical on (of people getting unexpected results).  That was definitely how it was for me and what, e.g., the search results here support this, I found one counter-example here. On the other hand, this new behavior will not confuse people when reading the manual, but when using the REPL.  Arguably the latter is worse."
technical,"Do you want to write var x = 0 to introduce every variable? That would also ""fix"" this, and be more like other languages.That is not how this works. You can't get any change to the language you want just by calling the current behavior a bug.  I reeeally don't think there should be a command line option for this. Then every piece of julia code will have to come with a comment or something telling you which option to use. Some kind of parser directive in a source file would be a bit better, but even better still would be to have a fixed rule. For example, hard scope inside modules only might make sense. Let me try again to provide an explanation of this that might be useful for avoiding the mania, hysteria, and carnage people are seeing in the classroom: ""Julia has two kinds of variables: local and global. Variables you introduce in the REPL or at the top level, outside of anything else, are global. Variables introduced inside functions and loops are local. Updating global variables in a program is generally bad, so if you're inside a loop or function and want to update a global, you have to be explicit about it by writing the global declaration again."" Perhaps that can be improved, suggestions welcome. I know, you'd rather not need any sort of explanation at all. I get that. But it doesn't seem so bad to me. I reeeally don't think there should be a command line option for this. Then every piece of julia code will have to come with a comment or something telling you which option to use. Some kind of parser directive in a source file would be a bit better, but even better still would be to have a fixed rule. I agree.  Sounds like a teaching and communication headache to me. Just so I understand: if I had a short script (not in a module!) in a .jl file which I had copied from an IJulia notebook, then if I ran that code in either the REPL directly or shift-enter in Juno, then it would behave consistently as soft-scope... but if I copied it instead of a module block then it would yell at me about globals?  But if I copied that code inside of functions inside of a module, then it should work.  If so, that makes complete sense,is very teachable and coherent.  Top-level scripts are an interactive interface for exploration, etc. but you would never put that kind of code in a module.  Modules are something that you should fill with functions are very carefully considered globals.  It would be easy to tell people about those rules."
technical,"From a practical point of view, getting this ""fixed"" in the REPL is not only important for teaching/non-programmer users. This behaviour also makes interactive debugging via the REPL (by copy-pasting parts) very unpractical. This mode of debugging can sometimes be preferable (even to a good debugger and) even for experienced programmers (and is often one of the reasons to prefer a dynamic language). Of course for the experienced programmers, being optional shouldn't be a problem, For novice users it would be preferably the default. As a naive programmer, I don't really see what is so wrong in viewing the global scope as a funny kind enclosing local scope, especially in dynamic languages. I do understand that from a compiler point of view it is not necessarily correct (in Julia), but it is a nice, easy and useful model for a (naive) programmer. (I also suspect it might be actually implemented that way in some languages). Julia also seems to presents it that way to the programmer: The following function function will give the error ""a not defined"", which it will not do if a=1 is put before the for loop.  which, unless I complete misunderstood, seems at odds with ""Whether an outer local variable exists, by design, does not depend on the order of appearance or execution of the expressions in the outer local scope"".  I very much agree with avoiding ""spooky action at a distance"", and much prefer explicit definition for using globals at the function/call stack level and would personally also like having something like loading from a file in its own scope, and requiring explicit definition for using global variables. At the level of loops is going a bit to far for me though, as the definitions/context is usually quite near. The 3 files example is a bit contrived (and fails with the expected ""a not defined"" error): You would normally put the initial definition in the same file. There is actual spooky danger in this (and I have been bitten by it in other languages) in that includes are run in the global scope, so you are inadvertently defining a global variable that may interfere with other code. However, having to use global in the loop is not a solution to this problem.  wrt to the long-running REPL session: The current behaviour replaces a very rare and easy to spot failure mode for running an online example in the REPL (you miss copy/pasting the initial definition of the variable before the loop, and already have the same variable defined globally from something previous) with not being able to run an online example correctly if it is part of a function (without adding global everywhere), and not solving the problem if it is not (if global is already there in the online code, you will still use the wrong value in the already existing global variable) I should have tuned into this earlier, but after a brief moment of concern all seems to be well.   We have actually never fully supported copying and pasting code from a function line-by-line into the REPL...The first thing that comes to mind is a REPL mode for line-by-line function debugging. Indeed Rebugger (which is exactly that) works properly on 1.0 only because it lacks the scope deprecation of 0.7, and could never be made to work on 0.6. However, I'm pleased to be able to verify that SoftGlobalScope.jl seems not to break that. For example, if you step deeply enough into show([1,2,4]) you get here. So it works fine on 1.0 (with or without softscope). On 0.7, evaluating this (with or without softscope) will yield julia So 0.7/1.0 are definitely a step forward, and if softscope makes certain things easier without breaking important functionality that's great.  The biggest concern, therefore, is simply how to intercept this appropriately without tanking other packages."
technical,"Do you mean that in the below code, a changes between the first and second for loops? In my mind, that's expected behavior, not a bug. I suppose adding a function can be done with just a minor version bump?  We can also warn exec'ing another file from exec'ed script and then put some pedagogical messages there to nudge them to use include."
technical,"I think this would be even more confusing, and would run counter to how notebooks are normally used.  It is common for the same variable to be used/modified in multiple cells, so requiring a global keyword for *all* inter-cell variables is a nonstarter to me ” it would require even *more* discussion of scope concepts than the issue with for loops we've been discussing here. I think as long as we all agree --- as we seem to --- that this is mostly or entirely an issue for interaction, then we have a way forward. If we special-case this in the REPL (as is being done for IJulia), the only bad case is developing something in the REPL and then moving it to top-level script code. Arguably that's the point where you should introduce functions, so I don't think it's so bad. Copy-pasting code between the REPL and function bodies will work (mostly), which is probably good enough. Then we also have the option of further justifying/clarifying the distinction by making REPL variables somehow local to the REPL --- i.e. not normal global variables, not available as Main.x. This is very similar to what StefanKarpinski just proposed above, but shared among all input blocks/cells."
technical,"I would point out that IJulia has the interesting possibility of making variables local do blocks by default. I.e. if you do this in a single block then it works:  ... and t is only visible within this evaluation block. If you wanted it to be visible outside, you would have to do this:  I have also considered a similar approach for Julia where the blocks are files rather than module. In other words just doing t = 0 at top scope creates a variable that is file-local rather than global. To declare a truly global variable, you'd need to write global t = 0 which would then be visible throughout the module. Perhaps too weird, but it has occurred to me many times over the years. I think this would be even more confusing, and would run counter to how notebooks are normally used.  It is common for the same variable to be used/modified in multiple cells, so requiring a global keyword for *all* inter-cell variables is a nonstarter to me ” it would require even *more* discussion of scope concepts than the issue with for loops we've been discussing here."
technical,"I guess I was responding more to: One other fix not mentioned here is to simply stop making ˜for' define a scope-block I very much appreciate this sentiment and for my use cases it would really help. From my perspective it is really about making the REPL as useful as possible rather than changing the scoping rules of the language directly. That said, the more I think about this problem the more I see the conflicting views I (personally) hold as to what the REPL should do. To be concrete, I'd very much like it if the REPL matched the scoping rules of a function body, i.e., variables are local rather than global and you can just copy-and-paste code directly from a function and know that it will work. I imagine a naive implementation would be something like let-block wrapping (as has been mention previously) of the form to be transformed into this.  Done properly (i.e., by someone who knows what they are doing), I imagine that this would have a number of benefits over the existing REPL. 1. previous workflows with interactive data analysis/computation just work. 2. far fewer posts on Discourse where the basic response is ""stop benchmarking with global variables"" - everything would be local and so hopefully fast! :) 3. copy-and-paste to/from a function body works as expected. 4. a workspace() like function is trivial if the backing store is some sort of Dict, just clear it out. 5. globals become explicit - things are local unless you specifically ask for them to be global, this is a big advantage from my perspective, I don't like implicitly creating globals. A very minor final point (and I hestiate to add this!), this would match the behaviour of Matlab making it easier for people transitioning - at the Matlab REPL all variables seem to be local unless explicitly annotated as global.  Until a few hours ago this story sounded great to me. But after Jeff's comment about functions I thought about pasting in stand-alone function definitions and how this approach would basically prevent that since function definitions should go in the global scope (at least, that is probably what is intended), but then what if they *were* intended to go into the local scope (an inner function)? There is no information to disambiguate the two possibilities. It would seem that two REPL modes are needed, one with local scope and one global scope. On one hand that could be very confusing (imagine the Discourse posts...) but on the other it could be extremely useful. (Having both REPL modes would also be non-breaking since you are just introducing new functionality :) )  Going for the halfway house of SoftGlobalScope.jl might end up being the least confusing compromise but my worry is that it's just another set of rules to remember (which things work in the REPL but not in my function body/global scope and vice-versa). Apologies for the long post but I think this is important for usability (and it helped me think it through!)."
technical,"What is the time-line on this?  It seems it would be a great improvement to user usability.  And at this ""critical"" time of Julia with 1.0 out, it would seem advantageous to get this fixed asap (in the way suggested by Jeff above) and tag a new Julia version or REPL version.  (Sorry for this arm-chair comment, as I certainly will not fix this!) I was going to argue that while this is true (for the implementation/compiler), the naive julia programmer cannot not see any different behaviour from his simpler conceptual model (a variable starts existing at the moment it is defined). Unfortunately you are right, the following code will not give an error, while it will give an error if you leave out the a=2 at the end. I'll explain the unfortunately: I can understand the behaviour (because i've worked with compiled languages before) but still find it confusing and unexpected. How bad must it be to someone with only scripting experience or new to programming. Also, I found some code that shows the behaviour, I do not see a useful application (maybe you can help me there)  On the REPL: I just got more convinced that changing the scoping back to ""normal"" at  least in the REPL (no need to add global in loops) is high priority: I was testing some things in the REPL today and got (again) bitten by it, taking some time to realize it. Given that I follow Julia already some time, really like a lot of it, am even following this thread about the problem, I would even call it a showstopper: A newbee (to the Julia) testing out the language is very likely not to find out the problem and just give up."
technical,"If we did something like this it seems like it might make sense for the module to determine how it works. So Main would be a ""soft scope"" module while by default other modules would be ""hard scope"" modules. I was interested to see if it was possible to monkey patch the REPL to use stevengj's globalize function and it appears it is without too much effort (though quite hacky). See the gist. This doesn't work with Juno (or anything else that calls Core.eval directly). I'm **not** going to be recommending this to people, but it's quite useful to me when doing quick-and-dirty data analysis. I would very much like to see a (better thought out) solution since it really is quite confusing for inexperienced and often reluctant coders (i.e., my students) when you can't copy and paste in code from a function into the REPL to see what it does and vice-versa. (BTW: the above is about as much testing as it has had!)"
technical,"Just a wee note: whilst special casing the global scope of the REPL, will allow copy-pasting code into and from functions, it will not allow copy-pasting into/from the global scope of another module. I wonder if we could adapt our approaches to teaching instead, eg introduce functions before loops, avoid loops in global scope.  This is totally impractical in a class that is not focused on teaching programming.   I might as well not use Julia in my classes if I can't use it interactively and/or have to write functions for everything first.  (And it's not just pedagogical.  Loops in global scope are *useful* for interactive work.  And one of the main reasons people like dynamic languages for technical computing is their facility for interactive exploration.  Not all coding is performance-oriented.)"
technical,"even your ""counter-example"" is about someone confused by *hard* scope, not by soft scope.  Making more scopes ""hard"" in 0.7 is certain to create more of this sort of confusion. I would point out that IJulia has the interesting possibility of making variables local do blocks by default. I.e. if you do this in a single block then it works:  ... and t is only visible within this evaluation block. If you wanted it to be visible outside, you would have to do this:  I have also considered a similar approach for Julia where the blocks are files rather than module. In other words just doing t = 0 at top scope creates a variable that is file-local rather than global. To declare a truly global variable, you'd need to write global t = 0 which would then be visible throughout the module. Perhaps too weird, but it has occurred to me many times over the years."
technical,"One other fix not mentioned here is to simply stop making ˜for' define a scope-block (just function and let would create new scope) I'd rather focus this discussion on things that we can do *before* Julia 2.0.   I agree that having interactive mode behave differently is only a stopgap, though, and we should seriously contemplate changing the scoping rules in a few years."
technical,"So you would turn a = 1 into something like a = let a, a = 1, end. And something like this would be turned into this. Frankly, I'm pretty annoyed that people are only giving this feedback now. This has change has been on master for ten months. I'm guilty of not having followed master very closed until recently, so this feedback is indeed a bit late. More than a concern for programmers (most for loops will be inside a function in library code) I'm afraid this is a concern for teaching. Often for loops are taught before functions or scopes (of course you need to understand scopes to really understand what's going on but in teaching things are often simplified). Here it becomes a bit difficult to teach a beginner how to sum numbers from 1 to 10 without explaining functions or global variables."
technical,"Can we please go back to focus on the issue at hand now, instead of having a meta discussion about how long people have had to test this. It is what it is right now, so let's look forward. I'm guilty of not having followed master very closed until recently, so this feedback is indeed a bit late. More than a concern for programmers (most for loops will be inside a function in library code) I'm afraid this is a concern for teaching. Often for loops are taught before functions or scopes (of course you need to understand scopes to really understand what's going on but in teaching things are often simplified). Here it becomes a bit difficult to teach a beginner how to sum numbers from 1 to 10 without explaining functions or global variables.  This is a big point. After finding out what the issue really is, it's surprising how little it actually shows up. It is less of an issue with a lot of Julia code in the wild and in tests, and it did reveal a lot of variables which were accidentally global (in both Julia Base's tests according to the original PR, and I noticed this on most of DiffEq's tests). In most cases it seems that the subtly wrong behavior isn't what you get (expecting a change in a loop), but rather expecting to be able to use a variable in a loop is what I've found to be the vast majority of where this shows up in updating test scripts to v1.0. So the good thing is that in most cases the user is presented with an error, and it's not difficult to fix.  The bad thing is that it's a little verbose to have to put global x inside of the loops, and now your REPL code is also different from the function code. Whether or not it's more intuitive behavior than before is a tough opinion because [there were definitely some edge cases in this) and so this is clearly easier to explain. But at the same time, while having a much more succinct explanation than the behavior of before, it's now easier to hit the edge cases where understanding scoping rules matters. '‚.  I for one would like to see the experiments with let blocking. This would keep the ""you didn't really want so many globals"" aspect of it, along with the simplified scoping explanation, while at the same time make REPL code behave like function interiors (which is seemingly what we've always wanted). Or inversely, making people specify variables they want to act as globals could be a nice way to keep the explicitness, and would make the ""REPL code is slow because of globals"" be much more obvious. The downside is that once again throwing things into a function would not require the global markers.  But given how this tends to show up, it's not really gamebreaking or a showstopper. I'd classify it as a wart  that should get a mention in any workshop but it's not like v1.0 is unusable because of it. I hope that changing this behavior isn't classified as breaking and require v2.0 though."
technical,"I'm guilty of not having followed master very closed until recently, so this feedback is indeed a bit late. More than a concern for programmers (most for loops will be inside a function in library code) I'm afraid this is a concern for teaching. Often for loops are taught before functions or scopes (of course you need to understand scopes to really understand what's going on but in teaching things are often simplified). Here it becomes a bit difficult to teach a beginner how to sum numbers from 1 to 10 without explaining functions or global variables.  This is a big point. After finding out what the issue really is, it's surprising how little it actually shows up. It is less of an issue with a lot of Julia code in the wild and in tests, and it did reveal a lot of variables which were accidentally global (in both Julia Base's tests according to the original PR, and I noticed this on most of DiffEq's tests). In most cases it seems that the subtly wrong behavior isn't what you get (expecting a change in a loop), but rather expecting to be able to use a variable in a loop is what I've found to be the vast majority of where this shows up in updating test scripts to v1.0. So the good thing is that in most cases the user is presented with an error, and it's not difficult to fix.  The bad thing is that it's a little verbose to have to put global x inside of the loops, and now your REPL code is also different from the function code. Whether or not it's more intuitive behavior than before is a tough opinion because [there were definitely some edge cases in this) and so this is clearly easier to explain. But at the same time, while having a much more succinct explanation than the behavior of before, it's now easier to hit the edge cases where understanding scoping rules matters. '‚.  I for one would like to see the experiments with let blocking. This would keep the ""you didn't really want so many globals"" aspect of it, along with the simplified scoping explanation, while at the same time make REPL code behave like function interiors (which is seemingly what we've always wanted). Or inversely, making people specify variables they want to act as globals could be a nice way to keep the explicitness, and would make the ""REPL code is slow because of globals"" be much more obvious. The downside is that once again throwing things into a function would not require the global markers.  But given how this tends to show up, it's not really gamebreaking or a showstopper. I'd classify it as a wart  that should get a mention in any workshop but it's not like v1.0 is unusable because of it. I hope that changing this behavior isn't classified as breaking and require v2.0 though. I'm not so sure I like the idea that the REPL should behave like a function interior.  It clearly isn't, so I expect it to behave like global scope.  To me the REPL not behaving like global scope would be potentially even more confusing than the discrepency that causes this issue. Regardless, at the very least I think that the documentation should be somewhat more explicit about this issue.  Casually reading the docs I would have assumed that you would need to use the local keyword to get the behavior occurs in global scope by default."
technical,"There have been dozens of threads and issues over the years in which people are confused or complaining about the old ""soft/hard scope"" distinction, so claiming that no one has ever been confused by or complained about the old behavior is just... not true. I could dig some of them up, but you were around, stevengj, so you can dig them up just as easily and I have a hard time believing that you didn't notice or don't remember these complaints and conversations. I'm specifically referring to people complaining that a global loop depends on global state.  I don't recall anyone complaining that this was bad behavior, nor can I find any examples of this. I agree that people have been confused about when and where assignment defines new variables, but it has usually been in the other direction ” they wanted local scopes to act more global (rather than vice versa), or to not have a distinction between begin and let.   IIRC, the complaint was never that assigning to a global variable in a global loop had the surprising side effect of modifying a global. The whole issue of scoping is confusing to new users, and it will continue to be so.  But the confusing part was not cases where assigning to a global variable name affected the global state.  The current behavior makes this worse, not better."
technical,"Not to mention that this is someone who clearly knows enough about programming languages to understand the nuances of scope. What about all of the matlab type users that are completely ignorant of these topics..., and probably will never invest enough time to understand the nuances. I've already answered multiple questions related to this on stackoverflow, mostly by new users, and even more in real life (last one just yesterday, from a Matlab user, who saw this as a no go)."
technical,"You may have misunderstood what I was saying (or I did not express it clearly). I was talking about courses that *use* Julia to teach something domain-specific (eg I taught numerical methods to econ grad students), not CS courses (which I have no experience with). The point that I was trying to make is that it is reasonable to expect a certain level of difference between Julia and language X (which may be Matlab), conversely, ignoring this can (and does) lead to problems. Personally, when learning a new language, I prefer to face these issues early on, also, I think simplicity and consistency of the language semantics is more important than similarity to other languages in the long run. But I recognize these preferences as subjective, and reasonable people can have different ones. I've created the (unregistered) package. If this seems reasonable, I can go ahead and register the package and then use it by default in IJulia (and perhaps submit PRs to Juno etcetera)."
technical,"(I guess) scoping rules cannot be changed in scripts because it would be backwards incompatible, i.e. would break the promise that any code written for 1.0 will run on any 1.* version. You are correct though that the same problem with scoping for the REPL also applies to scripts (naive user at a complete loss why his/her code does not work properly when run as a script). A way to solve/alleviate this problem without major incompatibilty would be to add an option to the julia cmdline to use  softscope (or alternative) , e.g. julia -f programfile, and show this option in any description/tutorial that a beginner is likely to come across. I also see a potential alternative for the softscope that may have some advantages (though i am probably overlooking disadvantages): What if a file (a called script) would always introduce its own local scope: scoping rules would be in complete consistency with those in functions, and with the expectations of a lot of users. It would also remove a lot of the performance liabilities with new users: No more unneeded globals (globals would have to be explicitly defined), and code might be compiled (How many times have you had to say to put everything in a function, and to avoid using globals?) I've just hit this and was completely boggled to be honest, having never seen it before in any other language. I'm planning on introducing an optional Julia course for advanced R users in my uni later this year once things have settled down, and my students will hit this on day 0 when they start randomly typing things in the REPL. And the fact that for loops behave differently from if statements just rubs salt in the wound, however logical this may be in terms of scoping. Scope inside functions is sufficiently hard to get biology students to grasp, the idea of having to explain  albeit perceived  glaring inconsistencies in it in the REPL / in a script / in a for loop / in an if statement (because that's what we're talking about here) in a way that is different from every other language on earth makes me very sad.  I understand the backward compatibility promise that was made, but having this work  as expected by every non-cs person on the planet (and most cs people I suspect)  seems like a bugfix rather than a backward compatibility issue - we're not saying that every bug will be reproduced for ever are we? The REPL fix is obviously essential, so it's great that you're proposing this, but then having to explain you can't copy a script into the REPL and expect the same behaviour seems as bad as or worse than the original problem.  Please, please, please think about treating this as a bugfix and pushing it out with scripts as well as the REPL - even if there's an switch to go to the ""old"" behaviour - and doing it as soon as possible in 1.0.1."
technical,"What would the proposed REPL-mode do to includeed scripts?  Would the evaluation of global statements depend on whether the REPL mode is activated?  If so, IMO this would be at odds with the 1.0 stability promise. If we did something like this it seems like it might make sense for the module to determine how it works. So Main would be a ""soft scope"" module while by default other modules would be ""hard scope"" modules."
technical,"What I'm trying to get at is that I feel a simple description of global vs. local is sufficient for early-stage teaching --- you don't even need to say the word ""scope"" (it does not occur at all in my explanation above). When you're just showing some simple expressions and loops in the REPL, you're not teaching people about testsets and you don't need an exhaustive list of the scoping behavior of everything in the language.  My only point is, this change does not suddenly make it necessary to teach lots of details about the language up front. You can still ignore the vast majority of stuff about scopes, testsets, etc., and a simple line on global vs. local should suffice. In a world where everyone started writing all of their code from scratch, I would agree completely.  The issue is that you need to teach students not just about scope, but also about understanding the scope of where they copy-pasted code they got from.  You need to teach them that if they copy-paste code that is on stackexchange within a function or a let block that they need to scan through it and find where to add ""global"" if they are pasting it into the REPL or a .jl file.  But if they are copying that code inside a function or into the Jupyter notebook. they shouldn't.  And if they find code inside of a stackexchange or tutorial page that has global variables in it, but they want to copy and modify that code inside of their own function, then they need to strip out the global.  And then students start asking why does for create this scope they need to worry about but not other things...."
technical,"I've already answered multiple questions related to this on stackoverflow, mostly by new users, and even more in real life (last one just yesterday, from a Matlab user, who saw this as a no go). In my ""spare time"", I've been adding scope, scoping, and global-variables tags to the SE questions.  I only stop because of lack of time, not because there aren't more."
technical,"Duplicate of #28523 and #28750. To those saying they don't want to teach people about global variables, I suggest teaching functions first, before for loops. Functions are more fundamental anyway, and this will help set the expectation that code should be written in functions. While I understand the inconvenience, this scoping behavior can be turned into a pedagogical advantage: ""In fact, global variables are such a bad idea, particularly using them in loops, that the language makes you bend over backwards to use them.""  Adding a non-default feature to the REPL for this seems ok to me though. JeffBezanson, remember that many of us would like to use Julia as a substitute for Matlab etcetera in technical courses like linear algebra and statistics.  These are *not* programming courses and the students often have no programming background.   We never do structured programming ” it's almost *all* interactive with short snippets and global variables.  Furthermore, the reason I'm using a dynamic language in the first place is to switch fluidly between interactive exploration and more disciplined programming.   The inability to use the same code in a global and a function context is a hindrance to that end, even for someone who is used to scoping concepts, and it is much worse for students from non-CS backgrounds."
technical,"How many mailing-list complaints and github issues have been filed about this by upset users? Zero, by my count. Why? Probably because this behavior is fundamentally unsurprising to people ” if you work in global scope, you depend on global state.  Hmm, did you really make a systematic study of this? I must have missed that. Nevertheless, this does not mean that this behavior is not a source of bugs or unexpected results, just that after the user has figured it out, it was recognized as correct behavior and thus did not prompt an issue/complaint. I sympathize with this problem. When I taught into some simple programming to econ students necessary for a course, I usually suggested that they go back and forth between wrapping code in functions, and simply commenting out function and end and running things in the global scope, so they could inspect what is happening. This pretty much made up for the lack of debugging infrastructure at that time in Julia.  It appears this approach is no longer feasible. But I wonder if it was really the right way to do it anyway, and in the meantime various things have improved a lot (#265 was fixed, Revise.jl and recently Rebugger.j.  It seems that this issue does not bother experienced users very much, the main concern is confusion in a pedagogical setting. I have not experimented with this myself yet, but I wonder if we could adapt our approaches to teaching instead, eg introduce functions before loops, avoid loops in global scope. These are elements of good style anyway and would benefit students. Just a wee note: whilst special casing the global scope of the REPL, will allow copy-pasting code into and from functions, it will not allow copy-pasting into/from the global scope of another module."
technical,"Making code harder to write interactively, forcing beginners writing their first loops to understand obscure scoping rules, and making code pasted from functions not work in global scopes does not help programmers write fast code in functions. It just makes it harder to use Julia interactively and harder for beginners. Making an ""unbreak me"" option the default seems wiser, especially an option that is aimed squarely at beginning users.   If it is a non-default option, then precisely those people who need it most will be those who don't have it enabled (and don't know it exists)."
technical,"Many of us Julia users have absolutely 0 CS background (including myself), but it seems to me that the proper attitude (*especially* for students) is a willingness to learn rather than demanding things be changed for the worse to accommodate our naivete. Now, I'm not necessarily implying that this particular change would be for the worse as I only have a limited understanding of what's going on here, but if it *is* the case that this is a significant complication or makes it excessively easy to write needlessly badly performing code it does not seem worth it to make a change in order to have a better lecture example.  You can't change the laws of physics so that the electrostatics examples you show to freshman are more applicable to real life. So my question as a non-CS user who also cares about performance is how would I be likely to screw up if this were made the default behavior.  Is it literally just the sorts of examples we are seeing here that are a problem (which I was already aware of), or are we likely to often screw this up badly in more subtle ways? For what it's worth, I do agree that having code behave differently depending on its enclosing scope is a generally undesirable feature. Making code harder to write interactively, forcing beginners writing their first loops to understand obscure scoping rules, and making code pasted from functions not work in global scopes does not help programmers write fast code in functions. It just makes it harder to use Julia interactively and harder for beginners."
technical,"JeffBezanson, remember that many of us would like to use Julia as a substitute for Matlab etcetera in technical courses like linear algebra and statistics.  These are *not* programming courses and the students often have no programming background.   We never do structured programming ” it's almost *all* interactive with short snippets and global variables.  Furthermore, the reason I'm using a dynamic language in the first place is to switch fluidly between interactive exploration and more disciplined programming.   The inability to use the same code in a global and a function context is a hindrance to that end, even for someone who is used to scoping concepts, and it is much worse for students from non-CS backgrounds. Many of us Julia users have absolutely 0 CS background (including myself), but it seems to me that the proper attitude (*especially* for students) is a willingness to learn rather than demanding things be changed for the worse to accommodate our naivete. Now, I'm not necessarily implying that this particular change would be for the worse as I only have a limited understanding of what's going on here, but if it *is* the case that this is a significant complication or makes it excessively easy to write needlessly badly performing code it does not seem worth it to make a change in order to have a better lecture example.  You can't change the laws of physics so that the electrostatics examples you show to freshman are more applicable to real life. So my question as a non-CS user who also cares about performance is how would I be likely to screw up if this were made the default behavior.  Is it literally just the sorts of examples we are seeing here that are a problem (which I was already aware of), or are we likely to often screw this up badly in more subtle ways? For what it's worth, I do agree that having code behave differently depending on its enclosing scope is a generally undesirable feature."
technical,"Another problematic case is a long-running REPL session. Try an example from somewhere online? It fails because you happened to have a global variable by the same name that the example uses for a local variable in a for loop or similar construct. So the notion that the new behavior is the only one that can cause confusion is definitely not accurate. I agree that the new behavior is a usability issue in the REPL but I just want to temper the conversation and present the other side clearly here. My small suggestion, that does not deal with the repl problem, but would be useful for didactic purposes when teaching the language not-interactively, at least: define a main block named ""program"", like can be done in fortran (it is the same as the ""let...end"" above, just with a more natural notation): one could teach the language without going into the scope details and only eventually discuss that point."
technical,"Absolutely not. Do you seriously want to go back to the pre-v0.2 world (see #1571 and #330) of loop scope?  We have actually never fully supported copying and pasting code from a function line-by-line into the REPL. So we can view this as an opportunity to make that work. Specifically, while it ""worked"" for for loops, it did not work for inner functions:  Inside a function, f will mutate the x from the first line. In the REPL it won't. But with a transformation like that in SoftGlobalScope.jl it could work. Of course, we probably wouldn't want that on by default since then pasting stand-alone function definitions wouldn't work. The first thing that comes to mind is a REPL mode for line-by-line function debugging. No, I want to go back to the 0.6 world."
technical,"the questioner appears to have been confused by it: ""I wonder if this is intuitive to beginning julia users. It was not intuitive to me ..."" Not to mention that this is someone who clearly knows enough about programming languages to understand the nuances of scope. What about all of the matlab type users that are completely ignorant of these topics..., and probably will never invest enough time to understand the nuances."
technical,"I was interested to see if it was possible to monkey patch the REPL to use stevengj's globalize function and it appears it is without too much effort (though quite hacky). See the gist. This doesn't work with Juno (or anything else that calls Core.eval directly). I'm **not** going to be recommending this to people, but it's quite useful to me when doing quick-and-dirty data analysis. I would very much like to see a (better thought out) solution since it really is quite confusing for inexperienced and often reluctant coders (i.e., my students) when you can't copy and paste in code from a function into the REPL to see what it does and vice-versa. (BTW: the above is about as much testing as it has had!) Nothing.   Basically, the proposal is that this would only be for code entered at an interactive prompt.   As soon as you start putting things in files, you need to learn the ""hard scope"" rules.   Hopefully, when you start putting code into files you should start using functions. It's not ideal for there to be pickier scoping rules for global code in files than at the prompt.   But I think that #19324 combined with the Julia 1.0 stability promise leaves us with no ideal options."
technical,"I've created the (unregistered) package. If this seems reasonable, I can go ahead and register the package and then use it by default in IJulia (and perhaps submit PRs to Juno etcetera). Obviously.  When I say ""use Julia instead of Matlab"", I don't mean I'm trying to teach them Matlab syntax in Julia, nor am I specifically targeting former Matlab users. It's not about differences from Matlab per se.  I would really rather not talk about global vs local scope and the utility of a global keyword for static analysis the first time I write a loop in front of non-CS students, or the first time they paste code from a function into the REPL to try it interactively.   I would rather focus on the math I'm trying to use the loop to express. **No one here is arguing for soft interactive scope just because that's what Matlab users expect.  We are arguing for it because that is what *all* first-time users will expect,** and because long **digressions into the unfamiliar concept of ""scope"" are certain to derail** any non-CS lecture where you are showing a loop for the first time.    (And even for experienced users, it's rather inconvenient to be forced to add global keywords when we are working interactively.)"
technical,"Here is a rough draft of a globalize. Okay, I've figured out how to implement a globalize include string function that preserves line-number information, and have added it to my gist.  A possible (non-breaking) way forward, if people like this approach: 1. Release a SoftGlobalScope.jl package with the globalize etc. functions. 2. Use SoftGlobalScope in IJulia (and possibly Juno, vscode, and OhMyREPL). 3. Fold the SoftGlobalScope functions into a future release of the REPL stdlib package and use it in the REPL.  Or is it practical to roll it into REPL.jl immediately?  I'm not completely clear on how stdlib updates work in 1.0. Please take a look at my implementation, in case I'm missing something that will cause it to be fragile."
technical,"Obviously.  When I say ""use Julia instead of Matlab"", I don't mean I'm trying to teach them Matlab syntax in Julia, nor am I specifically targeting former Matlab users. It's not about differences from Matlab per se.  I would really rather not talk about global vs local scope and the utility of a global keyword for static analysis the first time I write a loop in front of non-CS students, or the first time they paste code from a function into the REPL to try it interactively.   I would rather focus on the math I'm trying to use the loop to express. **No one here is arguing for soft interactive scope just because that's what Matlab users expect.  We are arguing for it because that is what *all* first-time users will expect,** and because long **digressions into the unfamiliar concept of ""scope"" are certain to derail** any non-CS lecture where you are showing a loop for the first time.    (And even for experienced users, it's rather inconvenient to be forced to add global keywords when we are working interactively.) One other fix not mentioned here is to simply stop making ˜for' define a scope-block (just function and let would create new scope)"
technical,"Folks, this discussion is verging on no longer being productive. Jeff knows very well that the old behavior was nice in the REPL. Who do you think designed and implemented it in the first place? We have already committed to changing the interactive behavior. A decision still needs to be made about whether a ""script"" is interactive or not. It sounds interactive when you call it ""a script"" but it sounds far less interactive when you call it ""a program""”yet they are exactly the same thing. Please keep the replies short and constructive and focused on the things which still must be decided. If there's comments that deviate from this, they may be hidden and the thread may be locked. One thought that I had but we dismissed as being ""too annoying"" and ""likely to cause the villagers to get out their pitchforks"" was that in non-interactive contexts, we could require a local or global annotation in ""soft scope"". That would guarantee that code from a module would work the same if pasted into the REPL. If we applied that to ""scripts""/""programs"" then the same would be true of them."
technical,"In a world where everyone started writing all of their code from scratch, I would agree completely.  The issue is that you need to teach students not just about scope, but also about understanding the scope of where they copy-pasted code they got from.  You need to teach them that if they copy-paste code that is on stackexchange within a function or a let block that they need to scan through it and find where to add ""global"" if they are pasting it into the REPL or a .jl file.  But if they are copying that code inside a function or into the Jupyter notebook. they shouldn't.  And if they find code inside of a stackexchange or tutorial page that has global variables in it, but they want to copy and modify that code inside of their own function, then they need to strip out the global.  And then students start asking why does for create this scope they need to worry about but not other things.... Pop quiz: in julia 0.6, is x global or local: The answer is that there's no way to know, because it depends on whether a global x has been defined before. Now, you can say for sure that it is local."
technical,"I agree... And also think that the scoping rules shouldn't necessarily change, just all of the interactive interfaces (i.e. the REPL, Jupyter, and Juno control enter). This is not just about beginners learning a new rule. If you can't copy and paste fragments of code into the REPL, jupyter etc and also into functions, it is a major annoyance for intermediate programmers as well. Of course, I also agree with the other posters... with beginners they going to take code fragments they see within functions, copy I into scripts, and be completely confused when it doesn't have the same behaviour when copied inside of a function, in juno, the repl, and jupyter. There will be 100 stack exchange questions which come down to the same issue. Intermediate programmers are going to have all sorts of homegrown solutions with wrapping in let blocks, etc which will confuse things further Possibly, but at this stage this is hypothetical (also the OP of the question linked is asking about the rationale for the scoping rule, as opposed to being confused about it). Also, while I respect the teaching experience of everyone who has concerns about this, whether this turns out to be a big deal in the classroom is something that time will tell."
technical,"Nothing.   Basically, the proposal is that this would only be for code entered at an interactive prompt.   As soon as you start putting things in files, you need to learn the ""hard scope"" rules.   Hopefully, when you start putting code into files you should start using functions. It's not ideal for there to be pickier scoping rules for global code in files than at the prompt.   But I think that #19324 combined with the Julia 1.0 stability promise leaves us with no ideal options. remember that many of us would like to use Julia as a substitute for Matlab etcetera in technical courses like linear algebra and statistics. These are not programming courses and the students often have no programming background. We never do structured programming ” it's almost all interactive with short snippets and global variables.  Having taught courses using Julia to students with prior exposure to Matlab/R/..., I sympathize with this concern. But at the same time, I don't think that using Julia just as a Matlab etc substitute is a viable approach: as demonstrated countless times by questions on Discourse and StackOverflow, this can lead to performance pitfalls that are difficult to fix and understand, possibly entailing an even larger cost than investing in understanding how Julia is different from these other languages (cf posts with topics ""I translated this code from Matlab and it is 10x slower"").  I think that the key issue is the *silent* failure, the issue *per se* is easy to understand and fix. I would suggest keeping the new behavior, but giving a warning in Main (by default, it should be possible to disable it)."
technical,"The behavior wouldn't be just to wrap everything in a let block”it's more complicated than that. You need to let-bind any global that's assigned inside the expression and then extract the let-bound value to a global at the end of the expression. So you would turn a = 1 into something like a = let a, a = 1, end. And something like this would be turned into this. Frankly, I'm pretty annoyed that people are only giving this feedback now. This has change has been on master for ten months."
technical,"I have the feeling that previously, the confusion with soft/hard scope was more of a theoretical nature (of people reading the manual) rather than of a practical on (of people getting unexpected results).  That was definitely how it was for me and what, e.g., the search results here support this, I found one counter-example here. On the other hand, this new behavior will not confuse people when reading the manual, but when using the REPL.  Arguably the latter is worse. SoftGlobalScope.jl is now a registered package.  My intention is to enable it by default (opt-out) for IJulia, at least this semester."
technical,"I should have tuned into this earlier, but after a brief moment of concern all seems to be well.   We have actually never fully supported copying and pasting code from a function line-by-line into the REPL...The first thing that comes to mind is a REPL mode for line-by-line function debugging. Indeed Rebugger (which is exactly that) works properly on 1.0 only because it lacks the scope deprecation of 0.7, and could never be made to work on 0.6. However, I'm pleased to be able to verify that SoftGlobalScope.jl seems not to break that. For example, if you step deeply enough into show([1,2,4]) you get here. So it works fine on 1.0 (with or without softscope). On 0.7, evaluating this (with or without softscope) will yield julia So 0.7/1.0 are definitely a step forward, and if softscope makes certain things easier without breaking important functionality that's great.  The biggest concern, therefore, is simply how to intercept this appropriately without tanking other packages. SoftScope does not touch the arguments of macro calls (since there is no way to know how the macro would rewrite it), so :(eval ...) is protected."
technical,"I suppose adding a function can be done with just a minor version bump?  We can also warn exec'ing another file from exec'ed script and then put some pedagogical messages there to nudge them to use include. Some thoughts I wrote down last night while trying to wrap my head around this issue (yet again) to try to figure out what the best course of action might be. No conclusion, but I think this lays out the problem quite clearly. After having thought about this issue for some years, I don't think there is any ""ideal solution"" this may be one of those problems where there are only suboptimal choices.  People naively view global scope as a funny kind enclosing local scope. This is why global scopes worked the way they did in Julia 0.6 and prior:  - If an outer local scope creates a local variable and an inner local scope assigns to it, then that assignment updates the outer local variable. - If an outer global scope creates a global variable and an inner local scope assigns to it, then that assignment previously updated the outer global variable.  The main difference, however, is:  - Whether an outer local variable exists, by design, does not depend on the order of appearance or execution of the expressions in the outer local scope. - Whether a global variable exists, however, cannot be independent of order, since one evaluates expressions in global scope, one at a time.  Moreover, since global scopes are often quite lengthy”not infrequently spread across multiple files”having the meaning of an expression depend upon other expressions an arbitrary distance from it, is a spooky action at a distance effect, and as such, quite undesirable.  This last observation shows why having the two different versions of a for loop at global scope behave differently is problematic. Also note that the contents of file1.jl and file3.jl are identical and we could simplify the example by including the same file twice with a different meaning and behavior each time."
technical,"For me, the larger issue is the perceived inconsistency. That is, I'm ok with Julia doing things differently, but: - Why should code pasted from a function not work in a REPL? - No other language I've ever used has this behavior, and it's another barrier to adoption - Why do for blocks behave differently from begin and if blocks? (if I sort of understand, but a block is [should be] a block.).  Regarding bullet 2, I think this is a bigger deal than we who have been using Julia for a while (and are committed to the language) might understand. I can tell you I'm currently 0 for 7 in convincing my group to write code in Julia, two of those were due to this for loop problem which I couldn't explain because I hadn't been exposed to it before. The remainder I guess we can chalk up to my lack of charisma.  My preference would be to ensure that code pasted from a function into a REPL behaves identically to the function, and that for loops do the expected thing when using them to analyze data interactively, that is, specifically, that they mutate external / global variables when directed without any special keywords. Sorry, but this argument is ridiculous to me.   I'm not talking about classes where I'm teaching programming.  There's a place for simple interactive computations, and in non-CS classes it's common to be introduced to programming languages as a ""glorified calculator"" to start with.  Teaching performance computing in Julia is an entirely different process but it doesn't hurt if they've already been using Julia as their ""calculator.""  If you start by introducing students to Matlab as their ""calculator,"" it's much harder to make the transition to ""real"" programming, because their first instinct is to do as much as possible with Matlab before jumping ship, at which point their bad habits are ingrained and they are reluctant to learn a new language.  In contrast, if you start with Julia as your glorified calculator, when it comes time to do more serious programming you have a much wider array of options available.  You don't have to train them to cram everything into ""vector"" operations or force them to do things badly before they do it right.  Are you saying I shouldn't use Julia in my linear-algebra course?   Or that I should only use it if I'm prepared to teach computer science as well as linear algebra?"
technical,"(Per, this is the relevant change) suggested here that one possibility to solve this issue is automatic wrapping of REPL entries in let blocks"
technical,"That's not what I'm saying. Clearly we should use the same solution in all those contexts. But implementing it as two different variations on scoping rules seems less clean than implementing it as a code transformation with one set of scoping rules. But perhaps those are functionally equivalent. However, it seems easier to explain in terms of the new simpler scoping rules + a transformation that takes REPL-style input and transforms it before evaluating it. That could be done as Meta.globalize(m::Module, expr::Expr) that transforms an expression by automatically annotating any globals which exist in the module as global if they are assigned inside of any top-level non-function scope. Of course, I think that's equivalent to what the old parser did, but a bit more transparent since you can call Meta.globalize yourself and see what the REPL will evaluate."
technical,"That could be done as Meta.globalize(m::Module, expr::Expr) that transforms an expression by automatically annotating any globals which exist in the module as global if they are assigned inside of any top-level non-function scope. Of course, I think that's equivalent to what the old parser did, but a bit more transparent since you can call Meta.globalize yourself and see what the REPL will evaluate. That could be done as this, that transforms an expression by automatically annotating any globals which exist in the module as global if they are assigned inside of any top-level non-function scope.  I actually started looking into implementing something like this a few minutes ago.  However, it looks like it would be *much* easier to implement as an option in julia-syntax.jl:  * Writing an external AST transformation is possible, but it seems like there are lots of tricky corner cases ” you basically have to re-implement the scoping rules ” whereas we already had the code to get it right in julia-syntax.scm. * It's even more tricky for something like IJulia that currently uses include string to evaluate a whole block of code and get the value of the last expression.  Not only would we have to switch to parsing expression by expression, but some hackery may be needed in order to preserve the original line numbers (for error messages etcetera).  (Though I found a hack for ChangePrecision.jl for this sort of thing that may work here also.) * Not to mention of the case of people that include external files, which would not be caught by your AST transformation.  However, it seems easier to explain in terms of the new simpler scoping rules + a transformation that takes REPL-style input and transforms it before evaluating it. I seriously doubt this would be easier to explain to new users than just saying that the rules are less picky for interactive use or for include with a certain flag."
technical,"For that matter, it's not crazy to me to default to --softscope=yes for any ""script"", i.e. for julia foo.jl, and only turn on the ""hard"" scoping rules for modules and include (at which point you should really be putting most code into functions). That.  The other one to seriously consider is Juno.  Remember that people will <shift-enter through their code to do interactive development(especially when working with the regression tests) and then later expect to be able to run the same file.   Should it matter if the code is in a testset or not (which I think might introduce a scope)?  It would be very confusing to the user if the same text changes when in a testset vs. not when using Atom's integration, and is inconsistent with doing ] test as well. It sure sounds to me like the best solution is that the hard-scope is simply an opt-in thing, where if every other usage (including include within scripts) uses softscope unless you say otherwise."
technical,"Having an option to choose between the old behavior and the new one is interesting but it feels very hacky. If someone implements an ""unbreak me"" soft-scope AST transformation, it will be very tempting to use it in IJulia, OhMyREPL, etcetera, at which point you get the even more problematic situation in which the default REPL is seen as broken. That's not what I'm saying. Clearly we should use the same solution in all those contexts. But implementing it as two different variations on scoping rules seems less clean than implementing it as a code transformation with one set of scoping rules. But perhaps those are functionally equivalent. However, it seems easier to explain in terms of the new simpler scoping rules + a transformation that takes REPL-style input and transforms it before evaluating it."
technical,"SoftScope does not touch the arguments of macro calls (since there is no way to know how the macro would rewrite it), so :(eval ...) is protected. The (outer) local variable a exists, but has not been assigned yet. If the loop tried to assign to a before reading it, the assignment would be visible outside as well. In general, creating a variable binding and assigning a value to it are separate steps."
technical,"But wouldn't that be confusing in that you couldn't do and use a after that? Unless global is inserted for all the toplevel assignments, I guess? The behavior wouldn't be just to wrap everything in a let block”it's more complicated than that. You need to let-bind any global that's assigned inside the expression and then extract the let-bound value to a global at the end of the expression."
technical,"Good point, this also needs import Future.scope. (I think this module/namespace/behavioral effect already is reserved/exists). As a reminder here, the change was to ensure that code behaves the same in all global scope environments, regardless of what else had previously been evaluated in that module. Before this change, you could get completely different answers (resulting from different scope assignment) simply by running the same code twice or by moving it around in a file. The number of complaints I saw about that in practice (zero) are certain to be dwarfed by the number of complaints and confusion you will see (and are already seeing) about the current behavior."
technical,"Possibly, but at this stage this is hypothetical (also the OP of the question linked is asking about the rationale for the scoping rule, as opposed to being confused about it). Also, while I respect the teaching experience of everyone who has concerns about this, whether this turns out to be a big deal in the classroom is something that time will tell. the questioner appears to have been confused by it: ""I wonder if this is intuitive to beginning julia users. It was not intuitive to me ..."""
technical,"I wonder if we could adapt our approaches to teaching instead, eg introduce functions before loops, avoid loops in global scope.  This is totally impractical in a class that is not focused on teaching programming.   I might as well not use Julia in my classes if I can't use it interactively and/or have to write functions for everything first.  (And it's not just pedagogical.  Loops in global scope are *useful* for interactive work.  And one of the main reasons people like dynamic languages for technical computing is their facility for interactive exploration.  Not all coding is performance-oriented.) There have been dozens of threads and issues over the years in which people are confused or complaining about the old ""soft/hard scope"" distinction, so claiming that no one has ever been confused by or complained about the old behavior is just... not true. I could dig some of them up, but you were around, stevengj, so you can dig them up just as easily and I have a hard time believing that you didn't notice or don't remember these complaints and conversations."
technical,"Conclusion after much discussion including triage: we're going to include something along the lines of SoftGlobalScope in Base and use it in the REPL and all other interactive evaluation contexts. JeffBezanson has pointed out that the way this is implemented is actually essentially the same as how soft scope was previously implemented, so to some extent we're coming around full circle. The difference is that now there is no scope behavior in modules or scripts, only in REPL-like contexts. I also think that  explaining  soft scope as a source rewrite is clearer than trying to distinguish between hard and soft scopes (which we never how Jeff explained it, I might point out). These two statements confuse me a bit as they seem a bit contradictory: Does this mean that the module Main has sometimes a soft scope (say at the REPL prompt) and sometimes a hard scope (say when julia -L script.jl)?  Would it not make sense to say that Main always has soft scope? And a module can opt-in to soft scope by using SoftGlobalScope?"
technical,"When I was first introduced to Julia (not a long time ago, and I come from a Fortran background mostly), I was taught that ""Julia is compiled and fast at the function level, thus everything that must be efficient must be done inside functions. In the main 'program' it behaves like a scripting language"". I found that fair enough, as I cannot imagine anyone doing anything too computationally demanding without understanding that statement. Therefore, if there is any sacrifice in performance at the main program for using the same notation and constructions than in the functions, I find that totally acceptable, much more acceptable than trying to understand and teach these scoping rules and not being able to copy and paste codes from one place to another.  By the way, I am a newbie in Julia yet, having chosen it to teach some high-school and undergraduate students some basics of simulations of physical systems. And I am already hopping this issue returns to the 'normal' behavior of previous versions, because it gives us quite a headache. This conversation is locked now and only Julia committers can comment."
technical,"I'm guilty of not having followed master very closed until recently, so this feedback is indeed a bit late. More than a concern for programmers (most for loops will be inside a function in library code) I'm afraid this is a concern for teaching. Often for loops are taught before functions or scopes (of course you need to understand scopes to really understand what's going on but in teaching things are often simplified). Here it becomes a bit difficult to teach a beginner how to sum numbers from 1 to 10 without explaining functions or global variables. To be fair, Julia 0.7 was released 13 days ago. This is a new change for most Julia users."
technical,"To be fair, Julia 0.7 was released 13 days ago. This is a new change for most Julia users. Unfortunately for those of us who can not handle living on the edge, its brand-new from our perspective."
technical,"I was going to argue that while this is true (for the implementation/compiler), the naive julia programmer cannot not see any different behaviour from his simpler conceptual model (a variable starts existing at the moment it is defined). Unfortunately you are right, the following code will not give an error, while it will give an error if you leave out the a=2 at the end. I'll explain the unfortunately: I can understand the behaviour (because i've worked with compiled languages before) but still find it confusing and unexpected. How bad must it be to someone with only scripting experience or new to programming. Also, I found some code that shows the behaviour, I do not see a useful application (maybe you can help me there)  On the REPL: I just got more convinced that changing the scoping back to ""normal"" at  least in the REPL (no need to add global in loops) is high priority: I was testing some things in the REPL today and got (again) bitten by it, taking some time to realize it. Given that I follow Julia already some time, really like a lot of it, am even following this thread about the problem, I would even call it a showstopper: A newbee (to the Julia) testing out the language is very likely not to find out the problem and just give up. we are both on long-awaited vacations (I should not be reading this). We can figure out what to do in a week or so. while the feedback is appreciated, scoping rules are not up for a broader debate or revision. The only thing on the table is special-casing interactive eval. The SoftGlobalScope package is already an excellent experimental implementation and it may just be a matter of making that part of Base and using it in the REPL."
technical,"I don't think treating this as a ""bugfix"" is in the cards, because it would break the 1.0 stability contract.   However, it seems reasonable to me to use softscope for scripts run with julia -i (i.e. ""interactive"" mode). (That is, there would be a flag --softscope={yes|no} and it would default to the value of isinteractive.) We'll have to consider the script mode choice."
technical,"I don't think treating this as a ""bugfix"" is in the cards, because it would break the 1.0 stability contract.   However, it seems reasonable to me to use softscope for scripts run with julia -i (i.e. ""interactive"" mode). (That is, there would be a flag --softscope={yes|no} and it would default to the value of isinteractive.) What I'm trying to get at is that I feel a simple description of global vs. local is sufficient for early-stage teaching --- you don't even need to say the word ""scope"" (it does not occur at all in my explanation above). When you're just showing some simple expressions and loops in the REPL, you're not teaching people about testsets and you don't need an exhaustive list of the scoping behavior of everything in the language.  My only point is, this change does not suddenly make it necessary to teach lots of details about the language up front. You can still ignore the vast majority of stuff about scopes, testsets, etc., and a simple line on global vs. local should suffice."
technical,"The (outer) local variable a exists, but has not been assigned yet. If the loop tried to assign to a before reading it, the assignment would be visible outside as well. In general, creating a variable binding and assigning a value to it are separate steps. What is the time-line on this?  It seems it would be a great improvement to user usability.  And at this ""critical"" time of Julia with 1.0 out, it would seem advantageous to get this fixed asap (in the way suggested by Jeff above) and tag a new Julia version or REPL version.  (Sorry for this arm-chair comment, as I certainly will not fix this!)"
technical,"This conversation is locked now and only Julia committers can comment. what would be the plan to implement the semantics you suggested in this discourse thread, initially only in the REPL and opt-in elsewhere?  It sounds like you are planning to put that directly into the lowering code (julia-syntax.scm), rather than as a syntax rewriting ala SoftScope.jl?  Or would you rather have it as syntax rewriting first (modifying SoftScope to the proposed rule and converting it to a stdlib), and defer putting it into the lowering code for a later Julia release?"
technical,"Making an ""unbreak me"" option the default seems wiser, especially an option that is aimed squarely at beginning users.   If it is a non-default option, then precisely those people who need it most will be those who don't have it enabled (and don't know it exists). What would the proposed REPL-mode do to includeed scripts?  Would the evaluation of global statements depend on whether the REPL mode is activated?  If so, IMO this would be at odds with the 1.0 stability promise."
technical,"One thought that I had but we dismissed as being ""too annoying"" and ""likely to cause the villagers to get out their pitchforks"" was that in non-interactive contexts, we could require a local or global annotation in ""soft scope"". That would guarantee that code from a module would work the same if pasted into the REPL. If we applied that to ""scripts""/""programs"" then the same would be true of them. When I was first introduced to Julia (not a long time ago, and I come from a Fortran background mostly), I was taught that ""Julia is compiled and fast at the function level, thus everything that must be efficient must be done inside functions. In the main 'program' it behaves like a scripting language"". I found that fair enough, as I cannot imagine anyone doing anything too computationally demanding without understanding that statement. Therefore, if there is any sacrifice in performance at the main program for using the same notation and constructions than in the functions, I find that totally acceptable, much more acceptable than trying to understand and teach these scoping rules and not being able to copy and paste codes from one place to another.  By the way, I am a newbie in Julia yet, having chosen it to teach some high-school and undergraduate students some basics of simulations of physical systems. And I am already hopping this issue returns to the 'normal' behavior of previous versions, because it gives us quite a headache."
technical,"I agree with stevengj both on the problem (teaching to non programmers becomes much harder) and on the solution (make things work in the REPL and the various IDEs). Including a script would still have the Julia 1.0 scoping rules but that's less of a concern, one just has to be careful to have the ""we can put our for loop in a function and then call the function"" class before the ""we can put our for loop in a file and include the file"" class.  This sounds like a good compromise as interactive debugging at the REPL doesn't become more painful than it needs to be (or more confusing to new users), while normal code in scripts has to follow strict scoping rules and is safe from bugs overwriting some variables accidentally. You may have misunderstood what I was saying (or I did not express it clearly). I was talking about courses that *use* Julia to teach something domain-specific (eg I taught numerical methods to econ grad students), not CS courses (which I have no experience with). The point that I was trying to make is that it is reasonable to expect a certain level of difference between Julia and language X (which may be Matlab), conversely, ignoring this can (and does) lead to problems. Personally, when learning a new language, I prefer to face these issues early on, also, I think simplicity and consistency of the language semantics is more important than similarity to other languages in the long run. But I recognize these preferences as subjective, and reasonable people can have different ones."
technical, (notice the ~ (tilde) which points to the node modules folder) Do you need to install a dependency to be able to use ~ instead of the node modules folder? I tried it but it didn't work.
technical,I would stay far away from bootstrap-loader. We are using it on a React project and it's been a pain in the butt to update to Bootstrap 4.0 beta. I ultimately want to go to Bootstrap 4 GA and I think I'm going to have to ditch bootstrap-loader to do it. +1 on the staying away from bootstrap-loader you will regret using it I had to rip it out after it randomly broke compilation due to buggy path resolve & bootstrap version code. After forking bootstrap-loader in an attempt to fix their code I quickly realized my mistake in adding it in the first place.
technical,"Sorry, nevermind. I made a mess out of it :D. My config was not in order, bootstrap-loader currently does not support Bootstrap 4."
technical,bootstrap-loader currently does not support Bootstrap 4. Closing since #22423 was merged.
technical,"So many different solutions, so many different workable options, yet none of them helped me in the past 3 hours getting bootstrap up and running... The webpack documentation page looks so simple but it is not that simple, apparently. Please include some more documentation on how to test if it is working and if there are any differences when using typescript. I just installed, included, imported and my app just runs fine, no errors, but yet no col-md or container styling working.. meh, i'll just return to CDN's... Feel free Ruud-cb to improve our docs if you found something, we cannot covered every use case"
technical,yes that page is great! But on https:getbootstrap.com no dependencies are mentioned in the npm section thats what caught me. Feel free to check out bootstrap 4 setup to see how I tackled this...
technical,Closing since #22423 was merged. For consideration:
technical,"(notice the ~ (tilde) which points to the node modules folder) Do you need to install a dependency to be able to use ~ instead of the node modules folder? I tried it but it didn't work. good question. I took this convention from sass-loader, but if in addition to sass-loader you're also using postcss than this could be related. Anyway you can try without the tilde, or worst case use '../../node modules'.  let me know how it goes."
technical,"Feel free Ruud-cb to improve our docs if you found something, we cannot covered every use case I lock this issue everything is here.  If someone want to improve our docs feel free to do it, or you can open an issue which point what is missing in our docs"
technical,"Feel free Ruud-cb to improve our docs if you found something, we cannot covered every use case I think a webpack configuration is something that many developers might find useful. I'll wrap it as a PR and leave it for you to decide."
technical,Feel free to check out bootstrap 4 setup to see how I tackled this... I would stay far away from bootstrap-loader. We are using it on a React project and it's been a pain in the butt to update to Bootstrap 4.0 beta. I ultimately want to go to Bootstrap 4 GA and I think I'm going to have to ditch bootstrap-loader to do it.
technical,"Seeing all the follow-up comments, do we need more docs updates here? I'm very new to webpack, but I have just set up a project and I have TypeScript and SCSS compiling correctly. I asm using webpack version 4.1.1. When I import bootstrap like this:  The TypeScript compiles, but I am given the following error on page load:  I have the following packages installed in my package.config: I imagine I also have to declare the  sign globally somewhere but this isn't documented in the bootstrap docs, but for me I can't seem to import the bootstrap js at all.  Is there something missing from the docs that I need to do?"
technical,"This thread helped me. But dont forget to install dependencies. Im surprised this is not mentioned more explicit in npm installation section. it's written here   Bootstrap is dependent on jQuery and Popper, these are defined as peerDependencies, this means that you will have to make sure to add both of them to your package.json using npm install --save jquery popper.js."
technical,"+1 on the staying away from bootstrap-loader you will regret using it I had to rip it out after it randomly broke compilation due to buggy path resolve & bootstrap version code. After forking bootstrap-loader in an attempt to fix their code I quickly realized my mistake in adding it in the first place. Seeing all the follow-up comments, do we need more docs updates here?"
technical,"thanks for reporting, a few questions to better diagnose the issue - 1. when importing scss, you do it directly from the js or from a secondary scss file? 2. If you use a secondary scss file, do rules before/after the import **are working** in the browser? 3. what version of style-loader are you using? Smells to me like could be related to this issue, try a couple of things for me -  1. use a secondary scss, and add this code **before** importing bootstrap - 2. upgrade style-loader to 0.17.0 3. take a look at a demo project implementing bootstrap"
technical,"I think a webpack configuration is something that many developers might find useful. I'll wrap it as a PR and leave it for you to decide. thanks for reporting, a few questions to better diagnose the issue - 1. when importing scss, you do it directly from the js or from a secondary scss file? 2. If you use a secondary scss file, do rules before/after the import **are working** in the browser? 3. what version of style-loader are you using?"
technical,For consideration: This thread helped me. But dont forget to install dependencies. Im surprised this is not mentioned more explicit in npm installation section.
technical,"I'm very new to webpack, but I have just set up a project and I have TypeScript and SCSS compiling correctly. I asm using webpack version 4.1.1. When I import bootstrap like this:  The TypeScript compiles, but I am given the following error on page load:  I have the following packages installed in my package.config: I imagine I also have to declare the  sign globally somewhere but this isn't documented in the bootstrap docs, but for me I can't seem to import the bootstrap js at all.  Is there something missing from the docs that I need to do? Update: I fixed my issue by using.  Apart from that the docs were enough for me to get going personally."
technical,"it's written here   Bootstrap is dependent on jQuery and Popper, these are defined as peerDependencies, this means that you will have to make sure to add both of them to your package.json using npm install --save jquery popper.js. yes that page is great! But on https:getbootstrap.com no dependencies are mentioned in the npm section thats what caught me."
technical,"Ah, I see. You are correct I did misread it, although now there's no explicit exit to the function. (See how that can get confusing to a reader? The function doesn't mark an exit, so I didn't think there was one). Anyway, there shouldn't be a special case to allow a developer to return the error which occurred without additional context. If they want to do that, they should do it explicitly with return err. Also, assuming you want to return the zero value for the other numbers is a dangerous game. For instance, let's say I wanted to write the following function: Returns how many occurrences of find exist in the UTF8 encoded reader. If an error occurs, I don't want to return n=0 because 0 is a valid return value of my function, I'd want to return n=-1. check/handle does this well because the return in it's system actually returns to the function, so there's no assumptions about what you're trying to return. Perhaps the handler should always be in the form (error) - (parent function's return values). This kind-of destroys the idea of reusable handler generators (ThenErr), though. (or anyone else that comes along), it would be great if you left your reason for the thumbs down.  The proposal has received a lot of useful critiques around the edge cases of language interaction. But I actually have not yet seen a single critique of the core idea of this proposal, including outside this go 2 process where I have shown it to others. similarly, it would be great to see critical comments of the core idea here and leave promotions of your proposal on the github issue that is already open for that."
technical,"To everyone else, please note that detailed discussion like this does not belong on the issue tracker. # Background (go 2 Process)  go 2 has laid out the problem of error handling (Please read before reading this proposal).  I am told that alternative proposals should be raised as git issues here. Please add anyone else you think would like to join the discussion.   # Introduction  It is amazing to see the error handling problem being properly addressed. The existing draft proposal  is good, but I think we can iterate to make it better. To avoid confusion by comparison to the existing proposal, I will avoid mentioning it in this one. However, if you are a supporter of the existing proposal, please separately read my critique of it.  It's useful to take a step back and clearly state what we are trying to do with our implementation:  * provide an abstraction that allows for the insertion of a return statement for errors. * compose handler functions together before they are used with the error return  In the existing go language, I cannot write a handler function which will create an early return from the function. There are a few approaches that use existing languages features for this control flow: * Macros (e.g. Rust originally used a try! macro). * Ruby supports anonymous functions that return from the enclosing function (method) * exceptions * Sequencing code with short-circuits. Some usage of monads in Haskell are a particularly good example of this.  For sequences with short-circuits, see the errors are values post for how this can be done in go. However, this severely alters how one would write go code.   # Proposal: handlers as functions, just a special check  Lets repeat our goals again:  * provide an abstraction that allows for the insertion of a return statement for errors. * compose handler functions together before they are used with the error return  Composition can be handled with ordinary functions that take and return an error.  That means we just need a mechanism to insert a return. For early return in my proposal, I will use a question mark operator ? rather than a check keyword. This is for two reasons * the operator can be used postfix, which has readability advantages * the original draft proposal used check, but it functions differently, so this may help avoid confusion.  See ""Appendix: Operator versus check function"" for a discussion on using ? or a check keyword.   ## Implementation as syntactic expansion  Where handler is a function that takes an error and returns one. Zero is the zero value for the (success) value returned before the error, assuming the function returns a single success value. A function that returns 4 values, the last one being an error, would have.  This is a simple, easy to understand transformation. It is easy to underestimate the value from being able to understand the usage site without searching for context. I am trying to avoid comparisons to other proposals, but I want to say that none of the others I have seen can be described this simply.  All of the transformation is performed entirely by ?. It inserts the nil check, the return, and creates the needed zero values. The handler is just a normal function and an argument to ?.  For some small convenience in writing cleanup handlers, the return section would actually expand to this:  See the section on handler types and the appendix section on ThenErr and ToModifyError.   ## Basic example from the original proposal, re-written  Putting this together, lets re-write SortContents, which wants different handlers in different places.  Let's show another example from the proposal (slightly simplified) that has handler composition: It is possible to combine handlers in the same way one would combine functions: The example uses a .ThenErr method (see appendix) as a way to compose error handler functions together.   ## Results  * This alternative proposal introduces just one special construct, ? * The programmer has control and flexibility in the error handling. * Handlers can be naturally composed as functions * The code is much more succinct and organized than current go error handling code. * errors can be returned from defer.   ## Checking error returns from deferred calls  This alternative proposal can support returning errors from defer: ## Notes on error handler function types  To respond to errors we want to do one of two things: * cleanup (side-effecting): (error) - nil or () - nil * modify the error: (error) - error  An error handler function must always have the type of the modifier, but we may not want the extra noise when writing a purely cleanup handler. The question mark operator can accept all forms. A cleanup function can be automatically converted to return the original error that would have been passed to it.  This is also true of helpers that compose error functions such as ThenErr. See the Appendix section on ThenErr to see how this is implemented.   # Appendix  ## Appendix: Handle and anonymous function syntax  This proposal is slightly more verbose than others that introduce a special anonymous function syntax that is lighter-weight and infers types. Without this syntax, the proposal would read:  I think it is worthwhile to explore having anonymous functions that are lighter-weight. However, I think this should be usable anywhere rather than just with a single keyword.  But please leave this for another proposal rather than throw it in the mix with error handlers!   ## Appendix: unary and binary.  The question mark operator can be used as a unary to just return the exception without any handlers running  I am favoring writing the unary form without any spaces in this case (more similar to Rust), but we should use whatever syntax the community finds best.   ## Appendix: Handling errors within the handler itself  A cleanup handler may generate a new error that should be propagated in addition to the current error. I believe this should just be handled by a multi-error technique, e.g. multierr.   ## Appendix: custom error types  The existing proposal seems like it would cast a concrete error type to the error interface when it is passed to a handler. I don't think this proposal is fundamentally different. I think this issue should be solved by the generics proposal.   ## Appendix: ThenErr and ToModifyErr An implementation of ThenErr  and ToModifyErr. See the syntax expansion section for how the ? operator uses ToModifyError. ## Appendix: Operator versus check function  The original proposal rejected the question mark and gave some reasons why. Some of those points are still valid with this proposal, and others are not.  Here is another proposal that I believe advocates the same solution proposed in this alternative, but with a check function. I would be happy with that as a solution, but below I give my preference for ?.  The original proposal had just one argument given to check. This alternative favors the question mark in large part because there are now 2 arguments. The original proposal states that there is a large readability difference in these two variants:  However, I think this is a matter of personal preference. Once there is a left-hand-side assignment, the readability opinion may also change.  Now lets add in a handlers and check our preference again.  I believe ? will be slightly nicer to use due to * fewer parantheses * putting error handling solely on the right-hand-side rather than both the left and right.  Note that it is also possible to put all the error handling on the left-hand-side of the error source. But I prefer keeping error handling on the right-hand-side for two reasons * a success result is still transferred to the left * it is possible to write an anonymous handler rather than being forced to declare it ahead of time   ## Appendix: built-in result type A go programmer that has used Rust, Swift, Haskell, etc will be missing a real result type. I would like to see a go 2 proposal for discriminated unions which includes a result type. However, I think both the original proposal and this alternative proposal would work fine with the addition of a result type. This is because go effectively already has a result type when dealing with errors. It is a tuple where the last member is of type error.  A future version of go with discriminated unions should be able to use ? for dealing with a discriminated union result type.  ## Appendix: intermediate bindings for readability Error handling on the right-hand-side may increase line length undesirably or seem to be easy to miss. Its always possible to use an intermediate binding.  ## Appendix: left-hand-side It is possible to support placing the handler on the left-hand-side. This could make more sense for check. One of the ideas behind this would be to emphasize the handler, for example in the case where f(...) is an enormous expression (see above section on intermediate bindings which is another way to handle this).  ## Appendix: returning the zero value  This proposal does not allow for the defensive practice of returning -1 as the success value, along with the error. Where -1 is useful because zero or a positive number are an allowed value in the problem domain, so someone may notice a -1 propagating. I don't think we need to support this use case for a few reasons. * It is not generally applicable anyways (consider a uint). * The contract of using the function is already that errors must be checked before looking at success values. * There are standard linters (errcheck) that will warn people about ignoring errors: we should instead ship this ability with go vet.   ## Appendix: all proposal examples re-written Below are the rest of the code snippets shown in the original proposal, transformed to this alternative proposal."
technical,"Error handling is a potential control flow changing point, so we care about it. The design in original draft introduces a new control flow changing rule, what we call ""special stacking"" or ""chained handler"". That brings confusion more than convenience. Some guys, include me, suggest to use a new control flow changing mark with normal function as error handler. However, how to implement this mark is controversial. A named catch block after check/?/etc does the trick nicely :-)"
technical,"deanveloper thanks for critiquing adding break/continue. I removed that section now because I don't like the idea either and it seems to distract from the proposal rather than to clarify. Ah, I see. You are correct I did misread it, although now there's no explicit exit to the function. (See how that can get confusing to a reader? The function doesn't mark an exit, so I didn't think there was one). Anyway, there shouldn't be a special case to allow a developer to return the error which occurred without additional context. If they want to do that, they should do it explicitly with return err. Also, assuming you want to return the zero value for the other numbers is a dangerous game. For instance, let's say I wanted to write the following function: Returns how many occurrences of find exist in the UTF8 encoded reader. If an error occurs, I don't want to return n=0 because 0 is a valid return value of my function, I'd want to return n=-1. check/handle does this well because the return in it's system actually returns to the function, so there's no assumptions about what you're trying to return. Perhaps the handler should always be in the form (error) - (parent function's return values). This kind-of destroys the idea of reusable handler generators (ThenErr), though."
technical,"Thanks for trying to keep the discussion on track: I would like to keep it focused on the proposal at hand. I see one point that is quite relevant to address: Is it bad to introduce this error handler concept if it doesn't scale down to simple usage? That is, if I don't need a shared handler, would I use this ""improved"" error handling? This is the benefit of the handler being a second argument to the check: you can write your handler inline without the overhead of naming it or registering it. So my version given in the above examples would still be:  I don't think this is inherently better than doing the traditional err != nil, but it is something you *could* do if you wanted to consistently do error handling the same way. Note that if you are not just side-effecting but also return error values, the ? will generate zero values for you also.  But this example is quite contrived as is: the real way you would write it. This is why I believe it is important to use normal functions as handlers: function composition is such a powerful tool.  It is probably possible to come up with an example more like the first that doesn't benefit from handler helper functions. If go lang had a good anonymous function syntax (type inference and no return), I would still prefer ?. Although I can understand the hint, I refuse the accusation by gregwebs that my objections had little to do with the proposal being discussed. The basic problems of the proposal remain."
technical,"If so, please tell me, why do so many programmers use syntax highlighting? Because they want to paint beautiful letters pictures? It seems we do not agree on this point. I think it's an obsession and a relic from other programming languages to think error's are something out of the ordinary (special cases) and not part of the actual code. It seem to me that your use case or expectation for GO may be inappropriate. We dont start to adapt Go for the styling of web pages, as a replacement of CSS. Because it's helpful. But that doesn't mean that a language should require syntax highlighting to be readable. I agree that errors really  shouldn't  have special treatment in Go. It's nice that in Go errors are just  values , but having all of the code repetition that we have is not okay and needs to be addressed. Code repetition leads to many issues, especially when it comes to refactoring. There is no current solution to the code repetition problem for handling error values, and that's what the check/handle construct is trying to aide.  Again, we only cared about 5 lines of code for that example I showed. But because we wanted to handle errors, we ended up needing to write 15 lines of code specifically to handle if something goes wrong. When did I say anything like this? CSS isn't programming, and it shouldn't be considered programming. I never said anything about Go doing things outside of programming. What I am saying is that Go is meant to be a language where what we write is what happens when it's run, contrasted to lots of OOP languages which have extremely confusing type systems and constructs. (Also, stop spelling it GO, it's spelled Go.)"
technical,"I would hereby like to follow freman's line of reasoning and would like to express my dislike of the new error handler proposals. Personally, I still consider the central principles of GO to be extremely precious and fear that the demands for syntactic sugar only lead to a dilution of the previous clarity and purity. The manual and explicit error checking/handling is in my opinion one of GO's core strengths.  Currently, type error is an interface like any other. A special syntax for handling errors would be a fundamental change. *Interesting observation* Many who start learning GO complain about the repetitive explicit error checking, but most get used to it and soon appreciate it in the vast majority of cases.  **cases:**  1. Should the repeated use of if err != nil {} be a visually disturbance for some users a different color scheme in the editor could easily solved this problem for them 1. Should the introduction of error handler be focused on enrichment of error information. It may be better to think about why the received error messages are incomplete. It might be beneficial to improve the code of the error returning function, instead of creating new syntax to iron out the initial fault.  1. leafbebop argued that error handler may improve chaining. And even though this may be true in some cases, I would like to question the premise here. In my opinion, chaining does not necessarily result in less writing effort, more comprehensible structures nor easy to maintain program code. **** In addition, chaining is only possible if all functions involved have a exact argument order. This would result in chaining being used in some cases and the conventional way in others, creating two parallel paradigms.  1. The proposal suggests that one of the goals is reducing the program code and/or the writing effort for the programmer, as well as a new syntactic expression for reformatting a received error. **** It is particularly difficult for me to follow this reasoning, because it is already optional to use a formatting function and also the amount / readability of the program code is not improved. **** The proposal introduces a new function type type errorHandler func(error)error. However the proposal seems to disregard that such a formatting function has only a limited selection of information, which is a bad prerequisite as a formatting function (only e). Unless such a function would be specially incorporated in the calling function and get access to all variables (with all disadvantages). **** The use of errorHandler's not only changes the language appearance, but also the reading flow. This effect is reinforced by errorHandler's defined outside of the calling function body. The confusion becomes more evident when different formatting functions are called. But I want to code in the Go language, not the Rust language - or I'd be coding in the Rust language :) Are our goals to be like other languages? Error handling Go results in the same questions and issues every time a new one of our devs picks it up. Over time, so long as they actually care to improve their craft they usually come to find it refreshing, especially as they learn of better ways of dealing with the errors. Sure it's not entry level easy, and it can be repetitive... but visual basic has on error resume next which has to be the easiest least repetitive way to ~~ignore errors~~ create unreliable code beyond underscoring them in Go. If I want more sugar, I can go back to perl where half of what I write in a maintainable way in Go can be squeezed into one line of code."
technical,"I meant refactoring the CopyFile function. If I wanted to change the error message, I'd need to remember to change it in all 4 places. Again, as I said, it really is nice for errors to just be treated as a value, but when we have an issue as bad as bad as how it is now (referencing code repetition), we need to weigh what matters more, and I believe that fixing the code repetition problem is more important than errors ""not being special"" but when we have an issue as bad as bad as how it is now (referencing code repetition), we need to weigh what matters more, and I believe that fixing the code repetition problem is more important than errors ""not being special"". I completely disagree. **question** If you do not care enough about the current error philosophy (if err != nil) , why not use the defer version? This would also solve your 'refactoring' objection... No changes needed."
technical,"it seems you have misread the proposal. Perhaps I wrote too much, let me know how I can make the section on  handler function types more clear. Currently it does state: ""A cleanup function will automatically be converted to return the original error that would have been passed to it.""When used as a handler, the error will still be passed along (see the section on ThenErr to show how this can be accomplished). In this proposal, the usage of check or ? always means that the function returns immediately if the error is not nil. deanveloper thanks for critiquing adding break/continue. I removed that section now because I don't like the idea either and it seems to distract from the proposal rather than to clarify."
technical,"I agree with that if err != nil, the function will exit. I believe exit on error is the common case. We should focus on the common case. If needing continue, just handle it with old style. I know you guys want to figure out a solution covering all cases. I hope it will be a lightweight one. I think heavy solution is against the philosophy of GO. And the design in original draft is already too heavy to me. Error handling consists of handler and trigger. Let me ask some questions.  1) Should trigger be bound with one or more handlers explicitly? In the original draft, ""check"" is the trigger. It cannot be bound with any handler explicitly. So a matching rule is needed.  2) Should handler be special or just a normal function?  3) Should trigger be a filter or just a consumer? In the original draft, ""check"" is the filter. It take return values from child function, and filters out the error. But in code, the trigger #err is just a consumer."
technical,"To expand on the whole function signature thing, here's what I mean: Again, in the check/handle made it clear: if err != nil, the function will exit Error handling is a potential control flow changing point, so we care about it. The design in original draft introduces a new control flow changing rule, what we call ""special stacking"" or ""chained handler"". That brings confusion more than convenience. Some guys, include me, suggest to use a new control flow changing mark with normal function as error handler. However, how to implement this mark is controversial."
technical,"Because it's helpful. But that doesn't mean that a language should require syntax highlighting to be readable. I agree that errors really  shouldn't  have special treatment in Go. It's nice that in Go errors are just  values , but having all of the code repetition that we have is not okay and needs to be addressed. Code repetition leads to many issues, especially when it comes to refactoring. There is no current solution to the code repetition problem for handling error values, and that's what the check/handle construct is trying to aide.  Again, we only cared about 5 lines of code for that example I showed. But because we wanted to handle errors, we ended up needing to write 15 lines of code specifically to handle if something goes wrong. When did I say anything like this? CSS isn't programming, and it shouldn't be considered programming. I never said anything about Go doing things outside of programming. What I am saying is that Go is meant to be a language where what we write is what happens when it's run, contrasted to lots of OOP languages which have extremely confusing type systems and constructs. (Also, stop spelling it GO, it's spelled Go.) Go is absolutely readable in its current form. I just said, in case it's not for a minority group of people... they could use it... Let's drop this, it does not help the actual Diskursion... Especially when refactoring functions, I can not grip your problem. Because using your example function CopyFile, would only require a single test. done!"
technical,"See link I posted above re ""critique"" for perspective on nesting calls that return error. (A ""pipe"" is an IPC or stream mechanism btw.) Good luck!"
technical,"A named catch block after check/?/etc does the trick nicely :-) I agree with that if err != nil, the function will exit. I believe exit on error is the common case. We should focus on the common case. If needing continue, just handle it with old style. I know you guys want to figure out a solution covering all cases. I hope it will be a lightweight one. I think heavy solution is against the philosophy of GO. And the design in original draft is already too heavy to me."
technical," I also read the proposals, but all examples do little change to my opinion. Both examples you put forward are specifically designed to provoke a constructed problem, knowing that both cases are far-fetched. Moreover, the current standard way is still shorter and more elegant, then your second example."
technical,"I also read the proposals, but all examples do little change to my opinion. Both examples you put forward are specifically designed to provoke a constructed problem, knowing that both cases are far-fetched. Moreover, the current standard way is still shorter and more elegant, then your second example. I encourage you to paste your first comment here into a gist linked from the feedback wiki, as it's mostly a critique of the error handlers concept, not the substance of this proposal :-). this issue isn't a forum to debate a commenter who took issue with handlers :-) Regarding gomarcus' critique of handlers, I have tried to document all the possible requirements for a new errors idiom (and thus benefits) on golang-dev."
technical, I mean that filter can work with pipe.
technical,"What I am saying is that Go is meant to be a language where what we write is what happens when it's run, contrasted to lots of OOP languages which have extremely confusing type systems and constructs Well, if this is your opinion, I wonder why you want to change this! That's the whole point let GO as it is! Don't put a square peg in a round hole. If something needs to be changed, add something useful ... like add generics but dont rush it. (Also, stop spelling it GO, it's spelled Go.) I meant refactoring the CopyFile function. If I wanted to change the error message, I'd need to remember to change it in all 4 places. Again, as I said, it really is nice for errors to just be treated as a value, but when we have an issue as bad as bad as how it is now (referencing code repetition), we need to weigh what matters more, and I believe that fixing the code repetition problem is more important than errors ""not being special"""
technical,"sorry for missing your actual concern. I think your level of programming defensiveness is probably appropriate given the lack of discriminated unions in go and the prevalence of zero values. However, it seems not generally applicable (what if I have a uint?) and unnecessary. The contract is always that the caller must check the error value before looking at the success value. We shouldn't weaken this proposal because someone is going to ignore errors. There are linters that check for this (errcheck): it would be much more powerful to add that capability to go vet or otherwise have this statically checked. I personally think that handle is sugar for goto rather than an anonymous function. It seems to be doing this. If you read it like that, the return makes perfect sense. Simplified example."
technical,"(or anyone else that comes along), it would be great if you left your reason for the thumbs down.  The proposal has received a lot of useful critiques around the edge cases of language interaction. But I actually have not yet seen a single critique of the core idea of this proposal, including outside this go 2 process where I have shown it to others. similarly, it would be great to see critical comments of the core idea here and leave promotions of your proposal on the github issue that is already open for that. I posted a link to a pure critique of check/handle, which largely applies to this proposal. It does not mention #id/catch. I urge you to read it.  I mentioned a catch block here as a solution to the control flow issue raised above, and used a prefix variation of your ? handler syntax with it."
technical,"I understand that the go team will be thorough and take time with proposals as they should. I am just trying to understand if I am following the proposal process properly. As to your comments on the proposal: I seems my proposal is hard to make it through parts without mis-reading. I think I can fix this by showing how it would actually be implemented in terms of syntax expansion. I will try that out, along with some re-wording, let me know how else I can make things more clear. The ? operator *always* returns the error from the function if one is present, a handler cannot alter this fact, it can only run side-effects and/or alter the error that gets returned. The right-hand-side function is treated the same way any function is treated in go. The special functionality comes only from the ? operator. We could require that a handler always return an error. As a convenience (and I am open to the possibility that this is a bad idea), we can allow a handler function to not return the error. I call this type of handler a cleanup handler. That just means that we transparently upgrade the cleanup handler to return its error (see the ThenErr appendix section for how this can be done). Yes, ? can be used with any function. The function must, however type check to avoid a compilation error. I agree user's will consolidate some handlers, but you actually lost me at the end. With this proposal one would write: I see, so the handler is passed the error, and can return a modified error, and that error is returned by the function using the ? operator? (For a language change proposal I find that it's normally best to present the suggested changes by themselves as a standalone document, rather than by comparison to a different change.)"
technical,"I think you got it now (should be easy to understand with the syntactic expansion section). I cleaned up the proposal by moving out critiques to a separate gist, let me know what else I can do to clarify things. I think our key goal is making the consequence of error handling more clear. When I read a check or ?, I want to know two things: 1. Which handler will be invoked. 2. What will happen to current function after error handling. This proposal let me understand them without searching in code context."
technical,"yes, I can see that the comparison is causing problems for everyone. I will remove it to a gist. I did now write a section ""Implementation as syntactic expansion"" that should hopefully make things very clear. I think you got it now (should be easy to understand with the syntactic expansion section). I cleaned up the proposal by moving out critiques to a separate gist, let me know what else I can do to clarify things."
technical,"It sounds like this proposal is suggesting that the ? operator take a function as the right hand argument.  That function is treated in an unusual manner: if that function returns, then the calling function, the one in which ? appears, returns.  But orthogonality suggests that if ? can be used with an ordinary function literal, then it can be used with any function.  What happens then?  That seems very confusing, not to mention hard to implement. It's not so far fetched to think that if this capability is available, people will naturally want to consolidate their error handlers in the form of a function.  When using handler from the design draft, this happens by writing something like With this one it seems unnecessary to do that at all.  But does it work? I understand that the go team will be thorough and take time with proposals as they should. I am just trying to understand if I am following the proposal process properly. As to your comments on the proposal: I seems my proposal is hard to make it through parts without mis-reading. I think I can fix this by showing how it would actually be implemented in terms of syntax expansion. I will try that out, along with some re-wording, let me know how else I can make things more clear. The ? operator *always* returns the error from the function if one is present, a handler cannot alter this fact, it can only run side-effects and/or alter the error that gets returned. The right-hand-side function is treated the same way any function is treated in go. The special functionality comes only from the ? operator. We could require that a handler always return an error. As a convenience (and I am open to the possibility that this is a bad idea), we can allow a handler function to not return the error. I call this type of handler a cleanup handler. That just means that we transparently upgrade the cleanup handler to return its error (see the ThenErr appendix section for how this can be done). Yes, ? can be used with any function. The function must, however type check to avoid a compilation error. I agree user's will consolidate some handlers, but you actually lost me at the end. With this proposal one would write:"
technical,"This is the most natural way for programmers who use the Rust language. There are some risks in introducing the 'check/handle' keywords. I would hereby like to follow freman's line of reasoning and would like to express my dislike of the new error handler proposals. Personally, I still consider the central principles of GO to be extremely precious and fear that the demands for syntactic sugar only lead to a dilution of the previous clarity and purity. The manual and explicit error checking/handling is in my opinion one of GO's core strengths.  Currently, type error is an interface like any other. A special syntax for handling errors would be a fundamental change. *Interesting observation* Many who start learning GO complain about the repetitive explicit error checking, but most get used to it and soon appreciate it in the vast majority of cases.  **cases:**  1. Should the repeated use of if err != nil {} be a visually disturbance for some users a different color scheme in the editor could easily solved this problem for them 1. Should the introduction of error handler be focused on enrichment of error information. It may be better to think about why the received error messages are incomplete. It might be beneficial to improve the code of the error returning function, instead of creating new syntax to iron out the initial fault.  1. leafbebop argued that error handler may improve chaining. And even though this may be true in some cases, I would like to question the premise here. In my opinion, chaining does not necessarily result in less writing effort, more comprehensible structures nor easy to maintain program code. **** In addition, chaining is only possible if all functions involved have a exact argument order. This would result in chaining being used in some cases and the conventional way in others, creating two parallel paradigms.  1. The proposal suggests that one of the goals is reducing the program code and/or the writing effort for the programmer, as well as a new syntactic expression for reformatting a received error. **** It is particularly difficult for me to follow this reasoning, because it is already optional to use a formatting function and also the amount / readability of the program code is not improved. **** The proposal introduces a new function type type errorHandler func(error)error. However the proposal seems to disregard that such a formatting function has only a limited selection of information, which is a bad prerequisite as a formatting function (only e). Unless such a function would be specially incorporated in the calling function and get access to all variables (with all disadvantages). **** The use of errorHandler's not only changes the language appearance, but also the reading flow. This effect is reinforced by errorHandler's defined outside of the calling function body. The confusion becomes more evident when different formatting functions are called."
technical,"There is no doubt in my mind that the next proposal draft from the Go team will add named handlers (or a func type) and drop implicit handler chains, given the feedback to date. It might drop check altogether.  However, that isn't enough IMO. My detailed critique: Golang, how dare you handle my checks! I'll say this a million times - I hate exiting from a function without a return. This should  never  happen unless there is a catastrophic error (panic). Also, returning an error without adding on to the context should be discouraged. The unary ? is just a bad idea for Go in general. (Both returning implicitly AND not adding context to the error). The rest of the proposal is interesting, but I'm not sure how much I like the idea of the ?. I think it means too many things in too many different languages, and it would add to the confusion. (Conditional operator (C), null-safe operation (Kotlin), Coallessing (C#), etc). I also feel like the built-in check function approach  feels  more like Go. I like that better than the ?. You guys discussed it being ""less noticable"" which is ""good"", but I'd say the opposite. It's the potential exit to a function, it  needs  to be noticable to be maintainable. Using check(...) instead of ? also resolves your ""should we allow break and continue as the RHS? The answer: no. Either way, this shouldn't be about syntax, syntax can be rethought. Let's discuss the idea behind it. I think having different function signatures doing different things is an interesting idea, but I don't like it. I think it makes reading code confusing, especially at the call site (""check site""?) of the handler function. If I'm reading someone else's code, I don't want to have to scroll back up to the top of the function to see if my code continues or not. The nice thing about the handle/check construct is that you  know  that  if the error is not nil, the function exits . I do like this idea though. Those are my critiques, I like the rest of the proposal. The use of anonymous functions rather than handler blocks is a good idea in my book."
technical,"Should the repeated use of if be a visually disturbance for some users a different color scheme in the editor could easily solved this problem for them. Syntax highlighting is never a solution to a problem and a programming language should NOT rely on syntax highlighting to make it readable. Here are a few lines of code peeled from the draft: 15 lines of error handling code, 5 lines of ""what we want to do"" code. That's why it's visually unappealing - there is way too much boilerplate and code repetition.  Go is all about writing programs that do what we want them to do, and it gets a lot harder to do that when we need  at least  3 lines of code on every function call that even has a  chance  of returning an error. If so, please tell me, why do so many programmers use syntax highlighting? Because they want to paint beautiful letters pictures? It seems we do not agree on this point. I think it's an obsession and a relic from other programming languages to think error's are something out of the ordinary (special cases) and not part of the actual code. It seem to me that your use case or expectation for GO may be inappropriate. We dont start to adapt Go for the styling of web pages, as a replacement of CSS."
technical,"Good luck! it seems you have misread the proposal. Perhaps I wrote too much, let me know how I can make the section on  handler function types more clear. Currently it does state: ""A cleanup function will automatically be converted to return the original error that would have been passed to it.""When used as a handler, the error will still be passed along (see the section on ThenErr to show how this can be accomplished). In this proposal, the usage of check or ? always means that the function returns immediately if the error is not nil."
technical,"Let me expand on that to say that nothing is going to happen in a hurry.  We're going to take the time required to make changes that seem good. It sounds like this proposal is suggesting that the ? operator take a function as the right hand argument.  That function is treated in an unusual manner: if that function returns, then the calling function, the one in which ? appears, returns.  But orthogonality suggests that if ? can be used with an ordinary function literal, then it can be used with any function.  What happens then?  That seems very confusing, not to mention hard to implement. It's not so far fetched to think that if this capability is available, people will naturally want to consolidate their error handlers in the form of a function.  When using handler from the design draft, this happens by writing something like With this one it seems unnecessary to do that at all.  But does it work?"
technical,"your objections are real. I only meant that some of them are not specific to this proposal (e.g. suggestions for adding special syntax highlighting, stating that the current verbosity is a non-problem, etc) but apply to essentially any proposal including this one. In that case we would prefer to keep it as general feedback on the wiki and just point to that on new error handling proposals instead of re-hashing the same conversations on every error handling proposal.  I did respond to the given code sample critique that was specific to the proposal. I know you did have some other specific points, but unfortunately I wasn't able to make sense of them (code is the clearest expression, nobody else was understanding my proposal until I wrote it in code). It's time for everyone to have a time out and cool off."
technical,There is a relatively slow moving Go2 proposal review process. Let me expand on that to say that nothing is going to happen in a hurry.  We're going to take the time required to make changes that seem good.
technical,"zero values: thanks for bringing that up. This proposal is essentially for discriminated unions. That is, the non-error value should not exist. I know that use cases do exist for actually returning a tuple of two meaningful values. However, I believe they are rare enough (I have seen thousands of lines of go code that never do this) that it is a mistake to place them as a design constraint on an error handling system. One can still use one of two approaches: * use the existing style of error handling * use an error type that gives back the value you want The latter looks something like this:  Returns how many occurrences of find exist in the UTF8 encoded reader. I believe you do need generics to be able to return the concrete type through an error check. no unary form of the check: I would be okay with always requiring an error handler that adds context. But I thought always requiring a handler was probably too heavy-handed for a go community that is not already consistently doing that. If you define a function identity, then you just have to write ? identity if you don't want to add anything to an error. So keep in mind it is easy to subvert the intent. An additional consideration is that some users may be satisfied enough by using stack traces that they don't feel the need to add context in every passing of an error."
technical," Operator ? looks less noticeable than ""check""."
technical,"I mean that filter can work with pipe. See link I posted above re ""critique"" for perspective on nesting calls that return error. (A ""pipe"" is an IPC or stream mechanism btw.)"
technical,"But I want to code in the Go language, not the Rust language - or I'd be coding in the Rust language :) Are our goals to be like other languages? Error handling Go results in the same questions and issues every time a new one of our devs picks it up. Over time, so long as they actually care to improve their craft they usually come to find it refreshing, especially as they learn of better ways of dealing with the errors. Sure it's not entry level easy, and it can be repetitive... but visual basic has on error resume next which has to be the easiest least repetitive way to ~~ignore errors~~ create unreliable code beyond underscoring them in Go. If I want more sugar, I can go back to perl where half of what I write in a maintainable way in Go can be squeezed into one line of code. Should the repeated use of if be a visually disturbance for some users a different color scheme in the editor could easily solved this problem for them. Syntax highlighting is never a solution to a problem and a programming language should NOT rely on syntax highlighting to make it readable. Here are a few lines of code peeled from the draft: 15 lines of error handling code, 5 lines of ""what we want to do"" code. That's why it's visually unappealing - there is way too much boilerplate and code repetition.  Go is all about writing programs that do what we want them to do, and it gets a lot harder to do that when we need  at least  3 lines of code on every function call that even has a  chance  of returning an error."
technical,"That's not what I'm trying to say here - what I'm saying is that the zero-value of int is meaningful in the CountOccurences function, so I would much rather return a -1 to make it clear that the function doesn't return meaningful information if an error occurs. I want to be clear. I don't want the caller to see 0, err, as it could be mistaken for ""zero occurrences were found before finding the following error"", I want them to see values from the function indicating that the function does not return useful information (other than the error) if an error occurs, which can be done by returning -1, err. Most of the time 0, err works, but in my experience, returning -1, err is not an uncommon case sorry for missing your actual concern. I think your level of programming defensiveness is probably appropriate given the lack of discriminated unions in go and the prevalence of zero values. However, it seems not generally applicable (what if I have a uint?) and unnecessary. The contract is always that the caller must check the error value before looking at the success value. We shouldn't weaken this proposal because someone is going to ignore errors. There are linters that check for this (errcheck): it would be much more powerful to add that capability to go vet or otherwise have this statically checked."
technical,"This is a very fair point. Although I think that -1 is still a pretty common thing to return when an error occurs. I've said this before, I really like the proposal. It feels very Go-like (at least when using a check built-in function), which is hard to come by for proposals not from the Go team themselves. I added a +1. Sorry if it seems like I'm nitpicking it pretty hard, just want to make sure everything is considered, this is a really good proposal. Yeah I was the same way. I saw handle as more of a goto than a function. Although both views work and I can see it going both ways. I think it personally makes more sense as a goto (it's how it probably works under the hood, AND just works better in general when it comes to how it returns). thanks for the  thorough review, and the good questions! Also, please help me promote usage of errcheck/gosec so that we don't have to bend over backwards with defensive coding practices! is there a process to moving this proposal forward with more reviews?"
technical,"I encourage you to paste your first comment here into a gist linked from the feedback wiki, as it's mostly a critique of the error handlers concept, not the substance of this proposal :-). this issue isn't a forum to debate a commenter who took issue with handlers :-) Regarding gomarcus' critique of handlers, I have tried to document all the possible requirements for a new errors idiom (and thus benefits) on golang-dev. Thanks for trying to keep the discussion on track: I would like to keep it focused on the proposal at hand. I see one point that is quite relevant to address: Is it bad to introduce this error handler concept if it doesn't scale down to simple usage? That is, if I don't need a shared handler, would I use this ""improved"" error handling? This is the benefit of the handler being a second argument to the check: you can write your handler inline without the overhead of naming it or registering it. So my version given in the above examples would still be:  I don't think this is inherently better than doing the traditional err != nil, but it is something you *could* do if you wanted to consistently do error handling the same way. Note that if you are not just side-effecting but also return error values, the ? will generate zero values for you also.  But this example is quite contrived as is: the real way you would write it. This is why I believe it is important to use normal functions as handlers: function composition is such a powerful tool.  It is probably possible to come up with an example more like the first that doesn't benefit from handler helper functions. If go lang had a good anonymous function syntax (type inference and no return), I would still prefer ?."
technical,"no unary form of the check: I would be okay with always requiring an error handler that adds context. But I thought always requiring a handler was probably too heavy-handed for a go community that is not already consistently doing that. If you define a function identity, then you just have to write ? identity if you don't want to add anything to an error. So keep in mind it is easy to subvert the intent. An additional consideration is that some users may be satisfied enough by using stack traces that they don't feel the need to add context in every passing of an error. That's not what I'm trying to say here - what I'm saying is that the zero-value of int is meaningful in the CountOccurences function, so I would much rather return a -1 to make it clear that the function doesn't return meaningful information if an error occurs. I want to be clear. I don't want the caller to see 0, err, as it could be mistaken for ""zero occurrences were found before finding the following error"", I want them to see values from the function indicating that the function does not return useful information (other than the error) if an error occurs, which can be done by returning -1, err. Most of the time 0, err works, but in my experience, returning -1, err is not an uncommon case"
technical,"It's time for everyone to have a time out and cool off. The right way to submit feedback and alternate designs like this is to post it somewhere else (a blog post, Medium, a Gist, etc) and then link it from the wiki page. Thanks."
technical,"thanks for the  thorough review, and the good questions! Also, please help me promote usage of errcheck/gosec so that we don't have to bend over backwards with defensive coding practices! is there a process to moving this proposal forward with more reviews? There is a relatively slow moving Go2 proposal review process."
technical,"you are right I should explicitly talk about how the question mark was mentioned in ""considered ideas"". In that, the case is made for check rather than ?. Some of those points are still valid with this proposal, and others are not. This is reviewed in the section ""Appendix: Operator versus check function"".  I hope we can move the conversation from comparing ? to check (either of which are acceptable to me) to the bigger picture of using regular functions instead of special stacking. There is no doubt in my mind that the next proposal draft from the Go team will add named handlers (or a func type) and drop implicit handler chains, given the feedback to date. It might drop check altogether.  However, that isn't enough IMO. My detailed critique: Golang, how dare you handle my checks!"
technical,"I personally think that handle is sugar for goto rather than an anonymous function. It seems to be doing this. If you read it like that, the return makes perfect sense. Simplified example. This is a very fair point. Although I think that -1 is still a pretty common thing to return when an error occurs. I've said this before, I really like the proposal. It feels very Go-like (at least when using a check built-in function), which is hard to come by for proposals not from the Go team themselves. I added a +1. Sorry if it seems like I'm nitpicking it pretty hard, just want to make sure everything is considered, this is a really good proposal. Yeah I was the same way. I saw handle as more of a goto than a function. Although both views work and I can see it going both ways. I think it personally makes more sense as a goto (it's how it probably works under the hood, AND just works better in general when it comes to how it returns)."
technical,"I think our key goal is making the consequence of error handling more clear. When I read a check or ?, I want to know two things: 1. Which handler will be invoked. 2. What will happen to current function after error handling. This proposal let me understand them without searching in code context. This is the most natural way for programmers who use the Rust language. There are some risks in introducing the 'check/handle' keywords."
technical,"The right way to submit feedback and alternate designs like this is to post it somewhere else (a blog post, Medium, a Gist, etc) and then link it from the wiki page. Thanks. To everyone else, please note that detailed discussion like this does not belong on the issue tracker."
technical,"I'll say this a million times - I hate exiting from a function without a return. This should  never  happen unless there is a catastrophic error (panic). Also, returning an error without adding on to the context should be discouraged. The unary ? is just a bad idea for Go in general. (Both returning implicitly AND not adding context to the error). The rest of the proposal is interesting, but I'm not sure how much I like the idea of the ?. I think it means too many things in too many different languages, and it would add to the confusion. (Conditional operator (C), null-safe operation (Kotlin), Coallessing (C#), etc). I also feel like the built-in check function approach  feels  more like Go. I like that better than the ?. You guys discussed it being ""less noticable"" which is ""good"", but I'd say the opposite. It's the potential exit to a function, it  needs  to be noticable to be maintainable. Using check(...) instead of ? also resolves your ""should we allow break and continue as the RHS? The answer: no. Either way, this shouldn't be about syntax, syntax can be rethought. Let's discuss the idea behind it. I think having different function signatures doing different things is an interesting idea, but I don't like it. I think it makes reading code confusing, especially at the call site (""check site""?) of the handler function. If I'm reading someone else's code, I don't want to have to scroll back up to the top of the function to see if my code continues or not. The nice thing about the handle/check construct is that you  know  that  if the error is not nil, the function exits . I do like this idea though. Those are my critiques, I like the rest of the proposal. The use of anonymous functions rather than handler blocks is a good idea in my book. To expand on the whole function signature thing, here's what I mean: Again, in the check/handle made it clear: if err != nil, the function will exit"
technical,"Go is absolutely readable in its current form. I just said, in case it's not for a minority group of people... they could use it... Let's drop this, it does not help the actual Diskursion... Especially when refactoring functions, I can not grip your problem. Because using your example function CopyFile, would only require a single test. done! What I am saying is that Go is meant to be a language where what we write is what happens when it's run, contrasted to lots of OOP languages which have extremely confusing type systems and constructs Well, if this is your opinion, I wonder why you want to change this! That's the whole point let GO as it is! Don't put a square peg in a round hole. If something needs to be changed, add something useful ... like add generics but dont rush it. (Also, stop spelling it GO, it's spelled Go.)"
technical,"I see, so the handler is passed the error, and can return a modified error, and that error is returned by the function using the ? operator? (For a language change proposal I find that it's normally best to present the suggested changes by themselves as a standalone document, rather than by comparison to a different change.) yes, I can see that the comparison is causing problems for everyone. I will remove it to a gist. I did now write a section ""Implementation as syntactic expansion"" that should hopefully make things very clear."
technical,"You are right. Less noticeable may be an advantage. you are right I should explicitly talk about how the question mark was mentioned in ""considered ideas"". In that, the case is made for check rather than ?. Some of those points are still valid with this proposal, and others are not. This is reviewed in the section ""Appendix: Operator versus check function"".  I hope we can move the conversation from comparing ? to check (either of which are acceptable to me) to the bigger picture of using regular functions instead of special stacking."
technical, You are right. Less noticeable may be an advantage.
technical,"Operator ? looks less noticeable than ""check"". you might want to state whether that is a good or a bad thing! I am assuming it is a critique. One advantage of this proposal is that it is not breaking any new ground, but instead following the lead of Rust (but adding a handler component). So we could survey Rust users to see if noticeably of ? is a problem. Although I have a preference for ?, I want to note that I would be perfectly happy with this proposal being accepted but modified to use check instead."
technical,"Although I can understand the hint, I refuse the accusation by gregwebs that my objections had little to do with the proposal being discussed. The basic problems of the proposal remain. your objections are real. I only meant that some of them are not specific to this proposal (e.g. suggestions for adding special syntax highlighting, stating that the current verbosity is a non-problem, etc) but apply to essentially any proposal including this one. In that case we would prefer to keep it as general feedback on the wiki and just point to that on new error handling proposals instead of re-hashing the same conversations on every error handling proposal.  I did respond to the given code sample critique that was specific to the proposal. I know you did have some other specific points, but unfortunately I wasn't able to make sense of them (code is the clearest expression, nobody else was understanding my proposal until I wrote it in code)."
technical,"I posted a link to a pure critique of check/handle, which largely applies to this proposal. It does not mention #id/catch. I urge you to read it.  I mentioned a catch block here as a solution to the control flow issue raised above, and used a prefix variation of your ? handler syntax with it. zero values: thanks for bringing that up. This proposal is essentially for discriminated unions. That is, the non-error value should not exist. I know that use cases do exist for actually returning a tuple of two meaningful values. However, I believe they are rare enough (I have seen thousands of lines of go code that never do this) that it is a mistake to place them as a design constraint on an error handling system. One can still use one of two approaches: * use the existing style of error handling * use an error type that gives back the value you want The latter looks something like this:  Returns how many occurrences of find exist in the UTF8 encoded reader. I believe you do need generics to be able to return the concrete type through an error check."
technical,"Ryan Dahl pinpoints the core problems of node.js. Don't confuse ""decisions he says he regrets"" with ""core problems of node.js"". The community, judging by the amount of stars on deno, feels the same.  Judging by the amount of stars seems like a pretty poor way to judge. ""I am interested in seeing where this project goes"" is not necessarily the same as ""Right on! I agree that the other project has huge problems!""  How do you feel about addressing those issues? Is it doable? May be a fork? Sorry, this isn't really much of a question appropriate for the issue tracker. This is more of a Call For Hot Takes, which is not particularly helpful here IMO.  Many of the core devs on Node.js have commented (e.g. on Twitter) on what aspects of the presentation they agree with and disagree with. You can do some web searches if you really care what the core devs think. Or maybe a few will weigh in here. (Sorry, I won't be one of them.)  If you have a very specific question, feel free to post it in an appropriate forum. Depending on the question, this issue tracker *might* be the right forum. (Sorry, I won't be one of them.)  OK, maybe I will comment a little bit. Some of these so-called ""core problems of node.js"" are just unimportant things we'll have to keep supporting or else break the entire ecosystem. So no, they won't get ""fixed"". But they're also not ""broken"". For example, supporting ""index.js"" as the default file for modules is maybe a less-than-perfect original decision, but now that there are hundreds of thousands of modules that do this, we're not going to change. The benefit of changing is negligible and the cost of changing is enormous."
technical,"Maybe to add one thing: If you, or somebody else, think Node.js *should* address one of the particular concerns, and are willing to help in making that happen, I'm sure we can arrange that. :slightly smiling face: Don't confuse ""decisions I regret"" with ""core problems of node.js"".  Ryan is not the first to mention them, there were others along the road who complained about packaging, gyp and node modules.  Security is a major issue IMHO, its only a matter of time until someone will plant a malware in some popular module. Possibly. Lets say only 1/3 of these people starred the project because they feel that node has problems - it is still a lot for a fresh project on GitHub."
technical,"Too heated doesn't really capture it, but none of the choices fit perfectly. On the whole, though, I agree with Trott that this is a topic best addressed through individual discussions, and trying to handle it all in a single issue is unproductive. How do you feel about addressing those issues? Is it doable? May be a fork?  I believe most of the points raised in the talk are not new to the project, but out of compatibility concerns it would be hard to do anything about them at this point. Deno is new and is free to experiment with ideas, but Node.js cannot afford that luxury with the enormous user base and the existing code out there - people may be upset when the existing design is not perfect, but they are usually more upset when their working code gets broken or when they are told to upgrade their massive code base to a more idiomatic style. Starting a general discussion around the issues may be helpful to make progress, but it is likely to go nowhere without anything actionable proposed.  The current ways of introducing significant changes into Node.js core are:  1. Start a working group, or a team, like nodejs/modules, that may or may not does periodic meetings and has their own issue tracker to tackle the cross-cutting concerns and reach consensus before starting the implementation in core 2. Start a new repo under this organization that contains a fork of Node.js core (there is usually a team started for it as well), work on the implementation there and sync with the upstream from time to time, but it's not going to have its own release (maybe that can be improved). When the team thinks it's ready, submit a PR back to the upstream. Past examples that have been merged into core:  nodejs/http2 and nodejs/abi-stable-node (prototype of N-API, you'll need to switch branches to find the prototypes and the collaboration happened there)  For both type of efforts we will usually have an item listed in the strategic initiatives and a champion (usually TSC member) who reports the status of the initiative every week at the TSC meeting, which is live-streamed on YouTube with a public Q&A session (there are also recordings in the channel) and the meetings minutes are available in the TSC repo.  The members of those initiatives do not have to be Node.js core collaborators or even members of this organization, but the team will decide how they recruit new members (it's usually a call-for-participants kind of thing). If you want to start an initiative, I think the best way is to go to the #node-dev IRC and find people who are interested in a particular item, and discuss about starting a new team for it. It is also important to make sure that the stakeholders are aware and willing to participate (like the VM vendors in the case of N-API), reaching out to them in private should help as well. A lot of work (coordination is work and it's hard) would be needed to make this happen so you'll either do it yourself, or find people who are interested in spending their time on that.  There used to be node-eps but it's not used anymore."
technical," Ryan Dahl pinpoints the core problems of node.js. Don't confuse ""decisions he says he regrets"" with ""core problems of node.js"". The community, judging by the amount of stars on deno, feels the same.  Judging by the amount of stars seems like a pretty poor way to judge. ""I am interested in seeing where this project goes"" is not necessarily the same as ""Right on! I agree that the other project has huge problems!""  How do you feel about addressing those issues? Is it doable? May be a fork? Sorry, this isn't really much of a question appropriate for the issue tracker. This is more of a Call For Hot Takes, which is not particularly helpful here IMO.  Many of the core devs on Node.js have commented (e.g. on Twitter) on what aspects of the presentation they agree with and disagree with. You can do some web searches if you really care what the core devs think. Or maybe a few will weigh in here. (Sorry, I won't be one of them.)  If you have a very specific question, feel free to post it in an appropriate forum. Depending on the question, this issue tracker *might* be the right forum."
technical,"How do you feel about addressing those issues? Is it doable? May be a fork?  I believe most of the points raised in the talk are not new to the project, but out of compatibility concerns it would be hard to do anything about them at this point. Deno is new and is free to experiment with ideas, but Node.js cannot afford that luxury with the enormous user base and the existing code out there - people may be upset when the existing design is not perfect, but they are usually more upset when their working code gets broken or when they are told to upgrade their massive code base to a more idiomatic style. Starting a general discussion around the issues may be helpful to make progress, but it is likely to go nowhere without anything actionable proposed.  The current ways of introducing significant changes into Node.js core are:  1. Start a working group, or a team, like nodejs/modules, that may or may not does periodic meetings and has their own issue tracker to tackle the cross-cutting concerns and reach consensus before starting the implementation in core 2. Start a new repo under this organization that contains a fork of Node.js core (there is usually a team started for it as well), work on the implementation there and sync with the upstream from time to time, but it's not going to have its own release (maybe that can be improved). When the team thinks it's ready, submit a PR back to the upstream. Past examples that have been merged into core:  nodejs/http2 and nodejs/abi-stable-node (prototype of N-API, you'll need to switch branches to find the prototypes and the collaboration happened there)  For both type of efforts we will usually have an item listed in the strategic initiatives and a champion (usually TSC member) who reports the status of the initiative every week at the TSC meeting, which is live-streamed on YouTube with a public Q&A session (there are also recordings in the channel) and the meetings minutes are available in the TSC repo.  The members of those initiatives do not have to be Node.js core collaborators or even members of this organization, but the team will decide how they recruit new members (it's usually a call-for-participants kind of thing). If you want to start an initiative, I think the best way is to go to the #node-dev IRC and find people who are interested in a particular item, and discuss about starting a new team for it. It is also important to make sure that the stakeholders are aware and willing to participate (like the VM vendors in the case of N-API), reaching out to them in private should help as well. A lot of work (coordination is work and it's hard) would be needed to make this happen so you'll either do it yourself, or find people who are interested in spending their time on that.  There used to be node-eps but it's not used anymore. Ryan Dahl pinpoints the core problems of node.js.The community, judging by the amount of stars on deno, feels the same.  How do you feel about addressing those issues? Is it doable? May be a fork?"
technical,"Don't confuse ""decisions I regret"" with ""core problems of node.js"".  Ryan is not the first to mention them, there were others along the road who complained about packaging, gyp and node modules.  Security is a major issue IMHO, its only a matter of time until someone will plant a malware in some popular module. Possibly. Lets say only 1/3 of these people starred the project because they feel that node has problems - it is still a lot for a fresh project on GitHub. There is quite a bit of work being done in each of those areas, most of which it seemed Ryan was unaware of. I suggest checking out the discussion in nodejs/modules, discussions around migrating away from gyp, and the Security Working Group. If you'd like to contribute to any of those and help us address the perceived issues you mentioned, we'd welcome contributions with open arms."
technical,"There is quite a bit of work being done in each of those areas, most of which it seemed Ryan was unaware of. I suggest checking out the discussion in nodejs/modules, discussions around migrating away from gyp, and the Security Working Group. If you'd like to contribute to any of those and help us address the perceived issues you mentioned, we'd welcome contributions with open arms. Too heated doesn't really capture it, but none of the choices fit perfectly. On the whole, though, I agree with Trott that this is a topic best addressed through individual discussions, and trying to handle it all in a single issue is unproductive."
technical,"thanks for  not  including those invectives. ˜…  We're working on the documentation around ""dynamic profiles,"" but until that lands:  There's a fine line we're walking between dynamic and user-generated content. Most of the dynamic generators are for system-dependent things like ""what WSL distributions do you have"" and ""which versions of powershell core are installed?"". Each distribution gets its own profile, as will each powershell version.  We want to make sure they're  found , but also that the user is clued into the fact that they can be customized. To that end, we add a ""stub"" entry to your settings file.  When we can't find it,  we assume it's because the generator never ran or discovered new content (like: a WSL distribution was installed) rather than that the user removed it.  With just the one settings file as a single source of truth, we aren't storing a bit somewhere saying ""we already generated {aaaa}, don't do it again"" anywhere but the actual profiles list.  Broadly speaking, you've got two options for disabling this profile (and any other dynamic/automatic profiles).  1. Hide the profile, live with it being in your settings, and continue to let us know that you're not happy that it's there. Set hidden to false, and it'll stop cluttering up your profile list and key bindings.  2.  Disable the whole dynamic generator . We have three dynamic profile generators today: * Windows.Terminal.Azure * Windows.Terminal.Wsl * Windows.Terminal.PowershellCore  If you add one of these names (or all of them!) to a new array at the top level of your settings file called disabledProfileSources, like this:  the generator won't even run and the profile will never come back. (The part about continuing to let us know was not meant to read like ""we're ignoring you, everybody please stop yelling"" -- it was intended to sound more like ""please express dissatisfaction, we want to make sure we're doing the right things"")"
technical,"So what you're saying is that every time I upgrade or update any software made by you and yours I have to scour the documentation and support and config files for any new ""features"" and traps you may have placed in it?  How about ASKING THE USER? Did that ever cross your mind?  ""Hey, I've detected that you've made a change in the config. I'm about to undo all your work and confuse the hell out of you, Would you like me to continue?"" # Steps to reproduce  Open profiles.json through the settings menu option and edit the profiles.json file. Remove the section for Azure Cloud Shell completely including the surrounding brackets and comma preceding the first curly bracket.  leaving the file valid but missing the Azure option. I have no need for it.  # Expected behavior  I expect to be able to remove the option that I do not have any use for.  # Actual behavior  When the Windows Terminal Program is run, the profile segment that was removed reappears as if by magic much to my amazement. If I mark the file as read-only to protect it from being reverted, Windows Terminal refuses to load.  I literally watched the section in question reappear in the configuration file open in VS code after I deleted it saved the file and re-run the Terminal program.  I want very much to include several invectives and expletives in this bug report. But I will refrain from doing so. But I want you to imagine what I would have said and you be sure and be creative.  The following is the text that I remove from the configuration file that reappears when the Windows Terminal program is run. There is no error or notification. It just reappears and the option for Azure Cloud is on the menu."
technical,"This really isn't cool, and the story about dynamic generators is really thin. You are shipping with a default config, and if a user removes a part of that config, re-adding it and not leaving any hints in the settings file how to disable your fancy ""dynamic generator"" is just a super user-hostile action.  Since you ship with a default config, you can be sure that it is  found  - because you shipped it that way. Getting rid of it was literally my first course of action on installing this software.  My next action was googling for ""wtfbshax why is this Azure rubbish coming back"" and, after reading this thread, my next action will be to get rid of this software immediately. Microsoft once again doesn't fail to disappoint. I deleted this app a while ago when I discovered they were blaming Microsoft Core devs for bugs they can't be bothered to fix or work around."
technical,"So am I to understand that you are saying that the Terminal program scans to see what's installed each time it is run and creates the profiles for the things it finds assuming that if there isn't already a profile for it that it should just make one? Indeed! It only scans for a small set of things -- WSL distributions and PowerShell Core instances. The Azure generator doesn't scan anything.  For what it's worth, this was informed by user requests in #1289, #1424, #1394, #1518, #1674, #2037, #2023, #2283, #2300, #2536 and #2804 (and the lack of autogeneration caused bugs like #1692 and #1449)  When we have a settings UI (#1564) there'll be a more natural way to configure what terminal does/doesn't do on your behalf, and a bit more visibility into the ""scanning"" process."
technical,"(The part about continuing to let us know was not meant to read like ""we're ignoring you, everybody please stop yelling"" -- it was intended to sound more like ""please express dissatisfaction, we want to make sure we're doing the right things"") So am I to understand that you are saying that the Terminal program scans to see what's installed each time it is run and creates the profiles for the things it finds assuming that if there isn't already a profile for it that it should just make one?"
technical," thanks for  not  including those invectives. ˜…  We're working on the documentation around ""dynamic profiles,"" but until that lands:  There's a fine line we're walking between dynamic and user-generated content. Most of the dynamic generators are for system-dependent things like ""what WSL distributions do you have"" and ""which versions of powershell core are installed?"". Each distribution gets its own profile, as will each powershell version.  We want to make sure they're  found , but also that the user is clued into the fact that they can be customized. To that end, we add a ""stub"" entry to your settings file.  When we can't find it,  we assume it's because the generator never ran or discovered new content (like: a WSL distribution was installed) rather than that the user removed it.  With just the one settings file as a single source of truth, we aren't storing a bit somewhere saying ""we already generated {aaaa}, don't do it again"" anywhere but the actual profiles list.  Broadly speaking, you've got two options for disabling this profile (and any other dynamic/automatic profiles).  1. Hide the profile, live with it being in your settings, and continue to let us know that you're not happy that it's there. Set hidden to false, and it'll stop cluttering up your profile list and key bindings.  2.  Disable the whole dynamic generator . We have three dynamic profile generators today: * Windows.Terminal.Azure * Windows.Terminal.Wsl * Windows.Terminal.PowershellCore  If you add one of these names (or all of them!) to a new array at the top level of your settings file called disabledProfileSources, like this:  the generator won't even run and the profile will never come back."
technical,"If it's worth stating that the file should be in UTF-8 format then it's worth mentioning the avoidance of a BOM.  But that's just my little ol' opinion. [ What if my editor is dumb and pre-checks the BOM box, for UTF-8 files, or just includes it without giving me any option? ]"
technical,"The reason is that back-compatibility with ASCII is a major benefit of UTF-8. For instance a Windows .bat/batch file saved as UTF-8 + BOM will crash when run because the command interpreter doesn't understand the BOM (ie. ”) prefix -- It's just garbage, which also displays in some text editors, which could easily confuse one as to whether to let this  mysterious garbage  persist, or should they remove it?  The following Unicode laced .bat file, saved as UTF-8, will work properly only when saved  **without**  a BOM: As the standard itself states, it neither requires nor recommends the use of a BOM, with UTF-8.  The so-called ""round-tripping"" scenario is really only appropriate when dealing with (pre-existing / external) text files, input into a program --not to the (Google) source code to be executed.  So ideally one shouldn't erase existing BOMs, willy-nilly, because they may be processed by another system which, against the standard's recommendation, expects their presence.  However, when fresh source files are created, the BOM is probably best avoided, in accordance with the Unicode/UTF-8 recommendation.  It follows the programming principle: ""Be liberal in what you accept and conservative in what you send."" I don't think this is worth adding. As you say, the UTF-8 spec already discourages using a BOM when feasible, and I don't see any reason why it being a Java source file introduces further complexity that merits calling this out in the style guide. I'm not certain I've ever even seen a UTF-8 file with a BOM..."
technical,"Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).  Once you've signed (or fixed any issues), please reply here with googlebot I signed it! and we'll verify it. #### What to do if you already signed the CLA  ##### Individual signers *   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your [email is set on your git commits.  ##### Corporate signers *   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go *   The email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits. *   The email used to register you as an authorized contributor must also be attached to your GitHub account). I signed it!"
technical,"I don't think this is worth adding. As you say, the UTF-8 spec already discourages using a BOM when feasible, and I don't see any reason why it being a Java source file introduces further complexity that merits calling this out in the style guide. I'm not certain I've ever even seen a UTF-8 file with a BOM... If it's worth stating that the file should be in UTF-8 format then it's worth mentioning the avoidance of a BOM.  But that's just my little ol' opinion."
technical,"[ What if my editor is dumb and pre-checks the BOM box, for UTF-8 files, or just includes it without giving me any option? ] It's not the goal of the Java Style Guide to address all possible configuration issues, we require Java source code to be UTF-8 for numerous reasons, but the presence or absence of a BOM is not something we need to make a ruling on. Leave it off when possible, as the UTF-8 spec already says."
technical,"Hi, first, let me say thank you for sending this PR, I should have done so in an earlier comment. Rejecting the PR is in no way a statement about you, nor your qualifications. Although it's true that Google's public style guides are under-curated (a situation we all lament, but no one has stepped up to address) we care very deeply about the contents of our style guides, and therefore are very conservative in what we add to it. My intent here was to bring closure to this issue, since it had unfortunately sat unaddressed for some time. You are absolutely right that there are situations where a BOM can matter, and it is reasonable to discourage their use. In our experience at Google this has not been a major issue (admittedly, most internal development happens on Linux, so issues related to .bat scripting are indeed something we don't typically run into). Looking into historical internal discussions around BOMs affecting Google Java developers I can find very little, and nothing related to BOMs in the .java source files themselves. None of which invalidates your concern, but in our opinion it doesn't rise to a level of severity to merit calling out in the Style Guide. Many best practices are intentionally not included in the guide simply because we don't want to make a ruling we don't have to. We trust teams to identify their own best practices where we don't make stipulations. Of note, this doesn't appear to be a Java-specific issue, and I notice only the HTML/CSS guide makes any mention of it. Perhaps we should consistently discourage BOMs across all our style guides, but I don't believe it's necessary to make the Java guide an outlier in this regard. I would ask that you do so with good-faith, but *please do* include a BOM if you believe it is relevant. We put a lot of faith in our tooling, and count on developers both internally and externally to help catch places where our tooling falls short. If a BOM breaks something we rely on, we will want to fix it. You might even get paid if you are able to find an exploit related to mishandling of BOMs.. One of the things I have deeply, deeply valued about my time at Google has been the candid and open discussions around issues of identity, gender inequality, and respect. We continue to fall short in many ways, but I am proud of the effort my peers have made to create a culture that is welcoming and inclusive. I don't believe anything I've said could fairly be labeled mansplaining, but I am sorry that I was curt. Again, thank you for raising this issue. Specify that no byte order mark should be used (when applicable) for source files."
technical," Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).  Once you've signed (or fixed any issues), please reply here with googlebot I signed it! and we'll verify it. #### What to do if you already signed the CLA  ##### Individual signers *   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your [email is set on your git commits.  ##### Corporate signers *   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go *   The email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits. *   The email used to register you as an authorized contributor must also be attached to your GitHub account)."
technical," The purpose of this commit is to avoid potential problems associated with BOMs, since they have been known to halt some script interpreters and compilers in their tracks.  Personally speaking, I have never experienced any problems by omitting the BOM.  But I write/code primarily in English.  If non-English writing/coding doesn't necessitate a BOM then it seems that the appropriate option would be to forgo its use, by default."
technical,"This doesn't really seem worth stipulating, the BOM has no meaning for UTF-8 What's the motivation for calling this out? Interpreters that don't handle BOMs sound like they aren't properly implementing the UTF-8 spec and should be fixed. The reason is that back-compatibility with ASCII is a major benefit of UTF-8. For instance a Windows .bat/batch file saved as UTF-8 + BOM will crash when run because the command interpreter doesn't understand the BOM (ie. ”) prefix -- It's just garbage, which also displays in some text editors, which could easily confuse one as to whether to let this  mysterious garbage  persist, or should they remove it?  The following Unicode laced .bat file, saved as UTF-8, will work properly only when saved  **without**  a BOM: As the standard itself states, it neither requires nor recommends the use of a BOM, with UTF-8.  The so-called ""round-tripping"" scenario is really only appropriate when dealing with (pre-existing / external) text files, input into a program --not to the (Google) source code to be executed.  So ideally one shouldn't erase existing BOMs, willy-nilly, because they may be processed by another system which, against the standard's recommendation, expects their presence.  However, when fresh source files are created, the BOM is probably best avoided, in accordance with the Unicode/UTF-8 recommendation.  It follows the programming principle: ""Be liberal in what you accept and conservative in what you send."""
technical,"The purpose of this commit is to avoid potential problems associated with BOMs, since they have been known to halt some script interpreters and compilers in their tracks.  Personally speaking, I have never experienced any problems by omitting the BOM.  But I write/code primarily in English.  If non-English writing/coding doesn't necessitate a BOM then it seems that the appropriate option would be to forgo its use, by default. This doesn't really seem worth stipulating, the BOM has no meaning for UTF-8 What's the motivation for calling this out? Interpreters that don't handle BOMs sound like they aren't properly implementing the UTF-8 spec and should be fixed."
technical,"The purpose of this commit is to avoid potential problems associated with BOMs, since they have been known to halt some script interpreters and compilers in their tracks.  Personally speaking, I have never experienced any problems by omitting the BOM.  But I write/code primarily in English.  If non-English writing/coding doesn't necessitate a BOM then it seems that the appropriate option would be to forgo its use, by default. [APPROVAL NOTIFIER] This PR is **APPROVED**  This pull-request has been approved by author. The full list of commands accepted by this bot can be found here. The pull request process is described here  Needs approval from an approver in each of these files. Approvers can indicate their approval by writing /approve in a comment Approvers can cancel approval by writing /approve cancel in a comment"
technical,"This isn't how we have technical disagreements. No amount of technical correctness can make up for treating other people poorly, please don't repeat that behavior. I will close both of these PRs. I'm available via slack or email if more explanation is needed. (additional note: please seek assistance rather than escalating.) Instead, I've asked him to send a fix for the issue in question. **What type of PR is this?** bug **What this PR does / why we need it**: Fixes Client should expose a mechanism to close underlying TCP connections. This PR should be merged after add http2 health check parameters for ConfigureTransport merged."
technical,"New changes are detected. LGTM label has been removed. Adding label do-not-merge/contains-merge-commits because PR contains merge commits, which are not allowed in this repository. Use git rebase to reapply your commits on top of the target branch. Detailed instructions for doing so can be found here."
technical," Adding the ""do-not-merge/release-note-label-needed"" label because no release-note block was detected, please follow our release note process to remove it. Instructions for interacting with me using PR comments are available here.  If you have questions or suggestions related to my behavior, please file an issue against the kubernetes/test-infra repository."
technical,[APPROVAL NOTIFIER] This PR is **APPROVED**  This pull-request has been approved by author. The full list of commands accepted by this bot can be found here. The pull request process is described here  Needs approval from an approver in each of these files. Approvers can indicate their approval by writing /approve in a comment Approvers can cancel approval by writing /approve cancel in a comment can you squash all the commits please?
technical,[APPROVAL NOTIFIER] This PR is **APPROVED**  This pull-request has been approved by author. The full list of commands accepted by this bot can be found here. The pull request process is described here  Needs approval from an approver in each of these files. Approvers can indicate their approval by writing /approve in a comment Approvers can cancel approval by writing /approve cancel in a comment have you run hack/update-vendor.sh?
technical,"Adding the ""do-not-merge/release-note-label-needed"" label because no release-note block was detected, please follow our release note process to remove it. Instructions for interacting with me using PR comments are available here.  If you have questions or suggestions related to my behavior, please file an issue against the kubernetes/test-infra repository. Hi. Thanks for your PR.  I'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step. Once the patch is verified, the new status will be reflected by the ok-to-test label.  I understand the commands that are listed here. Instructions for interacting with me using PR comments are available here.  If you have questions or suggestions related to my behavior, please file an issue against the kubernetes/test-infra repository."
technical,"Adding the ""do-not-merge/release-note-label-needed"" label because no release-note block was detected, please follow our release note process to remove it. Instructions for interacting with me using PR comments are available here.  If you have questions or suggestions related to my behavior, please file an issue against the kubernetes/test-infra repository. I may have opposition to hard-coding those Timeout values. Internally, there are several ways to detect dead connections. For example: error on readLoop, connection idle timeout... etc.  Most of detections kick-in during the readLoop except the one referred here.  The http2 Transport health check will kick-in when the readLoop is completely idle for ReadIdleTimeout seconds. It aims to do an additional health check when there is no incoming traffic for amount of time, but before IdleConnTimeout triggers idle connection recycling. Thus, I think the ReadIdleTimeout  should be smaller than IdleConnTimeout to achieve it's designed purpose.  In addition, I suggest keep the minimum PingTimeout value to default 15s. [From the source code in this), It uses time.AfterFunc(), not time.Ticker(), to trigger health check, and only Reset() the timer when the next readLoop kicks-in. It means that the health check will be performed ONLY ONCE when the readLoop blocks for amout of time. In such situation, there should be only one PingFrame on the fly, so the PingTimeout does not matter much. But setting the PingTimeout too low (eg: 2s in this PR) may lead to incorrect connection recycling during a network jitter or system load spike. Meanwhile, as the wide acceptance of http2, Kubernetes may have more additional default http2 Transport options to apply in the future. I suggest making a independent and well-commented helper function for the future configuration.  What I described above has already been done in my PR. Please consider my solution against this PR. To be honest(just technically, no offence), this PR looks nasty. It needs rebase&squash, and more comments in the code to describe the purpose of configuration and those ""magic number""."
technical,"can you squash all the commits please? it also fails with the following error  Vendor Verify failed. If you're seeing this locally, run the below command to fix your directories: These modules are pinned to versions different than the minimal preferred version. That means that without replace directives, a different version would be selected, which breaks consumers of our published modules. 1. Use hack/pin-dependency.sh to switch to the preferred version for each module 2. Run hack/update-vendor.sh to rebuild the vendor directory 3. Run hack/lint-dependencies.sh to verify no additional changes are required All pinned versions of checked dependencies match their preferred version.  you can check it in the pull-kubernetes-dependencies , I think that you can run make verify local to verify it"
technical,"can you squash all the commits please? Just adding a reference to my comments on #95898, which in the end is discussing both approaches."
technical,can you squash all the commits please? Looks like it should be based on this change in golang.
technical,Looks like it should be based on this change in golang. New changes are detected. LGTM label has been removed.
technical,"Since there is no need for uses to configure http2 health check, I do think we should add http2 health check by some reasonable default values, and if you guys think pingTimeout=2s is two short, we can tune it larger, maybe default 15s is a good option. Besides I can't find any significance that setting http2 Transport ReadIdleTimeout as the half of http Transport IdleConnTimeout, Is there any relationship between these two configurations? PR #95898 looks pretty but unnecessary, again, I insist my way to fix this problem, which is no doubt the most simple and efficient approach to this problem. And I don't care about which PR will be merged finally, but I do care about the most reasonable solution for this problem. If you guys could give reasons for setting http2 Transport ReadIdleTimeout as the half of http Transport IdleConnTimeout instead of some reasonable defaults, I'll be happy to close this PR ASAP. See this for context on locking. can you take a look at this and #95898 and give feedback on the best way to configure the PingTimeout/ReadIdleTimeout options?"
technical,"I may have opposition to hard-coding those Timeout values. Internally, there are several ways to detect dead connections. For example: error on readLoop, connection idle timeout... etc.  Most of detections kick-in during the readLoop except the one referred here.  The http2 Transport health check will kick-in when the readLoop is completely idle for ReadIdleTimeout seconds. It aims to do an additional health check when there is no incoming traffic for amount of time, but before IdleConnTimeout triggers idle connection recycling. Thus, I think the ReadIdleTimeout  should be smaller than IdleConnTimeout to achieve it's designed purpose.  In addition, I suggest keep the minimum PingTimeout value to default 15s. [From the source code in this), It uses time.AfterFunc(), not time.Ticker(), to trigger health check, and only Reset() the timer when the next readLoop kicks-in. It means that the health check will be performed ONLY ONCE when the readLoop blocks for amout of time. In such situation, there should be only one PingFrame on the fly, so the PingTimeout does not matter much. But setting the PingTimeout too low (eg: 2s in this PR) may lead to incorrect connection recycling during a network jitter or system load spike. Meanwhile, as the wide acceptance of http2, Kubernetes may have more additional default http2 Transport options to apply in the future. I suggest making a independent and well-commented helper function for the future configuration.  What I described above has already been done in my PR. Please consider my solution against this PR. To be honest(just technically, no offence), this PR looks nasty. It needs rebase&squash, and more comments in the code to describe the purpose of configuration and those ""magic number"". the following tests **failed**, say /retest to rerun all failed tests. Full PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR."
technical,"I may have opposition to hard-coding those Timeout values. Internally, there are several ways to detect dead connections. For example: error on readLoop, connection idle timeout... etc.  Most of detections kick-in during the readLoop except the one referred here.  The http2 Transport health check will kick-in when the readLoop is completely idle for ReadIdleTimeout seconds. It aims to do an additional health check when there is no incoming traffic for amount of time, but before IdleConnTimeout triggers idle connection recycling. Thus, I think the ReadIdleTimeout  should be smaller than IdleConnTimeout to achieve it's designed purpose.  In addition, I suggest keep the minimum PingTimeout value to default 15s. [From the source code in this), It uses time.AfterFunc(), not time.Ticker(), to trigger health check, and only Reset() the timer when the next readLoop kicks-in. It means that the health check will be performed ONLY ONCE when the readLoop blocks for amout of time. In such situation, there should be only one PingFrame on the fly, so the PingTimeout does not matter much. But setting the PingTimeout too low (eg: 2s in this PR) may lead to incorrect connection recycling during a network jitter or system load spike. Meanwhile, as the wide acceptance of http2, Kubernetes may have more additional default http2 Transport options to apply in the future. I suggest making a independent and well-commented helper function for the future configuration.  What I described above has already been done in my PR. Please consider my solution against this PR. To be honest(just technically, no offence), this PR looks nasty. It needs rebase&squash, and more comments in the code to describe the purpose of configuration and those ""magic number"". This PR requires add http2 health check parameters for ConfigureTransport merged, and I'm doing this currently."
technical,"Adding label do-not-merge/contains-merge-commits because PR contains merge commits, which are not allowed in this repository. Use git rebase to reapply your commits on top of the target branch. Detailed instructions for doing so can be found here. Yeah, I have update this PR using the latest http2.ConfigureTransports method."
technical,"Documentation on everything is always a issue, this will never go away. However its a simple concept, you're trying to add one quad to your item when rendered in the inventory. We have MANY things related to extending the item model, So I would tell you to start looking at the IModel interface, as well as the renderItemAndEffectIntoGUI/renderItemOverlayIntoGUI which gathers the model and passes through a hadleState function which you can easily build the extra bar onto. We also expose a model based font renderer so there's help with that. Combined with the MANY MANY posts and arguments that we've had on this issue tracker and the forums about the point of the model system is to REMOVE direct GL access from the mods/items. Your solution is to add a new hook that encourages modders to break GL. All you wont is your model to have one extra textured quad. Why does this need to be/encourage direct GL access? I don't care if you consider my response dickish, your initial post was. And im getting tired of people posting under the same veiled threats every damn time. ""Do this now or i'll hack it with a coremod!"" You may not intend for your post to come across that way but it does. So some takeaways: Don't open your Prs with 'Im gunna ASM this if you dont add it' Which is what your post comes under. Don't go against the entire point of moving to a model system buy adding YetAnother place of calling direct GL code. If you find something that you THINK you can't do in Forge open a issue, or use the forums. Explore your options. State what options you have explored. And again DO NOT open your argument with do this or i'll asm it! [Edited to remove being a dick]  Lex, you solution has 2 major drawbacks:  1. It requires a a code-based model. The step from putting a png and a 3-line json model into the resources folder to coding a custom IModel is huge. 2. The quads you add with your model are in model space. They will then be transformed using the Matrix4f derived from the json model. To add a quad that aligns to screen pixels, you need to apply those transforms in reverse and also know what the target coordinates (y!) are.  Both of these tasks are not trivial. Using the BufferBuilder to throw a single colored quad onto the screen *is* trivial, especially as anyone who has ever made a GUI for any block knows how to do it. (and if they don't, the code in renderItemOverlayIntoGUI already does exactly that and can be copied very easily.)  Now, is direct GL access the only way to do that? No, it isn't. I can think of dozens of ways to supply overlays that don't need GL calls but still are much simpler to use than baking models on-the-fly. Even a callback that asks for a texture path + u/v/w/h to overpaint over the item would fulfill 95% of all usescases. But the number 1 solution people will propose when they run into this problem is ""do it like vanilla"". And killing any discussion at that point will ensure that the same will happen again and again."
technical, All committers have signed the CLA.
technical,"If you would like to readdress this as a series of prebuilt utilities to render extra bars or numbers over items in the inventory. Then we can readdress it as that. But moving things back 5 years by encouraging people to do even more direct GL access is not going to fly. Feel free to create a new issue or PR to discuss that avenue of things.  As for the whole topic of ASM. Let me state this directly and for the record. It is, by definition, possible to do ANYTHING in a coremod. Stating that you can/are using a coremod in a PR serves literally NO purpose. It is NOTHING but a threat that gets tranted to ""Do this or i'll hack shit with a coremod"" I do not bed to threats.  So, new guideline (I've instructed Mezz to make it official). Any mention of ASM or hints at using a coremod to do shit in your issue will result in the thread being locked, and potentially you being banned from the Forge repo. End of story, i'm sick of being threatened by people and coremods. I have a tool which needs to have two durability bars. In order to draw the second one, I would have to ASM my own hook into RenderItem, or do some wonky rendering on the GuiContainerEvent.DrawForeground and RenderGameOverlayEvent.Post events. This prompted a PR for the hook instead."
technical,Abusive language. Locking and blocking. [removed - inappropriate content]
technical, Abusive language. Locking and blocking.
technical,"Hey thanks for the quick response, but this still feels like something is wrong. If uncleramsay had the wrong command and it was working fine and then all they upgraded was to 3.18 wouldn't that same wrong command still work.  Sure it would git push when he didn't want it to, but clearly it was working before. Unless, Lerna added some better error checking and this is an error that the command specified doesn't exist and it just didn't error out before, but that's probably still a break if bad commands worked before.  EDIT -  Removing the git in --no-git-push fixed the above command, but I still think it might be worth having somewhere in the release notes that bad commands will now error out instead of just being ignored. * --npm-client only applies to lerna bootstrap and lerna run. * --no-git-reset and --no-verify-access only apply to lerna publish, not lerna version."
technical,"I'd say that's much worse than sounds like you did me a favor, but all I got was extra work.   The whole point of SemVer is that this ""explicit change"" can be made by a machine. The author of a library categorized upgrades into breaking and non-breaking. Software like renovate can then pick certain updates and apply them automatically based on this categorization.  The condescending attitude aside, a lockfile alone does not help here. Updating lerna necessarily means my lockfile changes. My direct usage of yargs was ""protected"" by the lockfile, because the selector yargs12.0.1 still gave me 12.0.5 at node modules/yargs/. But without forced hoisting (and thus forced conflict resolutions) that has no impact on what is in node modules/lerna/node modules/yargs/ I am not saying you planned this. Your update broke something for some consumers without being SemVer major. That is unfortunate, but it's a mistake that can happen. However, your reaction to the mistake being brought to your intention is somewhere between condescending and hostile. You could have said ""sorry, this was not intended, let me roll back and re-release as 4.0.0"" and it would be fine. You could have said ""sorry, I didn't consider that the yargs API practically becomes part of the lerna API, I will make sure that future SemVer major updates of yargs only happen in SemVer major updates of lerna"" Instead you said that it's good that your mistake broke my build. ## Expected Behavior Prior to 3.18.0, this does not error out, and publishes without pushing to git.  ## Current Behavior Errors out as in above example  ## Steps to Reproduce (for bugs) ## Context I'm currently unable to publish alphas without locking to a lower version of lerna.  ## Your Environment Running on jenkins"
technical,"my build updated the package.json files and a later lerna publish pushed them to npm. That's exactly what I expected it to do. And SemVer doesn't care if breaking my build now is ""better"" than silent errors, it only cares about compatibility. lerna 3.18.x is not compatible to 3.17.x. If lerna does not want to do SemVer, than at the very least you should put a warning into your readme and possibly switch to version numbers that do not suggest SemVer, so consumers know not to let renovate auto-update this dependency."
technical,"Works fine with 3.17.0, breaks with 3.18.0 and 3.18.1 Hey thanks for the quick response, but this still feels like something is wrong. If uncleramsay had the wrong command and it was working fine and then all they upgraded was to 3.18 wouldn't that same wrong command still work.  Sure it would git push when he didn't want it to, but clearly it was working before. Unless, Lerna added some better error checking and this is an error that the command specified doesn't exist and it just didn't error out before, but that's probably still a break if bad commands worked before.  EDIT -  Removing the git in --no-git-push fixed the above command, but I still think it might be worth having somewhere in the release notes that bad commands will now error out instead of just being ignored."
technical,Your build had silent errors before. I'd say that's much worse than a temporarily broken build. my build updated the package.json files and a later lerna publish pushed them to npm. That's exactly what I expected it to do.
technical,"uncleramsay There is no --no-git-push option, it's called no-push  Which command elicited that error? That's very odd. Prior to 3.18.0 --no-git-push definitely works"
technical, Then you'll be making an argument for those technologies to have (at least) their client's implemented in .net core too.
technical,"Then you'll be making an argument for those technologies to have (at least) their client's implemented in .net core too. uncleramsay There is no --no-git-push option, it's called no-push  Which command elicited that error?"
technical,"I was quite polite until I was asked to be thankful for my builds breaking without warning. Unintended breaking changes happen, but blaming it on the consumers is not an appropriate response. When exactly did I say that?  * If you're not using a lockfile, then the only one to blame for your builds breaking ""without warning"" is yourself. * If you  are  using a lockfile, then it was an explicit change you made (upgrading lerna) that caused the breakage, not a dastardly plan on my part to cause you pain.  The fact remains that the long-standing  intention  of lerna's argument parsing was to be strict, throwing errors when unrecognized options were passed. There was a bug in yargs 12 that silently perverted this intention, and yargs 14 fixed it."
technical,"That's very odd. Prior to 3.18.0 --no-git-push definitely works Works fine with 3.17.0, breaks with 3.18.0 and 3.18.1"
technical,"* --npm-client only applies to lerna bootstrap and lerna run. * --no-git-reset and --no-verify-access only apply to lerna publish, not lerna version. Yargs has been configured with .strict() for almost two years now). It appears Yargs 14 fixes some bugs that were previously obscuring it."
technical,I honestly cannot believe how casual and dismissive you guys at MS are. Lose more games than the Jets and act like you are a team full of Tom Bradys. The hubris. I understand it must really difficult to do your job as a 1.75T company but can you try? Make some sort of effort. Please? Anything? **Describe the issue** I ran Test-ProxyLogon.ps1 it showed that my machine was impacted by the 0day CVEs. I ran MSERT and it reported nothing on a full scan. **Expected behavior**  I would expect that both of them would show IOCs or neither of them would show IOCs.  How is anyone supposed to know what is going on if two tools from the same company gives two different results?
technical,Test-ProxyLogon.ps1 and MSERT scan for two different things. They are not expected to show the same results. Please refer to the blog post for details. Quote the blog post to me one more time.
technical, Test-ProxyLogon.ps1 and MSERT scan for two different things. They are not expected to show the same results. Please refer to the blog post for details.
technical,"in general we haven't had that concept in the formatting engine yet.  But i'm personally amenable to it.  I think it's a reasonable approach that would give the user some level of control here in a manner that would be reasonably intuitive. **Design review summary:** The current general recommended approach is to add a blank line before the comment which should not have the extra indentation. However, to address the specific issue here, we would take a pull request which added a conditional formatting rule where comments  already aligned  where code would be indented on the same line will anchor at that location instead of the current behavior of anchoring to the trailing comment on the preceding line."
technical,"This seems by design to me...  it's a common pattern to have multi-line comments at the end of constructs, and to align them across multiple lines. In this case, we don't really have any way to know or believe that this isn't one of those cases, and aligning the comments as shown seems reasonable in terms of respecting the common pattern that is out there. Also referred here (edit: this was moved to #29647)"
technical,"Why is it moved to the backlog?  This keeps coming up and it is SO ANNOYING.  How about just adding an option to turn this nonsense off? as mentioned in this post here. In other words, if a community member wanted to contribute such a PR, roslyn would take it.  however, absent that, the advice would be to just put in a blank line in the code.  If this is something you are passionate about (which seem evidenced) perhaps you would be willing to help out here with a PR yourself? Note that there are a couple of good channels to help out people working to contribute toward roslyn (internal and external alike): Cheers!"
technical,"please keep your comments on topic here. As they have already elaborated on, the Roslyn team considered this bug, discussed the impact and potential solution in design meetings, and while they did arrive at a path forward decided pursuing it simply fell below their priority bar. The IDE team, as well as pretty much every other team, has a full load of work on their plate. Fixing this bug means there is another bug, which they value higher, that they cannot fix. In this case they've chosen to prioritize other work on their plate. At the same time they're willing to assist anyone else who wants to pursue the fix. I understand the frustration here at a bug that you prioritize not being fixed. At the same time though the IDE team has not been dismissive of your concerns. Rather they've given it careful thought and devoted design time to coming up with a potential solution."
technical,"I can't really agree with you because VS 15.8.2 also does it when the first comment is single-line  and the second comment is multi-line. The following shows the result after executing the ""Format Document"" command. I don't believe that anyone wants the above formatting. I never do that, and apparently multiple other people also never do that, therefore I suggest making a checkbox in the Options window, so that people can simply switch this behavior on or off as desired. However, in my opinion, the mixed example above seems to be a bug not a feature. do you recall if there was an outcome for this in design review, or does it still need to be discussed?"
technical,"I was talking about the Microsoft team's inaction on this bug, not yours.  You seem strangely passionate about defending their inaction.  To be clear, I think that contributing to open source is a wonderful and worthy endeavor.  We need more of that sort of collaboration in society.  But as verelpode pointed out, we all have a finite amount of time and other priorities in life.  When it comes to side projects, we have to be selective about what we say yes to.  We might have time to contribute to some but not others.  I think it's unreasonable to say that, because I'm a Visual Studio user and want a bug fixed, I must go down the rabbit hole of learning Roslyn.  While I appreciate your offer of assistance, I fail to grasp the practical value of it.  We're all programmers here.  If I'd go to the trouble of learning Roslyn then , for such a small improvement, I may as well figure it out myself.  If you sincerely want to help, rather than just pay lip service to the concept of helping, then please just fix this bug. Everyone already knows that if a Mormon, Christian, or Buddhist etc came to this .NET forum and frequently promoted and proselytized his/her religion/philosophy, then it would be a case of inappropriate behavior, regardless of whether he/she does it directly, indirectly, or somewhat subtly.  Some people think that if no god is involved, then it's not proselytism.  Nonsense!  In reality, just because your religion/philosophy doesn't involve a god, this doesn't suddenly cause a special exception that makes it appropriate for you to engage in proselytism.  For example, Buddhism doesn't involve a god.  Another example:  Many cults didn't or don't involve any god, but it's still bad behavior when the cult members engage in proselytism.  Whether or not a god is involved is irrelevant:  Either way, god or no gods, proselytism is unethical, inappropriate, and unprofessional behavior. That's the same behavior as when a fervent missionary ""innocently"" replies,  ""There is nothing wrong with providing support to people in need, to show them that Jesus loves them and will save them.  I'm helping people.  People should contribute to our church fund so that we can rescue even more people.  I was just simply showing people how they can contribute to our projects."""
technical,"**Design review summary:** The current general recommended approach is to add a blank line before the comment which should not have the extra indentation. However, to address the specific issue here, we would take a pull request which added a conditional formatting rule where comments  already aligned  where code would be indented on the same line will anchor at that location instead of the current behavior of anchoring to the trailing comment on the preceding line. Great, I think this solution will successfully prevent this issue being reported in future again as a bug.  To my surprise, it was even possible to satisfy all users automatically without the burden of creating yet another option in the Options window."
technical,"Wait, here is another alternative solution, better than my previous brainstorming. VS would only increase the indenting of a comment if it is  already  indented by at least one space or tab char.  Example:  First comment: VS would align this comment with the first comment because it is already indented. Blah blah: This comment would never be aligned with the previous comment.  Thus the final result is: First comment: VS would align this comment with the first comment because it is already indented. Blah blah: This comment would never be aligned with the previous comment.. In other words, if a comment is aligned with ""int x,"" then it remains at that level, but if it already indented beyond ""int x,"" by one or more space or tabs, then VS aligns it with the previous comment. I agree, but that's identical to the current behavior of VS 2019, therefore it would continue to trigger bug reports.  So how about making VS only increase the indenting of a comment when it is  already  indented further right than int x, ?    If a comment is currently exactly aligned with int x,, then why should VS think that a preexisting exact alignment is incorrect?"
technical,"Also referred here (edit: this was moved to #29647) I can't really agree with you because VS 15.8.2 also does it when the first comment is single-line  and the second comment is multi-line. The following shows the result after executing the ""Format Document"" command. I don't believe that anyone wants the above formatting. I never do that, and apparently multiple other people also never do that, therefore I suggest making a checkbox in the Options window, so that people can simply switch this behavior on or off as desired. However, in my opinion, the mixed example above seems to be a bug not a feature."
technical,"If the idea of putting it in the Options window is rejected, then I'm trying to brainstorm an alternative solution, and I thought about  maybe  making VS look for a tab character in the place marked with ""\t"" following. On my computer, I have Options - Text Editor - C# - Tabs set to ""Keep tabs"".  I can't stand the ""Insert spaces"" option.  I always use tab in-between "","" and "" comment"".  Despite the fact that I have VS set to ""Keep tabs"", when I copy & paste a line or otherwise trigger VS to format the line, VS changes my tabs to spaces.  I wish it wouldn't do that.  I write it with tabs like this...and VS 2019 changes my tabs to spaces against my wishes: I thnk i'd much rather have the rule that the s are aligned if they abut.  BUt not if there's a blank line between them.  So, if you have. Needing to understand that having a space (or not) would affect alignment of comments seems super strange to me.  Whereas, if there's this obvious gap between them, it would make it much more clear if they were intended to *not* be together."
technical,"It's also an open source project with a community that actively contributes to it and provides PRs when their needs don't align with the priorities of the main team itself.  I'm one of those people and have contributed as an external community member a large number of times. As i've already pointed out, the stated view of the team is that there is an easy workaround if hte current behavior is undesirable to you.  If that workaround is not tenable and you want to see this fixed, it will have to come from a PR from someone willing to view the change as valuable enough to invest their own time. I personally don't feel like it's worth fixing myself, but i'd be happy to help you with a PR to change if it is something that is affecting you. The people who's job it is to improve Visual Studio's user experience. As an open source project, ""the people"" includes the community (including you, me, and other interested parties). Yes, that's likely true.  But they're also tasked with work felt to be more important and a more valuable use of their time.  And so here we are. I'd welcome your assistance in making up for Microsoft's negligence.  As I've already mentioned, I don't have time to get into Roslyn.  How about I send you my kudos once you fix this bug?  But if your Roslyn skills aren't up to stuff, then don't worry about it."
technical,"Thanks for confirming, wanted to make sure.  I'll park these 2 until the next meeting then. I'm bringing this to a design review today. My proposal is as follows:  * The current behavior is generally by design * We could modify the current behavior in the case where a line comment starting with a space is followed by a line comment  not  starting with a space: This comment starts with a space This comment did not start with a space For this case, the first line comment not starting with a space would be aligned with int instead of the current behavior of aligning it with ."
technical,"This horribly annoying behavior in Visual Studio bothered me multiple times today.  It's so unbelievably annoying!  I'll upgrade to VS 2019 just for this improvement once it's available.  I'm anxiously awaiting. I've encountered this problem in Roslyn as well. The current behavior is annoying. See this for an example. Original code (with formatting warning). Code with formatting fix applied, but undesirable formatting: This is using 16.2p3."
technical,"Note that if the formatting of comments is influenced by the preexisting formatting/spacing (in the manner as I described above), then this behavior is  not at all  unusual nor inconsistent with other VS formatting behavior, because VS formatting is  already  influenced by the preexisting spacing.  For example, when I run the ""Format Selection"" command on the following code in VS 2019 on my own computer, VS doesn't change the code at all.  VS respects the fact that I wrote TestProperty2 on a single line and it leaves it on a single line.  In the same way as VS retains my preexisting formatting of a property, it can retain my preexisting formatting of a comment. Blah blah: This comment is already exactly aligned with ""int x,"" thus there is NO reason to think it's wrongly aligned. another comment: this comment does look wrong and should be reformatted. in general we haven't had that concept in the formatting engine yet.  But i'm personally amenable to it.  I think it's a reasonable approach that would give the user some level of control here in a manner that would be reasonably intuitive."
technical,"I agree, but that's identical to the current behavior of VS 2019, therefore it would continue to trigger bug reports.  So how about making VS only increase the indenting of a comment when it is  already  indented further right than int x, ?    If a comment is currently exactly aligned with int x,, then why should VS think that a preexisting exact alignment is incorrect? Note that if the formatting of comments is influenced by the preexisting formatting/spacing (in the manner as I described above), then this behavior is  not at all  unusual nor inconsistent with other VS formatting behavior, because VS formatting is  already  influenced by the preexisting spacing.  For example, when I run the ""Format Selection"" command on the following code in VS 2019 on my own computer, VS doesn't change the code at all.  VS respects the fact that I wrote TestProperty2 on a single line and it leaves it on a single line.  In the same way as VS retains my preexisting formatting of a property, it can retain my preexisting formatting of a comment. Blah blah: This comment is already exactly aligned with ""int x,"" thus there is NO reason to think it's wrongly aligned. another comment: this comment does look wrong and should be reformatted."
technical,"I'm bringing this to a design review today. My proposal is as follows:  * The current behavior is generally by design * We could modify the current behavior in the case where a line comment starting with a space is followed by a line comment  not  starting with a space: This comment starts with a space This comment did not start with a space For this case, the first line comment not starting with a space would be aligned with int instead of the current behavior of aligning it with . Note that some linters will complain if you don't start your comment without a space."
technical,"Note that some linters will complain if you don't start your comment without a space. Other linters will complain about comments at the end of a line of code. Most relevant to this issue though is the fact that most cases where I've seen this reported as a bug, including the original example above, do not have a space on the line of code that the user did not want to see indented."
technical,"Everyone already knows that if a Mormon, Christian, or Buddhist etc came to this .NET forum and frequently promoted and proselytized his/her religion/philosophy, then it would be a case of inappropriate behavior, regardless of whether he/she does it directly, indirectly, or somewhat subtly.  Some people think that if no god is involved, then it's not proselytism.  Nonsense!  In reality, just because your religion/philosophy doesn't involve a god, this doesn't suddenly cause a special exception that makes it appropriate for you to engage in proselytism.  For example, Buddhism doesn't involve a god.  Another example:  Many cults didn't or don't involve any god, but it's still bad behavior when the cult members engage in proselytism.  Whether or not a god is involved is irrelevant:  Either way, god or no gods, proselytism is unethical, inappropriate, and unprofessional behavior. That's the same behavior as when a fervent missionary ""innocently"" replies,  ""There is nothing wrong with providing support to people in need, to show them that Jesus loves them and will save them.  I'm helping people.  People should contribute to our church fund so that we can rescue even more people.  I was just simply showing people how they can contribute to our projects."" please keep your comments on topic here."
technical,"As they have already elaborated on, the Roslyn team considered this bug, discussed the impact and potential solution in design meetings, and while they did arrive at a path forward decided pursuing it simply fell below their priority bar. The IDE team, as well as pretty much every other team, has a full load of work on their plate. Fixing this bug means there is another bug, which they value higher, that they cannot fix. In this case they've chosen to prioritize other work on their plate. At the same time they're willing to assist anyone else who wants to pursue the fix. I understand the frustration here at a bug that you prioritize not being fixed. At the same time though the IDE team has not been dismissive of your concerns. Rather they've given it careful thought and devoted design time to coming up with a potential solution. Temporarily locking thread to give it time to cool down."
technical,"This one is still in the queue Thanks for confirming, wanted to make sure.  I'll park these 2 until the next meeting then."
technical,"do you recall if there was an outcome for this in design review, or does it still need to be discussed? This one is still in the queue"
technical," This seems by design to me...  it's a common pattern to have multi-line comments at the end of constructs, and to align them across multiple lines. In this case, we don't really have any way to know or believe that this isn't one of those cases, and aligning the comments as shown seems reasonable in terms of respecting the common pattern that is out there."
technical,"Yes, i think i would agree that mixed comment types should not align here. Wait, here is another alternative solution, better than my previous brainstorming. VS would only increase the indenting of a comment if it is  already  indented by at least one space or tab char.  Example:  First comment: VS would align this comment with the first comment because it is already indented. Blah blah: This comment would never be aligned with the previous comment.  Thus the final result is: First comment: VS would align this comment with the first comment because it is already indented. Blah blah: This comment would never be aligned with the previous comment.. In other words, if a comment is aligned with ""int x,"" then it remains at that level, but if it already indented beyond ""int x,"" by one or more space or tabs, then VS aligns it with the previous comment."
technical,"Temporarily locking thread to give it time to cool down. When you want to disable (but not delete) a field etc in a C# class, you comment it out by writing &quot,&quot, at the start of the line, but this causes VS 15.8.0 to incorrectly format the line when the previous line ends with a comment.  You can understand this much easier when you look at my screenshot attached.  To reproduce this bug, first put this class in a .cs file: Then select the text (select the entire class).  Then click menubar, Format Selection.  VS changes the class to: The bug also occurs when VS formats the class at other times. Thanks for investigating this!   This issue has been moved from   These are the original issue comments:  Thank you to check this.   These are the original issue solutions:  (no solutions)"
technical,"It's unethical to provide support to people to show how they can contribute to the Roslyn open source project? This is how much of the open source world works.  Projects are made freely available for people to use and modify and contribute back to.  When the priorities of the team don't align with your particular desires, you can choose to petition that they change, and/or you can choose to contribute your own efforts to align the project with your own desires. I have never expected ChainReactive to do anything. I have simply pointed out that this is something he can choose to contribute to if he wishes.  And, if he does wish to do so, i am 100% willing to provide my own time and experience toward aiding that effort.  There are many other community members who are willing to do that as well, and this has led to hundreds of community PRs and thousands of commits to the Roslyn repo that have improved the project for *everyone*.  This is the power and value of it being an open source project. Working with a community to improve an open source project sacrifices no ethical behavior as far as i can tell."
technical,"I thnk i'd much rather have the rule that the s are aligned if they abut.  BUt not if there's a blank line between them.  So, if you have. Needing to understand that having a space (or not) would affect alignment of comments seems super strange to me.  Whereas, if there's this obvious gap between them, it would make it much more clear if they were intended to *not* be together. Yes, i think i would agree that mixed comment types should not align here."
technical,"Working with a community to improve an open source project sacrifices no ethical behavior as far as i can tell. You've been doing more than that.  You're free to follow your choice of religion/philosophy, and various people such as ChainReactive, myself, and others are free to follow our own choices of religion/philosophy.  It's unethical and inappropriate if you frequently try to push or manipulate forum participants into joining your religion/philosophy. If I followed your recommendation and adopted your religion/philosophy, then it would substantially damage the upbringing of my children.  That would be even worse than unethical -- it would be monstrous to harm my children in that way."
technical,"Not all of us are React developers and we shouldn't have to change code in unrelated parts of the application. I empathize with your frustration, but the solution to it is to not run React-related rules on non-React-related parts of your codebase.  We're having a real problem with this because ESLint ignores the .eslintrc.json file (due to some other bug in ESLint), and it breaks the build. This seems like something you'd need to find a solution for. I'm going to lock this thread because this heated atmosphere is not constructive. **Do you want to request a *feature* or report a *bug*?** This seems like a bug.  **What is the current behavior?** 1. Follow the steps for adding the linting rules outlined on the Rules of Hooks page. 2. Create a plain JS function with a name that begins with use, e.g. useFoo. Call this function from within another plain JS function, e.g. testFoo. 3. The linter will complain:  React Hook ""useFoo"" is called in function ""testFoo"" which is neither a React function component or a custom React Hook function  react-hooks/rules-of-hooks   **What is the expected behavior?** Ideally, the rule would ignore functions that begin with use but are not React Hooks."
technical," In fact, how do you know your  useAnything  is not a hook? It can have nither  useEffect  nor  useState  and still be valid hook"
technical,"what are you doing to get around the issue with useWith from ramda? Telling eslint to ignore the line or rule for the file, entirely? Sorry for the delay, but I'm on vacation.  To be honest: I currently didn't use the entire eslint rule at the moment. But if I used it, I'd ignore the exact rule per line. But any automatism or at least a whitelist would be my preferred way"
technical,"In fact, how do you know your  useAnything  is not a hook? It can have nither  useEffect  nor  useState  and still be valid hook Sorry, I'm new to hooks so I just now saw this in the Build your own Hooks docs.   **Do I have to name my custom Hooks starting with use?** Please do. This convention is very important. Without it, we wouldn't be able to automatically check for violations of rules of Hooks because we couldn't tell if a certain function contains calls to Hooks inside of it.  I suppose the other side of this coin is that you're prohibited from naming any non-Hook function with a name that begins with ""use"" (or disable the eslint rule for that function). If that's the case and there's no way around this then feel free to close this. ˜„"
technical,"Sorry for the delay, but I'm on vacation.  To be honest: I currently didn't use the entire eslint rule at the moment. But if I used it, I'd ignore the exact rule per line. But any automatism or at least a whitelist would be my preferred way Thank you for the response! We did the same, but we also limited the rules of hooks in our eslint config to only our component folders like this:"
technical,"whats up with other functions beginning with use? For example ramdas useWith  Maybe the rule should check if any of the  problematic  functions like useState, useEffect etc. is used inside my hook, otherwise the checks can be handled differently.  Otherwise something like a whitelist of functions which are not hooks might be useful as a configuration parameter. what are you doing to get around the issue with useWith from ramda? Telling eslint to ignore the line or rule for the file, entirely?"
technical,"Keep up the great work. whats up with other functions beginning with use? For example ramdas useWith  Maybe the rule should check if any of the  problematic  functions like useState, useEffect etc. is used inside my hook, otherwise the checks can be handled differently.  Otherwise something like a whitelist of functions which are not hooks might be useful as a configuration parameter."
technical,"Sorry, I'm new to hooks so I just now saw this in the Build your own Hooks docs.   **Do I have to name my custom Hooks starting with use?** Please do. This convention is very important. Without it, we wouldn't be able to automatically check for violations of rules of Hooks because we couldn't tell if a certain function contains calls to Hooks inside of it.  I suppose the other side of this coin is that you're prohibited from naming any non-Hook function with a name that begins with ""use"" (or disable the eslint rule for that function). If that's the case and there's no way around this then feel free to close this. ˜„ Yeah it's intentional. We needed to pick a convention, and use was the least crowded out of descriptive short suffixes based on the JS codebases we've seen in open source and at FB. There's always bound to be some false positives. But at that point use convention is already so ingrained that people would likely *expect* it to be a Hook. So it's worth disallowing it anyway to avoid extra confusion."
technical,"I'm not sure how disallowing corporations that deal with ICE from using lerna would affect ICE's behavior - which I assume is the intent behind this PR.  Shoving politics into code will not help anyone. **Edit:** Just to clarify, I would not have personally made this change. I do however respect the existing maintainers of the projects decision to do so. I do not consider this project to be mine.  Locking this issue.  If you disagree with the license then you're welcome to use one of the alternatives or write your own.  If you're employed by a subsidiary listed, direct any questions about the usage of Lerna to your company lawyer. This license only applies to future versions, you're free to use old versions that do not contain this clause.  If you have concerns over the legality of relicensing. The MIT license allows sublicensing, which this falls under. Even still, all contributors implicitly agreed to the existing license, of which I am the original license holder, when they submitted code meaning we are within our rights to relicense.  If you're a contributor with active code in Lerna, and disagree with the relicense, feel free to privately message jamiebuilds, evocateur, or myself and we'll ensure that your contributions are either removed or rewritten to remove attribution.  Thanks everyone for your comments thus far."
technical,"Curious: Do ""parents"" who drag their children across the desert with the express intent of illegally entering the United States have no share of blame for what happens to these children? Are you protesting the right of the United States to choose who to allow to immigrate by deporting illegal aliens, or are you protesting the specific treatment of these illegal aliens in the context of their removal? If it's the former, then there's no point in bringing up their specific treatment as a point for why ICE should be protested (because it would be the case regardless of the treatment of any particular alien), and if it's the latter, then why not specify that, and protest by listing what exactly you would like changed about the treatment of aliens during their deportation? That would make what exactly you are protesting much clearer."
technical,"So what? Maximal open source permissiveness is not automatically correct and just. It's just one option.  And no, the terms are pretty clear, I suggest you read more carefully: ""The following license shall not be granted to the following entities or any subsidiary thereof."" It says nothing about people giving money to e.g. Microsoft or using Microsoft products. I'm not sure how disallowing corporations that deal with ICE from using lerna would affect ICE's behavior - which I assume is the intent behind this PR.  Shoving politics into code will not help anyone."
technical,"Lerna team, thank you for taking a stand  Please make sure that you amend your page on npmjs.org so that it does not state that the license of the project is ""MIT""."
technical,"I'm adding it to all my repos to prevent the lerna project from being used in any of those projects, both open source and commercial. Just as a matter of principal. Adding your political beliefs to your licensing is petty. The irony is that this very project is hosted on a resource owned by Microsoft... While I too abhor ICE, this isn't actually a valid license change. In order to re-license a project without a CLA, the consent of all contributors needs to be sought. There are a variety of scripts out there that accomplish this, the one I am most familiar with is the Rust Relicense assistant. I'd suggest using similar tooling to avoid potential legal issues."
technical,"While I too abhor ICE, this isn't actually a valid license change. In order to re-license a project without a CLA, the consent of all contributors needs to be sought. There are a variety of scripts out there that accomplish this, the one I am most familiar with is the Rust Relicense assistant. I'd suggest using similar tooling to avoid potential legal issues. Yep, noted that is is no longer Open Source Software, by definition.  The way this tirefire of a license can be interpreted, is that since a company uses Windows and pays Microsoft, they are complicit. Or Github for that matter, which is owned by Microsoft. Are the Lerna repo maintainers profiting on ICE, in direct opposition to their license? Github free accounts didn't come from nothingness."
technical,"I sent an email to check if my account is duplicated. **Edited:** ## Anyone coming here with this issue should read through this, or specifically go to this comment and send an email. **Describe your problem and how to reproduce it:** I've been trying to submit several projects for a week and I get the subject message.  **Add a Link to the page with the problem:**  All pages  **Tell us about your browser and operating system:** * Browser Name: * Browser Version: * Operating System: **If possible, add a screenshot here (you can drag and drop, png, jpg, gif, etc. in this box):**"
technical,"is not resolved After emailing support and having my duplicated accounts merged (for those following along, I had no idea that I had duplicated accounts - view issue #37457 for more info), I am able to submit my lessons and update my accounts settings. One buggy thing which I included in an email to support is that when I click the freeCodeCamp logo, it appears to log me out. I press Sign In and I am able to see my curriculum again though. Thanks for the help with this issue."
technical,"No, still not working. My original data is the same (1st post). I am reopening this issue, due to you still having the problem.  Are you using any browser extensions which could be blocking the requests?"
technical,"Now that my account has been merged #37457, the projects I submitted are showing up I am still experiencing the issue and I do not believe I have an account issue"
technical, i had the same problem
technical,"i had the same problem I haven't been able to reproduce the issue. Can both of you please check again? If the issue is still there, try checking your browser's proxy config and network settings in general"
technical,"Ok, I am unable to update my account settings. I receive this error when I try. I will follow the issue mentioned above I redceive the same error as Kelsie when I try to change my settings (e.g., switch to dark mode or submit an about message)."
technical,"I've been getting the same error message. Doesn't matter what browser I use, although at this point I can no longer log into my account (signed up w/Github acct). I'm still logged into a session on Opera. Browser is up to date. No extensions. I required a suggestion to solve Yahoo 404, I attempt all the steps known, and few more learned by internet if you have experience how to solve this please suggest the measures."
technical, I sent an email to check if my account is duplicated.
technical,"Thanks for reporting this macengr. It looks like this has been fixed and is waiting to be pushed to production. See this for more info. We can close this since it has been resolved. I'm not sure why you closed this because I am STILL unable to submit my projects. It's been over a week now. I've seen multiple people reporting this problem and it always ends the same way - oh, we fixed it, closed. It's not fixed until it works for the user."
technical,"Sorry for closing this prematurely. I can't reproduce this with any browser on High Sierra. I've been getting the same error message. Doesn't matter what browser I use, although at this point I can no longer log into my account (signed up w/Github acct). I'm still logged into a session on Opera. Browser is up to date. No extensions."
technical,"Not that I know of, at least, none that I wasn't using before this started. I've made a recording for you"
technical,"I am still experiencing the issue and I do not believe I have an account issue Is anyone that is experiencing this issue able to log in and edit their user account settings?  The reason I ask is that that specific message should only appear if the browser can't detect the internet or the fcc servers have not returned your user details. If not, then the problem is likely login issues, discussed further here. If yes and you can change your settings, submit other challenges and everything, aside from project submission, seems to be working, then this is definitely something new."
technical,"Thank you for your patience. We suspect this issue could be caused by of duplicate accounts. If you could kindly send us an email supportfreecodecamp.org with your username, we should be able to take a closer look and resolve this issue ASAP. is not resolved"
technical,"The reason this can be closed is because the issue has been fixed on our development environment. We don't need open issues for something that is fixed. I just verified that it is fixed locally. I'm not sure when the next time the code will be added to the live site will be, but once it is added, you will be able to submit your projects normally. Thanks. Edit: it actually seems to be working for me on production as well. Can you try one more time? No, still not working. My original data is the same (1st post)."
technical,"I am reopening this issue, due to you still having the problem.  Are you using any browser extensions which could be blocking the requests? Not that I know of, at least, none that I wasn't using before this started."
technical,"I required a suggestion to solve Yahoo 404, I attempt all the steps known, and few more learned by internet if you have experience how to solve this please suggest the measures. Now that my account has been merged #37457, the projects I submitted are showing up"
technical,"Is anyone that is experiencing this issue able to log in and edit their user account settings?  The reason I ask is that that specific message should only appear if the browser can't detect the internet or the fcc servers have not returned your user details. If not, then the problem is likely login issues, discussed further here. If yes and you can change your settings, submit other challenges and everything, aside from project submission, seems to be working, then this is definitely something new. Ok, I am unable to update my account settings. I receive this error when I try. I will follow the issue mentioned above"
technical,I've made a recording for you Sorry for closing this prematurely. I can't reproduce this with any browser on High Sierra.
technical,"I redceive the same error as Kelsie when I try to change my settings (e.g., switch to dark mode or submit an about message). Thank you for your patience. We suspect this issue could be caused by of duplicate accounts. If you could kindly send us an email supportfreecodecamp.org with your username, we should be able to take a closer look and resolve this issue ASAP."
technical,"I haven't been able to reproduce the issue. Can both of you please check again? If the issue is still there, try checking your browser's proxy config and network settings in general Thanks for reporting this macengr. It looks like this has been fixed and is waiting to be pushed to production. See this for more info. We can close this since it has been resolved."
technical," The reason this can be closed is because the issue has been fixed on our development environment. We don't need open issues for something that is fixed. I just verified that it is fixed locally. I'm not sure when the next time the code will be added to the live site will be, but once it is added, you will be able to submit your projects normally. Thanks. Edit: it actually seems to be working for me on production as well. Can you try one more time?"
technical,"For the last time: YES, they are a ""special type of type"". I'm tired of this bikeshedding. I tried to advocate for keeping this issue focused on the actual bug that can be fixed (syntax highlighting), but you keep bringing it back to advocating for breaking compatibility in the language design as if this was the same level of implication. **Godot version:** 3.2.3 **Issue description:** The types: int, bool, and float are all styled as keywords rather than types in the editor This introduces confusion to new users.  I'd also suggest int, bool, and float should be capitalised to be consistent with the rest of the types. One of the reasons that PHP has such a poor reputation in many circles is because of its lack of consistency. For example its naming of substr or str replace and countless other string functions. Edit: enum type too?"
technical,"No, it wouldn't. The way GDScript tokenizer works in terms of syntax highlight has nothing to do with the capitalization. This is not what was suggested. I only propose we solve the problem presented here with a separate option for primitive types. By default, it would be the same color as keywords, as it is now, but whoever would like to highlight them differently, be it matching the color of classes or another color, would be able to do so. In the end, it will be as colorful as each individual user wants, which is fitting, as this issue is about individual user's preferences. Again, there's two things in this issue:  - The original report that primitive types are wrongly colored. That's a bug and I'm not even sure why it warrants discussion, they should be colored like any other type. PR welcome, that's likely easy to fix.  - Renaming primitive types to Int, Bool, Float I'm not interested in discussing this further here for the sake of perfectionism."
technical,"Again, there's two things in this issue:  - The original report that primitive types are wrongly colored. That's a bug and I'm not even sure why it warrants discussion, they should be colored like any other type. PR welcome, that's likely easy to fix.  - Renaming primitive types to Int, Bool, Float I'm not interested in discussing this further here for the sake of perfectionism. As far as I know, the problem is not in the tokenizer, but in the mechanism for highlighting the code of the TextEdit class. It's just that int is highlighted like a built-in python-like function (like len and str) and this overrides the type highlighting. If we split it into int (function) and Int (type), then the problem will be solved automatically.  There are no bool(), int(), float() functions on the GDScript documentation page (although they are highlighted as built-in functions), in fact they are built-in type constructors like String() and Vector2(). It's funny that there is no difference between str() and String(), apparently str() was created to make GDScript look more like Python.  This is just a note, actually the problem is broader, for example load is always highlighted in red, even in func load() and self.load. Does the user have a real need for this? Why do we need to provide the user with the ability to highlight ""primitive"" types with a different color than non-primitive ones? Maybe we need to add an option to change the color for each type? I mean, primitive types aren't that different from non-primitive types that they deserve a separate color."
technical,"It GDScript really all that high-level?  It basically just interfaces with Godot's built-in C++ classes (a limited subset, at that), with python-like syntax and a few handy shortcuts like iterating over arrays more easily.  And since GDScript does not exist outside of Godot”and never will”implementation details are important, since  GDScript   is   its implementation, nothing more .  There is nothing outside of the implementation that dictates what GDScript is supposed to be, or how it is supposed to work (the docs are only meant to describe what it is).  This is a major difference between GDScript and ""mainstream"" languages.  GDScript has a lot of ties to the C++ of the engine it interacts with”those ""implementation details"" being some of them.  If Godot used semicolons and braces instead of line breaks, colons and indents, would you feel the same way about this?  The real problem, though, is that any potential benefits of a change do not outweigh the obvious costs. True, there's no reason GDScript  has  to use ""int"" instead of ""Int"", but changing it will break every current Godot coder's code and intuition with no real benefit.  One reason it may have ended up this way is that it mirrors the names of those things in C++ (what the engine devs are seeing all day), where all the types added by the engine are PascalCase, and primitive types are all lowercase.  Since the C++ functions GDScript interacts with all use the C++ built-in bool, int and float types, and Godot's own Vector2, Variant, Array, etc. types, the current situation makes GDScript code nicely mirror the same code in C++.  This isn't very useful to those who only use GDScript, but is nice for anyone also using other languages with Godot.  So I'll have to disagree with your that ""it would be nice"" to change it.  In general, I'd say the engine's syntax highlighting (and, relatedly, which things in GDScript are reserved words) is a little strange in 3.x (why are built-in functions like print and floor reserved?), but I just did some testing, and it seems master doesn't really have globally reserved words (probably because of the parser rewrite).  It does, however, still have weird highlighting if for some inexplicable reason you do var Node or var int (Node is highlighted green and int red when they should be white like other variable names). GDScript is a high-level language because there are no pointers or other low-level memory manipulations. In terms of design and syntax, GDScript is a mixture of Python, JavaScript, and C++. But some things taken from C++ don't fit well with the fact that GDScript is a language whose goal is to be simple for beginners. I disagree with the opinion that GDScript should look familiar to C++ developers.  This change is highly optional and insignificant, as most people don't care. But even if it were accepted, the old code would be easy to fix automatically, unlike some of the other recent GDScript changes.  To summarize: in GDScript, primitive types are written in lowercase **only** because they are written this way in C++. There is no other reason."
technical,"GDScript is a high-level language because there are no pointers or other low-level memory manipulations. In terms of design and syntax, GDScript is a mixture of Python, JavaScript, and C++. But some things taken from C++ don't fit well with the fact that GDScript is a language whose goal is to be simple for beginners. I disagree with the opinion that GDScript should look familiar to C++ developers.  This change is highly optional and insignificant, as most people don't care. But even if it were accepted, the old code would be easy to fix automatically, unlike some of the other recent GDScript changes.  To summarize: in GDScript, primitive types are written in lowercase **only** because they are written this way in C++. There is no other reason. I disagree that there are two things here. There's two flavours of the same thing. Put simply:  Are primitives a special type of Type? Yes = Then different styling and capitalisation could make sense No = Then they should be styled and capitalised the same way as other types  This is not a ""perfectionism"" thing. This is a consistency thing. It's in the same vein of why Godot has a contributors style guide. Because anything that reduces the cognitive load on a developer is a good thing and consistency does that. Now personally I don't see how primitives are special to GDScript users, but dalexeev has done a better job covering that than I did."
technical,"As far as I know, the problem is not in the tokenizer, but in the mechanism for highlighting the code of the TextEdit class. It's just that int is highlighted like a built-in python-like function (like len and str) and this overrides the type highlighting. If we split it into int (function) and Int (type), then the problem will be solved automatically.  There are no bool(), int(), float() functions on the GDScript documentation page (although they are highlighted as built-in functions), in fact they are built-in type constructors like String() and Vector2(). It's funny that there is no difference between str() and String(), apparently str() was created to make GDScript look more like Python.  This is just a note, actually the problem is broader, for example load is always highlighted in red, even in func load() and self.load. Does the user have a real need for this? Why do we need to provide the user with the ability to highlight ""primitive"" types with a different color than non-primitive ones? Maybe we need to add an option to change the color for each type? I mean, primitive types aren't that different from non-primitive types that they deserve a separate color. I respect your efforts to maintain order in discussions, but the two issues are related. I tried to count the differences between primitive types and non-primitive types and counted two differences:  1. Primitive types are highlighted in a different color (recognized as a bug). 2. Primitive types are written with a lowercase letter (a tradition of many C-like languages).  I did not find any more differences (maybe someone will find it). Please note that both differences concern only appearance (color and case)."
technical,"It's not a matter of engine devs, it's very common in most programming languages that primitive types are identified as such with their case, as they typically have properties which different from non-primitive types (passing by value vs reference, copy/clone, etc.). For example, C# uses PascalCase for classes but its primitive types bool, float, int are lowercase. Another example: Rust primitive data types are lowercase. But it's non-primitive data types (e.g. structs, enums) are PascalCase. In Godot, types like Vector3 or Color are not primitive data types (they're collections of primitive data types, here float) and in a language like Rust, they'd typically be represented by structs (in C++ they're classes). There's no *in*consistency here, this is on purpose, and this has not been a problem in the past 6 years for Godot users, whether experienced or beginners. I second Akien about capitalization, but I want to remind everyone that the first part of this issue is about syntax highlight '. So to address that... Of the top of my head I cannot remember if any other language in any other editor highlighted primitive types the same as keywords or not, but this can probably be arranged anyway on our end. We can make a color for primitives a separate setting and let users decide if they want it to be the same as keywords, as classes, or have a unique color all together."
technical,"I second Akien about capitalization, but I want to remind everyone that the first part of this issue is about syntax highlight '. So to address that... Of the top of my head I cannot remember if any other language in any other editor highlighted primitive types the same as keywords or not, but this can probably be arranged anyway on our end. We can make a color for primitives a separate setting and let users decide if they want it to be the same as keywords, as classes, or have a unique color all together. I see your C#/Rust examples, but isn't Godot meant to be Python like? Is this distinction a thing in Godot? As far as I can see the Class types are a mixture of Reference (i.e. Array) and Value (i.e. PoolStringArray). As a end-user why do I care about whether something is primitive or classed? I can't see much of any difference, and if there is a different I'd suggest it should be explicitly documented in the docs rather than hoping the dev intuits it from capitalisation. Of the top of my head I cannot remember if any other language in any other editor highlighted primitive types the same as keywords or not. PyCharm groups all built-ins which includes types (str) and functions(len()), they're distinct from keywords. I prefer Godot's way in that regard in that types get their own style."
technical,"int, float and bool are plain old data types (*Edit:* primitive types is more correct, POD types is a wider and potentially more confusing concept). Array and String are templates, which are not trivial. So it makes sense for templated types to be capitalized like other non trivial types (e.g. Object). I would ask you both: from the end user's perspective do they care whether something is a primitive type or a class type? That seems like an academic difference that's only pertinent to the engine devs, not to the vast majority of game developers. In python behind the scenes everything is a Class, but to the users they see types, classes, even functions. The built-in types are all consistently lower-case (str, dict, tuple...) with the exception of None which is also a special value (as True / False). It's hard to understate the value of consistency for a developer."
technical,"No, there **is** inconsistency with the following naming conventions:  All types  except  these three have a capitalized name. Any  exception  is inconsistency. The only question is whether there are any advantages to having the exception. This is only because this inconsistency affects only appearance, but not behavior. Just like the fact that these types are highlighted in a different color than the rest.  Sorry, but in my opinion this is useless. All types should be highlighted with the same color. Otherwise it will be too colorful: primitive types - in one color, built-in types - in another, classes - in a third, custom classes - in the fourth, 2D types - in the fifth, 3D types - in the sixth, etc. Absolutely agree. What does knowing whether the type is primitive or not? What is the difference between a primitive type and a non-primitive type? I could understand if types passed by value were written with a small letter, and types passed by reference with a capital letter. Or built-in types with small, and classes with capital. The only reason these three types are written with small letters is tradition, habit, and blind copying of C/C ++. Yes, you are right, it is, although it is absolutely pointless. But in some languages (Kotlin, Haxe, Ada, etc) all types (even ""primitive"" ones) start with a capital letter, and this warms my perfectionist soul. But since it does not affect behavior, this is not a change I would fight for. It's just a little annoying that people think in such fictitious categories as primitive types. In comparison, the illogical and counter-intuitive behavior of the division operator creates much bigger problems, but people don't want to notice them and react extremely negatively due to the strength of their habits. In this regard, in GDScript bool/int/float behave exactly like String, Vector2, Packed*Array and most other types, i.e. passed by value. Only Array, Dictionary and Object are passed by reference."
technical,"This has nothing to do with consistency and not the same as PHP's problems. Primitive types are not capitalized in most C-like languages, while classes can be capitalized as per individual style guide. If anything, Array and especially String being capitalized is inconsistent. But I guess this is borrowed from the engines C++ core. int, float and bool are plain old data types (*Edit:* primitive types is more correct, POD types is a wider and potentially more confusing concept). Array and String are templates, which are not trivial. So it makes sense for templated types to be capitalized like other non trivial types (e.g. Object)."
technical,"These are not significant differences, just implementation details. In JavaScript, for example, you can do 1.2345.toFixed(2). GDScript is a high-level language, so the low-level details of C++ are not important here enough to emphasize these three types. Knowing about their primitiveness in C++ does nothing for a GDScript developer. Once again, this is not a thing that I would fight for. I refer to it as ""It would be nice."" I just wanted to show that ""primitive"" types are a strange tradition. But I face the objection that there are some real reasons to classify primitive types into a separate category. It surprises me. It GDScript really all that high-level?  It basically just interfaces with Godot's built-in C++ classes (a limited subset, at that), with python-like syntax and a few handy shortcuts like iterating over arrays more easily.  And since GDScript does not exist outside of Godot”and never will”implementation details are important, since  GDScript   is   its implementation, nothing more .  There is nothing outside of the implementation that dictates what GDScript is supposed to be, or how it is supposed to work (the docs are only meant to describe what it is).  This is a major difference between GDScript and ""mainstream"" languages.  GDScript has a lot of ties to the C++ of the engine it interacts with”those ""implementation details"" being some of them.  If Godot used semicolons and braces instead of line breaks, colons and indents, would you feel the same way about this?  The real problem, though, is that any potential benefits of a change do not outweigh the obvious costs. True, there's no reason GDScript  has  to use ""int"" instead of ""Int"", but changing it will break every current Godot coder's code and intuition with no real benefit.  One reason it may have ended up this way is that it mirrors the names of those things in C++ (what the engine devs are seeing all day), where all the types added by the engine are PascalCase, and primitive types are all lowercase.  Since the C++ functions GDScript interacts with all use the C++ built-in bool, int and float types, and Godot's own Vector2, Variant, Array, etc. types, the current situation makes GDScript code nicely mirror the same code in C++.  This isn't very useful to those who only use GDScript, but is nice for anyone also using other languages with Godot.  So I'll have to disagree with your that ""it would be nice"" to change it.  In general, I'd say the engine's syntax highlighting (and, relatedly, which things in GDScript are reserved words) is a little strange in 3.x (why are built-in functions like print and floor reserved?), but I just did some testing, and it seems master doesn't really have globally reserved words (probably because of the parser rewrite).  It does, however, still have weird highlighting if for some inexplicable reason you do var Node or var int (Node is highlighted green and int red when they should be white like other variable names)."
technical,"I would ask you both: from the end user's perspective do they care whether something is a primitive type or a class type? That seems like an academic difference that's only pertinent to the engine devs, not to the vast majority of game developers. In python behind the scenes everything is a Class, but to the users they see types, classes, even functions. The built-in types are all consistently lower-case (str, dict, tuple...) with the exception of None which is also a special value (as True / False). It's hard to understate the value of consistency for a developer. It's not a matter of engine devs, it's very common in most programming languages that primitive types are identified as such with their case, as they typically have properties which different from non-primitive types (passing by value vs reference, copy/clone, etc.). For example, C# uses PascalCase for classes but its primitive types bool, float, int are lowercase. Another example: Rust primitive data types are lowercase. But it's non-primitive data types (e.g. structs, enums) are PascalCase. In Godot, types like Vector3 or Color are not primitive data types (they're collections of primitive data types, here float) and in a language like Rust, they'd typically be represented by structs (in C++ they're classes). There's no *in*consistency here, this is on purpose, and this has not been a problem in the past 6 years for Godot users, whether experienced or beginners."
technical,"In this regard, in GDScript bool/int/float behave exactly like String, Vector2, Packed*Array and most other types, i.e. passed by value. Only Array, Dictionary and Object are passed by reference. No, it wouldn't. The way GDScript tokenizer works in terms of syntax highlight has nothing to do with the capitalization. This is not what was suggested. I only propose we solve the problem presented here with a separate option for primitive types. By default, it would be the same color as keywords, as it is now, but whoever would like to highlight them differently, be it matching the color of classes or another color, would be able to do so. In the end, it will be as colorful as each individual user wants, which is fitting, as this issue is about individual user's preferences."
technical,"I see your C#/Rust examples, but isn't Godot meant to be Python like? Is this distinction a thing in Godot? As far as I can see the Class types are a mixture of Reference (i.e. Array) and Value (i.e. PoolStringArray). As a end-user why do I care about whether something is primitive or classed? I can't see much of any difference, and if there is a different I'd suggest it should be explicitly documented in the docs rather than hoping the dev intuits it from capitalisation. Of the top of my head I cannot remember if any other language in any other editor highlighted primitive types the same as keywords or not. PyCharm groups all built-ins which includes types (str) and functions(len()), they're distinct from keywords. I prefer Godot's way in that regard in that types get their own style. No, there **is** inconsistency with the following naming conventions:  All types  except  these three have a capitalized name. Any  exception  is inconsistency. The only question is whether there are any advantages to having the exception. This is only because this inconsistency affects only appearance, but not behavior. Just like the fact that these types are highlighted in a different color than the rest.  Sorry, but in my opinion this is useless. All types should be highlighted with the same color. Otherwise it will be too colorful: primitive types - in one color, built-in types - in another, classes - in a third, custom classes - in the fourth, 2D types - in the fifth, 3D types - in the sixth, etc. Absolutely agree. What does knowing whether the type is primitive or not? What is the difference between a primitive type and a non-primitive type? I could understand if types passed by value were written with a small letter, and types passed by reference with a capital letter. Or built-in types with small, and classes with capital. The only reason these three types are written with small letters is tradition, habit, and blind copying of C/C ++. Yes, you are right, it is, although it is absolutely pointless. But in some languages (Kotlin, Haxe, Ada, etc) all types (even ""primitive"" ones) start with a capital letter, and this warms my perfectionist soul. But since it does not affect behavior, this is not a change I would fight for. It's just a little annoying that people think in such fictitious categories as primitive types. In comparison, the illogical and counter-intuitive behavior of the division operator creates much bigger problems, but people don't want to notice them and react extremely negatively due to the strength of their habits."
technical,"I respect your efforts to maintain order in discussions, but the two issues are related. I tried to count the differences between primitive types and non-primitive types and counted two differences:  1. Primitive types are highlighted in a different color (recognized as a bug). 2. Primitive types are written with a lowercase letter (a tradition of many C-like languages).  I did not find any more differences (maybe someone will find it). Please note that both differences concern only appearance (color and case). Primitive data types have a few properties different from other data types: - They don't act like objects, and so don't have functions (things like Vector2.normalized()) - Faster to move around and perform operations on  Also, I was just messing around in GDScript, and it seems anything inheriting from Object (Reference, Node, etc.) isn't reserved, allowing you to do things like var Control : int = 10, which prevents you from accessing Control until your variable goes out of scope (can't do Control.new(), for example)."
technical,"Primitive data types have a few properties different from other data types: - They don't act like objects, and so don't have functions (things like Vector2.normalized()) - Faster to move around and perform operations on  Also, I was just messing around in GDScript, and it seems anything inheriting from Object (Reference, Node, etc.) isn't reserved, allowing you to do things like var Control : int = 10, which prevents you from accessing Control until your variable goes out of scope (can't do Control.new(), for example). These are not significant differences, just implementation details. In JavaScript, for example, you can do 1.2345.toFixed(2). GDScript is a high-level language, so the low-level details of C++ are not important here enough to emphasize these three types. Knowing about their primitiveness in C++ does nothing for a GDScript developer. Once again, this is not a thing that I would fight for. I refer to it as ""It would be nice."" I just wanted to show that ""primitive"" types are a strange tradition. But I face the objection that there are some real reasons to classify primitive types into a separate category. It surprises me."
technical," This has nothing to do with consistency and not the same as PHP's problems. Primitive types are not capitalized in most C-like languages, while classes can be capitalized as per individual style guide. If anything, Array and especially String being capitalized is inconsistent. But I guess this is borrowed from the engines C++ core."
technical," **Please do not add a +1 comment to this issue.**  Instead, please add a  reaction to the issue parent above.  Many of us are subscribed to the comments here as we are waiting for an update if/when this becomes solved or a workaround is found.  Every ""+1"" comment triggers another email, essentially spamming the repo owners *and* everybody subscribed.  We are all anxious to find a solution here, but please be mindful of your fellow devs and do not add any additional ""+1"" comments as they do not work towards finding a solution (you can continue to voice support with a  above)."
technical,"I was able to work around that by changing count to **3**, plan/apply-ing, then reverting the count to the dynamic value. # Another part to this: Consider each of these modules is for user management of accounts. the parent module creates groups and roles and allows users in this account to assume roles in other accounts. It takes a list of the children account IDs to setup allowing this. The child modules create roles with no groups. This is because I want to sign into the parent account and assume roles into other accounts controlled by what group you are placed in the parent account. When you run terraform plan you get:  # Error output:  Error refreshing state: 1 error(s) occurred: There is a provider in the child module that assumes the myrole in the child account. But it can't until the module.parent allows it to. It works fine if I comment out the child module then run it. This is why the error only references child b and not child a, I did the commenting out workaround to get it to work for child a. But due to #9653 I can't run a destroy on these either without manual intervention. So, I'm trying to make sure that permissions checks are taken into consideration with the addition of depends on to modules. It would also be nice if you could pass a map into a provider key of a module that way I could define my providers outside the module and have them specified by map keys inside the module but that's a side note."
technical,"#16983 Is this the same error?? This is causing us issues.. do not understand why i cannot just depend on resources from other modules as well, why do we need the output, let me tag resources as public or private. It doesn't work using output anyways.. ## my use-case I want to use the output of one module (a list) in order to generate a number of resources. The number of resources to be created should match the size of the modules list.  ## code snippet #### module bar This module provides just a list of sg ids. One is of a generated resource and one is hardcoded #### Error output"
technical,"Can't resources have depends on on other resources in different modules? I understand that modules can't depend on other modules, but resources should be able to depend on other resources, despite of modules.  I have two modules under my application module, docker-swarm and docker-app. Before running app I need to provision and initialize the Docker Swarm.  So in my docker-app module I have a resource aws instance.docker-app and he has depends on = [""module.applications.docker-swarm.aws instance.swarm-manager""] but I always get: #16983 Is this the same error?? This is causing us issues.. do not understand why i cannot just depend on resources from other modules as well, why do we need the output, let me tag resources as public or private. It doesn't work using output anyways.."
technical,"Can't resources have depends on on other resources in different modules? I understand that modules can't depend on other modules, but resources should be able to depend on other resources, despite of modules.  I have two modules under my application module, docker-swarm and docker-app. Before running app I need to provision and initialize the Docker Swarm.  So in my docker-app module I have a resource aws instance.docker-app and he has depends on = [""module.applications.docker-swarm.aws instance.swarm-manager""] but I always get: +1 in our use case, instance in the private subnet has to be create after the NAT gateway is created. But, here instances are launching prior to the NAT gateway because of that user data is not executing."
technical,"It has been almost 3 years since this issue was opened, and I can verify that there is no way of structuring a complex infrastructure into simple terraform module building blocks that depends on each other(try creating k8s namespaces in one module that needs to be done right after another module that is responsible for GKE node pool creation).   I hope I am missing something here.   What do you need for this feature to be implemented? Can we help you? I think some of us could write terraform from scratch in 3 years... Have some faith on the community. mitchellh A lot of the work in 0.12.0 was laying the groundwork for this to be possible. We cut 0.12.0 without it so we can ship sooner, but a lot of the features we intended on shipping are being worked on in 0.12.x. For example, we shipped resource for each a couple releases ago.  Kristin (one of our core team here) gave a great talk at HashiConf EU titled ""A (2nd) Tour of Terraform 0.12"" where she outlines some of the roadmap. She notes that ""module depends on"" is on the way (slide 52) and it is our intention to build that into the 0.12.x release series."
technical,"AirbornePorcine In your second module you need to actually use the wait for cluster variable somewhere that establishes a dependency. Such as in a template or a trigger on a null resource, etc. The only change I'd make to his workaround is to maybe make the dummy dependency resource use a trigger so it changes when the instance changes Ah, that makes sense! Should have known that Terraform would optimize away my variable if I didn't use it. :) I tried out your suggestion with the trigger and that didn't do it, but setting the depends id on a tag on some resource in module 2 seems to have done it. Hacky but oh well I guess."
technical,"Thanks, that doesn't entirely fit what I think this issue is talking about though - making a module able to depend on another one. I don't have any resources that the module should depend on - it should depend on another module being created.  I've tried doing stuff like this: Main terraform template: But this doesn't do anything - my module two is still created at basically the same time as module 1. AirbornePorcine In your second module you need to actually use the wait for cluster variable somewhere that establishes a dependency. Such as in a template or a trigger on a null resource, etc. The only change I'd make to his workaround is to maybe make the dummy dependency resource use a trigger so it changes when the instance changes"
technical,"work around to use -target if you have dependency in module that uses  count = ""{length(var.some var generated)}""  A way to fix it is to use shell script and trigger missing dependencies ex belwo 3 modules 1 vpn creation 2 subnets and 3rd is routing  run vpc and subnets before route module An alternate work-around, for my own use-case, is to set a bool explicitly so that the length need not be calculated at the initial planning phase."
technical,"## my use-case I want to use the output of one module (a list) in order to generate a number of resources. The number of resources to be created should match the size of the modules list.  ## code snippet #### module bar This module provides just a list of sg ids. One is of a generated resource and one is hardcoded #### Error output Another use-case  I have a module which creates an ECS service, sets up autoscaling, cloudwatch alarms, etc.  It takes the task definition as an input, which tends to be where the differences between similar services lie.  Different services need different access to other AWS resources, however, which means I need to create a task role and assign policies to it.  In most contexts, I would have the service module define the role and return the arn and name as outputs, to have policies attached after the module returns.  However, because it may sometimes be necessary to have certain policies in place BEFORE the service starts up (to access a KMS key to decrypt secrets in the config file, for example), and the service module waits for the service to start.  For this reason, I need to create the role, attach various policies, and then run the service module.  However, creating the role is a separate act from attaching a policy to it, so the service module ends up dependent on the role because the role arn is passed in, but it starts executing before the policy attachment resource has completed.  It would be easy enough to make the service module depend on the policy attachment resource, except that isn't possible.  So I have to workaround it by passing an output of the attachment resource to the module.  I'm sure the race between policy attachment and actually needing it will almost always work in my favour, but it will inevitably not be guaranteed and it WILL happen, probably the first time I'm not around to babysit a service launch in-person.  Have to define the task role and pass it to the stem-job module because the job may not be able to start up if permissions aren't granted first, so we cannot attach permissions to the role after the service is started in the module"
technical,Don't know if a workaround could be decent :D but you you can output a resource and fill this output as input of your module. Any time frame for this? Are we expecting this to be fixed soon?
technical,"# Another part to this: Consider each of these modules is for user management of accounts. the parent module creates groups and roles and allows users in this account to assume roles in other accounts. It takes a list of the children account IDs to setup allowing this. The child modules create roles with no groups. This is because I want to sign into the parent account and assume roles into other accounts controlled by what group you are placed in the parent account. When you run terraform plan you get:  # Error output:  Error refreshing state: 1 error(s) occurred: There is a provider in the child module that assumes the myrole in the child account. But it can't until the module.parent allows it to. It works fine if I comment out the child module then run it. This is why the error only references child b and not child a, I did the commenting out workaround to get it to work for child a. But due to #9653 I can't run a destroy on these either without manual intervention. So, I'm trying to make sure that permissions checks are taken into consideration with the addition of depends on to modules. It would also be nice if you could pass a map into a provider key of a module that way I could define my providers outside the module and have them specified by map keys inside the module but that's a side note. Any update on implementing this feature please ?"
technical,"If you're looking for additional use cases, that require something like depends on to work in a module, here's a simple one.  Several infrastructure servers need to be up and running, such as puppet, consul/vault, docker, before other servers that depend on them can even be spun up.  Because of several limitations in terraform resources, modules are the only way to implement configurations in a concise and efficient  manner, versus lengthy and numerous individual resource configurations.  Adding depends on, cuts my code from 100+ files and thousands of lines of code to 1 file and a couple hundred lines that that have to do nothing more than specify the variables for modules, and a dozen files for the called modules that become a one stop edit for configurations. Any update on this?"
technical,"zygimantas I believe yours is a use case you can solve today by adding depends on attributes to your output blocks.  You VPC module could look something like this: This will prevent your VPC module from exposing the vpc id until the subnets are ready, which should allow your ALB module to lookup the subnets with a data block as you would expect. any workaround here in case of module/submodule structure? When module depends on submodule?"
technical,"is this planned to be solved? Anyone have a decent workaround for doing this in Terraform 0.11.x?  My use case is simple, I want the module to depend on a resource that must be created first before the module does anything."
technical,is this planned to be solved? Anyone have a good workaround for this in the meantime?
technical,"Hi,  For my case I have a module for specific groupings of components. I originally raised this ticket so that I could guarantee the data (RDS) module and a cache (elasticache) module was finished before launching the site (EC2) module, as the site module fails to complete, [null resource does a deployment to the EC2 instances], due to not have a database/cache to connect to.  I resolved this by making an output for each module and then pass the output to the site module ""rds complete = module.rds.instance id"" using the mechanism you describe. apparentlymart - for my use-case I wanted to make an entire module dependent on a single resource, not depend on an entire module's resources.  I had a module that launched a cluster of various different EC2 clusters/ASGs that, when launched, would run a particular application.  I wanted to make the entire module depend on the existence of an RDS database that would be created outside the module.  I suppose I could pass in a computed value from the RDS instance as a variable and then reference that variable in some dummy way for the EC2 ASGs, but it would be convenient to be able to specify depends on on the module itself and have all of the resources contained within depend on the external resource."
technical,"Please take the complaints to twitter or reddit.  I'm subscribed to this thread for news about the feature only. Based on the feedback here, I've locked this discussion. We're actively working on module expansion. The engineers are making good progress, and I am excited about 0.13. For folks who are really upset, I'm ddreierhashicorp.com and I'm happy to talk."
technical,This is a really important feature to have in my opinion. It would make the code much more readable and maintainable than by using one of the hack that we find on blog posts. Can anyone please give a time line on when this feature will be released?
technical,"To clarify, my issue isn't at all with the lack of the feature - it's with the lack of communication.  Thanks for clarifying.  I would be fine with Hashicorp never developing this feature, but I take issue with Hashicorp not being able to even take a guess as to when it will be completed, if at all. It's great that they've communicated that this is a top priority, as that address the 'if it will happen', but they haven't addressed the 'when'.  I can understand that frustration, I agree Hashicorp could improve their communication with us as users of various kinds. However, WRT this general topic of first-class modules, I believe Hashicorp has been pretty clear, and within the bounds of what's possible (and also avoiding making unnecessary expectations which they may fail to deliver on).  For example, in the 3 years that this has transpired, we've seen the following: 1) many, rather verbose and elaborate discussions of the use cases and needs and possibilities, the problems and blocks, as well as how to get there. 2) Hashicorp realizes their fsck up, and acknowledges that TF core needs to be refactored in major ways. 3) Hashicorp lays down the roadmaps for 0.11 and 0.12, with various breaking/major changes that will facilitate the next round of changes 4) Hashicorp plows through 0.11 and 0.12 releases, following through as planned. 5) Hashicorp starts work on 0.13 which is focusing on modules as a first-class citizen  Particularly for those last two,  no timelines (with unreasonable expectations) were laid down. Yea, I would have loved to see 0.12 completed sooner, or dev on 0.13 to have started sooner, but I'd rather this than to have a product which is all over the place, poorly developed, poorly designed, lots of major breaking changes, and just generally moving to the whims of the wind as it blows. can you please lock this issue to contributors only? I believe at this point just about every *constructive* comment that could be made, has already been made.  There are likely hundreds of people subscribed to this issue, and we don't all need to get an email every time someone wants to complain. I'll be happy if the next time I hear about this feature is when it's done."
technical,"I need this plz Can't resources have depends on on other resources in different modules? I understand that modules can't depend on other modules, but resources should be able to depend on other resources, despite of modules.  I have two modules under my application module, docker-swarm and docker-app. Before running app I need to provision and initialize the Docker Swarm.  So in my docker-app module I have a resource aws instance.docker-app and he has depends on = [""module.applications.docker-swarm.aws instance.swarm-manager""] but I always get:"
technical,"what you need to do is pass ""something"" from the outputs of database module to the cluster module as an input, creating a dependency on creation could you please elaborate with a piece of pseudo code? I have been using terraform only for a couple of days now and I have a hard time imagining what the solution would be like."
technical,"Any update on this? Do you expect that the aws ecs service depends on race condition will be satisfied with cross module variables?  ie moduleA outputs aws iam role policy.moduleA.id and moduleB is declared using that output - is that enough for a aws ecs service in moduleB to not have a race condition, or is a null resource necessary?"
technical,No. It doesn't.  The first module outputs.  The cluster module I have. And this is the result: doesnt exist in that context. you cant depend on it at root module level
technical,"Anyone have a decent workaround for doing this in Terraform 0.11.x?  My use case is simple, I want the module to depend on a resource that must be created first before the module does anything. Don't know if a workaround could be decent :D but you you can output a resource and fill this output as input of your module."
technical,"Hi,  Thank you for the explanation. I sensed it would require some major redesign/refactoring. Here's my use case:  What I'm trying to do is wrap some main resource creation, such as aws security group, etc.. around modules of their own so I can enforce special naming conventions and other default behaviors pertaining to our environment so that other developers can then use these modules as building blocks to create/provision their infrastructure components.  Hence, I have a module that wraps around aws elb resource instantiation. But then this module requires security groups dependency. I then have a module that creates a security group with our naming conventions, etc... And after that, I'm passing in module.security group.id as an input parameter into my elb module. This means terraform must know to create the ""security group module"" first and have its outputs computed so the compute values are passed into the ""elb module"", but that does not happen. I get a failure which appears to indicate the ""security group module"" is not computed yet when ""elb module"" is trying to be instantiated.  Thanks. dtserekhman-starz from what you have described here it sounds like you're passing the security group id to the ELB module and thus Terraform should, as you assumed, be able to infer that dependency and create things in the correct order. If that's not working, that sounds like a bug distinct from what this issue is talking about and so I'd encourage you to open a new top-level issue with some config examples so we can dig into it some more. Thanks for sharing this problem!"
technical,I am seeing the same error: module root: module example: depends on is not a valid parameter  My Terraform version is: Terraform v0.8.2  My use case is that I want a module to depend on a resource. Error loading Terraform: module root: module etcdbastion: depends on is not a valid parameter.  Seeing it here too
technical,Thanks is good news.Time will come to remove all the work arounds soon then... Glad to hear! super exciting
technical,"Thanks for the update and putting the emphasis on this, it is greatly appreciated. Great to hear you guys are working on this. Have you guys been able to scope and figure out a timeline for the project yet? Roughly when this is planned to be released is one of the deciding factors for if my team will be using TFC or switching to Pulumi. Can you provide any broad information on when this will be released (week/month/year)?  Edit: Others might still need an ETA but we don't. The lack of communication (both on GitHub and via support email) about any estimates as to when Hashicorp will complete features they've committed to has resulted in my team canceling our TFC subscription. We're now moving to Pulumi."
technical,"That prioritization makes a lot of sense to me. Looking forward to it! Great, 3 years in, looking forward to having this needed feature."
technical,"This is still failing in 0.9.10.  :/ Hi all! Thanks for the great discussion here.  I wanted to share a little background on what's going on here to help explain why Terraform behaves the way it does today and why this is not a simple thing to implement:  Terraform modules today are an organizational/namespacing construct for configuration but they are  not  actually part of the graph. Rather than the module  itself  appearing in the graph, you instead find the individual variables and outputs as separate graph nodes, which for  many  cases is the better model since it means that parts of the calling module can start to get created as soon as their required outputs are finished, rather than waiting until the entire module completes.  The fact that modules themselves don't exist in the graph is why depends on isn't supported for modules. Instead, any interpolation used in either a variable passed into a module or in an output returned from a module creates an implied dependency with  that specific variable or module .  So with all of this said, I understand that the current behavior is proving inconvenient for some of you in certain cases. If you'd be willing, I'd like to learn more about what these situations are, since there may be other solutions that can address specific use-cases in a way that would be less complex to implement and thus more likely to get done sooner. If any of you are able to share some real-world examples of situations where you've needed a whole-module dependency (that is, where dependency on specific outputs is insufficient) -- and, ideally, some configuration examples illustrating those -- this would be really helpful in figuring out what change is best to make here to address these use-cases.  Thanks again for the discussion here and sorry this issue has been quiet for so long."
technical,"I heard an update weeks ago in a Hashicorp event telling us ""soon"". :) Maybe Christmas comes early this year ,) Hi guys, any release ETA for this so much expected feature ?"
technical,"I'm very excited to announce that beta 1 of terraform 0.13.0 will be available on June 3rd, and will include module depend on. I've pinned an issue with more details about the beta program, and posted a discuss thread for folks who want to talk about it more. Hi there,   ### Terraform Version ### Affected Resource(s) module   ### Terraform Configuration Files  ### Debug Output Error loading Terraform: module root: module legacy site: depends on is not a valid parameter module root: module legacy site: depends on is not a valid parameter  ### Expected Behavior I am trying to use the new depends on instead of the above outputs, so I create and provision my app once I know database and caches are built.  ### Actual Behavior Nothing as terraform errors out as above.  ### Steps to Reproduce  1. terraform apply  ### References depends on can reference modules. This allows a resource or output to depend on everything within a module. (#10076)"
technical,"I have seen many times, in order to depend on multiple resources from a module, developers create a null resource and export it as depends-id.  Than other modules are taking input variable as this depends-id resource. This process is pretty much ugly and not readable at all.  I can see current architecture does not support modules as first class citizen in the graph. I am not an expert for internals but I have an idea:  1. Create an implicit hidden resource for each module. 2. This implicit resource must be depends on all resources on the modules. 3. Resources in the module which depends on other module can simply implicitly depends on resources which declared at (1) .  Is this doable? What do you think? Hi,  For my case I have a module for specific groupings of components. I originally raised this ticket so that I could guarantee the data (RDS) module and a cache (elasticache) module was finished before launching the site (EC2) module, as the site module fails to complete, [null resource does a deployment to the EC2 instances], due to not have a database/cache to connect to.  I resolved this by making an output for each module and then pass the output to the site module ""rds complete = module.rds.instance id"" using the mechanism you describe."
technical,"Thanks for sharing your use-cases, everyone. That helps a lot.  I wanted to note that the depends on mechanism is intended not as the primary means of dealing with dependencies but rather as a workaround for situations where dependencies cannot be inferred automatically. For example, the resources for Amazon API Gateway have some issues here because there are some restrictions on what order resources can be created in that aren't represented in the resource attributes themselves. It's always preferable to use interpolation to create dependencies where it is possible to do so, since it's a more direct description of the desired end-state, rather than of the details of how to get there.  So if possible I'd like to collect some more use-cases where interpolation  can't  solve the problem via implied dependencies. chrisrlong's use-case seems like it could be an example of this: the dependency between the application's compute resources and its data store are presumably not explicit in the attributes, I assume in this case that some other configuration store like Consul is acting as the glue to help the code running on the EC2 instance find the RDS and ElastiCache addresses?  FlorinAndrei I understand and respect that perspective. I wasn't explaining how it works as an excuse for not supporting these use-cases but more as context for my later request that we talk about use cases  rather than  implementation strategy in case there's a different way to get the outcomes we need here without losing the benefits of the current implementation. I  do  want to solve this, but as engineers many of us have a tendency to focus on a particular solution to a problem rather than digging into the problem itself, and I want to be open to other solutions that may allow this problem to get solved sooner or with fewer drawbacks. Hi,  Thank you for the explanation. I sensed it would require some major redesign/refactoring. Here's my use case:  What I'm trying to do is wrap some main resource creation, such as aws security group, etc.. around modules of their own so I can enforce special naming conventions and other default behaviors pertaining to our environment so that other developers can then use these modules as building blocks to create/provision their infrastructure components.  Hence, I have a module that wraps around aws elb resource instantiation. But then this module requires security groups dependency. I then have a module that creates a security group with our naming conventions, etc... And after that, I'm passing in module.security group.id as an input parameter into my elb module. This means terraform must know to create the ""security group module"" first and have its outputs computed so the compute values are passed into the ""elb module"", but that does not happen. I get a failure which appears to indicate the ""security group module"" is not computed yet when ""elb module"" is trying to be instantiated.  Thanks."
technical,"I think #12570 and similar issues would be addressed by this enhancement. Right? Hi, any ETA of when this issue fix/enhancement may be available?  Terraform modules is a powerful feature for abstraction and code-reuse, but without inter-modules dependencies support we can only go so far with this feature. It's blocking us right now for the cases described above."
technical,"I think #12570 and similar issues would be addressed by this enhancement. Right? I am experiencing this issue as well, but not sure whether I am making a mistake here or whether the fact that I am using ARM templates as a deployment mechanism for the Logic Apps that I am deploying.  I have a file structure of: - main.tf -- shared/main.tf -- logic-apps/main.tf -- logic-apps/module1/main.tf -- logic-apps/module2/main.tf  Main is an orchestration level that is lightweight and creates a Resource Group and just called modules below it. Shared is supposed to creates a servicebus, auth rule and api connection to the service bus for use in the 'another' module. This currently outputs a dependency id as below.  Main.tf then passes that dependency id to the logic apps module that then creates some logic apps that should point at the servicebus api connection that was created above. Logic-apps.tf then passes this same variable to module1.tf to create the logic apps relevant to module1.  The module1.tf is using an Arm template in the same way as above, but has a depends on the dependency id that has been passed all the way from shared/main.tf.  At this point i am getting the following error Is this something that is caused by my use of ARM scripts within the Terraform code and are they not completing in the right timespan to allow the later creation of the logic apps that depend on them?  As a side note, if i run an Apply on this a second time it works because of the pre-existence on the service bus and the connection to it that can now be used, but not sure the client with accept that as a suitable solution. :-D"
technical,My use case: the slave virtual machine depends on master virtual machine to exist I am seeing the same error: module root: module example: depends on is not a valid parameter  My Terraform version is: Terraform v0.8.2  My use case is that I want a module to depend on a resource.
technical,"Another use-case  I have a module which creates an ECS service, sets up autoscaling, cloudwatch alarms, etc.  It takes the task definition as an input, which tends to be where the differences between similar services lie.  Different services need different access to other AWS resources, however, which means I need to create a task role and assign policies to it.  In most contexts, I would have the service module define the role and return the arn and name as outputs, to have policies attached after the module returns.  However, because it may sometimes be necessary to have certain policies in place BEFORE the service starts up (to access a KMS key to decrypt secrets in the config file, for example), and the service module waits for the service to start.  For this reason, I need to create the role, attach various policies, and then run the service module.  However, creating the role is a separate act from attaching a policy to it, so the service module ends up dependent on the role because the role arn is passed in, but it starts executing before the policy attachment resource has completed.  It would be easy enough to make the service module depend on the policy attachment resource, except that isn't possible.  So I have to workaround it by passing an output of the attachment resource to the module.  I'm sure the race between policy attachment and actually needing it will almost always work in my favour, but it will inevitably not be guaranteed and it WILL happen, probably the first time I'm not around to babysit a service launch in-person.  Have to define the task role and pass it to the stem-job module because the job may not be able to start up if permissions aren't granted first, so we cannot attach permissions to the role after the service is started in the module I am using modules simply to reduce the amount of boilerplate I need to write to set up an instance template, a managed instance group, etc. for my machine clusters.  I want to be able to pass in additional resources like load balancer pools, disk images, etc. but I can't set my module to depend on these passed in resources."
technical,Error loading Terraform: module root: module etcdbastion: depends on is not a valid parameter.  Seeing it here too I believe this one can solve unnested modules problem too. There were some issues regarding this #10883 before. This solution would be intuitive and address these issues.
technical,"Hi all! Thanks for the great discussion here.  I wanted to share a little background on what's going on here to help explain why Terraform behaves the way it does today and why this is not a simple thing to implement:  Terraform modules today are an organizational/namespacing construct for configuration but they are  not  actually part of the graph. Rather than the module  itself  appearing in the graph, you instead find the individual variables and outputs as separate graph nodes, which for  many  cases is the better model since it means that parts of the calling module can start to get created as soon as their required outputs are finished, rather than waiting until the entire module completes.  The fact that modules themselves don't exist in the graph is why depends on isn't supported for modules. Instead, any interpolation used in either a variable passed into a module or in an output returned from a module creates an implied dependency with  that specific variable or module .  So with all of this said, I understand that the current behavior is proving inconvenient for some of you in certain cases. If you'd be willing, I'd like to learn more about what these situations are, since there may be other solutions that can address specific use-cases in a way that would be less complex to implement and thus more likely to get done sooner. If any of you are able to share some real-world examples of situations where you've needed a whole-module dependency (that is, where dependency on specific outputs is insufficient) -- and, ideally, some configuration examples illustrating those -- this would be really helpful in figuring out what change is best to make here to address these use-cases.  Thanks again for the discussion here and sorry this issue has been quiet for so long. I have seen many times, in order to depend on multiple resources from a module, developers create a null resource and export it as depends-id.  Than other modules are taking input variable as this depends-id resource. This process is pretty much ugly and not readable at all.  I can see current architecture does not support modules as first class citizen in the graph. I am not an expert for internals but I have an idea:  1. Create an implicit hidden resource for each module. 2. This implicit resource must be depends on all resources on the modules. 3. Resources in the module which depends on other module can simply implicitly depends on resources which declared at (1) .  Is this doable? What do you think?"
technical,"Likewise, not having depends on with modules, nor for each, discourages use of modules in terraform since you have to give up critical features to take advantage. I heard an update weeks ago in a Hashicorp event telling us ""soon"". :) Maybe Christmas comes early this year ,)"
technical,Any update on implementing this feature please ? I need this plz
technical,"any workaround here in case of module/submodule structure? When module depends on submodule? I produced an output (like an id on a null resource) on one module, and then the other module can be fed that id as a variable from your main template.  This variable can be used in the depends on list, and as a trigger."
technical,"An alternate work-around, for my own use-case, is to set a bool explicitly so that the length need not be calculated at the initial planning phase. I think #12570 and similar issues would be addressed by this enhancement. Right?"
technical,"Thanks we are looking forward to that !  As I cannot see anyone else reporting a specific issue related to module dependencies that I'm encountering, I'll report it here. I hope that the support of ""depends on"" on modules will resolve that, but I'm not quite sure. Any idea ?  With some code like this: The ""count"" value depends on resource attributes that cannot be determined until apply, so Terraform cannot predict how many instances will be created. To work around this, use the -target argument to first apply only the resources that the count depends on. I think I managed to create a valid workaround for this.  It requires modules to have some boilerplate, but otherwise it seems to work fine."
technical,"I'm seeing a very similar issue, but am trying to reference a module output in a resource in the root:  Attempting to plan this will render the following error:  Error running plan: 1 error(s) occurred I was able to work around that by changing count to **3**, plan/apply-ing, then reverting the count to the dynamic value."
technical,"This is now working.  I thought however that this was the reason for the terraform remote state. Is to be able to exchange data between module without creating input streams like this one.  Is this fixed in any of the new versions? i would take this from this ticket, as it isnt a problem with tf but your code implementation. try the maililing list or ideally one of the many slacks were we discuss terraform. you can find me on the og-aws and hangops"
technical,"in my case, I have a module that creates security groups and then calls it again to create the rules for each SG, passing the corresponding SG id. Some SG rules reference other SG id for inbound targets.  the code that creates the rules depends on the SG already existing. I'm seeing a very similar issue, but am trying to reference a module output in a resource in the root:  Attempting to plan this will render the following error:  Error running plan: 1 error(s) occurred"
technical,"Based on the feedback here, I've locked this discussion. We're actively working on module expansion. The engineers are making good progress, and I am excited about 0.13. For folks who are really upset, I'm ddreierhashicorp.com and I'm happy to talk. I'm very excited to announce that beta 1 of terraform 0.13.0 will be available on June 3rd, and will include module depend on. I've pinned an issue with more details about the beta program, and posted a discuss thread for folks who want to talk about it more."
technical,"Based on the feedback here, I've locked this discussion. We're actively working on module expansion. The engineers are making good progress, and I am excited about 0.13. For folks who are really upset, I'm ddreierhashicorp.com and I'm happy to talk. If you're looking for additional use cases, that require something like depends on to work in a module, here's a simple one.  Several infrastructure servers need to be up and running, such as puppet, consul/vault, docker, before other servers that depend on them can even be spun up.  Because of several limitations in terraform resources, modules are the only way to implement configurations in a concise and efficient  manner, versus lengthy and numerous individual resource configurations.  Adding depends on, cuts my code from 100+ files and thousands of lines of code to 1 file and a couple hundred lines that that have to do nothing more than specify the variables for modules, and a dozen files for the called modules that become a one stop edit for configurations."
technical,"Thanks for helping, but my codes are fine. Currently I fixed it with below work around.  It works, but every time, when run terraform plan, always report there is one resource module.app.aws ecs service.app to be changed.  But when you apply it, no change at all. in my case, I have a module that creates security groups and then calls it again to create the rules for each SG, passing the corresponding SG id. Some SG rules reference other SG id for inbound targets.  the code that creates the rules depends on the SG already existing."
technical,"Do you expect that the aws ecs service depends on race condition will be satisfied with cross module variables?  ie moduleA outputs aws iam role policy.moduleA.id and moduleB is declared using that output - is that enough for a aws ecs service in moduleB to not have a race condition, or is a null resource necessary? In my case, I need set depends on to a resource in a module, any work around for me?  I got the same error: Always get error that resource is not found"
technical,"I am using modules simply to reduce the amount of boilerplate I need to write to set up an instance template, a managed instance group, etc. for my machine clusters.  I want to be able to pass in additional resources like load balancer pools, disk images, etc. but I can't set my module to depend on these passed in resources. In your sample, you can move the codes about iam role, policy to that module stem-job as well.  I did similar without depend on issue."
technical,"Managing to spawn the different modules using a dependency workaround is possible, yet it gets really messy when you want to destroy the stack. (Cycle errors happens and they are hard to read, even if there is no real cycle error in the module dependencies for what I know).  IMO this feature is a must to do what I imagine of ""clean architecture code"" with Terraform for large stack. I still need some shell script to deploy / destroy the modules in the correct order because with every workaround I could find on the net I never managed to have a fully working solution.  Keep up the good work TF Team we're waiting for this ! is this planned to be solved?"
technical,"So....... this is open for a very long time? I cannot understand why one module cannot depend on another.  I have a module that creates webapps, it needs to receive some items from a different module.. how do i go about doing that??  So basic.. is this still an open issue ? any feature is being worked on for this ask ?"
technical,"This is very strange - we introduced depends on for modules in 0.8.0-beta2   Paul  I'm not sure that is correct, using 0.11 and a module block cannot contain a depends on   error: module ""example"": ""depends on"" is not a valid argument It has been almost 3 years since this issue was opened, and I can verify that there is no way of structuring a complex infrastructure into simple terraform module building blocks that depends on each other(try creating k8s namespaces in one module that needs to be done right after another module that is responsible for GKE node pool creation).   I hope I am missing something here.   What do you need for this feature to be implemented? Can we help you? I think some of us could write terraform from scratch in 3 years... Have some faith on the community. mitchellh"
technical,"This is very strange - we introduced depends on for modules in 0.8.0-beta2   Paul  I'm not sure that is correct, using 0.11 and a module block cannot contain a depends on   error: module ""example"": ""depends on"" is not a valid argument It's hard to diagnose without seeing the module structure. It looks like you're perhaps not passing in the output to the module for the cluster. You'd need something like:  (With endpoint as a valid variable within your cluster module). And then consume the endpoint value there. Make sense?"
technical,"I produced an output (like an id on a null resource) on one module, and then the other module can be fed that id as a variable from your main template.  This variable can be used in the depends on list, and as a trigger. Jesus, this issue is not resolved still! I am facing the same issue."
technical,"Jesus, this issue is not resolved still! I am facing the same issue. Jesus, this issue is not resolved still! I am facing the same issue. and the outputs dont work for you?"
technical,"I am experiencing this issue as well, but not sure whether I am making a mistake here or whether the fact that I am using ARM templates as a deployment mechanism for the Logic Apps that I am deploying.  I have a file structure of: - main.tf -- shared/main.tf -- logic-apps/main.tf -- logic-apps/module1/main.tf -- logic-apps/module2/main.tf  Main is an orchestration level that is lightweight and creates a Resource Group and just called modules below it. Shared is supposed to creates a servicebus, auth rule and api connection to the service bus for use in the 'another' module. This currently outputs a dependency id as below.  Main.tf then passes that dependency id to the logic apps module that then creates some logic apps that should point at the servicebus api connection that was created above. Logic-apps.tf then passes this same variable to module1.tf to create the logic apps relevant to module1.  The module1.tf is using an Arm template in the same way as above, but has a depends on the dependency id that has been passed all the way from shared/main.tf.  At this point i am getting the following error Is this something that is caused by my use of ARM scripts within the Terraform code and are they not completing in the right timespan to allow the later creation of the logic apps that depend on them?  As a side note, if i run an Apply on this a second time it works because of the pre-existence on the service bus and the connection to it that can now be used, but not sure the client with accept that as a suitable solution. :-D Just another use case: We have one module that creates a template lambda function, it conditionally includes a separate module that provides default cloudwatch alarms for that lambda.  What I'd like to do is create a default alarm that watches the log output of the lambda and alerts when occurrences of terms like ERROR and WARN reach certain thresholds.  Issue is that the cloudwatch log group isn't created when I try to create my log metric filter, but I can't specify that my log metric filter depends on my lambda function. It crashes, saying the cloudwatch group doesn't exist yet. Trying to specify that the alarm module depends on the lambda function resource also doesn't work.  It works if I do everything in a single lambda function though, because I can set dependency between my log metric filter and my lambda function. I will try the output hack in the mean time tho."
technical,"dtserekhman-starz from what you have described here it sounds like you're passing the security group id to the ELB module and thus Terraform should, as you assumed, be able to infer that dependency and create things in the correct order. If that's not working, that sounds like a bug distinct from what this issue is talking about and so I'd encourage you to open a new top-level issue with some config examples so we can dig into it some more. Thanks for sharing this problem! Just to clarify, here's how my module is defined which creates a security group which it then exposes through its output variable below:  And this is instantiation of another module which relies on the computed ecs elb security group.id value from above:  So, from above, ""{module.ecs elb security group.id}"" value should already be computed when passed to security groups of the ecs elb module, but it's not. I believe this does fall into the same realm as other use cases in this ticket? Terraform needs to be able to compute that ecs elb security group module is a dependency to ecs elb module and should already instantiate all resources for the ecs elb security group before instantiating ecs elb module.  Thanks."
technical,"Any time frame for this? Are we expecting this to be fixed soon? Likewise, not having depends on with modules, nor for each, discourages use of modules in terraform since you have to give up critical features to take advantage."
technical,"Any time frame for this? Are we expecting this to be fixed soon? Managing to spawn the different modules using a dependency workaround is possible, yet it gets really messy when you want to destroy the stack. (Cycle errors happens and they are hard to read, even if there is no real cycle error in the module dependencies for what I know).  IMO this feature is a must to do what I imagine of ""clean architecture code"" with Terraform for large stack. I still need some shell script to deploy / destroy the modules in the correct order because with every workaround I could find on the net I never managed to have a fully working solution.  Keep up the good work TF Team we're waiting for this !"
technical,"Since it's unlock. Does that means that this issue was implemented? My case: two separate modules - VPC and ALB. In VPC module, VPC and subnets are created. Output variable is ""vpc id"". ALB module has vpc id input variable and use aws subnet ids data source to find subnets for ALB.  Unfortunatelly, that does not work. I suspect that data source is evaluated once VPC is created, but before subnets are created. My workaround was to return subnets as output variable from VPC module, and that is not an elegant solution."
technical,"Anyone have a good workaround for this in the meantime? My pseudo workaround is to add a list variable inside the module: Then when using the module, pass it a computed value from the resource I want it to depend on"
technical,"Retagging as enhancement, since this isn't broken functionality, its functionality that doesn't exist yet. :) My use case: the slave virtual machine depends on master virtual machine to exist"
technical,"Jesus, this issue is not resolved still! I am facing the same issue. and the outputs dont work for you? No. It doesn't.  The first module outputs.  The cluster module I have. And this is the result:"
technical,"please take a deep breath and calm down.  Imo Hashicorp market themselves as a software provider that provides solutions for businesses. If it was some dude working on his own in his basement then of course i wouldn't post such ignorant comments, however the fact that this feature has been open for over three years with no communication is a little disconcerting. I apologise if you felt somehow attacked by my comment. Please take the complaints to twitter or reddit.  I'm subscribed to this thread for news about the feature only."
technical,"thanks for getting back to me - you are indeed correct - modules cannot (currently) depend on other modules but can depend on a resource only  Will change the tag on this from bug to enhancement :)  Keep an eye out for it soon Retagging as enhancement, since this isn't broken functionality, its functionality that doesn't exist yet. :)"
technical,"This has been a requested feature for about 2 years now according to the other ticket, any progress on this? Seems like enough people need this. It's an important feature to be able to manage modules depending on other modules. In our case we use a variable from module 1 in the provisioner of module 2 and terraform does not detect that. I am thinking that I shouldn't even need depends on, but I was hoping to patch that problem with it."
technical,This is so basic. I am running into issues with some large stacks. Hopefully this issue gets fixed soon. Since it's unlock. Does that means that this issue was implemented?
technical,"Just another use case: We have one module that creates a template lambda function, it conditionally includes a separate module that provides default cloudwatch alarms for that lambda.  What I'd like to do is create a default alarm that watches the log output of the lambda and alerts when occurrences of terms like ERROR and WARN reach certain thresholds.  Issue is that the cloudwatch log group isn't created when I try to create my log metric filter, but I can't specify that my log metric filter depends on my lambda function. It crashes, saying the cloudwatch group doesn't exist yet. Trying to specify that the alarm module depends on the lambda function resource also doesn't work.  It works if I do everything in a single lambda function though, because I can set dependency between my log metric filter and my lambda function. I will try the output hack in the mean time tho. So I have a dependancies on the module creating the resource first to retrieve the id, is this the only way to pass on wait for deployment ? The module will create the resource and then the data must interrogate to retrieve the ids.  the module creates fine, when I add in the data and output it does not recognize resources as not created yet."
technical,"In your sample, you can move the codes about iam role, policy to that module stem-job as well.  I did similar without depend on issue. So....... this is open for a very long time? I cannot understand why one module cannot depend on another.  I have a module that creates webapps, it needs to receive some items from a different module.. how do i go about doing that??  So basic.."
technical,"Ah, that makes sense! Should have known that Terraform would optimize away my variable if I didn't use it. :) I tried out your suggestion with the trigger and that didn't do it, but setting the depends id on a tag on some resource in module 2 seems to have done it. Hacky but oh well I guess. Still getting ""depends on is not a valid parameter"" when used within a module definition. I see a lot of +1s asking to be able to depend on another module itself, but the feature that's supposed to work (depend on a module's resource), is broken.  This is biting me HARD because of the big warning at the top of the ecs service page which describes needing to use depends on to prevent a race condition. Due to this bug, **you cannot prevent it**."
technical,"apparentlymart - for my use-case I wanted to make an entire module dependent on a single resource, not depend on an entire module's resources.  I had a module that launched a cluster of various different EC2 clusters/ASGs that, when launched, would run a particular application.  I wanted to make the entire module depend on the existence of an RDS database that would be created outside the module.  I suppose I could pass in a computed value from the RDS instance as a variable and then reference that variable in some dummy way for the EC2 ASGs, but it would be convenient to be able to specify depends on on the module itself and have all of the resources contained within depend on the external resource. Sure, but that's an explanation from a low level perspective. That's just the implementation. You're saying ""this car cannot make right-hand turns because we've decided to use chrome-plated thingamajiggers in the steering linkage, so let's work around this limitation by making lots of left-hand turns instead"".  The way it looks for a user, you're creating an abstraction (the module) that behaves in many ways like other entities that can use depends on, but then this abstraction turns around and makes itself an exception to that rule. It's jarring. It really seems to me like this should have been taken into account from the beginning."
technical,"I have complied master from version Terraform v0.8.0-dev, which should include beta2 features. (I did this to get 10337  and 10338 fixes)  It seems it would work if you set a resource to depend on a module, but you cannot set a module to depend on another module......  Thanks for the great tool btw ,) thanks for getting back to me - you are indeed correct - modules cannot (currently) depend on other modules but can depend on a resource only  Will change the tag on this from bug to enhancement :)  Keep an eye out for it soon"
technical,"You shouldn't need any depends on setup here. Looks like you're referencing your variable incorrectly inside your module. Thanks for helping, but my codes are fine. Currently I fixed it with below work around.  It works, but every time, when run terraform plan, always report there is one resource module.app.aws ecs service.app to be changed.  But when you apply it, no change at all."
technical,"Sure, but that's an explanation from a low level perspective. That's just the implementation. You're saying ""this car cannot make right-hand turns because we've decided to use chrome-plated thingamajiggers in the steering linkage, so let's work around this limitation by making lots of left-hand turns instead"".  The way it looks for a user, you're creating an abstraction (the module) that behaves in many ways like other entities that can use depends on, but then this abstraction turns around and makes itself an exception to that rule. It's jarring. It really seems to me like this should have been taken into account from the beginning. Thanks for sharing your use-cases, everyone. That helps a lot.  I wanted to note that the depends on mechanism is intended not as the primary means of dealing with dependencies but rather as a workaround for situations where dependencies cannot be inferred automatically. For example, the resources for Amazon API Gateway have some issues here because there are some restrictions on what order resources can be created in that aren't represented in the resource attributes themselves. It's always preferable to use interpolation to create dependencies where it is possible to do so, since it's a more direct description of the desired end-state, rather than of the details of how to get there.  So if possible I'd like to collect some more use-cases where interpolation  can't  solve the problem via implied dependencies. chrisrlong's use-case seems like it could be an example of this: the dependency between the application's compute resources and its data store are presumably not explicit in the attributes, I assume in this case that some other configuration store like Consul is acting as the glue to help the code running on the EC2 instance find the RDS and ElastiCache addresses?  FlorinAndrei I understand and respect that perspective. I wasn't explaining how it works as an excuse for not supporting these use-cases but more as context for my later request that we talk about use cases  rather than  implementation strategy in case there's a different way to get the outcomes we need here without losing the benefits of the current implementation. I  do  want to solve this, but as engineers many of us have a tendency to focus on a particular solution to a problem rather than digging into the problem itself, and I want to be open to other solutions that may allow this problem to get solved sooner or with fewer drawbacks."
technical,"I think I managed to create a valid workaround for this.  It requires modules to have some boilerplate, but otherwise it seems to work fine. Thanks for the update and putting the emphasis on this, it is greatly appreciated."
technical,Thank you epic update and the transparency is appreciated! I think we can finally all rest easy that this is coming Thanks is good news.Time will come to remove all the work arounds soon then...
technical,"Glad to hear! super exciting Thanks we are looking forward to that !  As I cannot see anyone else reporting a specific issue related to module dependencies that I'm encountering, I'll report it here. I hope that the support of ""depends on"" on modules will resolve that, but I'm not quite sure. Any idea ?  With some code like this: The ""count"" value depends on resource attributes that cannot be determined until apply, so Terraform cannot predict how many instances will be created. To work around this, use the -target argument to first apply only the resources that the count depends on."
technical,"My pseudo workaround is to add a list variable inside the module: Then when using the module, pass it a computed value from the resource I want it to depend on Thanks, that doesn't entirely fit what I think this issue is talking about though - making a module able to depend on another one. I don't have any resources that the module should depend on - it should depend on another module being created.  I've tried doing stuff like this: Main terraform template: But this doesn't do anything - my module two is still created at basically the same time as module 1."
technical,"Just to clarify, here's how my module is defined which creates a security group which it then exposes through its output variable below:  And this is instantiation of another module which relies on the computed ecs elb security group.id value from above:  So, from above, ""{module.ecs elb security group.id}"" value should already be computed when passed to security groups of the ecs elb module, but it's not. I believe this does fall into the same realm as other use cases in this ticket? Terraform needs to be able to compute that ecs elb security group module is a dependency to ecs elb module and should already instantiate all resources for the ecs elb security group before instantiating ecs elb module.  Thanks. that does sound like a bug, separate from this feature request. If you could open a new issue with these details that'd be very helpful to try to figure out what's going on there."
technical,"While I appreciate the need for you and others to plan, we are not yet in a position to communicate any timelines. My hope above was to begin communicating what we're currently focused on more publicly.  I can give a few details on the larger first-class module project that this is a part of to help you with your planning:  - **improving modules is the main feature of the next terraform release**: we're expecting to ship  make modules first-class in 0.13, which is the next major release. The only other thing that we'll block 0.13.0 on is a set of critical bug fixes that require backward-incompatible behavior changes. We're not planning on holding back 0.13.0 for any other features. - **depends on may ship after count and for each**: depends on is a subset of the larger project of first-class modules. As part of that project, we'll make depends on, count, and for each work in modules. I can't tell you, yet, whether we'll ship depends on, and so count and for each all at once, or what order they'll ship in - we may ship for each and count in 0.13.0 and then quickly follow up with a minor release that adds depends on. We do intend to batch any breaking changes in 0.13.0, so that the later addition of depends on would be purely additive. That prioritization makes a lot of sense to me. Looking forward to it!"
technical,"i would take this from this ticket, as it isnt a problem with tf but your code implementation. try the maililing list or ideally one of the many slacks were we discuss terraform. you can find me on the og-aws and hangops the state only gets updated when the apply finishes. you are creating two resources IN THE SAME state... if those were two different statefiles, the 2nd would consume the resource outputs of the 1st"
technical,"man...  Many people ""pay a monthly"" and wait patiently for a new feature. This specific feature requires lot of tech resources and time, no need for such an outcry because it does not help anyone.  They decided to implement it which is great news, crass comments like these do not help anyone. There are various workarounds for this feature that currently exists, such using templates to generate tf files, so its not really a showstopper. We should be thankful that they even prioritized this at all."
technical,"We have not actually got to deploy consol yet, <watch this space currently we just have terraform, chef and capistrano. Terraform ""sticks"" it together itself using Route 53. Longs we stick to the naming conventions. This has been a requested feature for about 2 years now according to the other ticket, any progress on this?"
technical,"Great, 3 years in, looking forward to having this needed feature. This is a really important feature to have in my opinion. It would make the code much more readable and maintainable than by using one of the hack that we find on blog posts."
technical,It's hard to diagnose without seeing the module structure. It looks like you're perhaps not passing in the output to the module for the cluster. You'd need something like:  (With endpoint as a valid variable within your cluster module). And then consume the endpoint value there. Make sense? This is now working.  I thought however that this was the reason for the terraform remote state. Is to be able to exchange data between module without creating input streams like this one.  Is this fixed in any of the new versions?
technical,is this still an open issue ? any feature is being worked on for this ask ? This is so basic. I am running into issues with some large stacks. Hopefully this issue gets fixed soon.
technical,"Hi, any ETA of when this issue fix/enhancement may be available?  Terraform modules is a powerful feature for abstraction and code-reuse, but without inter-modules dependencies support we can only go so far with this feature. It's blocking us right now for the cases described above. This is still failing in 0.9.10.  :/"
technical,you already have it in your example. you have an output. use that as the input for the other one. consume it inside it. done profit This is throwing another error. I got this from the documentation: No profit.
technical,"So I have a dependancies on the module creating the resource first to retrieve the id, is this the only way to pass on wait for deployment ? The module will create the resource and then the data must interrogate to retrieve the ids.  the module creates fine, when I add in the data and output it does not recognize resources as not created yet. This is very strange - we introduced depends on for modules in 0.8.0-beta2   Paul  I'm not sure that is correct, using 0.11 and a module block cannot contain a depends on   error: module ""example"": ""depends on"" is not a valid argument"
technical, This is very strange - we introduced depends on for modules in 0.8.0-beta2  can you run terraform version and post the output here for me?
technical,"Wow, over three years and still nothing?! three years is nothing in terraform time"
technical,"There are various workarounds for this feature that currently exists, such using templates to generate tf files, so its not really a showstopper. We should be thankful that they even prioritized this at all. To clarify, my issue isn't at all with the lack of the feature - it's with the lack of communication.  Thanks for clarifying.  I would be fine with Hashicorp never developing this feature, but I take issue with Hashicorp not being able to even take a guess as to when it will be completed, if at all. It's great that they've communicated that this is a top priority, as that address the 'if it will happen', but they haven't addressed the 'when'.  I can understand that frustration, I agree Hashicorp could improve their communication with us as users of various kinds. However, WRT this general topic of first-class modules, I believe Hashicorp has been pretty clear, and within the bounds of what's possible (and also avoiding making unnecessary expectations which they may fail to deliver on).  For example, in the 3 years that this has transpired, we've seen the following: 1) many, rather verbose and elaborate discussions of the use cases and needs and possibilities, the problems and blocks, as well as how to get there. 2) Hashicorp realizes their fsck up, and acknowledges that TF core needs to be refactored in major ways. 3) Hashicorp lays down the roadmaps for 0.11 and 0.12, with various breaking/major changes that will facilitate the next round of changes 4) Hashicorp plows through 0.11 and 0.12 releases, following through as planned. 5) Hashicorp starts work on 0.13 which is focusing on modules as a first-class citizen  Particularly for those last two,  no timelines (with unreasonable expectations) were laid down. Yea, I would have loved to see 0.12 completed sooner, or dev on 0.13 to have started sooner, but I'd rather this than to have a product which is all over the place, poorly developed, poorly designed, lots of major breaking changes, and just generally moving to the whims of the wind as it blows."
technical,I believe this one can solve unnested modules problem too. There were some issues regarding this #10883 before. This solution would be intuitive and address these issues. Waiting for this feature to be added
technical,"I believe this one can solve unnested modules problem too. There were some issues regarding this #10883 before. This solution would be intuitive and address these issues. We have not actually got to deploy consol yet, <watch this space currently we just have terraform, chef and capistrano. Terraform ""sticks"" it together itself using Route 53. Longs we stick to the naming conventions."
technical,"Hi guys, any release ETA for this so much expected feature ? we just picked this up and started working on it. Two engineers on the terraform core team recently started working on supporting count, for each, and depends on work in modules.  Modules should be first-class, and the message is really clear from the terraform community that having some language features not work in modules makes it hard to adopt modules as intended, which has downstream consequences like making it harder to write maintainable, well organized terraform code. We saw how many people asked for it, we understand why not having it is painful, have prioritized it, and are actively working on it.  This is one of the terraform core team's current top priorities, and it's also a pretty significant amount of work, more than you'd imagine, because it's part of a larger effort to make modules first-class. The project just started, and I don't have an ETA.  As the project is currently running, it looks like for each and count will be done first, and then depends on will follow afterward. However, it may be that we batch them all up in the same release to minimize churn."
technical,"doesnt exist in that context. you cant depend on it at root module level what you need to do is pass ""something"" from the outputs of database module to the cluster module as an input, creating a dependency on creation"
technical,"Great to hear you guys are working on this. Have you guys been able to scope and figure out a timeline for the project yet? Roughly when this is planned to be released is one of the deciding factors for if my team will be using TFC or switching to Pulumi. Can you provide any broad information on when this will be released (week/month/year)?  Edit: Others might still need an ETA but we don't. The lack of communication (both on GitHub and via support email) about any estimates as to when Hashicorp will complete features they've committed to has resulted in my team canceling our TFC subscription. We're now moving to Pulumi. While I appreciate the need for you and others to plan, we are not yet in a position to communicate any timelines. My hope above was to begin communicating what we're currently focused on more publicly.  I can give a few details on the larger first-class module project that this is a part of to help you with your planning:  - **improving modules is the main feature of the next terraform release**: we're expecting to ship  make modules first-class in 0.13, which is the next major release. The only other thing that we'll block 0.13.0 on is a set of critical bug fixes that require backward-incompatible behavior changes. We're not planning on holding back 0.13.0 for any other features. - **depends on may ship after count and for each**: depends on is a subset of the larger project of first-class modules. As part of that project, we'll make depends on, count, and for each work in modules. I can't tell you, yet, whether we'll ship depends on, and so count and for each all at once, or what order they'll ship in - we may ship for each and count in 0.13.0 and then quickly follow up with a minor release that adds depends on. We do intend to batch any breaking changes in 0.13.0, so that the later addition of depends on would be purely additive."
technical,"**Please do not add a +1 comment to this issue.**  Instead, please add a  reaction to the issue parent above.  Many of us are subscribed to the comments here as we are waiting for an update if/when this becomes solved or a workaround is found.  Every ""+1"" comment triggers another email, essentially spamming the repo owners *and* everybody subscribed.  We are all anxious to find a solution here, but please be mindful of your fellow devs and do not add any additional ""+1"" comments as they do not work towards finding a solution (you can continue to voice support with a  above). work around to use -target if you have dependency in module that uses  count = ""{length(var.some var generated)}""  A way to fix it is to use shell script and trigger missing dependencies ex belwo 3 modules 1 vpn creation 2 subnets and 3rd is routing  run vpc and subnets before route module"
technical,"Can anyone please give a time line on when this feature will be released? Wow, over three years and still nothing?!"
technical,could you please elaborate with a piece of pseudo code? I have been using terraform only for a couple of days now and I have a hard time imagining what the solution would be like. you already have it in your example. you have an output. use that as the input for the other one. consume it inside it. done profit
technical,"In my case, I need set depends on to a resource in a module, any work around for me?  I got the same error: Always get error that resource is not found You shouldn't need any depends on setup here. Looks like you're referencing your variable incorrectly inside your module."
technical,This is throwing another error. I got this from the documentation: No profit. Your output reference is incorrect. The output from the module called database is simply called endpoint.   Therefore your output reference is module.database.endpoint
technical,"My case: two separate modules - VPC and ALB. In VPC module, VPC and subnets are created. Output variable is ""vpc id"". ALB module has vpc id input variable and use aws subnet ids data source to find subnets for ALB.  Unfortunatelly, that does not work. I suspect that data source is evaluated once VPC is created, but before subnets are created. My workaround was to return subnets as output variable from VPC module, and that is not an elegant solution. zygimantas I believe yours is a use case you can solve today by adding depends on attributes to your output blocks.  You VPC module could look something like this: This will prevent your VPC module from exposing the vpc id until the subnets are ready, which should allow your ALB module to lookup the subnets with a data block as you would expect."
technical,"here's my answer, i modified Maarondesigns answer by chaining the functions but had to add the Math.sign(num) !== -1 to get rid of the -2 **This is my solution:**  change code below this line change code above this line **And this can work as well:**  change code below this line  change code above this line"
technical,"The issue, as described in my original post, and corrected by  merge was that neither filter, map, nor chaining of functions had been introduced at that point in the curriculum.  Presumably you are now completing this problem after it has already been moved to its new home.  It's not that people are making it ""too complicated"", it's that the groundwork had not been laid for this challenge in its original location. #### Describe your problem and - if possible - how to reproduce it Challenge ES6 - Write Higher Order Arrow Functions The examples show moving from normal function to the ES6 arrow function.  However, the challenge is about ""chaining"" multiple functions together.  It is super unclear how the example code would lead one to chaining a filter and map function together.  This is really a better discussion for the functional programming section.  Additionally, the filter function requires that users understand how to determine if a number is an integer or not.  There is nothing in the prior curriculum which would make it clear to a new coder how to do this.  While you do talk about the ""parseInt"" function, it's never used in the context of comparing a number to itself.  A better filter function might be ""positive numbers"" instead, which is a much simpler comparison.  I was attempting to help someone understand this one today and I was initially flummoxed as to how to solve it.  Once I realized it was about chaining, I was able to write a solution like this:  I don't believe that there is another way to solve this using higher order functions and arrow functions.  There is no solution in the seed file, so I'm not sure exactly what the authors had in mind.  In short, this challenge badly needs clarification and simplification.  It may be entirely inappropriate for this section of challenges.  I could see it after a new challenge called ""How to Chain Higher Order functions"" or something like that."
technical,"I had to just skip it for now. Agreed, I was chugging along fine and then hit this one which felt like a massive leap in difficulty from any of the previous.  Mostly due that a lot of the concepts required to complete it were completely foreign as they had not been explained in prior lessons.  Skipping for now also."
technical,"Look who is here..! I know this tracker is strictly for issue related discussion, but its awesome to see you here.  Thanks for the report. Are these no longer being tagged/assigned?  Is someone responsible for the new curriculum?"
technical,"I've got a PR for this that's been there for already since November. const squaredIntegers = arr.reduce((accumulator, currentValue) = { if(currentValue  0 && currentValue % 1 === 0) { accumulator.push(currentValue * currentValue), } return accumulator, }, []),  Yes I UNDERSTAND most of this yet for the life of me I cannot understand why we truncate the decimal numbers or also current value % 1 === 0 so I have been looking at this problem for 1 hour at least... I do have strong math skills and understanding yet really modulus is just remainder yet what 1===0 so does this represent NAN I seem to be confused by word definitions relative to programming. I try to run the numbers through the function in my head yet this throws me off how am I filtering the decimal numbers.... I thought what if I can just pop that -2 of and make it not exist with pop() to no avail. Moving on now to much time on this."
technical,"Thanks for this post! I was super confused trying to solve this as well and I've even been exposed to .map() and higher order functions previously. I thought for sure there would be a way to solve it with only .filter() because they haven't even said what the other functions do when you get to this exercise, or that it's possible to chain them together. I managed to solve it with a really ugly workaround, still using .map() but forgot about .parseInt() and that you can chain the functions... Cheers! Glad that I'm not alone. Please add more steps leading to that exercise. In the meanwhile, I'm going to leave ES6 module, move ahead and come back to it when I'll be able to understand. It's the very first time since I got started that I genuinely can say something negative about the curriculum...all the previous exercises made sense in the linear progression...not this one."
technical,"I started on this. 2 problems:  1. I can't see my changes in local dev. I posted about it here. It must be something like I am looking at the wrong file  2. Where is the code for the tests? The YAML has this: should be a constant variable (by using this), Is this the actual test? What runs this? It's hard to read. What is the best way to work on functions inside a file like this?  I was going to re-write the problem a bit to check for syntax and change the tests. Not really sure how these tests within a markdown file work. great that you started on this!  Seeing local changes is a bit of a process. First you need to reseed the db with npm run seed, then enter npm run bootstrap. When the last command is finished, enter npm run develop, then refresh the page when that's ready. You should be able to see your changes after that.  As for number 2, yes, that's the test. Tests come in pairs with a string for text and another for the testString. Text is what the user will see on the left side of the page before tests are run. The test string itself is made up of two parts--the first is the actual test, then the string after the comma which is shown in the console if the test fails.  As for the actual test suite, I believe we're using Chai. There's a cheat sheet that might help you write your tests here: There are a number of ways you could work with functions here. How were you thinking of rewriting the challenge? A lot of the times the tests look at the return value of the function. The third test currently does this:"
technical,"Agreed, I was chugging along fine and then hit this one which felt like a massive leap in difficulty from any of the previous.  Mostly due that a lot of the concepts required to complete it were completely foreign as they had not been explained in prior lessons.  Skipping for now also. here's my answer, i modified Maarondesigns answer by chaining the functions but had to add the Math.sign(num) !== -1 to get rid of the -2"
technical,"Hi! I'm a new coder who just got to this point in the challenges. It definitely took some serious google searching to figure out a solution, since .map(), .filter(), and .reduce() had not been previously mentioned. I also only really grasped that I could chain and nest functions like: due to having learned some through Grasshopper before I started with FCC. I think some challenge on that concept before this one would've been useful, as well as challenges on filter, map, and reduce.  I know the chaining isn't necessarily required... but at this point in the challenges where the theme seems to be on condensing code, IMO it's the perfect time to bring it up.  Or in the meantime, maybe we could just point the text for the map, filter, and reduce to their relevant challenges later on, or to relevant tutorials elsewhere on the internet if FCC doesn't currently have challenges for those concepts? Hi All, This one was a little fun to figure it out... Here are the steps I took to get the answer. I know it's a slower approach but I had to make sure that I could isolate each step before getting to the next one. I hope this helps!"
technical,"Maybe map, filter, reduce need to be explained beforehand. But chaining is not required and there are multiple solutions I suppose. Hi! I'm a new coder who just got to this point in the challenges. It definitely took some serious google searching to figure out a solution, since .map(), .filter(), and .reduce() had not been previously mentioned. I also only really grasped that I could chain and nest functions like: due to having learned some through Grasshopper before I started with FCC. I think some challenge on that concept before this one would've been useful, as well as challenges on filter, map, and reduce.  I know the chaining isn't necessarily required... but at this point in the challenges where the theme seems to be on condensing code, IMO it's the perfect time to bring it up.  Or in the meantime, maybe we could just point the text for the map, filter, and reduce to their relevant challenges later on, or to relevant tutorials elsewhere on the internet if FCC doesn't currently have challenges for those concepts?"
technical,"Just came to put my two cents in. I agree with the initial assessment of SaintPeter. I got totally lost on this one and I'm not new to programming, just new to JavaScript. I quickly realized there was a knowledge gap between what I know about these methods and what is expected to solve the problem. I agree, several more challenges need to be added to the ES6 section to fill in said knowledge gaps. SaintPeter had some really great suggestions for possible challenge topics that could be added before this one, but I haven't taken the time to write them up yet.  If you have any suggestions for possible challenges you'd like to see before this one, please leave a comment. Or even better, please help us make a new challenge! There's still some time left in Hacktoberfest"
technical,"const squaredIntegers = arr.reduce((accumulator, currentValue) = { if(currentValue  0 && currentValue % 1 === 0) { accumulator.push(currentValue * currentValue), } return accumulator, }, []),  Yes I UNDERSTAND most of this yet for the life of me I cannot understand why we truncate the decimal numbers or also current value % 1 === 0 so I have been looking at this problem for 1 hour at least... I do have strong math skills and understanding yet really modulus is just remainder yet what 1===0 so does this represent NAN I seem to be confused by word definitions relative to programming. I try to run the numbers through the function in my head yet this throws me off how am I filtering the decimal numbers.... I thought what if I can just pop that -2 of and make it not exist with pop() to no avail. Moving on now to much time on this. I had lots of trouble with this too, until finding this post.  I even tried to copy and paste the solution under ""get a hint"" so that I could work backwards and the hint doesn't pass the test."
technical,"Hi All, This one was a little fun to figure it out... Here are the steps I took to get the answer. I know it's a slower approach but I had to make sure that I could isolate each step before getting to the next one. I hope this helps! I had to just skip it for now."
technical,"**This is my solution:**  change code below this line change code above this line **And this can work as well:**  change code below this line  change code above this line I have an integrated Intel card ( Intel HD graphics 620). As for cheap hardware and vms, my answer would be no. Any way to fix?"
technical,"I would agree with this issue - I got real stuck on it knowing quite well how to use the array methods - it doesn't say anything about using .map and it is not used or taught at all up to this point - same with testing if it's an integer - that's not too tough to figure out I don't think, but it does complicate things - the challenge description only talks about using .filter and the instructions say...  Use arrow function syntax to compute the square of only the positive integers (fractions are not integers) in the array realNumberArray and store the new array in the variable squaredIntegers.  From those instructions it seems like the challenge wants you to use just .filter to accomplish this - challenges are supposed to teach a single thing, this one introduces too much and feels way too complicated - and it isn't clear to me what we are even supposed to be learning on this one I just reviewed the curriculum leading up to this challenge. 1. There doesn't appear to be an introduction to the concept of a callback function or functions as first class object. 2. No introduction to anonymous functions (except two challenges prior) 3. No mention of either map or filter  All told, I think this challenge should just be removed or moved to the functional programming section."
technical,"My suggestion would be to greatly simplify the problem such that it can be solved by changing a normal function definition into an arrow function definition.  It's not even important that the output of the function be checked, just the syntax.  Just make sure that the instructions line up and that the tests check for the correct syntax.  I think that would be the most direct solution.  I'd say go for it!  Making these types of changes was how I got started.  It's great experience! I started on this. 2 problems:  1. I can't see my changes in local dev. I posted about it here. It must be something like I am looking at the wrong file  2. Where is the code for the tests? The YAML has this: should be a constant variable (by using this), Is this the actual test? What runs this? It's hard to read. What is the best way to work on functions inside a file like this?  I was going to re-write the problem a bit to check for syntax and change the tests. Not really sure how these tests within a markdown file work."
technical,"P.S: Your baby Contributors Chat room is still the place where everyone hangs out, so if you would like to catch up, you know where to find us. I would agree with this issue - I got real stuck on it knowing quite well how to use the array methods - it doesn't say anything about using .map and it is not used or taught at all up to this point - same with testing if it's an integer - that's not too tough to figure out I don't think, but it does complicate things - the challenge description only talks about using .filter and the instructions say...  Use arrow function syntax to compute the square of only the positive integers (fractions are not integers) in the array realNumberArray and store the new array in the variable squaredIntegers.  From those instructions it seems like the challenge wants you to use just .filter to accomplish this - challenges are supposed to teach a single thing, this one introduces too much and feels way too complicated - and it isn't clear to me what we are even supposed to be learning on this one"
technical,"Learning arrow functions is very similar to learning regex (which was excellent btw, kudos to the creators).  One needs to slow down, have lots of practice runs of increasing complexity and then you'll get it.  I am still quite far away from playing with Chai but if no one is actively working on this I'll give it a try.   It can be broken up into three successively difficult challenges if that's OK... I've got a PR for this that's been there for already since November."
technical,"I just reviewed the curriculum leading up to this challenge. 1. There doesn't appear to be an introduction to the concept of a callback function or functions as first class object. 2. No introduction to anonymous functions (except two challenges prior) 3. No mention of either map or filter  All told, I think this challenge should just be removed or moved to the functional programming section. in general the whole es6 section seems to too advanced and/or complexly written (or at least some of it), and the javascript section as a whole seems a little scattered and not simple enough for beginners, or maybe just doesn't progress gradually enough or something - I want to suggest some rearranging of some sections/challenges but Im not sure on a better way yet - there's more than a few forum posts from campers struggling in this area of the curriculum"
technical,"im really feeling stupid for not being able to figure this one out :( Is this still an issue? If so, then if I understand the problem correctly, the solution will be creating a text base explanation of how arrow functions work over ES5, emphasizing the chaining of arrow functions, right? I am pretty green PR-wise so not sure if I should just start, I don't want to do it wrong, etc. If the above is correct I'll do this."
technical,"I have an integrated Intel card ( Intel HD graphics 620). As for cheap hardware and vms, my answer would be no. Any way to fix? Just came to put my two cents in. I agree with the initial assessment of SaintPeter. I got totally lost on this one and I'm not new to programming, just new to JavaScript. I quickly realized there was a knowledge gap between what I know about these methods and what is expected to solve the problem."
technical,"Reran all the npm scripts and my changes are reflected after a seed, but my changes are still not reflected on save. I need to run npm run bootstrap to see them. Is there a way around this?  I'm familiar with Chai but will have to play around with the tests to fully get what is going on.  As for what I was going to re-write, just going to focus more on the syntax and remove the need for the function to be written in a particular way, i.e. it has to have an array named a certain way inside of it. This kind of change is what SaintPeter suggested I do above. Learning arrow functions is very similar to learning regex (which was excellent btw, kudos to the creators).  One needs to slow down, have lots of practice runs of increasing complexity and then you'll get it.  I am still quite far away from playing with Chai but if no one is actively working on this I'll give it a try.   It can be broken up into three successively difficult challenges if that's OK..."
technical," Look who is here..! I know this tracker is strictly for issue related discussion, but its awesome to see you here.  Thanks for the report."
technical,"Glad that I'm not alone. Please add more steps leading to that exercise. In the meanwhile, I'm going to leave ES6 module, move ahead and come back to it when I'll be able to understand. It's the very first time since I got started that I genuinely can say something negative about the curriculum...all the previous exercises made sense in the linear progression...not this one. Maybe map, filter, reduce need to be explained beforehand. But chaining is not required and there are multiple solutions I suppose."
technical,"sorry about the delay in getting back to you.  Yup.. you are right at the moment we do not have anyone to actively oversee the curriculum's quality. We would love some help though. Its been a little while since the new curriculum was first brought in (I mean added to the beta) by the contributors, so yeah it could use the polishing up.  I know of a lot of contributors who have done some great work like ahmadabdolsaheb mstellaluna and scissorsneedfoodtoo recently, just to name a few off the top of my head.  The other thing is that us shipping the platform (splitting up the curriculum infra and the user backend) has broken the contributing pipeline. The new challenge infra is supper snappy and powerful BTW. So some real potential there, that we did not have previously. No more monkey patching console.log !!  The challenge and user schema has changed, the tooling has changed (for good though, which we will see in a few days hopefully, when we fix things for the local setup).  We are working toward improving to the DX for contributors, as soon as we have the production really stable for us to focus on the contributions. There has been a great surge in people wanting to contribute. We are just a little swamped the moment with the support and user priority issues. P.S: Your baby Contributors Chat room is still the place where everyone hangs out, so if you would like to catch up, you know where to find us."
technical,"I had lots of trouble with this too, until finding this post.  I even tried to copy and paste the solution under ""get a hint"" so that I could work backwards and the hint doesn't pass the test. people are just making it too complicated . here is my Solution it's very simple"
technical,"great that you started on this!  Seeing local changes is a bit of a process. First you need to reseed the db with npm run seed, then enter npm run bootstrap. When the last command is finished, enter npm run develop, then refresh the page when that's ready. You should be able to see your changes after that.  As for number 2, yes, that's the test. Tests come in pairs with a string for text and another for the testString. Text is what the user will see on the left side of the page before tests are run. The test string itself is made up of two parts--the first is the actual test, then the string after the comma which is shown in the console if the test fails.  As for the actual test suite, I believe we're using Chai. There's a cheat sheet that might help you write your tests here: There are a number of ways you could work with functions here. How were you thinking of rewriting the challenge? A lot of the times the tests look at the return value of the function. The third test currently does this: Reran all the npm scripts and my changes are reflected after a seed, but my changes are still not reflected on save. I need to run npm run bootstrap to see them. Is there a way around this?  I'm familiar with Chai but will have to play around with the tests to fully get what is going on.  As for what I was going to re-write, just going to focus more on the syntax and remove the need for the function to be written in a particular way, i.e. it has to have an array named a certain way inside of it. This kind of change is what SaintPeter suggested I do above."
technical,"Are these no longer being tagged/assigned?  Is someone responsible for the new curriculum? sorry about the delay in getting back to you.  Yup.. you are right at the moment we do not have anyone to actively oversee the curriculum's quality. We would love some help though. Its been a little while since the new curriculum was first brought in (I mean added to the beta) by the contributors, so yeah it could use the polishing up.  I know of a lot of contributors who have done some great work like ahmadabdolsaheb mstellaluna and scissorsneedfoodtoo recently, just to name a few off the top of my head.  The other thing is that us shipping the platform (splitting up the curriculum infra and the user backend) has broken the contributing pipeline. The new challenge infra is supper snappy and powerful BTW. So some real potential there, that we did not have previously. No more monkey patching console.log !!  The challenge and user schema has changed, the tooling has changed (for good though, which we will see in a few days hopefully, when we fix things for the local setup).  We are working toward improving to the DX for contributors, as soon as we have the production really stable for us to focus on the contributions. There has been a great surge in people wanting to contribute. We are just a little swamped the moment with the support and user priority issues."
technical,"people are just making it too complicated . here is my Solution it's very simple The issue, as described in my original post, and corrected by  merge was that neither filter, map, nor chaining of functions had been introduced at that point in the curriculum.  Presumably you are now completing this problem after it has already been moved to its new home.  It's not that people are making it ""too complicated"", it's that the groundwork had not been laid for this challenge in its original location."
technical,"We discussed this issue on the SIG-arch call of 20200130, and have unanimously agreed that we will keep the current naming for the aforementioned reasons. **What would you like to be added**: Rename ""taint"" to something less vulgar. **Why is this needed**: In common English, a taint is an area in the nether regions of the human body. This makes discussing Kubernetes ""taints"" very difficult in a professional or public setting, especially if anyone who doesn't already know of Kubernetes ""taints"" is present. We should rename ""taint"" to be either ""perineum"", the official name for the taint, or something more fitting like ""restriction"" or ""stigma."""
technical,"I'm opposed to the change at this stage due to the overall impact it would have, there is plenty of history and literature that does not pertain to the urban-dictionary definition. Agreed. A rename at this point is not practical due to API compatibility guarantees, and the term as used corresponds to the legitimate and inoffensive definition of the word."
technical,"it might be too late for this one, without breaking thousands of users. but ""taint"" in general is already used in computing and i don't think many people have a problem with that. I don't agree with that rationale. I think we can all agree that we should avoid using vulgarities in our code, documentation, and general professional communication. Regardless of whether or not the word was once accepted in computation, it is now a common vulgarity. Therefore, we should move away from it. Doing so will only increase the appeal of Kubernetes by making it appear more mature and forward thinking."
technical,"Thanks for the feedback.  Since these issues historically attract a lot of unhelpful arguments and attacks in the threads, I'm going to lock this issue for now and when we add this to sig-architecture agenda I'll update with details for discussion there. I'll bring this up at the sig-arch meeting today, however as neolit noted * our API guarantees prevent these sorts of renames on short time scales (we might introduce a new name, but the old name would be preserved effectively for the rest of the v1 lifetime) * the term is well established in both having common descriptive usage (outside of the less-common vulgar usage) so I think it's unlikely we would change this.  Will update after the meeting."
technical,"I'll bring this up at the sig-arch meeting today, however as neolit noted * our API guarantees prevent these sorts of renames on short time scales (we might introduce a new name, but the old name would be preserved effectively for the rest of the v1 lifetime) * the term is well established in both having common descriptive usage (outside of the less-common vulgar usage) so I think it's unlikely we would change this.  Will update after the meeting. I'm opposed to the change at this stage due to the overall impact it would have, there is plenty of history and literature that does not pertain to the urban-dictionary definition."
technical," it might be too late for this one, without breaking thousands of users. but ""taint"" in general is already used in computing and i don't think many people have a problem with that."
technical,"I don't agree with that rationale. I think we can all agree that we should avoid using vulgarities in our code, documentation, and general professional communication. Regardless of whether or not the word was once accepted in computation, it is now a common vulgarity. Therefore, we should move away from it. Doing so will only increase the appeal of Kubernetes by making it appear more mature and forward thinking. Thanks for the feedback.  Since these issues historically attract a lot of unhelpful arguments and attacks in the threads, I'm going to lock this issue for now and when we add this to sig-architecture agenda I'll update with details for discussion there."
technical,"Agreed. A rename at this point is not practical due to API compatibility guarantees, and the term as used corresponds to the legitimate and inoffensive definition of the word. We discussed this issue on the SIG-arch call of 20200130, and have unanimously agreed that we will keep the current naming for the aforementioned reasons."
technical,"Alright guys, locking.  What is asked here is not possible with the way spriteframes work in Godot, and a proxy parameter was added to allow the use case described here without breaking compatibility.  If you want the frame property to follow a different logic, or be removed and replaced altogether, this is a breaking change and it should be discussed (constructively) on godot-proposals. *Issue description:** Working in Frame-by-frame Animation with Spritesheets is a bit tedious right now, see discussion here: However another, maybe far more important issue, is the exponential workload created when adding frames to an existing animation in a spritesheet animation. **This makes iteration almost impossible.** Adding frames to the bottom of the spritesheet is no problem at all, as increasing the Vframe does not change existing keyframes. Adding frames horizontally however, breaks the animations, because frame: count the cells in the spritesheet from left to right and keyframes don't update their ""frame:"" value to added cells in every row.  My current workaround involves numbering each frame in the spritesheet with current and previous keyframe numbers in an external graphics editor, then go to every Animation and every track and every keyframe and manually set the new value. I hope you can see how crazy this is and becomes even more so the more animations/tracks/keyframes you have. In addition to the stupidity of the task, it's super easy to incorporate mistakes in the procedure, and hence demands a lot of concentration. I do not wish this job on my fiercest enemy.  **Proposed solution:** Update Keyframe value ""frame:"" when Hframes changed depending on what VFrame the frame: was set and how many Hframes where added. For instance: EDIT: I spend a day and a halve making another graphic in hopes newcomers to this issue can gasp it quickly:   **Steps to reproduce:**  1. Create new Scene with Sprite and AnimationPlayer 2. Create a new Animation 3. Add Texture 6x2gdicon.png to the Sprite, in the Sprite Inspector under ""Animation"" set Vframe 6 and Hframe 2 4. Keyframe frames 0 to 11 5. Override or change 6x2gdicon.png with 6x3gdicon.png and **change Hframe to 3** 6. Watch a broken Animation"
technical,"I am **once again**, going to explain how the frame property works within the engine: you select a frame, which is numbered from left to right, then from top to bottom. So it **is expected** so that changing the number of column changes the ID of your frames. It's easy to understand, and without considering this texture resize problem, it does the job quite well.   It is not expected by users as you can see here in this thread.  Sorry, but this issue did not receive enough upvotes to be significantly representative about how the feature should work. But anyway, what users think is not what matters to define what is a bug or not. A bug happens when a software does not behave as expected from the programmer point of view, not the user one. The user might just have, like here apparently, misunderstood how the feature was designed and the implied advantages/flaws. So the only bug here is maybe about adding a warning to the documentation, but I don't really call that a bug.   If you don't want to call it a bug, it's very bad design at the very least.  Yeah, in that case we can call it bad design. So for now this issue will not be opened again as this is not a bug, and you can now easily workaround it using the frame coords feature. The update the ""frame"" animation to the ""frame coord"" animation is another problem though, so if you face problem updating this should be another issue.  If you have a better design in mind, please open an issue on the godot-proposal repository. A bug happens when a software does not behave as expected from the programmer point of view, not the user one  Have you ever worked with a QA team? You don't see the problem with this philosophy at all?    Yeah, in that case we can call it bad design. So for now this issue will not be opened again as this is not a bug  This issue was not labeled as bug. To be reopend it does not need to be labeled as bug. You yourself labeled it as "" enhancement topic:editor usability"" I asked akien-mga who closed it to reopen it, because nothing about it has been fixed. And it still needs fixing!   you can now easily workaround it using the frame coords feature.  No you cannot. This would mean destroying and completely redoing each and every single Animation. In my case that's a year's worth of work.  Once again, this issue is about the Frame property. Not the newly added Coords. If the Frame property is still available in the engine, it still needs fixing.   explain how the frame property works within the engine: you select a frame, which is numbered from left to right, then from top to bottom.  This is not how the Frame property works for the user. **The frame property expects an integer**. The user therefore expects the same behavior if the spritesheet is expanded vertically or horizontally, again, see OPs graphic in the original post.   If you have a better design in mind, please open an issue on the godot-proposal repository.  The original post already has a fix proposed: **The cell IDs have to update if the Hframe property changed.**"
technical,"Let makes things clear. A bug is when something happens in a way it was not designed for. No one designed Godot to keep the indices if you change your texture size, it is written down nowhere in the documentation. So right now, everything works as expected (indeed from the developers point of view), even if the current design has limitations.  I do not mean to minimize the problem, the enhancement would be a significant improvement. But no, it is not a bug. A bug is when something happens in a way it was not designed for.  The design is to let the user assign a specific keyframe. Godot was not designed to change the displayed keyframe once it has been assigned unless the user resigns it. It works as specified in the design when expanding vertically. But not horizontally. Godot is also designed to allow changes in the spritesheet dimension. (HFrame and VFrame Inspector settings).  To me that's as clear cut a bug as the definition can be. The way I read and understand it, also according to your own definition."
technical,"Yeah, that's a year of work. Next time you will have to think a little bit more about how a feature works before using it. And we won't solve your problem by introducing workarounds in the code, sorry. And once again, there is nothing to fix here, unless you have a real ""solution"" to the problem. Otherwise, stop wasting our time.  What the hell? Alright guys, locking.  What is asked here is not possible with the way spriteframes work in Godot, and a proxy parameter was added to allow the use case described here without breaking compatibility.  If you want the frame property to follow a different logic, or be removed and replaced altogether, this is a breaking change and it should be discussed (constructively) on godot-proposals."
technical,"Yeah, that's a year of work. Next time you will have to think a little bit more about how a feature works before using it. And we won't solve your problem by introducing workarounds in the code, sorry. And once again, there is nothing to fix here, unless you have a real ""solution"" to the problem. Otherwise, stop wasting our time.  What the hell? Anyway, I might be able to solve the problem easily. I'll try something."
technical,"There is no way to know what's actually on the sprite sheet.  Godot does not need to know what's on the sprite sheet. All it needs to know is that I when I add a column (increase of Hframes) it will change all already existing keyframes with the ""frame:"" value according to the algorithm proposed in the solution above.  I do not see how this is ** not ** a bug, to be honest: **If you already have keyframes set for your frame-by-frame animation, why would you want the  displayed  frames of already set keyframes change arbitrarily, only because you add a column or row to your spritesheet?**  It makes iteration impossible, because it forces you to either have a fixed number of animations or a fixed number of frames per animation when you start your project. The only case when this would be less of a problem/bug is when you are creating an exact clone or mini game and you how exactly how many animations or how many frames you will need. But are these the the only kind of games we want for this engine? At least on my end HFrames to 3 corrects the animation.  Sadly it does not. What this does and what your gif shows is that it defines the correct cell size of the sprite sheet. What breaks the animation is that different cells are shown. Because ""frame:"" counts the cells from left to right until the end of the first row, then the second row from left to right and so on. Maybe make sure your animation plays slow, you can pause and see what frame it used to be, I've written it below the number of the current keyframe. The red frame for instance is 10, while it was and still should be 7.  If this would be a runcycle, the character would not run anymore because the frames would be all mixed up. I will add another example with a runcycle when I get home, maybe the issue is easier and quicker to grasp then. It will be a few hours though."
technical,"Hey guys I made another graphic, in hopes it's easier to understand for people hearing about this the first time, let me know what you think. groud and others who have labeling rights: **I kindly request the ""enhancement"" label to be reviewed.** Please have a look at the graphic I made. If you still don't think this is a bug, please be so kind and help me understand how this could be intended behavior. It makes spritesheet work hard for no benefit and the trouble the issue causes increases exponentially with the size of the project. Everyone, no matter what their workflow is or how they structure their spritesheet or whether or not they use a spritesheet packer tool or not, everyone has to deal with this issue and the additional workload it creates. A workload, that becomes unfeasible to manage if the project becomes large.  We do not have an option yet to import timing for animation, so when designing our animation, we are depended on keyframes.  I'm just at the beginning of incorporating animation into my project. Luckily, I haven't set all my keyframes yet, just a tiny percentage. Still, I had to meticulously and painstakingly spend hours and hours selecting each frame, on every track on every animation, comparing the previous value with my desired current one and reset it, all the while praying I don't fuck up. There is no point in doing this. It's tedious and an invite for error. How was this ever intended behavior? And even if it was, it's buggy as hell."
technical,"Wow, congrats on working on such a beautiful game!  Of course .. TexturePacker  **can**  make a lot of sense even for pixel games, depending on your target platform ect ... If you want to release on C64, you most likely will have to dig much deeper than TexturePacker to make things work.  The Sprite doesn't know that it has animations linked to it and the AnimationPlayer just has a track that points to a property of a node (in this case to the frame property of a Sprite) and doesn't know anything else about it (it doesn't even know that it's pointing to a specific Sprite just a NodePath)  So maybe the Animation Player  should  be notified when there is a change made of a Hframes property down the Node Path? What's wrong with having the Sprite Node emitting a signal to do just that?  The position of your sprites in the spritesheet doesn't matter because it's you (or your programmer) the one that will give meaning to it.  It does matter a whole lot to whoever is in the process of creating the spritesheet and keyframes if they have an interest to stay mentally sane. Once they are all done you are totally right. You can scramble and optimize the sprite frames order and position to their most efficient arrangement. That is as long as your code is supporting the simultaneous and automatic re-setting of your ""frame:"" values in all your keyframes. But if we would have this functionality in Godot already, we would not need to have this discussion here in the first place. Hey guys I made another graphic, in hopes it's easier to understand for people hearing about this the first time, let me know what you think."
technical,"I'm curious, are there any examples you have showing another game engine doing this kind of re-sequencing?  Pretty sure this is how the sprite sheets are designed, so it wouldn't be a ""bug"".  The idea behind it in the current form is that as long as you keep your frames in some kind of sequence going from left to right, top to bottom, it doesn't matter how you arrange the sheets.  The frames should still read in the correct frame order.  Like doing this for example, wouldn't break your animation keyframes.  ## From 6 by 2 ## Into a 2 by 8 So you need to add some frames to this 2x8 sheet later, they would go in positions to the right of 11 and wrap into the new row.  And if you have a new row, increase it.  You would just have to organize and export your spritesheets to maintain this order, and nothing should break later.  I think I understand that you have some particular workflow you want accommodated, where you use  completely symmetrical spritesheets, and need to extend or contract all cycles uniformly?  Again, not entirely sure.  It sounds at least like you want to toss in a new column, update the frame count, and have the engine modify all your keyframes in all animation players and animations that use that sprite sheet.  If that is roughly it, then what you're suggesting solves perhaps only just for that, but what about non-uniform spritesheets?  In my work at least, we stuff in animations of all differing lengths, and rarely do we have a set that even give us perfectly filled sheets.  In my case, having an update to a spritesheet property do widespread changes to the keyframes is undesirable.  I would think just the action of fiddling with the Hframe and Vframe properties will start mangling all our keyframes if it was making assumptions about how its organized. hey, first of all, thank you sticking with me and trying to understand my arguments, I really appreciate it!  I have no experience using large spritesheets and frame-by-frame animation in other engines, so sadly I cannot give you an example how this would be dealt with elsewhere.  The idea behind it in the current form is that as long as you keep your frames in some kind of sequence going from left to right, top to bottom, it doesn't matter how you arrange the sheets. The frames should still read in the correct frame order.  Yes I know. That's precisely the root of the problem. I do not think I have a particular workflow I want accommodated, the issue applies to the workflow you explained in your last comment just the same as soon as you want iteration and more than a few animations in a single sprite sheet for drawcall and performance reasons.  Please bear with me while I try to explain: If you know of any other way or option to organize, edit, manage and maintain larger amount of animations in a spritesheet, please do share."
technical,"I'm trying to use AnimatedSprites controlled by AnimationPlayer not even using spritesheets.. but individual, animated .png's. ""Only"" trying to get it somehow to work since about 40 hours. There is zero documentation for frame-animation. There's a few examples... but none of them go into different animations: Always a single kind of animation. When asking in help not enough people seem to be using this kind of animation as programmers probably just use some cutout sprites and stick to AnimationPlayer. I've set myself some workflow with other applications where I can decently animate something new and simple within 5 minutes or less. And I'm sure I'm not the only one who prefers NOT to directly animate in Godot if a different, more efficient pipeline is already setup. Please... do not enforce people to rigid workflows.. but enable them to use whatever they need. By: Enabling simple use of individual .pngs or Spritesheets in AnimatedSprites.. and letting those be controlled through AnimationPlayer. This is not about ""enhacement"". In the current state.. it seems impossible to actually use and get anything done. When I try to change the played Animation in AnimationPlayer... the only keyframed Animations played are actually the ones selected in ""Animation"" property of the AnimatedSprite. Having different animations in AnimatedSprites. Therefore completely has no effect. I am now forced to use a separate ""AnimatedSprite"" for each kind of animation. However that leads to all animations being shown on screen at once. This cumbersome workflow can't be intended by design: Especially since both AnimatedSprites and AnimationPlayer already have separate tracks for different animation-types... built... in. I also have this issue and think it is a bug!"
technical,"The only downside I see with dead pixels is a larger download. The upside is a MUCH better overview, which is absolute indispensable, when you are doing pixelart, get more animations and frames and lower resolution sprites. Currently I'm working on a 2D Pixelart project (something Godot praises itself for being a great fit) which means I have tons of frames that look almost indistinguishable next to each other. On top of that, naming each frame even each animation is impossible with frames just 32 pixel or 48 pixel wide which also have to accommodate the sprite. My current (very incomplete) player base sprite has 67 rows and 8 columns, so 535 frames. The png is 112KB in size, so I honestly don't care a bit about unused pixels on my spritesheet. Even if it would be ten times as large, the ability to find animations and frames I'm looking for outweighs the 0.01 seconds additional download time a billion to one. I know you are not doing pixelart in your game (which looks absolutely awesome btw! I've been following for a while) but in pixelart animation you select move and copy previously created frames and past them into new animations all the time. That's the core of any pixelart animation. Every single pixelpush has a huge impact to your animation. So the ability to quicky jump through your spritesheet, find what you are looking for copy and paste it to someplace else in your spritesheet or compare single pixel locations from various places in your sheet, is absolutely necessary. You also have to do this because your newly created frame must match previously created ones by the pixel. Think a change from one idle into another.  Yes, if we have to expand the frame count, well we have to just bite the bullet and re-keyframe some things. It has been done a couple times and it only takes a few minutes for a dozen or so animations.  This is what this issue is about. For you it might be not that much of an issue. I suppose this is  because  you are not making a pixel game and  because  you are making a sidescoller with two directions of movement, not 8. Hence your spritesheets are a lot more clearly laid out by default and while the re-keyframing or rearranging of your spritesheets might be an inconvenience, it is not so much an issue. For me however, and I dare say anyone who wants to do a slightly more professional looking pixel art game with decently complex animation, I dare say it not just an inconvenient issue, it's clearly a Starship Trooper sized brain bug in Godots design. **I therefore would really appreciate if someone would review the ""enhancement"" tag and maybe consider replacing it with a ""bug"" tag.** I did not see our way in there, so you may feel this is illegitimate  You way or organizing the spritesheet is just as legitimate as any other. It is however affected by this issue just the same. Hence not a solution. When scaling up production, your way of organizing it would fail exactly like the other options I've layed out so far. And for the same reason:  because keyframes don't update their ""frame:value"" when HFrames changes.  So you either end up having to rework all the sprites in your spritesheet externally and make the spritesheet less readable in the process, or re-set all keyframe ""frame:"" values in Godot."
technical,"I did not see our way in there, so you may feel this is illegitimate  You way or organizing the spritesheet is just as legitimate as any other. It is however affected by this issue just the same. Hence not a solution. When scaling up production, your way of organizing it would fail exactly like the other options I've layed out so far. And for the same reason:  because keyframes don't update their ""frame:value"" when HFrames changes.  So you either end up having to rework all the sprites in your spritesheet externally and make the spritesheet less readable in the process, or re-set all keyframe ""frame:"" values in Godot. I don't understand this issue. What you are asking is not something any framework/engine I've used before supports. How do you expect Godot to understand that the image that you told it had 67 rows and 8 columns now has 70 rows and 8 columns and it should magically (?) offset all the frames of the other animations? That image doesn't even know that it has animations linked to it. Making sense of where one animation starts and when it ends is your job or yours programmer job. Godot doesn't know that your first row is the ""walk"" animation and your second is the ""run"" animation that's something you tell it and you will need to update if you change anything that affects it. The only thing that Godot knows and cares is that an image has x rows and y columns and uses that information to calculate where in that image the frame 10 is anything else is up to you.  Also, transparent pixels are still uploaded to the GPU and taking space in the video memory. PNGs are compressed lossless image files but the data you upload to the GPU is uncompressed so that 112KB png file is ~2.2MB uncompressed if my math isn't wrong (which isn't much but you can see why it can add up to a lot and why there are tools like TexturePacker that can remove all the transparent pixels around the sprite and pack them tightly into a smaller image). It has nothing to do with download speeds."
technical,"At least on my end HFrames to 3 corrects the animation.  Sadly it does not. What this does and what your gif shows is that it defines the correct cell size of the sprite sheet. What breaks the animation is that different cells are shown. Because ""frame:"" counts the cells from left to right until the end of the first row, then the second row from left to right and so on. Maybe make sure your animation plays slow, you can pause and see what frame it used to be, I've written it below the number of the current keyframe. The red frame for instance is 10, while it was and still should be 7.  If this would be a runcycle, the character would not run anymore because the frames would be all mixed up. I will add another example with a runcycle when I get home, maybe the issue is easier and quicker to grasp then. It will be a few hours though. I'm curious, are there any examples you have showing another game engine doing this kind of re-sequencing?  Pretty sure this is how the sprite sheets are designed, so it wouldn't be a ""bug"".  The idea behind it in the current form is that as long as you keep your frames in some kind of sequence going from left to right, top to bottom, it doesn't matter how you arrange the sheets.  The frames should still read in the correct frame order.  Like doing this for example, wouldn't break your animation keyframes.  ## From 6 by 2 ## Into a 2 by 8 So you need to add some frames to this 2x8 sheet later, they would go in positions to the right of 11 and wrap into the new row.  And if you have a new row, increase it.  You would just have to organize and export your spritesheets to maintain this order, and nothing should break later.  I think I understand that you have some particular workflow you want accommodated, where you use  completely symmetrical spritesheets, and need to extend or contract all cycles uniformly?  Again, not entirely sure.  It sounds at least like you want to toss in a new column, update the frame count, and have the engine modify all your keyframes in all animation players and animations that use that sprite sheet.  If that is roughly it, then what you're suggesting solves perhaps only just for that, but what about non-uniform spritesheets?  In my work at least, we stuff in animations of all differing lengths, and rarely do we have a set that even give us perfectly filled sheets.  In my case, having an update to a spritesheet property do widespread changes to the keyframes is undesirable.  I would think just the action of fiddling with the Hframe and Vframe properties will start mangling all our keyframes if it was making assumptions about how its organized."
technical,"groud and others who have labeling rights: **I kindly request the ""enhancement"" label to be reviewed.** Please have a look at the graphic I made. If you still don't think this is a bug, please be so kind and help me understand how this could be intended behavior. It makes spritesheet work hard for no benefit and the trouble the issue causes increases exponentially with the size of the project. Everyone, no matter what their workflow is or how they structure their spritesheet or whether or not they use a spritesheet packer tool or not, everyone has to deal with this issue and the additional workload it creates. A workload, that becomes unfeasible to manage if the project becomes large.  We do not have an option yet to import timing for animation, so when designing our animation, we are depended on keyframes.  I'm just at the beginning of incorporating animation into my project. Luckily, I haven't set all my keyframes yet, just a tiny percentage. Still, I had to meticulously and painstakingly spend hours and hours selecting each frame, on every track on every animation, comparing the previous value with my desired current one and reset it, all the while praying I don't fuck up. There is no point in doing this. It's tedious and an invite for error. How was this ever intended behavior? And even if it was, it's buggy as hell. I'm trying to use AnimatedSprites controlled by AnimationPlayer not even using spritesheets.. but individual, animated .png's. ""Only"" trying to get it somehow to work since about 40 hours. There is zero documentation for frame-animation. There's a few examples... but none of them go into different animations: Always a single kind of animation. When asking in help not enough people seem to be using this kind of animation as programmers probably just use some cutout sprites and stick to AnimationPlayer. I've set myself some workflow with other applications where I can decently animate something new and simple within 5 minutes or less. And I'm sure I'm not the only one who prefers NOT to directly animate in Godot if a different, more efficient pipeline is already setup. Please... do not enforce people to rigid workflows.. but enable them to use whatever they need. By: Enabling simple use of individual .pngs or Spritesheets in AnimatedSprites.. and letting those be controlled through AnimationPlayer. This is not about ""enhacement"". In the current state.. it seems impossible to actually use and get anything done. When I try to change the played Animation in AnimationPlayer... the only keyframed Animations played are actually the ones selected in ""Animation"" property of the AnimatedSprite. Having different animations in AnimatedSprites. Therefore completely has no effect. I am now forced to use a separate ""AnimatedSprite"" for each kind of animation. However that leads to all animations being shown on screen at once. This cumbersome workflow can't be intended by design: Especially since both AnimatedSprites and AnimationPlayer already have separate tracks for different animation-types... built... in."
technical, Is this not a bug? I cannot believe this is how it was intended to work.
technical,"What you are asking is not something any framework/engine I've used before supports. How do you expect Godot to understand that the image that you told it had 67 rows and 8 columns now has 70 rows and 8 columns and it should magically (?) offset all the frames of the other animations? That image doesn't even know that it has animations linked to it. Making sense of where one animation starts and when it ends is your job or yours programmer job. Godot doesn't know that your first row is the ""walk"" animation and your second is the ""run"" animation that's something you tell it and you will need to update if you change anything that affects it.  I'm not asking for any of that. Please have a look at my proposal again.  The only thing that Godot knows and cares is that an image has x rows and y columns and uses that information to calculate where in that image the frame 10 is anything else is up to you.  Yes I know. If it remembers what the previous setting for Hframes and Vframes was and what the current new setting is, that's also all it needs to know to fix the issue and adjust keyframes with ""frame:"" values.  Also, transparent pixels are still uploaded to the GPU and taking space in the video memory. PNGs are compressed lossless image files but the data you upload to the GPU is uncompressed so that 112KB png file is ~2.2MB uncompressed if my math isn't wrong  Thank you for pointing this out. I was not sure about how the GPU handles the png compression. Still, even 22 MB uncompressed are nothing compared to an inability to iterate on animation, add frames or add animations in the production process. Texture Packer makes a lot of sense for Tilemaps in 2D HD games, not so much for animation in low res 2D pixelart games in Godot. At least I would not know how you could marry those two when it comes to frame-by-frame pixel animation. If you know, or have a tool, please let us know. It does make sense in 2D pixelart games. For example, I had to package all the sprites of this game I worked on (this isn't a Godot game) into big spritesheets because we were having issues running out of vram on mobile (although we didn't end releasing it on mobile) Like this: (the spritesheets are transparent but windows is being windows)  Those are ~14200 sprites (most of them is the japanese font ~10k ˜… )  The position of your sprites in the spritesheet doesn't matter because it's you (or your programmer) the one that will give meaning to it.   Yes I know. If it remembers what the previous setting for Hframes and Vframes was and what the current new setting is, that's also all it needs to know to fix the issue and adjust keyframes with ""frame:"" values.  The Sprite doesn't know that it has animations linked to it and the AnimationPlayer just has a track that points to a property of a node (in this case to the frame property of a Sprite) and doesn't know anything else about it (it doesn't even know that it's pointing to a specific Sprite just a NodePath)"
technical,"Is this not a bug? I cannot believe this is how it was intended to work. It's probably not a bug, unless I'm missing something.  There is a lot mentioned in the topic, so I wasn't too sure exactly where the focus was for it.  Regarding the steps to reproduce.  At least on my end HFrames to 3 corrects the animation.  **Edit** - Am I missing something? There is no way to know what's actually on the sprite sheet.  It just uses row and column math to slice it up into identical rectangles.  You'll always have to specify these numbers and conform a spritesheet to fit that alignment.  As far as the workflow things, if you want to keep your keyframes correct, as you were saying, additions will probably have to go on the end.  I'm not exactly sure what kind of algorithm you could use that will know where you want to offset the values of existing keyframes to match the insertions into the middle of an updated spritesheet.  There is still nothing that knows the content of your spritesheet or what you want at what location.  In my experience at least, when I mangle these things in a large project, its just best to write a toolscript to automate the adjustments and fixes of AnimationPlayer keyframes.  Otherwise if its small, doing it by hand may be fastest."
technical,"I also have this issue and think it is a bug! Let makes things clear. A bug is when something happens in a way it was not designed for. No one designed Godot to keep the indices if you change your texture size, it is written down nowhere in the documentation. So right now, everything works as expected (indeed from the developers point of view), even if the current design has limitations.  I do not mean to minimize the problem, the enhancement would be a significant improvement. But no, it is not a bug."
technical,"Oh and thanks for proposing a solution (From 6 by 2 to 2 by 8)! While this is certainly feasibly for small projects and small Spritesheets, I do not see how it would be feasible for regular sized professional productions meaning anything more than 20 animations per sheet. Essentially, you would have to do the same tedious restructuring with the only difference being you would have to do it outside of Godot. Plus, if you do this for a while you get really ""unreadable"" spritesheets. So for me, that's actually even a bit worse than reassigning keyframe values inside Godot. No problem, I'm just trying to at least understand what you have in mind.  This is a very nice graphic you've made.  And apologies, what you've outlined is all new to me.  This is my first time seeing anyone organize spritesheets that way, so maybe I'm completely in the dark about something.   There are basically two ways to organize frame by frame animation.  I did not see our way in there, so you may feel this is illegitimate.  That's the order we go with and it fits well with how Godot functions presently.  Yes, if we have to expand the frame count, well we have to just bite the bullet and re-keyframe some things.  It has been done a couple times and it only takes a few minutes for a dozen or so animations.  My impression of what you have explained above is that there appears to be a lot of wasted space in there.  This examples is a 15x11 and has 47 empty frames.  (28% empty?)  When I think about a 400x300ish frame size it would amount to 400x300x47 = 5,640,000 dead pixels.  Spritesheets already tend to be somewhat wasteful in favor of convenience, but just speaking for myself I wouldn't want to voluntarily take on so much more for this method.  If I were to try to put what you have into my own words, it resembles to me something like sub-spritesheets inside of spritesheets, or possibly a hybrid between a spritesheet and an atlas.  An atlas with the requirements that a texture be in a uniform size and then be organized in some adjacent manner.  Unfortunately, I really think I'd have to see this one implemented in the wild and in practice to understand and evaluate it."
technical,"hey, first of all, thank you sticking with me and trying to understand my arguments, I really appreciate it!  I have no experience using large spritesheets and frame-by-frame animation in other engines, so sadly I cannot give you an example how this would be dealt with elsewhere.  The idea behind it in the current form is that as long as you keep your frames in some kind of sequence going from left to right, top to bottom, it doesn't matter how you arrange the sheets. The frames should still read in the correct frame order.  Yes I know. That's precisely the root of the problem. I do not think I have a particular workflow I want accommodated, the issue applies to the workflow you explained in your last comment just the same as soon as you want iteration and more than a few animations in a single sprite sheet for drawcall and performance reasons.  Please bear with me while I try to explain: If you know of any other way or option to organize, edit, manage and maintain larger amount of animations in a spritesheet, please do share. Oh and thanks for proposing a solution (From 6 by 2 to 2 by 8)! While this is certainly feasibly for small projects and small Spritesheets, I do not see how it would be feasible for regular sized professional productions meaning anything more than 20 animations per sheet. Essentially, you would have to do the same tedious restructuring with the only difference being you would have to do it outside of Godot. Plus, if you do this for a while you get really ""unreadable"" spritesheets. So for me, that's actually even a bit worse than reassigning keyframe values inside Godot."
technical,"they made a pull request and already committed the fix. Above your comment you can see the purple graphic saying the pull request has been merged and added to Godot 3.2 Please reopen this issue (not sure if OP is still around), this does not fix this issue at all, animating the Sprite Frame property is still incredibly broken in 3.2 Alpha2 as described in the original post of this issue. I have a lot of animations and would really need to have this fixed.  **Minimal test project:** Run the project, click on the color buttons to see a spritesheet animation playing. Each animation plays a separate row of the ""test spritesheet.png"" spritesheet.  To test the issue, - drag the ""test spritesheet expanded.png"" into the Sprite texture, - set ""Hframes"" from 7 to 11 - run the project again to see the wrong cells being displayed in lower rows of the spritesheet (any but green, since green is the first row, the cell attribution for green obviously is unchanged.)"
technical,"Anyway, I might be able to solve the problem easily. I'll try something. please, respect the way we organize the work to be done  I certainly do. So far noone has explained why this would be intended behavior and I am completely unfamiliar with the usecase you mention here."
technical,"No problem, I'm just trying to at least understand what you have in mind.  This is a very nice graphic you've made.  And apologies, what you've outlined is all new to me.  This is my first time seeing anyone organize spritesheets that way, so maybe I'm completely in the dark about something.   There are basically two ways to organize frame by frame animation.  I did not see our way in there, so you may feel this is illegitimate.  That's the order we go with and it fits well with how Godot functions presently.  Yes, if we have to expand the frame count, well we have to just bite the bullet and re-keyframe some things.  It has been done a couple times and it only takes a few minutes for a dozen or so animations.  My impression of what you have explained above is that there appears to be a lot of wasted space in there.  This examples is a 15x11 and has 47 empty frames.  (28% empty?)  When I think about a 400x300ish frame size it would amount to 400x300x47 = 5,640,000 dead pixels.  Spritesheets already tend to be somewhat wasteful in favor of convenience, but just speaking for myself I wouldn't want to voluntarily take on so much more for this method.  If I were to try to put what you have into my own words, it resembles to me something like sub-spritesheets inside of spritesheets, or possibly a hybrid between a spritesheet and an atlas.  An atlas with the requirements that a texture be in a uniform size and then be organized in some adjacent manner.  Unfortunately, I really think I'd have to see this one implemented in the wild and in practice to understand and evaluate it. The only downside I see with dead pixels is a larger download. The upside is a MUCH better overview, which is absolute indispensable, when you are doing pixelart, get more animations and frames and lower resolution sprites. Currently I'm working on a 2D Pixelart project (something Godot praises itself for being a great fit) which means I have tons of frames that look almost indistinguishable next to each other. On top of that, naming each frame even each animation is impossible with frames just 32 pixel or 48 pixel wide which also have to accommodate the sprite. My current (very incomplete) player base sprite has 67 rows and 8 columns, so 535 frames. The png is 112KB in size, so I honestly don't care a bit about unused pixels on my spritesheet. Even if it would be ten times as large, the ability to find animations and frames I'm looking for outweighs the 0.01 seconds additional download time a billion to one. I know you are not doing pixelart in your game (which looks absolutely awesome btw! I've been following for a while) but in pixelart animation you select move and copy previously created frames and past them into new animations all the time. That's the core of any pixelart animation. Every single pixelpush has a huge impact to your animation. So the ability to quicky jump through your spritesheet, find what you are looking for copy and paste it to someplace else in your spritesheet or compare single pixel locations from various places in your sheet, is absolutely necessary. You also have to do this because your newly created frame must match previously created ones by the pixel. Think a change from one idle into another.  Yes, if we have to expand the frame count, well we have to just bite the bullet and re-keyframe some things. It has been done a couple times and it only takes a few minutes for a dozen or so animations.  This is what this issue is about. For you it might be not that much of an issue. I suppose this is  because  you are not making a pixel game and  because  you are making a sidescoller with two directions of movement, not 8. Hence your spritesheets are a lot more clearly laid out by default and while the re-keyframing or rearranging of your spritesheets might be an inconvenience, it is not so much an issue. For me however, and I dare say anyone who wants to do a slightly more professional looking pixel art game with decently complex animation, I dare say it not just an inconvenient issue, it's clearly a Starship Trooper sized brain bug in Godots design. **I therefore would really appreciate if someone would review the ""enhancement"" tag and maybe consider replacing it with a ""bug"" tag.**"
technical,"It's probably not a bug, unless I'm missing something.  There is a lot mentioned in the topic, so I wasn't too sure exactly where the focus was for it.  Regarding the steps to reproduce.  At least on my end HFrames to 3 corrects the animation.  **Edit** - Am I missing something? There is no way to know what's actually on the sprite sheet.  It just uses row and column math to slice it up into identical rectangles.  You'll always have to specify these numbers and conform a spritesheet to fit that alignment.  As far as the workflow things, if you want to keep your keyframes correct, as you were saying, additions will probably have to go on the end.  I'm not exactly sure what kind of algorithm you could use that will know where you want to offset the values of existing keyframes to match the insertions into the middle of an updated spritesheet.  There is still nothing that knows the content of your spritesheet or what you want at what location.  In my experience at least, when I mangle these things in a large project, its just best to write a toolscript to automate the adjustments and fixes of AnimationPlayer keyframes.  Otherwise if its small, doing it by hand may be fastest. There is no way to know what's actually on the sprite sheet.  Godot does not need to know what's on the sprite sheet. All it needs to know is that I when I add a column (increase of Hframes) it will change all already existing keyframes with the ""frame:"" value according to the algorithm proposed in the solution above.  I do not see how this is ** not ** a bug, to be honest: **If you already have keyframes set for your frame-by-frame animation, why would you want the  displayed  frames of already set keyframes change arbitrarily, only because you add a column or row to your spritesheet?**  It makes iteration impossible, because it forces you to either have a fixed number of animations or a fixed number of frames per animation when you start your project. The only case when this would be less of a problem/bug is when you are creating an exact clone or mini game and you how exactly how many animations or how many frames you will need. But are these the the only kind of games we want for this engine?"
technical,"Ummmm, why this is closed? Newb here. Is there a solution already? What is this ""frame coords"" thing and how does it work? Thank you devs for your hard work btw xD they made a pull request and already committed the fix. Above your comment you can see the purple graphic saying the pull request has been merged and added to Godot 3.2"
technical,"please, respect the way we organize the work to be done  I certainly do. So far noone has explained why this would be intended behavior and I am completely unfamiliar with the usecase you mention here. Ummmm, why this is closed? Newb here. Is there a solution already? What is this ""frame coords"" thing and how does it work? Thank you devs for your hard work btw xD"
technical,"I don't understand this issue. What you are asking is not something any framework/engine I've used before supports. How do you expect Godot to understand that the image that you told it had 67 rows and 8 columns now has 70 rows and 8 columns and it should magically (?) offset all the frames of the other animations? That image doesn't even know that it has animations linked to it. Making sense of where one animation starts and when it ends is your job or yours programmer job. Godot doesn't know that your first row is the ""walk"" animation and your second is the ""run"" animation that's something you tell it and you will need to update if you change anything that affects it. The only thing that Godot knows and cares is that an image has x rows and y columns and uses that information to calculate where in that image the frame 10 is anything else is up to you.  Also, transparent pixels are still uploaded to the GPU and taking space in the video memory. PNGs are compressed lossless image files but the data you upload to the GPU is uncompressed so that 112KB png file is ~2.2MB uncompressed if my math isn't wrong (which isn't much but you can see why it can add up to a lot and why there are tools like TexturePacker that can remove all the transparent pixels around the sprite and pack them tightly into a smaller image). It has nothing to do with download speeds. What you are asking is not something any framework/engine I've used before supports. How do you expect Godot to understand that the image that you told it had 67 rows and 8 columns now has 70 rows and 8 columns and it should magically (?) offset all the frames of the other animations? That image doesn't even know that it has animations linked to it. Making sense of where one animation starts and when it ends is your job or yours programmer job. Godot doesn't know that your first row is the ""walk"" animation and your second is the ""run"" animation that's something you tell it and you will need to update if you change anything that affects it.  I'm not asking for any of that. Please have a look at my proposal again.  The only thing that Godot knows and cares is that an image has x rows and y columns and uses that information to calculate where in that image the frame 10 is anything else is up to you.  Yes I know. If it remembers what the previous setting for Hframes and Vframes was and what the current new setting is, that's also all it needs to know to fix the issue and adjust keyframes with ""frame:"" values.  Also, transparent pixels are still uploaded to the GPU and taking space in the video memory. PNGs are compressed lossless image files but the data you upload to the GPU is uncompressed so that 112KB png file is ~2.2MB uncompressed if my math isn't wrong  Thank you for pointing this out. I was not sure about how the GPU handles the png compression. Still, even 22 MB uncompressed are nothing compared to an inability to iterate on animation, add frames or add animations in the production process. Texture Packer makes a lot of sense for Tilemaps in 2D HD games, not so much for animation in low res 2D pixelart games in Godot. At least I would not know how you could marry those two when it comes to frame-by-frame pixel animation. If you know, or have a tool, please let us know."
technical,"It does make sense in 2D pixelart games. For example, I had to package all the sprites of this game I worked on (this isn't a Godot game) into big spritesheets because we were having issues running out of vram on mobile (although we didn't end releasing it on mobile) Like this: (the spritesheets are transparent but windows is being windows)  Those are ~14200 sprites (most of them is the japanese font ~10k ˜… )  The position of your sprites in the spritesheet doesn't matter because it's you (or your programmer) the one that will give meaning to it.   Yes I know. If it remembers what the previous setting for Hframes and Vframes was and what the current new setting is, that's also all it needs to know to fix the issue and adjust keyframes with ""frame:"" values.  The Sprite doesn't know that it has animations linked to it and the AnimationPlayer just has a track that points to a property of a node (in this case to the frame property of a Sprite) and doesn't know anything else about it (it doesn't even know that it's pointing to a specific Sprite just a NodePath) Wow, congrats on working on such a beautiful game!  Of course .. TexturePacker  **can**  make a lot of sense even for pixel games, depending on your target platform ect ... If you want to release on C64, you most likely will have to dig much deeper than TexturePacker to make things work.  The Sprite doesn't know that it has animations linked to it and the AnimationPlayer just has a track that points to a property of a node (in this case to the frame property of a Sprite) and doesn't know anything else about it (it doesn't even know that it's pointing to a specific Sprite just a NodePath)  So maybe the Animation Player  should  be notified when there is a change made of a Hframes property down the Node Path? What's wrong with having the Sprite Node emitting a signal to do just that?  The position of your sprites in the spritesheet doesn't matter because it's you (or your programmer) the one that will give meaning to it.  It does matter a whole lot to whoever is in the process of creating the spritesheet and keyframes if they have an interest to stay mentally sane. Once they are all done you are totally right. You can scramble and optimize the sprite frames order and position to their most efficient arrangement. That is as long as your code is supporting the simultaneous and automatic re-setting of your ""frame:"" values in all your keyframes. But if we would have this functionality in Godot already, we would not need to have this discussion here in the first place."
technical,"Nobody's saying it shouldn't use the network.  It just shouldn't use the network *before the user has given it permission*, because otherwise it is a **data leak**.  That's *why the consent dialog exists*.  Very obviously this is a bug: the auto-update mechanism, which is itself a form of telemetry so long as it uses the network (transmitting to the manufacturer that the user has installed Atom), must be serialized to occur *only after* their consent (or lack thereof) to telemetry is indicated.  Doing it beforehand renders the telemetry opt-out entirely ineffective, as you *still end up sending telemetry* to the ""what version is current"" service, even when the user doesn't want it sent.  That seems to be a recurring issue, as well: #20185 elaborates on that. *The second dialog you show is from the telemetry package which clearly states that if you opt out it will send an anonymous report of an opt out and then ignore future data*  FYI that central.github.com connection happens *before* I click *anything* in the consent dialog (including the opt out button), so it's clearly sending something in advance of knowing whether it is permitted to or not."
technical,"Thanks to everyone for the feedback and for sharing your concerns.  We obviously disagree over some core definitions here. It appears that the core idea being asserted is that making any network connection is, in essence, sharing private information with whomever might have access to sniffing the network at any point between the source and the destination. Secondarily, that Atom has a responsibility to prevent any network access that the user doesn't explicitly grant consent to.  Whether we agree on the first point or not, the way that Atom is designed, it  cannot  prevent all network access in all circumstances. If you install a package that opens a network connection, there's nothing that the core Atom code can do currently to prevent that and that ability isn't something that the Atom maintainer team is going to spend time investigating or implementing. So, if you want an editor that will work completely offline with no network connections whatsoever, Atom is not going to be the appropriate editor for you.  We agree that the telemetry package shouldn't send information before a button is clicked, so we're definitely going to investigate premature connections to central.github.com before the user has explicitly clicked a button and we are tracking that in this.  We can appreciate that different people make different tradeoffs when it comes to network exposure versus functionality. We've made choices that we feel strike a good balance for the majority of users, so we'll be leaving the rest, specifically auto-update checking, the way it currently is designed. ### Description  Atom is contacting Microsoft/GitHub processes running on Amazon servers on first launch **without consent**, and leaking my IP address and timestamp to the manufacturer, as well as transmitting the fact that I use Atom (via outbound request) to thousands of other people and organizations.  ### Steps to Reproduce  1. Launch Atom for the first time  **Expected behavior:**  No telemetry is sent.  **Actual behavior:**  Telemetry is sent.  Atom transmits my IP address (and implicit timestamp) to the manufacturer and thousands of other people.  At no point am I prompted for consent prior to this happening.  This happens *silently*.  Only **after** this information has been sent out of my machine does the main application window open with the consent dialog (which has its own issues).  **Reproduces how often:**  100% of the time on first launch  ### Versions  ### Additional Information  A user's IP address, as well as the tracking/telemetry/analytics/autoupdate target host IP are both transmitted from the user's machine at time of first launch (adding a timestamp to these first two pieces of data).  This tuple of (user source IP, atom.io destination ip, TCP port, TLS SNI hostname, timestamp) **leaks usage information** to thousands of different people when it is sent from the user's computer: ISP, hosting providers, network interchanges, intelligence services (hi Ed!), Microsoft internal systems administrators, GitHub systems administrators, and Amazon network administrators.  The user is given **no opportunity** to opt out of this, to prevent it, and is **not even made aware of it happening**.  This means that the work on #12281 is *incomplete*.  The software is still **transmitting user data without consent** before the consent dialog even appears.  Wikipedia defines spyware as:   Spyware is a software that aims to gather information about a person or organization, sometimes without their knowledge, and send such information to another entity without the consumer's consent.  ## Required Elements  1. software :white check mark: atom is software 1. *gathers information about a person* :white check mark: information that the user is launching Atom for the first time 1. *without their knowledge* :white check mark: no information is displayed to the user when the request is made, or any time thereafter 1. *send information to another entity* :white check mark: sends data to ISP, routers, hosting companies and staff, GitHub, Microsoft, Amazon, and NSA 1. *without the user's consent* :white check mark: no consent was asked or provided, and indeed, did not exist  Presently, Atom does *all of these* on first launch."
technical,"If you bundled auto-update consent in with telemetry consent, that seems like an unfortunate decision for the user.  Auto-updates, while they are revealing, are different than pure telemetry by most definitions.  It's probably to be expected that a) most people turn off telemetry when given an option and b) most people want auto-updates turned on.  I'm not sure how it would A/B test, but my guess is that having auto-updates default to off would result in a lot less automatically updated Atom clients. As already stated, if that form of ""telemetry"" is an issue for you feel free to block the network access or create a version of Atom that doesn't check for updates. This isn't something the Atom team is currently interested in changing though.  Folks can reasonably disagree about the privacy impact of something like this, and/or the feasibility of personal workarounds. But I fail to see how anyone fairminded could accept that an identifying outgoing connection is sent before the user presented with an opt-in/out. That's a dark pattern. Any organization that practices it is staining its own credibility. It's just a falsehood, and a bad precedent on every level.   Atom is designed to run in an internet connected environment  Is this documented somewhere? And why is this a design constraint? Speaking generally, I would propose the opposite, that a text editor be designed to be fully capable in all respects when offline. When I look at your home page bullet points, none of them relate to having an internet connection. And when I look at the ""short version"" of your unequivocal privacy statement:   We only collect the information you choose to give us  it's pretty hard to square with the current behavior.  You're collecting something users haven't chosen to give you."
technical,"You'll know that the work is done on #12281 when the app can be first-time launched on a fresh system, opt-out of metrics/telemetry/reporting selected by the user, a file created, typed into, saved, and the application quit, without a single network request being made (meaning that the usage of Atom is not leaked to the LAN, the ISP, the intermediate networks, anyone monitoring those networks, or any staff member of Microsoft/GitHub (including network administrators or network security monitoring devices)). Atom is designed to run in an internet connected environment, doing things such as checking for updates (your first dialog) without prompting the user. The second dialog you show is from the telemetry package which clearly states that if you opt out it will send an anonymous report of an opt out and then ignore future data. You are certainly free to block the network access and Atom will work in an offline mode if that is your preference, if that is not what you desire though there are plenty of other editors out there that may fit your needs better. Atom is fully open source so you are welcome to build your own version that fits your requirements, note that the  branding  is copyrighted so if you do that and decide to publish it you can't call it Atom.  Thanks for reaching out!  We have passed along the information to the maintainers team. Because we treat our issues list as the Atom team's backlog, we close feedback issues after passing along the information to the maintainers to keep our backlog clean and focused. In the future, if you want to send feedback to the maintainers you can do so more directly by sending email to atomgithub.com."
technical,"As already stated, if that form of ""telemetry"" is an issue for you feel free to block the network access or create a version of Atom that doesn't check for updates. This isn't something the Atom team is currently interested in changing though.  Folks can reasonably disagree about the privacy impact of something like this, and/or the feasibility of personal workarounds. But I fail to see how anyone fairminded could accept that an identifying outgoing connection is sent before the user presented with an opt-in/out. That's a dark pattern. Any organization that practices it is staining its own credibility. It's just a falsehood, and a bad precedent on every level.   Atom is designed to run in an internet connected environment  Is this documented somewhere? And why is this a design constraint? Speaking generally, I would propose the opposite, that a text editor be designed to be fully capable in all respects when offline. When I look at your home page bullet points, none of them relate to having an internet connection. And when I look at the ""short version"" of your unequivocal privacy statement:   We only collect the information you choose to give us  it's pretty hard to square with the current behavior.  You're collecting something users haven't chosen to give you. Auto-updates, while they are revealing, are different than pure telemetry by most definitions.  It doesn't matter how we define them, *practically*, immediate auto update checks on launch *amount to telemetry*.  It's a rose by any other name.  It sends out a launch event across the network, regardless of what the person who implemented the feature intended it to accomplish.  It must be treated as such, regardless of intent, because that is the *effect*.   I'm not sure how it would A/B test, but my guess is that having auto-updates default to off would result in a lot less automatically updated Atom clients.  Is this a worse thing than every single Atom user reporting to their ISP and national governments and Microsoft for permanent logging exactly when and where they open their text editor every single time?"
technical,"If you're going to insist on the autoupdate thing, couldn't you just do it once per month, at a random number of cumulative hours delay (say, 1-12) after startup, so that it doesn't function as an ad hoc ""user has launched the app"" telemetry event to everyone watching and logging the network?  It would still work just fine, and wouldn't be transmitting user activity on every launch. If you bundled auto-update consent in with telemetry consent, that seems like an unfortunate decision for the user.  Auto-updates, while they are revealing, are different than pure telemetry by most definitions.  It's probably to be expected that a) most people turn off telemetry when given an option and b) most people want auto-updates turned on.  I'm not sure how it would A/B test, but my guess is that having auto-updates default to off would result in a lot less automatically updated Atom clients."
technical,"This whole issue is extremely simple: Atom should not send data out to the world without asking the user. Period.  Asking the user to block network access is tangential and does not inspire confidence in the Atom. Internet ""connected"" app means that the App can connect to the internet if needed and invoked by the user. Not willy-nilly send out telemetry data to the world.  Atom should also ask the user if it can check for updates. It's an expected scenario to check for updates, it's not a constraint though.  **It is not an expected scenario from the user's perspective.** With great respect, you're just wildly mistaken. Consider the following flow: User looks at your privacy policy. User reads the first sentence and thinks that consent will be requested for outbound connections. User obtains an installer. User launches app and app immediately phones home without consent.  I think we all understand that there are strong Business Intelligence incentives to want to collect  installation data. Those incentives only get answered by pushback, in micro-processes like this thread. Would it persuade the BI audience if they sat back and contemplated a legal review of this under the CCPA? Just a reminder that enforcement starts July 1, 2020.   You can even turn off checking for updates in the settings if you want, although that wouldn't prevent the first run connection this issue is about.  You've simply restated the problem."
technical,"I don't disagree with you, just trying to play devil's advocate a bit. There's clearly a reason Atom's team believes auto-updates on by default is of high importance. Imploring maintainers of an open source software is not going to yield the best results, I think. Nor is telling users to fork your app if they don't like it.  I do think that while delivered in a dictatorial manner, neilpanchal's comment is worth discussing more. It's not dictatorial, it's principled.  Using a user's computer to do things against them, silently, without their consent, is unethical.  He's speaking plainly about the ethics of the matter, which comes across as pretty harsh, but ultimately it's no more harsh than asserting 1 as true and zero as false.  It's the simple result of the principle ""the computer belongs to the user and should respect the user's wishes and not silently obey those of a remote party instead""."
technical,"It's not dictatorial, it's principled.  Using a user's computer to do things against them, silently, without their consent, is unethical.  He's speaking plainly about the ethics of the matter, which comes across as pretty harsh, but ultimately it's no more harsh than asserting 1 as true and zero as false.  It's the simple result of the principle ""the computer belongs to the user and should respect the user's wishes and not silently obey those of a remote party instead"". Thanks to everyone for the feedback and for sharing your concerns.  We obviously disagree over some core definitions here. It appears that the core idea being asserted is that making any network connection is, in essence, sharing private information with whomever might have access to sniffing the network at any point between the source and the destination. Secondarily, that Atom has a responsibility to prevent any network access that the user doesn't explicitly grant consent to.  Whether we agree on the first point or not, the way that Atom is designed, it  cannot  prevent all network access in all circumstances. If you install a package that opens a network connection, there's nothing that the core Atom code can do currently to prevent that and that ability isn't something that the Atom maintainer team is going to spend time investigating or implementing. So, if you want an editor that will work completely offline with no network connections whatsoever, Atom is not going to be the appropriate editor for you.  We agree that the telemetry package shouldn't send information before a button is clicked, so we're definitely going to investigate premature connections to central.github.com before the user has explicitly clicked a button and we are tracking that in this.  We can appreciate that different people make different tradeoffs when it comes to network exposure versus functionality. We've made choices that we feel strike a good balance for the majority of users, so we'll be leaving the rest, specifically auto-update checking, the way it currently is designed."
technical,"Atom is designed to run in an internet connected environment   Is this documented somewhere? And why is this a design constraint? Speaking generally, I would propose the opposite, that a text editor be designed to be fully capable in all respects when offline.  It's an expected scenario to check for updates, it's not a  constraint  though. As I have stated multiple times you can definitely work with Atom while offline (or with network requests blocked). You can even turn off checking for updates in the settings if you want, although that wouldn't prevent the first run connection this issue is about. This whole issue is extremely simple: Atom should not send data out to the world without asking the user. Period.  Asking the user to block network access is tangential and does not inspire confidence in the Atom. Internet ""connected"" app means that the App can connect to the internet if needed and invoked by the user. Not willy-nilly send out telemetry data to the world.  Atom should also ask the user if it can check for updates."
technical," You'll know that the work is done on #12281 when the app can be first-time launched on a fresh system, opt-out of metrics/telemetry/reporting selected by the user, a file created, typed into, saved, and the application quit, without a single network request being made (meaning that the usage of Atom is not leaked to the LAN, the ISP, the intermediate networks, anyone monitoring those networks, or any staff member of Microsoft/GitHub (including network administrators or network security monitoring devices))."
technical," /dev/sdb is not your C drive, it's a VHD that contains your root filesystem."
technical,"Thought of a clearer way to put this. Your ext4 filesystem is 250GiB. It doesn't resize every time you consume a few more bytes in Windows. damn, okay, it seems there is an opportunity to reduce the size of this disk, even so. ty"
technical,"Mystic8b - I don't see MB anywhere in the screenshot you posted, I might be looking at it wrong but I see. Exactly. Where does 251gb come from? About megabytes, I never said a word"
technical,"This isn't constructive. I ACKNOWLEDGE THE FOLLOWING BEFORE PROCEEDING: 1. If I delete this entire template and go my own path, the core team may close my issue without further explanation or engagement. 2. If I list multiple bugs/concerns in this one issue, the core team may close my issue without further explanation or engagement. 3. If I write an issue that has many duplicates, the core team may close my issue without further explanation or engagement (and without necessarily spending time to find the exact duplicate ID number). 4. If I leave the title incomplete when filing the issue, the core team may close my issue without further explanation or engagement. 5. If I file something completely blank in the body, the core team may close my issue without further explanation or engagement.  All good? Then proceed! This bug tracker is monitored by Windows Subsystem for Linux development team and other technical folks.  Important: When reporting BSODs or security issues, DO NOT attach memory dumps, logs, or traces to Github issues. Instead, send dumps/traces to securemicrosoft.com, referencing this GitHub issue. Ideally, please configure your machine to capture minidumps, repro the issue, and send the minidump, You can find instructions to do that here:  If this is a console issue (a problem with layout, rendering, colors, etc.), please post the issue to the Terminal tracker For documentation improvements, please post to the documentation tracker For any other questions on contributing please see our contribution guidelines Please fill out the items below.  # Steps to reproduce What you're doing and what's happening. Copy&paste the full set of specific command-line steps necessary to reproduce the behavior, and their output. Include screenshots if that helps demonstrate the problem. -- If you'd like to provide logs you can provide an strace(1)  log of the failing command (if some command is failing, then run strace -o some command.strace -f some command some args, and link the contents of some command.strace in a gist. More info on strace can be found hereYou can use Github gists to share the output  Additionally, For WSL launch issues, please collect detailed logs, instructions here. #  Expected behavior Free space on wsl corresponds to reality  A description of what you're expecting, possibly containing screenshots or reference material.   # Actual behavior  Free / occupied space does not correspond to reality  What's actually happening? --"
technical,"Very normal. You can have a dozen 250 GiB virtual disks (total of 3TiB of space that doesn't really exist) one 128 GiB SSD. [Which is why this is tag by-design already.] is right, the disk is dynamic and that is the maximum size."
technical,"Where is 250mb here? There 251GB I understand that this is a virtual device, but it must have a certain size occupied on the disk, as on the disks of virtual machines in a workstation, etc. Mystic8b - I don't see MB anywhere in the screenshot you posted, I might be looking at it wrong but I see."
technical,"Yes, but there should be as much space on it as there is on the current drive, isn't it? No, because it isn't your drive. There is 250GiB on ""it"" (by default). Where the ""it"" here isn't a real drive. It is a virtual block device. The size of the  virtual device  is 250GiB whether the  backing storage  for the device lives on a 128 gigabyte ssd or a 128 terrabyte raid array. The size isn't real because the device isn't real."
technical,"Type-o obv. 250 GiB (shows as 251GiB). That number is arbitrary ref #4373. Okay, edited his post, first he wrote 250mib So you want to say that a 250GB virtual disk with a real free space of 111GB is normal?"
technical,"damn, okay, it seems there is an opportunity to reduce the size of this disk, even so. ty This isn't constructive."
technical, Thought of a clearer way to put this. Your ext4 filesystem is 250GiB. It doesn't resize every time you consume a few more bytes in Windows.
technical,"Exactly. Where does 251gb come from? About megabytes, I never said a word Type-o obv. 250 GiB (shows as 251GiB). That number is arbitrary ref #4373."
technical,"Okay, edited his post, first he wrote 250mib So you want to say that a 250GB virtual disk with a real free space of 111GB is normal? Very normal. You can have a dozen 250 GiB virtual disks (total of 3TiB of space that doesn't really exist) one 128 GiB SSD. [Which is why this is tag by-design already.]"
technical,"No, because it isn't your drive. There is 250GiB on ""it"" (by default). Where the ""it"" here isn't a real drive. It is a virtual block device. The size of the  virtual device  is 250GiB whether the  backing storage  for the device lives on a 128 gigabyte ssd or a 128 terrabyte raid array. The size isn't real because the device isn't real. Where is 250mb here? There 251GB I understand that this is a virtual device, but it must have a certain size occupied on the disk, as on the disks of virtual machines in a workstation, etc."
technical,"/dev/sdb is not your C drive, it's a VHD that contains your root filesystem. Yes, but there should be as much space on it as there is on the current drive, isn't it?"
technical,"We've discussed this issue quite a bit. We the Angular team, strongly believe in building an inclusive community where everyone feels welcome, and we see how especially ng-repeat could be problematic in certain scenarios. It was never our intention. ""ng"" simply stands for a*NG*ular - the two characters in the middle of the name. And the name Angular has its roots in angle brackets that are the key part of HTML syntax that Angular uses. After reviewing the current names of APIs that are prefixed with ng we concluded that ng-repeat was the only one that stood out as potentially problematic. Fortunately, the latest versions of Angular use ngFor instead of  ng-repeat. ng-repeat is no longer used by Angular and was deprecated along with AngularJS a few years ago, so this api is no longer used in new development. For these reasons, we will not make any changes in our current APIs, but we will be very mindful of this problem when creating future APIs. Thank you for bringing it to our attention. #  feature request ### Description I led a workshop on web development for the youth. A girl asked me a question what a certain line on an angular template meant. She was unusually shy and had trouble pointing at the line. Later she said in private that she didn't know how to read ng-repeat out loud. Spelling it reminded her of an ugly racial slur. I understand that it is just an unfortunate abbreviation, but it limits the pool of the talented folks who could learn angular. ### Describe the solution you'd like Replacing the ng with ang or something similar would be awesome. It will make the project more inclusive. It also matches the library name better, so it's easier to remember. There is evidence that it is a confusing name.  ### Describe alternatives you've considered This is a big change. We could start it by improving documentation and accepting two prefixes by default."
technical,"By this logic ang- is a terrible choice as it could stand for anything from negative words like ang-ry to religious words like ang-el. Trying to find two or three letters that are universally innocent in every language in a way that you cannot somehow turn them into something negative is probably an impossible task. did you mean ""vast minority""? As mentioned, ng-repeat is from AngularJS, not Angular (this repo) where *ngFor is used. Also please note that AngularJS has reached LTS and is no longer being modified other than for security issues or major breaking changes to browser or jQuery support."
technical,check pull request ## Events and Control Color Swap  ### Blocks (Vertical & Horizontal) ### Blocks in Context empty editor  ### Color Palette
technical,## Events and Control Color Swap  ### Blocks (Vertical & Horizontal) ### Blocks in Context empty editor  ### Color Palette After further discussion we have decided to keep these color relationships as-is in an effort to unify colors / grammars across ScratchJr and Scratch.
technical, check pull request
technical,"After further discussion we have decided to keep these color relationships as-is in an effort to unify colors / grammars across ScratchJr and Scratch. In Scratch 2.0 the colors for ""control"" and ""events"" are an orange-ish brown and an orange-ish yellow respectively. As part of our initial color explorations for Scratch 3.0 we strived to normalize the block category palettes between Scratch 2.0 and ScratchJr as well as resolve design issues with the ""muddy"" brown color that was being used for the ""events"" category. The result of this was to shift the ""events"" category to yellow and keep the ""control"" category with a similar orange to what it has today.  An unfortunately side-effect of this change that we have observed is that users who are familiar with the Scratch 2.0 category colors sometimes mistakenly select the ""control"" category when they intend to select the ""events"" category. We have observed that this issue goes away after continuous use of Scratch 3.0 (it appears to be temporary / transitional), but nonetheless we should discuss if corrective action should be taken.  ### Scratch 2.0  ### ScratchJr ### Scratch 3.0 (Vertical Grammar) ### Scratch 3.0 (Horizontal Grammar)"
technical,"I am officially drawing a line under this request. ERCC will never be merged upstream while I remain involved with OpenRA. At this point it has nothing to do with any of the in-game aspects, it is entirely about the behaviour of the people championing it.  While most of the competitive RA community are good people, there is a rotten core of toxic entitlement that manifests as abuse and belittlement, often focused around ERCC, and usually focused personally against me. This behaviour has forever soured ERCC, and integrating it into the upstream RA mod now would validate that behaviour as an effective strategy for influencing OpenRA's development.  Why now? Two recent incidents targeting me (from N/A in the community discord, and Longely/Widow in the competitive discord - the two main personalities behind ERCC) were incredibly offensive and belittling, and are the straw that has broken the camels back when added on top of many other generally toxic events this year. People in the competitive community were having fun meming about ""us vs the devs"" earlier this year, but at that time I was seriously considering quitting OpenRA completely due to how unhappy and embarrased the behaviour of some people in the community (and the general tolerance of that behaviour as acceptable by everybody else) was making me. Ultimately, I decided that I enjoy working on the project, and making things better for everybody else in the community. I *do not* enjoy interacting with the assholes, and intend to be increasingly blunt in dismissing their toxic behaviour. This is the first concrete action towards that.  I can certainly appreciate the desire for solving directional map imbalance, to the point where I fully implemented the code for one solution (rotatable structures) and developed a working prototype for a second (allowing harvesters to dock from any direction, visually jumping docking point). I think there is scope for some ERCC alternative, but I suggest that the competitive community as a whole put some effort into solving its attitude problem before they try to revisit such ideas here. ## Issue Summary As it has become the standard in RAGL and proven its value, it might be a good idea to replace it everywhere for better balancing.  What is ERCC? ERCC provides more exits for harvesters (north exit) which makes harvesting from all sides more balanced. Furthermore, the outline of the refinery got changed to square to alleviate map-inconveniences.  TODO: More detailed guide by widow about ERCC."
technical,"I understand that but it's missleading. Fact is that majority of above avarage skilled players prefer playing with ERCC refineries.  OpenRA supposted to improve gameplay of the originals, so players that who actually understand openra's gameplay are forced to play on a mod map doesn't give it a good look  I'm not saying that ERCC has to be implemented in its current form, after all it was merelly an experiment to see if refineries could be fixed. I'm just saying that it's a very relevant issue that needs a solution Additionally the person responsible for ERCC is not interested in refining it further, so it falls on the shoulders of someone else"
technical,It this has a footprint while the current one has this. it looks like this while the OG looks like this. ERCC currently suffers from #18232
technical," I am officially drawing a line under this request. ERCC will never be merged upstream while I remain involved with OpenRA. At this point it has nothing to do with any of the in-game aspects, it is entirely about the behaviour of the people championing it.  While most of the competitive RA community are good people, there is a rotten core of toxic entitlement that manifests as abuse and belittlement, often focused around ERCC, and usually focused personally against me. This behaviour has forever soured ERCC, and integrating it into the upstream RA mod now would validate that behaviour as an effective strategy for influencing OpenRA's development.  Why now? Two recent incidents targeting me (from N/A in the community discord, and Longely/Widow in the competitive discord - the two main personalities behind ERCC) were incredibly offensive and belittling, and are the straw that has broken the camels back when added on top of many other generally toxic events this year. People in the competitive community were having fun meming about ""us vs the devs"" earlier this year, but at that time I was seriously considering quitting OpenRA completely due to how unhappy and embarrased the behaviour of some people in the community (and the general tolerance of that behaviour as acceptable by everybody else) was making me. Ultimately, I decided that I enjoy working on the project, and making things better for everybody else in the community. I *do not* enjoy interacting with the assholes, and intend to be increasingly blunt in dismissing their toxic behaviour. This is the first concrete action towards that.  I can certainly appreciate the desire for solving directional map imbalance, to the point where I fully implemented the code for one solution (rotatable structures) and developed a working prototype for a second (allowing harvesters to dock from any direction, visually jumping docking point). I think there is scope for some ERCC alternative, but I suggest that the competitive community as a whole put some effort into solving its attitude problem before they try to revisit such ideas here."
technical,"One thing that I find the most offensive about this extended trainwreck of a discussion is that it basically boils down to a few competitive players, who already have access to ERCC via map-mods, insisting that everybody else is playing the game wrong and should have the option to use the classic refineries taken away from them. This is not cool, IMO. I can get behind what pchote is saying. I think this already has a nice place in map-mods. And if ruleset-only mods are ever implemented then the ERCC (and other balance mods and tests) could be easily applied to any map.  And even if the ERCC gets accepted to the core game then it should be optional and not the default."
technical,"This could also be solved using PlaceBuildingVariants and inverted artwork. I tested it and that problem doe snot (really) exist with ERCC Punsho . The only thing I observed is that you have to click a bit ""below"" the husk which is probably due to the logical layout/overlay structure. But you can certainly recover husks from the ERCC refinery.  There's more than one place where a harvester can die, i had a game where 2 harvesters were dead on a refinery. I was able to revive one by the method you just mentioned, but the other was completelly unclickable. This is how this bug was discovered"
technical," I understand that but it's missleading. Fact is that majority of above avarage skilled players prefer playing with ERCC refineries.  OpenRA supposted to improve gameplay of the originals, so players that who actually understand openra's gameplay are forced to play on a mod map doesn't give it a good look  I'm not saying that ERCC has to be implemented in its current form, after all it was merelly an experiment to see if refineries could be fixed. I'm just saying that it's a very relevant issue that needs a solution"
technical, It this has a footprint while the current one has this. it looks like this while the OG looks like this.
technical,"Please also include screenshots that show how the harvester clips through solid walls and the roof for a fair comparison. It works most of the time, but if you do these exact paths it clips. One of the problems is that the harvester art is bigger than once cell. that also causes problems with the current ref but these don't exist with ERCC"
technical,"It works most of the time, but if you do these exact paths it clips. One of the problems is that the harvester art is bigger than once cell. that also causes problems with the current ref but these don't exist with ERCC it would be much better if it used a new design that was not a space-warping bodge of the original refinery  get rid of the overhang completely  it is never going to be accepted as a replacement for the original refinery, but if you want to be added as a distinct thing that could be optionally enabled then it needs to make sense as its own thing"
technical,"it would be much better if it used a new design that was not a space-warping bodge of the original refinery  get rid of the overhang completely  it is never going to be accepted as a replacement for the original refinery, but if you want to be added as a distinct thing that could be optionally enabled then it needs to make sense as its own thing One thing that I find the most offensive about this extended trainwreck of a discussion is that it basically boils down to a few competitive players, who already have access to ERCC via map-mods, insisting that everybody else is playing the game wrong and should have the option to use the classic refineries taken away from them. This is not cool, IMO."
technical,ERCC currently suffers from #18232 Please also include screenshots that show how the harvester clips through solid walls and the roof for a fair comparison.
technical,I can get behind what pchote is saying. I think this already has a nice place in map-mods. And if ruleset-only mods are ever implemented then the ERCC (and other balance mods and tests) could be easily applied to any map.  And even if the ERCC gets accepted to the core game then it should be optional and not the default. The mutator thing should be mentioned here if it is raised as an issue somewhere.
technical,"Additionally the person responsible for ERCC is not interested in refining it further, so it falls on the shoulders of someone else This could also be solved using PlaceBuildingVariants and inverted artwork."
technical,"I tested it and that problem doe snot (really) exist with ERCC Punsho . The only thing I observed is that you have to click a bit ""below"" the husk which is probably due to the logical layout/overlay structure. But you can certainly recover husks from the ERCC refinery.  There's more than one place where a harvester can die, i had a game where 2 harvesters were dead on a refinery. I was able to revive one by the method you just mentioned, but the other was completelly unclickable. This is how this bug was discovered This could be in the form of an tickbox at lobby, probably it would be default off."
technical,"I'm also going to close since as it stands now this is still a no-op on our end. ## Required Information. Entering this information will route you directly to the right team and expedite traction.  **Question, Bug, or Feature?** *Type*: Bug  **Enter Task Name**: ## Environment - Server - Azure Pipelines or TFS on-premises? Azure Pipelines  - If using Azure Pipelines, provide the account name, team project name, build definition name/build number: these are private data in private builds not to be tracked publicly.  - Agent - Hosted or Private:  Hosted  - If using Hosted agent, provide agent queue name: Hosted Ubuntu 1604  ## Issue Description  ### Test 1 Using the following to set prepend the path results in an extra quote in the PATH  output1: output2  PATH: Expected result:  PATH: This seems to be causing some paths on added not be used in the search"
technical,"Updated the wording in my comment above for clarity   To be clear, I think the behavior is undesirable for this use case, but in general it  is  the behavior we want. The contract we have with the end user is essentially: any time you write something to stdout that matches our ##vso commands, we'll process it. That includes when tools write something to stdout, or a command writes something to stdout. We have no way of differentiating between you trying to write something and the script accidentally writing something.  IMO the only potentially questionable behavior here is that we don't require the ##vso command to be at the start of a line. I see pros and cons on that one, but that's somewhat moot at this point because we wouldn't be able to change that without risking breaking a significant portion of customers. Even with that, its not a bug as much as a design question that you  could  disagree with.  Do you think that's fair? Bash is setting the   variable to the first arg of the previous command that was executed (see here for more info on this behavior). When you call set, that is getting written out to stdout. Since the previous command was a ## command, it writes out. Sorry, I do not think that's fair.  If as you say setting   to an arbitrary value prior to invoking ##vso is a circumvention, it is very clear to me that ##vso is making an unjustified assumption about its execution environment, and that needs to be fixed, even if it is not an easy fix. Is that not clear to you?  If I invoke ##vso in a script, it behaves differently (undesirably) depending on the immediately previous command, and this is a design feature?  In other words, your demonstrated circumvention adding  =blah before calling ##vso begs the question, well azure, maybe instead of asking customers to add this line to their scripts maybe ##vso should always do something similar under the covers?  Your proposed feature enhancement leaves existing customers with non-read-only variables vulnerable to this undesirable behavior, bottom line ##vso setvariable without the readOnly=true flag are so undesirable as to be useless.  Further, it seems to me, that were the proposed feature enhancement implemented, and customers embraced the readOnly flag, that azure would occasionally issue warning messages ""attempt to update read only variable"" instead of occasionally tacking on a single quote, I guess MS could ask customers to add  =blah before calling ##vso to suppress the warning message, but that hardly seems like progress to me.  Please add ##vso setvariable to the issue description so that other concerned parties, customer and MS, can more accurately recognize the scope of this issue and get more eyes on the proposed feature enhancement. Thank you."
technical," Hey - this looks like it might be the same issue as this  Could you try running set +x at the start of your script? If that doesn't work, could you try queuing a build with system diagnostics turned on and sharing the output?"
technical,"Yesterday i lost a day cleaning up a spurious trailing single quote. Today I put together a minor variation on your ""Set variables in scripts"" YAML example on your ""Variables"" concept page. I hope this suggests some simple test cases. Hey per my comment above, I think this is all probably related to #10331 - we're working on a better long term fix, but in the short term adding set +x should fix the problem."
technical,"Bash is setting the   variable to the first arg of the previous command that was executed (see here for more info on this behavior). When you call set, that is getting written out to stdout. Since the previous command was a ## command, it writes out. Sorry, I do not think that's fair.  If as you say setting   to an arbitrary value prior to invoking ##vso is a circumvention, it is very clear to me that ##vso is making an unjustified assumption about its execution environment, and that needs to be fixed, even if it is not an easy fix. Is that not clear to you?  If I invoke ##vso in a script, it behaves differently (undesirably) depending on the immediately previous command, and this is a design feature?  In other words, your demonstrated circumvention adding  =blah before calling ##vso begs the question, well azure, maybe instead of asking customers to add this line to their scripts maybe ##vso should always do something similar under the covers?  Your proposed feature enhancement leaves existing customers with non-read-only variables vulnerable to this undesirable behavior, bottom line ##vso setvariable without the readOnly=true flag are so undesirable as to be useless.  Further, it seems to me, that were the proposed feature enhancement implemented, and customers embraced the readOnly flag, that azure would occasionally issue warning messages ""attempt to update read only variable"" instead of occasionally tacking on a single quote, I guess MS could ask customers to add  =blah before calling ##vso to suppress the warning message, but that hardly seems like progress to me.  Please add ##vso setvariable to the issue description so that other concerned parties, customer and MS, can more accurately recognize the scope of this issue and get more eyes on the proposed feature enhancement. Thank you. I think I maybe haven't made the problem here clear, there seems to be a little bit of misunderstanding.  If as you say setting   to an arbitrary value prior to invoking ##vso is a circumvention, it is very clear to me that ##vso is making an unjustified assumption about its execution environment, and that needs to be fixed, even if it is not an easy fix. Is that not clear to you?  If I invoke ##vso in a script, it behaves differently (undesirably) depending on the immediately previous command, and this is a design feature?  The issue is not that ##vso commands are dependent on the previous line - its actually that in Bash, set gives you different output dependent on the previous line (since it is very intentionally dependent on the environment). Since you have an environment variable   now set to, set prints that line which invokes the ## command a second time. So what is happening here is:  The ## commands get masked from output which makes this a little challenging to see, but if you turn on diagnostics you can see that's what is happening. That's also why your case doesn't repro if you remove the set command.   In other words, your demonstrated circumvention adding  =blah before calling ##vso begs the question, well azure, maybe instead of asking customers to add this line to their scripts maybe ##vso should always do something similar under the covers?  This is mostly addressed by what I said previously in this comment, but to be clear, you'd actually want the  =blah before set since that's the problematic command. So it would look like: You wouldn't need this if you were running a different command which doesn't print the environment.  I would also kindly ask that we keep the conversation friendly and respectful. Please refer to our code of conduct:"
technical,Our syntax is documented here -  If you think we need further documentation please raise an issue in the docs repo - this link should prepoluate it with the info needed to get it triaged/responded to fastest.  I'm going to lock this thread since I think it has ceased being productive and respectful. I'm also going to close since as it stands now this is still a no-op on our end.
technical,"May I respectfully advocate for intuitive behavior in our software, and may I respectfully suggest that if i need to turn on diagnostic tracing to understand how a bug is feature, perhaps it is a bug.  May i respectfully remind you and readers that the MS published idiom for ##vso usage is embedded in echo commands. Respectfully I do not agree that a fix is impossible. I think ##vso needs to be smarter.  I think this issue and the proposed solution needs more eyes. I'm going to ask again that we keep the conversation respectful going forward and work towards resolving the problem - otherwise I'll lock the thread to let the conversation cool off.  As far as the issue itself goes, I'd again reiterate that I think its working as intended, but we do have the read only feature coming to address use cases like yours. If you disagree, I'd appreciate clarification on how you believe the feature is working right now and how you believe it should work.  FWIW, the relevant design is as follows:  Any time a ##vso command appears in stdout it is processed - regardless of where it appears in the output. If you set a variable twice, the second time you set it wins out. ##vso commands are wiped from the output (though you can see that they were successfully processed in diagnostic mode)."
technical,"I think I maybe haven't made the problem here clear, there seems to be a little bit of misunderstanding.  If as you say setting   to an arbitrary value prior to invoking ##vso is a circumvention, it is very clear to me that ##vso is making an unjustified assumption about its execution environment, and that needs to be fixed, even if it is not an easy fix. Is that not clear to you?  If I invoke ##vso in a script, it behaves differently (undesirably) depending on the immediately previous command, and this is a design feature?  The issue is not that ##vso commands are dependent on the previous line - its actually that in Bash, set gives you different output dependent on the previous line (since it is very intentionally dependent on the environment). Since you have an environment variable   now set to, set prints that line which invokes the ## command a second time. So what is happening here is:  The ## commands get masked from output which makes this a little challenging to see, but if you turn on diagnostics you can see that's what is happening. That's also why your case doesn't repro if you remove the set command.   In other words, your demonstrated circumvention adding  =blah before calling ##vso begs the question, well azure, maybe instead of asking customers to add this line to their scripts maybe ##vso should always do something similar under the covers?  This is mostly addressed by what I said previously in this comment, but to be clear, you'd actually want the  =blah before set since that's the problematic command. So it would look like: You wouldn't need this if you were running a different command which doesn't print the environment.  I would also kindly ask that we keep the conversation friendly and respectful. Please refer to our code of conduct: May I respectfully advocate for intuitive behavior in our software, and may I respectfully suggest that if i need to turn on diagnostic tracing to understand how a bug is feature, perhaps it is a bug.  May i respectfully remind you and readers that the MS published idiom for ##vso usage is embedded in echo commands. Respectfully I do not agree that a fix is impossible. I think ##vso needs to be smarter.  I think this issue and the proposed solution needs more eyes."
technical,"Hey per my comment above, I think this is all probably related to #10331 - we're working on a better long term fix, but in the short term adding set +x should fix the problem. No, adding set +x does not fix the problem, I'll test it if you won't  Also, I note the potential duplicate you reference  #10331 is currently closed as no fault found which does not suggest a fix is in progress."
technical,"Please add ##vso[task.setvariable] to the description.Thanks. No, adding set +x does not fix the problem, I'll test it if you won't:  Oh, I see - this is the same type of root cause, but actually slightly different - sorry about that. Its actually the set command that is causing problems here which is why I didn't initially capture the issue.  Bash is setting the   variable to the first arg of the previous command that was executed (see here for more info on this behavior). When you call set, that is getting written out to stdout. Since the previous command was a ## command, it writes out see here for a repro with diagnostics turned on that shows this behavior.  So this is actually the correct behavior here - I'd recommend wiping   before running set, which I've confirmed works here   Also, I note the potential duplicate you reference #10331 is currently closed as no fault found which does not suggest a fix is in progress.  Sure, probably should have been a little clearer here - right now, the system is working as designed. However, as you've seen here, this behavior is probably not desirable for all use cases. Since its not a bug, we're not going to change behavior here, but we are looking at a feature to workaround this by making some variables read only (so they can't get overwritten). You can see the spec for that feature here"
technical,"Dear friend, may I respectfully ask where is this documented? My sincere apologies if I missed something. Our syntax is documented here -  If you think we need further documentation please raise an issue in the docs repo - this link should prepoluate it with the info needed to get it triaged/responded to fastest.  I'm going to lock this thread since I think it has ceased being productive and respectful."
technical,"No, adding set +x does not fix the problem, I'll test it if you won't  Also, I note the potential duplicate you reference  #10331 is currently closed as no fault found which does not suggest a fix is in progress. Please add ##vso[task.setvariable] to the description.Thanks."
technical,"No, adding set +x does not fix the problem, I'll test it if you won't:  Oh, I see - this is the same type of root cause, but actually slightly different - sorry about that. Its actually the set command that is causing problems here which is why I didn't initially capture the issue.  Bash is setting the   variable to the first arg of the previous command that was executed (see here for more info on this behavior). When you call set, that is getting written out to stdout. Since the previous command was a ## command, it writes out see here for a repro with diagnostics turned on that shows this behavior.  So this is actually the correct behavior here - I'd recommend wiping   before running set, which I've confirmed works here   Also, I note the potential duplicate you reference #10331 is currently closed as no fault found which does not suggest a fix is in progress.  Sure, probably should have been a little clearer here - right now, the system is working as designed. However, as you've seen here, this behavior is probably not desirable for all use cases. Since its not a bug, we're not going to change behavior here, but we are looking at a feature to workaround this by making some variables read only (so they can't get overwritten). You can see the spec for that feature here Please add to the description.Thanks.  I think its probably the same issue, I'd prefer to leave it to the issue author's discretion."
technical,"We agree, this behavior is not desirable. We disagree, it is bug. After you implement a new category of variables, read-only variables, and ask customers to flag variables as read-only with a new parameter at variable creation, you will maintain the undesirable behavior for non-read-only variables, because it is a design feature? Updated the wording in my comment above for clarity   To be clear, I think the behavior is undesirable for this use case, but in general it  is  the behavior we want. The contract we have with the end user is essentially: any time you write something to stdout that matches our ##vso commands, we'll process it. That includes when tools write something to stdout, or a command writes something to stdout. We have no way of differentiating between you trying to write something and the script accidentally writing something.  IMO the only potentially questionable behavior here is that we don't require the ##vso command to be at the start of a line. I see pros and cons on that one, but that's somewhat moot at this point because we wouldn't be able to change that without risking breaking a significant portion of customers. Even with that, its not a bug as much as a design question that you  could  disagree with.  Do you think that's fair?"
technical,"Please add to the description.Thanks.  I think its probably the same issue, I'd prefer to leave it to the issue author's discretion. We agree, this behavior is not desirable. We disagree, it is bug. After you implement a new category of variables, read-only variables, and ask customers to flag variables as read-only with a new parameter at variable creation, you will maintain the undesirable behavior for non-read-only variables, because it is a design feature?"
technical,"Hey - this looks like it might be the same issue as this  Could you try running set +x at the start of your script? If that doesn't work, could you try queuing a build with system diagnostics turned on and sharing the output? Yesterday i lost a day cleaning up a spurious trailing single quote. Today I put together a minor variation on your ""Set variables in scripts"" YAML example on your ""Variables"" concept page. I hope this suggests some simple test cases."
technical,"We're actively working on solutions to shader jank, which you can follow along in this issue. I'm going to close this thread, as the conversation isn't really productive after this comment, I'm afraid. ## Steps to Reproduce  You must include full steps to reproduce so that we can reproduce the problem. --  1. Install and run your app on a recent iOS device using a release build.  **Expected results:**  what did you want to see?  Flutter apps should be just as buttery as native apps, especially on the first run when it's the customer's first time opening the app getting their first impression of it.  **Actual results:**  what did you see? The very first start after installing is extremely janky and laggy. The next time you start it, it will be sort of fine. After a few more starts it will be smooth.  * Yes this is a release build of the app.  Here are some gifs for examples on an iPhone 8 running iOS 13.5.1.  ### Very first time starting the app after a fresh install * Notice most of the frames after the splash screen and before the screen slides up are dropped. This has the most jank (first run). * You can really see the jank when the loader animation stops (when that screen is disposed) and the next screen starts to slide up, but it just kind of appears with no frames in between. ### Second time opening the app * The second time opening the app you can see the initial transition is  better  but there is still some very noticeable jank as the screen slides up from the bottom. ### Third time opening the app * Now the third time and every time going forward it is smooth just like you'd expect. * But if you uninstall or update the app, then it will be janky again until the 2nd or 3rd time you open/use it.  ### Screen transitions example The same thing happens for screen transitions too. * The first time you run a new screen transition it will be laggy. * All future screen transitions of the same type are smooth. * If you use a different kind of transition somewhere in your app, that one will be laggy too the first time it runs. I have already tried following this doc related to SkSL caching. It fixed some of the first start jank issues on Android but has no effect on iOS (likely because of metal which is mentioned there). It also kind of is a bummer that if I want my app to be buttery smooth, I will always need to write integration tests (or do it manually) and save a dump of every transition / animation, and ensure to keep that skia shader capture file updated every time I change my app.  To be completely honest, this is not acceptable to ship the app in its current state and I am disappointed I got this far with Flutter (7+ months) before I noticed or even heard of this kind of issue. It seems to not be Flutter's fault but Skia's, but still I think new devs should know this problem exists before they get neck deep into it. It's the kind of thing you won't notice until you have a somewhat significantly-sized app and can easily overlook at first.  I really hope this can be fixed soon because it has an extremely detrimental effect on the perceived quality of your app when it's this laggy the very first time you open it, especially on brand new ios devices where native apps don't lag at all.  Run your application with flutter run --verbose and attach all the log output below between the lines with the backticks. If there is an exception, please see if the error message includes enough information to explain how to solve the issue.  Run flutter analyze and attach any output of that command below. If there are any analysis errors, try resolving them before filing this issue. Finally, paste the output of running flutter doctor -v here. No issues found!"
technical,"I'm also voicing my opinion that this should be labeled as highest priority. This shader jank is something that severely affects the perceived quality of our apps. The very first thing that someone asked about my latest app was ""why is it so laggy?""... Something that no developer would want to hear after spending countless hours working on it. Anything that affects the quality of the app in such magnitude should be considered as highest priority. Also want to add that I have 10's of thousands of active users of my iOS app every day, deployed with Flutter, and my 5-star (thousands of reviews) app is getting 1-star reviews, especially from iPad users, for ""freezes"" and ""choppy"" animations.  Via support conversations I've confirmed it's jank they're speaking of.  This is a critical issue for me as well, and I hope this is as high a priority as possible."
technical,"For those that are having issues on Android, you can follow this to solve this now. For those that are having issues on iOS, I've updated a simpler instruction in this on how to try SkSL warm-up by opting out Metal. If that instruction works, please let us know what performance tradeoff you're seeing from your app. These are the short-term solutions. Chinmay and I will give more details on the longer-term solutions later. any news on the doc?"
technical, AoT compilation
technical,"Honestly, this issue made me re-consider of cross-platform frameworks being owned by the same company (Google here) will be supported at best, and ignoring quality metrics delivery on other platforms (iOS). This issue has no problem on Android, but  it is  on iOS. I think if it was on Android, it will be solved faster. Anyways, Flutter is great, but please ensure issues are solved with the same priority on both Android and iOS. Apple should make SwiftUI crossplatform, so at least there would be competition ,)"
technical,"Experiencing same issue on Android devices. For instance, the Flutter Gallery app is very laggy on the majority of medium-range Android devices like Xiaomi MI A2. The navigation part between tabs takes about 1 second and frame drop is very significant. It makes the whole application look like a web-view or built with cross-platform not native solutions, like PhoneGap :) Can we change the title to include Android as well?"
technical,Disappointing that this is not being considered high priority.  Can't submit to App Store with extreme jank. Did apple say anything about your app being slow?
technical,"any news on the doc? Does the feedback on #61045 help the iOS case for you at all? On the Android side, I saw that the GitHub link's been moved to the web site, I'm not sure what other action is necessary here."
technical,"Same issue. I spent a lot of time trying to solve this problem. But I didn't find anything, except to put a forced delay before the animation (but this fix does not completely solve the issue). How long wait for a fix (week/month)? Thanks you for the great framework. Experiencing same issue on Android devices. For instance, the Flutter Gallery app is very laggy on the majority of medium-range Android devices like Xiaomi MI A2. The navigation part between tabs takes about 1 second and frame drop is very significant. It makes the whole application look like a web-view or built with cross-platform not native solutions, like PhoneGap :)"
technical,"Looking closer at the slowed down animations (this time timeDilation = 25), I think it is worse than I thought in the last comment because you can see it skip more than a few times.  ### First app open It seems to skip more than once here. ### First screen transition This one is not as bad, I just see two skips. Experiencing same issues on both stable and beta versions, but never thought that this is an issue of flutter or Skia. Thanks for your research."
technical,"he is planning on writing a doc about how to address this, should be available later this week. For those that are having issues on Android, you can follow this to solve this now. For those that are having issues on iOS, I've updated a simpler instruction in this on how to try SkSL warm-up by opting out Metal. If that instruction works, please let us know what performance tradeoff you're seeing from your app. These are the short-term solutions. Chinmay and I will give more details on the longer-term solutions later."
technical,"Yeah, I've had metal disabled for a number of weeks now. It greatly improved our app's performance with that jank gone. We've not noticed any serious performance issues that would make us want to go back. I hope in time metal support will become better, and I hope it's easier to work around it soon. Right now flutter upgrades are quite painful as I need to recompile my engine every time I upgrade flutter versions, so I generally do it every few weeks as it takes a few hours. He is a little busy recently in some other work so the doc has been delayed. BTW, this issue's title now covers both iOS and Android shader compilation jank so it looks like a duplicate of this."
technical,"Just updated to include Android. I think the real issue here is that there is no viable fix for iOS at the moment. With android you can do the SkSL warmup routine outlined here. But this will not do anything for iOS metal devices yet (any iOS device since the iPhone 5S). I've been trying to compile my own flutter engine with iOS metal disabled to test OpenGL but I cannot get my engine to build. If anyone can help, here is where I'm stuck now. he is planning on writing a doc about how to address this, should be available later this week."
technical,"Same problem on iOS, also. I always use Github's reaction feature when ever I stumble on an issue that I'm also experiencing, in order not to be redundant. Sadly not today........... Same issue on IOS too."
technical,"It looks like no one from Flutter team cares about the issue. I did some more investigation and it looks like there is just a bug in Skia working on Metal.  Why I think there is Skia bug: 1. Jank is not depend on device performance, in my case it is clearly visible on one specific **1 second long** animation  - on Metal on ALL devices I tested, there is only 2 frames displayed on first run, first frame and last. In case of OpenGL number of frames depend on how powerful device is. 2. For the same animation I took slowest possible Android device I could find, and it showed 4 frames. :) How is that possible that 5 years old low level Android device works faster than iPhone11, both running Flutter?Flutter team, PLEASE INVESTIGATE THIS ISSUE PROPERLY! IMHO there is no need to postpone resolution until SkSL warmup for Metal is ready. Even more - SkSL warmup for Metal wont fix this issue, just hide it for some use cases. Reason to get 1* still will be present. I am developing my app for more than a year and I tried a lot of flutter releases. There wasn't any animation jank until 1.12 (inclusively). As I know after 1.12 flutter started to use Metal instead of Open GL and every animation started to lag and hang. All animations in my app are very simple (like route changing, dialog showing). I used to think that I did something wrong but I didn't. I tried to downgrade flutter and every animation became smooth. You may think that the problem is only in using Metal but I don't think so. I tried flutter engines with Metal disabled and without shader precompilation animation janks still persist. And it seems weird for me that the solution for this problem is not fixing what is broken but some weird actions with shader precompilation. May be this problem took place before 1.12 on old android devices I don't know, I tested only on my iPhone 6s, then actions with shader precompilation seem necessary for these old devices, but not for others. And I agree with totalerex. I used my app without shader precompilation and it's unusable. Someone said even simple demo apps from flutter gallery are very janky. So my point is there is nothing more important than fixing this issue."
technical,I hope this gets fixed as soon as possible. This issue completely ruins the first impression of an app. I am hoping this issue is fixed before my initial release on iOS
technical,"Just a reminder to anyone new here - I am doing my best to maintain this repository offering the iOS profile/release engines with metal disabled if you want to try using OpenGL with SkSL caching. It's worth a shot if you cannot wait for the long term solution here. Some considerations: * It's experimental disabling metal and not a lot of data is collected yet, so please help report any issues you find so the Flutter team can work with it here. * Some people have reported OOM crashes with heavy animations with OpenGL, so test your app thoroughly.  * Some people have reported some Lottie animations have distortions when using OpenGL, so check your animations thoroughly.  * There may be a performance trade-off with overall animation smoothness being worse on OpenGL, but the trade-off for having no first-run jank could make it worth it for your case. I have problem the same"
technical,"Experiencing same issues on both stable and beta versions, but never thought that this is an issue of flutter or Skia. Thanks for your research. I hope this gets fixed as soon as possible. This issue completely ruins the first impression of an app."
technical,"Also want to add that I have 10's of thousands of active users of my iOS app every day, deployed with Flutter, and my 5-star (thousands of reviews) app is getting 1-star reviews, especially from iPad users, for ""freezes"" and ""choppy"" animations.  Via support conversations I've confirmed it's jank they're speaking of.  This is a critical issue for me as well, and I hope this is as high a priority as possible. I should also add that unfortunately, many users force close their apps because they think it makes their device perform better.  You can imagine how this exacerbates the jank issue."
technical,"Also want to add that I have 10's of thousands of active users of my iOS app every day, deployed with Flutter, and my 5-star (thousands of reviews) app is getting 1-star reviews, especially from iPad users, for ""freezes"" and ""choppy"" animations.  Via support conversations I've confirmed it's jank they're speaking of.  This is a critical issue for me as well, and I hope this is as high a priority as possible. i think Flutter has done it right by building everything from scratch, just need to fix these performance issues and things will be golden"
technical,"I always use Github's reaction feature when ever I stumble on an issue that I'm also experiencing, in order not to be redundant. Sadly not today........... Same issue on IOS too. I'm also voicing my opinion that this should be labeled as highest priority. This shader jank is something that severely affects the perceived quality of our apps. The very first thing that someone asked about my latest app was ""why is it so laggy?""... Something that no developer would want to hear after spending countless hours working on it. Anything that affects the quality of the app in such magnitude should be considered as highest priority."
technical,"He is a little busy recently in some other work so the doc has been delayed. BTW, this issue's title now covers both iOS and Android shader compilation jank so it looks like a duplicate of this. it doesn't look good that priority of the ""extreme jank"" issue has been decreased. There is no workaround for the issue. The opting out Metal doesn't really work since Lottie and Flare animations rendering is bad in OpenGl on iPhone X/11 (see example here). Formally I can't submit PR against OpenGL iOS rendering, since it is not officially used. So, there is closed loop, for my application at least. We  cannot deliver to customer junky application and/or bad quality Lottie/Flare animations. Please instead of changing priority to lower values, increase priority as much as possible."
technical,"i think Flutter has done it right by building everything from scratch, just need to fix these performance issues and things will be golden Just a reminder to anyone new here - I am doing my best to maintain this repository offering the iOS profile/release engines with metal disabled if you want to try using OpenGL with SkSL caching. It's worth a shot if you cannot wait for the long term solution here. Some considerations: * It's experimental disabling metal and not a lot of data is collected yet, so please help report any issues you find so the Flutter team can work with it here. * Some people have reported OOM crashes with heavy animations with OpenGL, so test your app thoroughly.  * Some people have reported some Lottie animations have distortions when using OpenGL, so check your animations thoroughly.  * There may be a performance trade-off with overall animation smoothness being worse on OpenGL, but the trade-off for having no first-run jank could make it worth it for your case."
technical,"Can we change the title to include Android as well? Just updated to include Android. I think the real issue here is that there is no viable fix for iOS at the moment. With android you can do the SkSL warmup routine outlined here. But this will not do anything for iOS metal devices yet (any iOS device since the iPhone 5S). I've been trying to compile my own flutter engine with iOS metal disabled to test OpenGL but I cannot get my engine to build. If anyone can help, here is where I'm stuck now."
technical,"Some additional findings: * When I slow the global animation speed down by 50x, I can see it's only the very first frame of the animation that is dropped. * After that initial jank, it is smooth for the rest of the (very slowed down) animation. Looking closer at the slowed down animations (this time timeDilation = 25), I think it is worse than I thought in the last comment because you can see it skip more than a few times.  ### First app open It seems to skip more than once here. ### First screen transition This one is not as bad, I just see two skips."
technical,"Some additional findings: * When I slow the global animation speed down by 50x, I can see it's only the very first frame of the animation that is dropped. * After that initial jank, it is smooth for the rest of the (very slowed down) animation. My Bad! I Saw P3 - P2.."
technical,"I should also add that unfortunately, many users force close their apps because they think it makes their device perform better.  You can imagine how this exacerbates the jank issue. nezoat please try my repo here which contains the flutter engine compiled with metal disabled so you can use SkSL warmup to reduce the jank."
technical,"Our customers have complained after opening the application for the first time :( Same issue. I spent a lot of time trying to solve this problem. But I didn't find anything, except to put a forced delay before the animation (but this fix does not completely solve the issue). How long wait for a fix (week/month)? Thanks you for the great framework."
technical,"yep, the same. Really not cool. 60fps was marketing killer-feature. I think that the priority of this problem should be high. Same problem on iOS, also."
technical,"AoT compilation Some additional findings: * When I slow the global animation speed down by 50x, I can see it's only the very first frame of the animation that is dropped. * After that initial jank, it is smooth for the rest of the (very slowed down) animation."
technical,"Did apple say anything about your app being slow? Sorry, should clarify - Not because of Apple, but because of not meeting a level of production quality"
technical,"Did apple say anything about your app being slow? That type of comment will only achieve one result: getting this conversation locked (it should, by the way.)"
technical,"it doesn't look good that priority of the ""extreme jank"" issue has been decreased. There is no workaround for the issue. The opting out Metal doesn't really work since Lottie and Flare animations rendering is bad in OpenGl on iPhone X/11 (see example here). Formally I can't submit PR against OpenGL iOS rendering, since it is not officially used. So, there is closed loop, for my application at least. We  cannot deliver to customer junky application and/or bad quality Lottie/Flare animations. Please instead of changing priority to lower values, increase priority as much as possible. The priority was increased. Check here the Label description. Edit: My Bad! It was indeed a downgrade.. This should bem P2 or P1.."
technical,"My Bad! I Saw P3 - P2.. We are actively working on this, although not with the alacrity that requires it surfacing in our weekly critical issue triage, which is why it was awarded a P3 instead of a P2 --- P0-P2 bugs get reviewed for status every week, P3 issues are things we either are working on or hope to work on in short order (see triaging issues for an explanation of our priority scheme). Thanks for your feedback in letting us know how important this is to you."
technical,"I am hoping this issue is fixed before my initial release on iOS We're actively working on ""Test-based shader warmup #53609"" to solve this, but that may take some time. Before that, you can follow to compile a custom Flutter engine that turns off Metal and uses OpenGL so the SkSL warm-up would solve this issue. Flutter still used OpenGL on all iOS devices a few months ago so its performance would not be too far behind. Depending on the workload, your app may prefer trading Metal's other improvements for OpenGL's vastly faster shader warm-up performance. Admittedly, compiling the Flutter engine takes much more time than just clicking a button, turn on a flag, or running a single command. We could have made it much easier by providing an opt-out-Metal option. However, we don't know if that will bring more harm than good. That's why your feedback is valuable to us, and we'd love to hear feedback from as many app developers as possible."
technical,"That type of comment will only achieve one result: getting this conversation locked (it should, by the way.) We're actively working on solutions to shader jank, which you can follow along in this issue. I'm going to close this thread, as the conversation isn't really productive after this comment, I'm afraid."
technical,"Does the feedback on #61045 help the iOS case for you at all? On the Android side, I saw that the GitHub link's been moved to the web site, I'm not sure what other action is necessary here. Yeah, I've had metal disabled for a number of weeks now. It greatly improved our app's performance with that jank gone. We've not noticed any serious performance issues that would make us want to go back. I hope in time metal support will become better, and I hope it's easier to work around it soon. Right now flutter upgrades are quite painful as I need to recompile my engine every time I upgrade flutter versions, so I generally do it every few weeks as it takes a few hours."
technical,"I have problem the same yep, the same. Really not cool. 60fps was marketing killer-feature. I think that the priority of this problem should be high."
technical,"Also: It would be nice to at least give an explanation before just closing an issue, this way nothing will get resolved, ## Summary As discussed in #4182, I'd like to request the addition of the ""override ext"" front matter tag.  ## Motivation I'm using php code on my website, like many others, too. I can create a ""test.php"" file, and the generated file will have the "".php""-extension, all right. However, I want to use markdown for my blog posts, and these "".md"" files are automatically generated to "".html"" files! And because the posts have a .""php"" layout with php code, but the file ends up as html, the site does not work. OK, I could create a permalink as suggested in the issue linked above, but that's a bad workaround because I don't want to hardcode the file name and directory. A similar request has been discussed already, but I find the excuses quite weak. As of now, it's hardcoded for markdown files to be converted to html files, and that's simply not right. Give us an option here!  Edit: OK, it's not really needed for other files where I can already set my custom extension manually, this would just add more confusion. But for all files which jekyll automatically converts into a hardcoded format, this should be changeable - most likely in config.yml."
technical,"This issue has been automatically marked as stale because it has not been commented on for at least two months.  The resources of the Jekyll team are limited, and so we are asking for your help.  If this is a **bug** and you can still reproduce this error on the latest <code3.x-stable</code or <codemaster</code branch, please reply with all of the information you have about it in order to keep the issue open.  If this is a **feature request**, please consider building it first as a plugin. Jekyll 3 introduced hooks which provide convenient access points throughout the Jekyll build pipeline whereby most needs can be fulfilled. If this is something that cannot be built as a plugin, then please provide more information about why in order to keep this issue open.  This issue will automatically be closed in two months if no further activity occurs. Thank you for all your contributions. This is already possible based on answers here. An arbitrary extension or no-extension file will be parsed with liquid if it has frontmatter and then jekyll outputs as .html And if you set the permalink in the file you can force the extension to be .json, .php , etc. Maybe set the permalink pattern globally in the config as a collection. You can set permalink for post but the response there was that it doesn't work in this case I think"
technical," This issue has been automatically marked as stale because it has not been commented on for at least two months.  The resources of the Jekyll team are limited, and so we are asking for your help.  If this is a **bug** and you can still reproduce this error on the latest <code3.x-stable</code or <codemaster</code branch, please reply with all of the information you have about it in order to keep the issue open.  If this is a **feature request**, please consider building it first as a plugin. Jekyll 3 introduced hooks which provide convenient access points throughout the Jekyll build pipeline whereby most needs can be fulfilled. If this is something that cannot be built as a plugin, then please provide more information about why in order to keep this issue open.  This issue will automatically be closed in two months if no further activity occurs. Thank you for all your contributions."
technical,"This is already possible based on answers here. An arbitrary extension or no-extension file will be parsed with liquid if it has frontmatter and then jekyll outputs as .html And if you set the permalink in the file you can force the extension to be .json, .php , etc. Maybe set the permalink pattern globally in the config as a collection. You can set permalink for post but the response there was that it doesn't work in this case I think Yeah, that's the problem: I don't want to hardcode the path with permalinks, I just want to be able to adjust the file extension..."
technical,Women can also be a construction worker ## Use case  While calling a web service on Flutter web it will not work because of CORS Policy  ## Proposal I saw a chrome window when I start debugging a Flutter web application I wonder if we could add a flag to disable the CORS policy.
technical,"Unless you allow Notepad to save it as a .txt file. Use a proper programmer's text editor, preferably, but until then, rename the file after editing, if necessary. Unfortunately, I have no idea at all what you mean by the request method and options. This should have nothing to do with something like that. Using this batch file simply means that Chrome is started with an extra option in the command line that makes it not observe the CORS requirement during debug runs. Which is basically a temporary solution because, in the long run, you have to make sure the server is set up to provide the required CORS settings, there's no way out of this requirement. This isn't a Flutter limitation at all, Flutter has nothing to do with it, it's a browser limitation and cannot be circumvented because that would make it dangerous and no browser vendor would ever do that. **In other words, if you have your server you call set up properly, you don't need this even in debug.** It just works normally. ### SOLUTION IS LISTED FOR WIKIPEDIA CORS ERROR A api request to wikipedia from flutter web and the error was very simple the ""**CORS error**"". as it happens i did what we all do ""Search The Google"" and to many result specially associate with flutter, but most important is so many issues are still not closed. This person tries to understand what these errors are and why it happen. f**irebase, google storage CORS error** better check this out on stackoverflow. **wikipedia api CORS error** check this out. **at my side** what i was missing was  a &origin=* url parametere with value. previous url  with error. new updated working url here."
technical,"YES.  This is not the problem, however. The directory a-temp-location has been successfully created in my HOME folder by Chrome itself, and is full of files also create by it. A quick Googling seems to suggest that in sh, the variable to include all passed parameters is . I used * on Windows but neither that nor your %* might be accepted in sh."
technical,So basically no viable solution for this? A very simply batch file and it works.I've been using it since my first Flutter Web day and it's still perfect. :-)
technical,developers suggested using an extension or opening chrome with CORS disabled So I have 2 questions: 1- Why the app is not launching as a new tab in the same browser? and is there any possibility I can change the default behavior? because the chrome instance which got launched doesn't have extensions like my default chrome. 2-How could I configure the app to run on Firefox instead of chrome? But i hosted my website on Firebase also but still same problem any idea ? Or how to allow this api call from my localhost because i am using few microsoft sharepoint api's which i can't request Microsoft for modification. Pls help.
technical,developers suggested using an extension or opening chrome with CORS disabled So I have 2 questions: 1- Why the app is not launching as a new tab in the same browser? and is there any possibility I can change the default behavior? because the chrome instance which got launched doesn't have extensions like my default chrome. 2-How could I configure the app to run on Firefox instead of chrome? can you show me how did you do with the bash profile?
technical,"I have followed the presented steps using .bash executable. In which I had to add the extension to the .bash profile like this after that you should source the path of the .bash profile and google-chrome-unsafe.bash it will open the unsafe chrome version, so it means you're good to go, if not restart your machine so the changes are applied  In my case when I ran the debug it didn't start, and running flutter doctor -v gives Permission denied You should run this In the app directory so it's able to execute the executable  Another scenario that might occur is to have a Permission denied errno = 13 which states you can resolve by running flutter clean, with more detailed view flutter clean -v but might not work as expected so you must delete the shown file manually on the correspondent paths, in my case were .dart tool and build. After that you just need to run it and should work, it will rebuild the project and run. Could you reconsider opening the issue as it got a lot of attention? Thanks"
technical,it is not a bug. it is security permission like when you call twitter from facebook then facebook is not allowed to call twitter. So backend developer should allow you to call his web service from your localhost. this security feature is for browsers only but not on mobile or postman. developers suggested using an extension or opening chrome with CORS disabled So I have 2 questions: 1- Why the app is not launching as a new tab in the same browser? and is there any possibility I can change the default behavior? because the chrome instance which got launched doesn't have extensions like my default chrome. 2-How could I configure the app to run on Firefox instead of chrome?
technical,it is not a bug. it is security permission like when you call twitter from facebook then facebook is not allowed to call twitter. So backend developer should allow you to call his web service from your localhost. this security feature is for browsers only but not on mobile or postman. Did you leave a-temp-location verbatim? :-)
technical, Disabling the CORS checks locally would lead to drastically different behavior between a debug and deployed application - you can't very well ask all users of your website to disable CORS locally. Adding this flag would be a mistake
technical,It is absolutely not the same thing flutter web working locally and remotely ? I am also facing this issue with flutter web but it works fine on Mobile. this is a bug in Flutter web ?
technical,A very simply batch file and it works.I've been using it since my first Flutter Web day and it's still perfect. :-) Hello This batch will be used on release version ?  If i did understand well no ? Because my problem is calling some webservice  who don't accept CORS. Calling those webservices from my release version.
technical,Tell Mateus how. :-) Here it is:  Device: Macbook Pro OS: macOS Catalina  1. Add this to .bash profile: 2. Create a shell-script file (named: google-chrome-unsafe) at the same directory where the original chrome executable is with this one single line. OBS: the * is basically to allow Flutter to provide more flags so don't remove it. Edit 1: Remeber that it is important to have the shell-script at the same directory where the original chrome executable is placed otherwise flutter will not show chrome as a device to run your project. That is why the CHROME EXECUTABLE points to a shell script that is inside the chrome application directory.
technical,"A quick Googling seems to suggest that in sh, the variable to include all passed parameters is . I used * on Windows but neither that nor your %* might be accepted in sh. Hi guys, can you help me? I'm on Mac and this bash profile is not working.  What is wrong with it and how should I do that? When I run my IDE with this bash profile chrome is not being listed as device, but if I remove the last line it and restart the IDE it is being listed."
technical,"In AWS you could whitelist a specific port. So I am sure Sharepoint has the same feature. By the way, you can Add the whitelisted port for every run in the launch.json file. Sharepoint CORS. Hi team I am facing this CORS policy issue from last 2 month, still not able to figure it out how to fix. Access to XMLHttpRequest at this from origin has been blocked by CORS policy: Response to preflight request doesn't pass access control check: It does not have HTTP ok status. - I have hosted my flutter web app on AWS - Apache environment  please if you any solution please help me."
technical,"The solution: create a batch file (or whatever your platform calls the shell scripts) that calls Chrome with the appropriate command line. (Note the %* at the end, important!) Set up an environment variable named CHROME EXECUTABLE to point to this file. Flutter will then use this file to start Chrome. How flutter will use it? is it automatically?  Flutter will then use this file to start Chrome."
technical,"The solution: create a batch file (or whatever your platform calls the shell scripts) that calls Chrome with the appropriate command line. (Note the %* at the end, important!) Set up an environment variable named CHROME EXECUTABLE to point to this file. Flutter will then use this file to start Chrome. how to create the batch file like deakjahn had mentioned??"
technical,"I've updated my comment above. I have followed the presented steps using .bash executable. In which I had to add the extension to the .bash profile like this after that you should source the path of the .bash profile and google-chrome-unsafe.bash it will open the unsafe chrome version, so it means you're good to go, if not restart your machine so the changes are applied  In my case when I ran the debug it didn't start, and running flutter doctor -v gives Permission denied You should run this In the app directory so it's able to execute the executable  Another scenario that might occur is to have a Permission denied errno = 13 which states you can resolve by running flutter clean, with more detailed view flutter clean -v but might not work as expected so you must delete the shown file manually on the correspondent paths, in my case were .dart tool and build. After that you just need to run it and should work, it will rebuild the project and run."
technical,"I've updated my comment above. I have it in System but it shouldn't actually matter. The Flutter doctor display shows the value when it sees it, try that first."
technical,"Why is this closed anyway?  Is there some solution I'm missing? I ended up setting up a reverse proxy script to modify traffic headers (the flutter host/port are hard-coded near the top of the script). (The script is from somewhere else on the web, but I modified it for py3.x and had to make some other changes to get it working.) You run this, it'll transparently transfer your traffic from port 9000, to 8989 where I have flutter running, removing the origin headers (and some other stuff the original script stripped), and adding in the access-control-allow-origin header. This worked for me, although I'm getting some infinite ""font"" declaration dropped errors now.  Script below that screenshot. I really wish there was a simple solution to this. It makes developing my Flutter app that interacts with a legacy web-server very difficult. The simplest option would be to give a way of enabling other domains for the debug webserver that AndroidStudio starts, or give me away to enable source-maps in the standalone flutter webserver. I can proxy through a server, but then data errors result in incomprehensible errors because source maps never load. OR I can run directly out of Android Studio which gives me good errors, but then I can't test any network interactions.  The arguments about fooling people into having problems in production is about as valid as saying that debuggers shouldn't be supported because it is not how production works. Making development difficult WILL NOT increase Flutter Web usage."
technical,Disabling the CORS checks locally would lead to drastically different behavior between a debug and deployed application - you can't very well ask all users of your website to disable CORS locally. Adding this flag would be a mistake I wonder why web services work in Android debugging mode while there is no Internet permission in the Manifest file? For sure the simple answer is easy debugging until adding the permission for the release version
technical,Here it is:  Device: Macbook Pro OS: macOS Catalina  1. Add this to .bash profile: 2. Create a shell-script file (named: google-chrome-unsafe) at the same directory where the original chrome executable is with this one single line. OBS: the * is basically to allow Flutter to provide more flags so don't remove it. Edit 1: Remeber that it is important to have the shell-script at the same directory where the original chrome executable is placed otherwise flutter will not show chrome as a device to run your project. That is why the CHROME EXECUTABLE points to a shell script that is inside the chrome application directory. I've updated my comment above.
technical,"Thanks for your response but its giving below error: Access to XMLHttpRequest at this token from origin has been blocked by CORS policy: No 'Access-Control-Allow-Origin' header is present on the requested resource. In AWS you could whitelist a specific port. So I am sure Sharepoint has the same feature. By the way, you can Add the whitelisted port for every run in the launch.json file. Sharepoint CORS."
technical,"I wonder why web services work in Android debugging mode while there is no Internet permission in the Manifest file? For sure the simple answer is easy debugging until adding the permission for the release version Internet Permission for Android is the same as CORS in Chrome Here is the PR for Android Permission that makes Internet works debug mode only  This is not an ""injection"" at build time, but rather a separate manifest for debug builds  Debug mode Internet permission PR **move INTERNET permission to debug/AndroidManifest.xml**  Profile mode Internet permission PR **Add a manifest for profile builds that enables INTERNET permission. It will be **optional** -flag only until the backend developer add whitelist for my local or in AWS config"
technical,"Internet Permission for Android is the same as CORS in Chrome Here is the PR for Android Permission that makes Internet works debug mode only  This is not an ""injection"" at build time, but rather a separate manifest for debug builds  Debug mode Internet permission PR **move INTERNET permission to debug/AndroidManifest.xml**  Profile mode Internet permission PR **Add a manifest for profile builds that enables INTERNET permission. It will be **optional** -flag only until the backend developer add whitelist for my local or in AWS config It is absolutely not the same thing"
technical,flutter web working locally and remotely ? I am also facing this issue with flutter web but it works fine on Mobile. this is a bug in Flutter web ? it is not a bug. it is security permission like when you call twitter from facebook then facebook is not allowed to call twitter. So backend developer should allow you to call his web service from your localhost. this security feature is for browsers only but not on mobile or postman.
technical,"Hello This batch will be used on release version ?  If i did understand well no ? Because my problem is calling some webservice  who don't accept CORS. Calling those webservices from my release version. No, I thought you meant debugging, this is what the OP talked about. :-) In release mode, you need correct CORS settings. I don't think Flutter can do anything about this, this is a browser security limitation. No webapp can override the security features of the browser, if it could, that would make them completely useless. So there is actually a web service not under your control that doesn't have correct CORS settings? If it is so, how is anybody supposed to use it, quite independent from Flutter?"
technical,"I really wish there was a simple solution to this. It makes developing my Flutter app that interacts with a legacy web-server very difficult. The simplest option would be to give a way of enabling other domains for the debug webserver that AndroidStudio starts, or give me away to enable source-maps in the standalone flutter webserver. I can proxy through a server, but then data errors result in incomprehensible errors because source maps never load. OR I can run directly out of Android Studio which gives me good errors, but then I can't test any network interactions.  The arguments about fooling people into having problems in production is about as valid as saying that debuggers shouldn't be supported because it is not how production works. Making development difficult WILL NOT increase Flutter Web usage. So basically no viable solution for this?"
technical,"I have it in System but it shouldn't actually matter. The Flutter doctor display shows the value when it sees it, try that first. So disabling CORS in Chrome on my development machine will fix it, but once deployed release mode in production the user's wont face any CORS issue?"
technical,"Well, how do you normally create files? That way. :-) So what I did is, I opened Notepad, wrote those lines and saved it as a .bat file. And then in environment variables, I created a new variable called CHROME EXECUTABLE and set its value as the path to my .bat file. And the request method, when I inspect the network tab has changed to POST instead of OPTION. Am I doing this right? Sorry I'm a complete noob :/"
technical,Thanks everyone I finally got it working on Mac. Tell Mateus how. :-)
technical,can you show me how did you do with the bash profile? Thanks everyone I finally got it working on Mac.
technical,"You can start flutter web server on random port using command flutter run -d web-server, and then go to localhost:port in your browser or you could use release app using following guide. Thanks for your response but its giving below error: Access to XMLHttpRequest at this token from origin has been blocked by CORS policy: No 'Access-Control-Allow-Origin' header is present on the requested resource."
technical,"your script solution workes like a charm  thanks! I have adapted your script solution in Linux as described below and it works perfectly!  Create a google-chrome-unsafe.sh with the following content: It's better to create and use a dedicated folder in the home directory instead of /tmp for the temporary location, as Chrome will create some folders there to work properly. Then, make it executable. The environment variable can be set in ~/.bashrc or ~/.bash aliases (if included from .bashrc)."
technical,"Hi team I am facing this CORS policy issue from last 2 month, still not able to figure it out how to fix. Access to XMLHttpRequest at this from origin has been blocked by CORS policy: Response to preflight request doesn't pass access control check: It does not have HTTP ok status. - I have hosted my flutter web app on AWS - Apache environment  please if you any solution please help me. The solution: create a batch file (or whatever your platform calls the shell scripts) that calls Chrome with the appropriate command line. (Note the %* at the end, important!) Set up an environment variable named CHROME EXECUTABLE to point to this file. Flutter will then use this file to start Chrome."
technical,"So disabling CORS in Chrome on my development machine will fix it, but once deployed release mode in production the user's wont face any CORS issue? They will, unless you setup your servers properly. This is something you can't skip."
technical,"Now that Flutter Web has gone mainstream, this seems to resurface again. So, let's put it straight, with the largest, boldest letters: **THIS IS NOT A FLUTTER ISSUE, There is no Flutter solution or workaround and never will be.**  If you use a web service, you have to abide by CORS rules. Check out the web service to see how they want you to do it. If it's your own server, set it up properly. **Flutter has nothing to do with it.** While you might be able to disable CORS as described above for yourself in **debug**, there's absolutely no way to do it in **release** for your users. Simply impossible and would be so dangerous no browser vendor would ever allow it.  (For a simple description: CORS means that the web service you try to use has the ability to decide whether it accepts and handles incoming requests from your web app. Obviously, if it refuses to serve you, your Flutter app cannot force it to do so because the browser it runs in will enforce these rules, no matter what you do. So, you have to look around and see how the **web service** you want to use proposes to handle the situation. If it's an established web service, there will be a solution because CORS applies to every one of their API users, not just Flutter ones.) To keep it simple, it's not Flutter issue. You can setup a server and enable cors headers or write a **Chrome extension** that receive a **message** from your app, and get data from external api resource using **XMLHttpRequest**."
technical,"So what I did is, I opened Notepad, wrote those lines and saved it as a .bat file. And then in environment variables, I created a new variable called CHROME EXECUTABLE and set its value as the path to my .bat file. And the request method, when I inspect the network tab has changed to POST instead of OPTION. Am I doing this right? Sorry I'm a complete noob :/ Unless you allow Notepad to save it as a .txt file. Use a proper programmer's text editor, preferably, but until then, rename the file after editing, if necessary. Unfortunately, I have no idea at all what you mean by the request method and options. This should have nothing to do with something like that. Using this batch file simply means that Chrome is started with an extra option in the command line that makes it not observe the CORS requirement during debug runs. Which is basically a temporary solution because, in the long run, you have to make sure the server is set up to provide the required CORS settings, there's no way out of this requirement. This isn't a Flutter limitation at all, Flutter has nothing to do with it, it's a browser limitation and cannot be circumvented because that would make it dangerous and no browser vendor would ever do that. **In other words, if you have your server you call set up properly, you don't need this even in debug.** It just works normally."
technical,"how to create the batch file like deakjahn had mentioned?? Well, how do you normally create files? That way. :-)"
technical,"Would it help if we had ""flutter -d web-server"" accept additional headers, like this. Edit: It appears more might be needed, but I'm inexperienced in this. The server currently sends out some other headers that would probably also need to be modified: Why is this closed anyway?  Is there some solution I'm missing? I ended up setting up a reverse proxy script to modify traffic headers (the flutter host/port are hard-coded near the top of the script). (The script is from somewhere else on the web, but I modified it for py3.x and had to make some other changes to get it working.) You run this, it'll transparently transfer your traffic from port 9000, to 8989 where I have flutter running, removing the origin headers (and some other stuff the original script stripped), and adding in the access-control-allow-origin header. This worked for me, although I'm getting some infinite ""font"" declaration dropped errors now.  Script below that screenshot."
technical,"They will, unless you setup your servers properly. This is something you can't skip. Would it help if we had ""flutter -d web-server"" accept additional headers, like this. Edit: It appears more might be needed, but I'm inexperienced in this. The server currently sends out some other headers that would probably also need to be modified:"
technical,"Did you leave a-temp-location verbatim? :-) YES.  This is not the problem, however. The directory a-temp-location has been successfully created in my HOME folder by Chrome itself, and is full of files also create by it."
technical,"How flutter will use it? is it automatically?  Flutter will then use this file to start Chrome. Yes. Flutter checks this env variable before it launches Chrome. If you try to start flutter run -d chrome on a system that doesn't have Chrome installed, Flutter will specifically ask you to either put it into the default location or to tell it where it is using CHROME EXECUTABLE (this is actually how I learned of its existence :-) )."
technical,"But i hosted my website on Firebase also but still same problem any idea ? Or how to allow this api call from my localhost because i am using few microsoft sharepoint api's which i can't request Microsoft for modification. Pls help. You can start flutter web server on random port using command flutter run -d web-server, and then go to localhost:port in your browser or you could use release app using following guide."
technical,"Yes. Flutter checks this env variable before it launches Chrome. If you try to start flutter run -d chrome on a system that doesn't have Chrome installed, Flutter will specifically ask you to either put it into the default location or to tell it where it is using CHROME EXECUTABLE (this is actually how I learned of its existence :-) ). your script solution workes like a charm "
technical,"Current status: Seems to recover in about 18 minutes (when the OS informs the app the connection is recovered, most likely).  Workaround: Call goOffline() followed by goOnline().  While calling goOffline/goOnline is a reasonable workaround until there's a fix, doesn't feel like clients should have to monitor and manage the connectivity, our SDKs should handle this.  Locking discussion to save eng time answering ETA requests, but keeping this open as a bug for discussion and resolution. We'll update here when there is a status update or news of a fix. Until then, please use the workaround and assume there's no release date available. ### [REQUIRED] Step 2: Describe your environment  * Firebase Component: Database * Component version: 19.2.0, but I think it has been problem since 2018  ### [REQUIRED] Step 3: Describe the problem  Using realtime database change listener normally in android mobile with wifi connection. It work perfectly for most of the times. But it would eventually (2-3 days to 2-3 weeks) not be able to connect to firebase realtime database and get the realtime update, even if the PC of the same router still able to connect. I need to shut the wifi of my android to use mobile 4g network, or reset the router. Resetting router make it usable but eventually after a while it will fail again  I myself using unity. But there was already some people happen to face the same problem so I think it could be common to many people. Also, surprisingly, the web that use firebase still work in the same network state. So I think this problem is because native network library  I think the actual source of problem could be a problem of router setting. But that might be critical. Because realtime update could be fail for anyone in misconfig router (in workplace for instance) but it result in the whole app failed unknowingly to us. It could also be a critical feature of that app. Isn't it possible to have firebase native to have failsafe to fallback?"
technical,"Is there any alternative to solve this problem? At this point, I don't see an obvious fix since this happens very low in the network stack."
technical,"I've also encountered this problem before and only resetting the customer's router ended up working. Writes over wifi to another REST API were working during this period, but setValue() to my Firebase RTDB were not. I unfortunately don't have the logs from that period. Chiming in since this seems to be an issue in our usage of Realtime Database too. For years already, we have received support requests from Android app users who are not receiving data from RTDB. I've tried many things over the years to mitigate this, but never have found a way to fix the issue completely. Switching away from wifi usually fixes the situation. It's not a very widespread issue, but one that has been very hard to reproduce. Common symptoms include: - Only affects our Android app, not iOS (which is accessing the exact same database) - Other apps / internet connection in general works on the phone, it's just RTDB things that fail - Seems to affect devices from many manufacturers (Samsung, OnePlus, Huawei, Nokia) - Has been a consistent problem over many Android versions (7-10)"
technical,"Firebase support got back to me and in my case, after an initial internet connectivity disconnect occurs with device wifi remaining connected, the correct approach is to simply call goOffline() followed by goOnline(). I verified this immediately restores connection to the Firebase database, though not sure if it's the same wifi issue others are facing. Current status: Seems to recover in about 18 minutes (when the OS informs the app the connection is recovered, most likely).  Workaround: Call goOffline() followed by goOnline().  While calling goOffline/goOnline is a reasonable workaround until there's a fix, doesn't feel like clients should have to monitor and manage the connectivity, our SDKs should handle this.  Locking discussion to save eng time answering ETA requests, but keeping this open as a bug for discussion and resolution. We'll update here when there is a status update or news of a fix. Until then, please use the workaround and assume there's no release date available."
technical,"I happen to have the same issue, after reading this thread I contact our clients experimenting this issue, I suggested to them to use their mobile data instead of their wifi and the app loaded the info. On my research I found that the problem could be even more complex.  One of our clients have two phones, Samsung A30s and Huawei P20 pro, the 2 devices were connected to the same wifi router. The samsung is having the issue, the Huawei is not. Firebase support got back to me and in my case, after an initial internet connectivity disconnect occurs with device wifi remaining connected, the correct approach is to simply call goOffline() followed by goOnline(). I verified this immediately restores connection to the Firebase database, though not sure if it's the same wifi issue others are facing."
technical,"I don't know which specific device have this problem except one samsung of my acquaintance I don't know detail about. And my own Nokia 7 plus  This problem could occur in many router setting even though it could use internet normally and the most disturbingly confusion is, in the same wifi that cause problem to android, realtime database in ios of the same app still work under it properly  And while RTDB won't work, at the same time in the same device that RTDB can't load anything, firestore could work properly  So this issue is really specifically a problem of only RTDB android library in wifi Firestore and RTDB uses a very different networking stack. Firestore uses GRPC, whereas the RTDB used WebChannel which more heavily relies on Android's built in networking.  If you are able to reproduce this somewhat reliable in an emulator, then we could probably come up with a reproduction. It's a bit tough to diagnose issues that point to issues with the DNS resolver from afar."
technical,"One thing I am going to try (which I already do in another version of my app, for different reasons) is to create a dummy keep-alive setValue() write to my RTDB every 5 minutes or so. I've never gotten a report from that version of my app that the connection has broken, so I am wondering if it has something to do with it.  **Update:**  I was actually able to reproduce this, or some version of this, it seems. I connected my test device to my phone's hotspot wifi connection and then turned off the LTE mobile data. I sent my setValue() request with an onComplete listener and the connection hangs. DatabaseError is never thrown. I then re-enable the mobile LTE data on my hotspot device - subsequent attempts also fail, DatabaseError is never thrown so the else statement never runs. I opened a browser and confirmed internet connection is present. Restarting the activity in the reconnected state did not help as well.  The fix was to disable and re-enable wifi on the device. How long did you wait for the connections to recover? This would be useful for comparing to the socket connection timeouts."
technical,"Thank you for sharing the logs. It looks like the error might be a fundamental issue with the device's network (Unable to resolve host ""s-usc1c-nss-246.firebaseio.com"": No address associated with hostname).  Can you let us know if this problem occurs on multiple devices? If so, would it be possible to share a reproduction for us that we could run on Emulator? I don't know which specific device have this problem except one samsung of my acquaintance I don't know detail about. And my own Nokia 7 plus  This problem could occur in many router setting even though it could use internet normally and the most disturbingly confusion is, in the same wifi that cause problem to android, realtime database in ios of the same app still work under it properly  And while RTDB won't work, at the same time in the same device that RTDB can't load anything, firestore could work properly  So this issue is really specifically a problem of only RTDB android library in wifi"
technical,"The glimpse of the cause is in this comment  ""The network stack"" that rely on underlying android network library is the root cause, there was many bug about networking in android OS itself, not only firebase but facebook SDK also suffer from another android networking library bug and google themselves never ever had publicly announcement to fix them  FireStore use GRPC instead and they avoid those android bug I happen to have the same issue, after reading this thread I contact our clients experimenting this issue, I suggested to them to use their mobile data instead of their wifi and the app loaded the info. On my research I found that the problem could be even more complex.  One of our clients have two phones, Samsung A30s and Huawei P20 pro, the 2 devices were connected to the same wifi router. The samsung is having the issue, the Huawei is not."
technical,"How long did you wait for the connections to recover? This would be useful for comparing to the socket connection timeouts. I let the setValue() run twice and both times it recovered after around 18 minutes. I'm happy to email you the entire stacktrace if that's helpful, it looks like there's some relevant stuff going on there."
technical,"katowulf and I connected offline and this is expected behavior due to the Android OS basically neglecting to informing apps when the connection is restored. I can't say if this is the same exact issue the original poster and others faced, but in my case (Nexus 10 tablet, Android 5.1.1) it took around 18 minutes to reconnect to the socket and for the setValue() request to complete after an initial internet disconnect (when no change in wifi network connection occurs).  Restarting the app or disabling and re-enabling wifi appears to fix it. I managed to get some additional info about this issue from one of our users: sees like that using an Android device on wifi over a certain wifi router/modem (Sagemcom F-3686ACv2) from a certain Finnish mobile carrier (DNA) caused Firebase's websocket connection to not work. HTTP traffic worked normally through the same wifi. This issue was ""solved"" by rebooting or resetting the router. Unfortunately I don't have logs about this case.  While the blame in this case falls to the router and/or carrier, it's very unfortunate that this happens for Realtime Database usage only. And it's weird that this seems to affect Android only. I wish I could even get an error from Firebase SDK in these cases (currently no callbacks are fired when connection does not work). Our usage of Realtime database is read-only."
technical," I've also encountered this problem before and only resetting the customer's router ended up working. Writes over wifi to another REST API were working during this period, but setValue() to my Firebase RTDB were not. I unfortunately don't have the logs from that period."
technical,"Thank you for your information. Sadly I don't have times, so I would switch to firestore instead as of now it release unity SDK Is there any alternative to solve this problem?"
technical,"I let the setValue() run twice and both times it recovered after around 18 minutes. I'm happy to email you the entire stacktrace if that's helpful, it looks like there's some relevant stuff going on there. katowulf and I connected offline and this is expected behavior due to the Android OS basically neglecting to informing apps when the connection is restored. I can't say if this is the same exact issue the original poster and others faced, but in my case (Nexus 10 tablet, Android 5.1.1) it took around 18 minutes to reconnect to the socket and for the setValue() request to complete after an initial internet disconnect (when no change in wifi network connection occurs).  Restarting the app or disabling and re-enabling wifi appears to fix it."
technical,"Thanks for reporting, Thaina. From what I understand, Realtime Database works on mobile/cellular connection properly, but not on Wi-Fi connection.  We usually mention to developers that the issue may be isolated to certain network providers that possibly blocks access to certain services. However, if this isn't the case, then there may be something else that is causing it. With that, I would like to ask for the following details in order for me to understand the issue further:  - A set of verbose logs captured upon running your app through your IDE (can be found in your Logcat)  - To capture them, you need to enable the debug logging after getting an instance by using the following line - Any other information that would be helpful On monday I try to build and find the log but cannot see anything related to firebase database. It just silence  I have upgrade to 6.11 yesterday and will try to log and search for problem again soon  But one thing I would like to report is, while realtime database face the problem from wifi. Firestore (I have alpha privilege) can listen to change from that wifi normally. Is there anything related to network connection system difference between these two library?"
technical,"Chiming in since this seems to be an issue in our usage of Realtime Database too. For years already, we have received support requests from Android app users who are not receiving data from RTDB. I've tried many things over the years to mitigate this, but never have found a way to fix the issue completely. Switching away from wifi usually fixes the situation. It's not a very widespread issue, but one that has been very hard to reproduce. Common symptoms include: - Only affects our Android app, not iOS (which is accessing the exact same database) - Other apps / internet connection in general works on the phone, it's just RTDB things that fail - Seems to affect devices from many manufacturers (Samsung, OnePlus, Huawei, Nokia) - Has been a consistent problem over many Android versions (7-10) One thing I am going to try (which I already do in another version of my app, for different reasons) is to create a dummy keep-alive setValue() write to my RTDB every 5 minutes or so. I've never gotten a report from that version of my app that the connection has broken, so I am wondering if it has something to do with it.  **Update:**  I was actually able to reproduce this, or some version of this, it seems. I connected my test device to my phone's hotspot wifi connection and then turned off the LTE mobile data. I sent my setValue() request with an onComplete listener and the connection hangs. DatabaseError is never thrown. I then re-enable the mobile LTE data on my hotspot device - subsequent attempts also fail, DatabaseError is never thrown so the else statement never runs. I opened a browser and confirmed internet connection is present. Restarting the activity in the reconnected state did not help as well.  The fix was to disable and re-enable wifi on the device."
technical," Thank you for sharing the logs. It looks like the error might be a fundamental issue with the device's network (Unable to resolve host ""s-usc1c-nss-246.firebaseio.com"": No address associated with hostname).  Can you let us know if this problem occurs on multiple devices? If so, would it be possible to share a reproduction for us that we could run on Emulator?"
technical,"Firestore and RTDB uses a very different networking stack. Firestore uses GRPC, whereas the RTDB used WebChannel which more heavily relies on Android's built in networking.  If you are able to reproduce this somewhat reliable in an emulator, then we could probably come up with a reproduction. It's a bit tough to diagnose issues that point to issues with the DNS resolver from afar. Thank you for your information. Sadly I don't have times, so I would switch to firestore instead as of now it release unity SDK"
technical," Thanks for reporting, Thaina. From what I understand, Realtime Database works on mobile/cellular connection properly, but not on Wi-Fi connection.  We usually mention to developers that the issue may be isolated to certain network providers that possibly blocks access to certain services. However, if this isn't the case, then there may be something else that is causing it. With that, I would like to ask for the following details in order for me to understand the issue further:  - A set of verbose logs captured upon running your app through your IDE (can be found in your Logcat)  - To capture them, you need to enable the debug logging after getting an instance by using the following line - Any other information that would be helpful"
technical,"I managed to get some additional info about this issue from one of our users: sees like that using an Android device on wifi over a certain wifi router/modem (Sagemcom F-3686ACv2) from a certain Finnish mobile carrier (DNA) caused Firebase's websocket connection to not work. HTTP traffic worked normally through the same wifi. This issue was ""solved"" by rebooting or resetting the router. Unfortunately I don't have logs about this case.  While the blame in this case falls to the router and/or carrier, it's very unfortunate that this happens for Realtime Database usage only. And it's weird that this seems to affect Android only. I wish I could even get an error from Firebase SDK in these cases (currently no callbacks are fired when connection does not work). Our usage of Realtime database is read-only. The glimpse of the cause is in this comment  ""The network stack"" that rely on underlying android network library is the root cause, there was many bug about networking in android OS itself, not only firebase but facebook SDK also suffer from another android networking library bug and google themselves never ever had publicly announcement to fix them  FireStore use GRPC instead and they avoid those android bug"
technical,"I've the same issue. I've tried revoking and granting access with Github and login with email and Google, but nothing. I hope you solve it soon. Thanks ### Chances are you have a duplicate account. **How to fix?**   #### Please email us at supportfreecodecamp.org and we will get this resolved for you. **But why would I have duplicate accounts?**  Back in June 2018, we had updated our platform. At that time, a bug had caused account duplications for some users. With the recent Oct, 2019 update to the platform, we have identified that the bug was fixed but the account duplications remain in the database. These unfortunately have to fixed by our team manually. Users have progressed with the curriculum since last year and a programmatic fix may not be possible. None of your progress is gone or lost. They are simply lying in a different account with the same email address or username attached to it.  **Does this affect how I login? Can I use GitHub or other logins** We simply rely on the email address that is received by us, as long as GitHub, Facebook, Google are returning the same email you have with us you will be signed into your correct account. Thanks for your patience, and understanding with this. The newer platform is more resilient and we look forward to your feedback"
technical, Chances are you have a duplicate account. Please email us at supportfreecodecamp.org and We will try and get this resolved for you.
technical,"### Chances are you have a duplicate account. **How to fix?**   #### Please email us at supportfreecodecamp.org and we will get this resolved for you. **But why would I have duplicate accounts?**  Back in June 2018, we had updated our platform. At that time, a bug had caused account duplications for some users. With the recent Oct, 2019 update to the platform, we have identified that the bug was fixed but the account duplications remain in the database. These unfortunately have to fixed by our team manually. Users have progressed with the curriculum since last year and a programmatic fix may not be possible. None of your progress is gone or lost. They are simply lying in a different account with the same email address or username attached to it.  **Does this affect how I login? Can I use GitHub or other logins** We simply rely on the email address that is received by us, as long as GitHub, Facebook, Google are returning the same email you have with us you will be signed into your correct account. Thanks for your patience, and understanding with this. The newer platform is more resilient and we look forward to your feedback Closing as stale. Please read the comment above if you are having issues with your account. Thanks and happy coding"
technical,"Closing as stale. Please read the comment above if you are having issues with your account. Thanks and happy coding I'm trying to login for the last 3 days, using my google/github account but everytime getting the message: ""Oops! Something went wrong. Please try again in a moment."" I've tried clearing cache/cookies/incognito. But nothing is working. Also getting the same error when starting a project task."
technical,"Chances are you have a duplicate account. Please email us at supportfreecodecamp.org and We will try and get this resolved for you. I've the same issue. I've tried revoking and granting access with Github and login with email and Google, but nothing. I hope you solve it soon. Thanks"
technical,"your tone and style of commenting is violating our Code of Conduct. We have marked the problematic comments as abusive and blocked you for 7 days. If this behavior persists, we're going to block you permanently.  It's OK to be passionate but attacking another open source project for any reason is not okay. This is not the kind of community we want to have and such behavior will not be tolerated. ### Description  A multi-platform app UI toolkit with the name of   maui   was introduced by Microsoft on May 19th, 2020 according to this.   **MAUI** simplifies the choices for .NET developers, providing a single stack that supports all modern workloads: Android, iOS, macOS, and Windows. The native features of each platform and UI control are within reach in a simple, cross-platform API for you to deliver no-compromise user experiences while sharing even more code than before.  There has been maui linux for a long time:  According to Wikipedia, Maui Linux has been around sine 2016. Also, there has been MauiKit for a long time: There is clearly a name clash.  ### Expected Behavior Microsoft chooses names not already used in the Linux community.  ### Actual Behavior Microsoft chooses names already used in the Linux community."
technical,"I hope this is not Microsoft's general attitude towards open source projects these days... Because it's a recent project. It has been under development for two years, but it's public. It was - until today - literally the first result when searching for ""maui framework"". Microsoft's Maui has 21 issues. 10 issues bad, 21 issues good?"
technical,"this is less about a legal issue, but more or less about a name conflict.  ... not sure if you expect us all to email you now, but please be prepared... Email me. Stealing is not ok and has not been intended. Happy to work through conflicts."
technical,"Imagine trying to put up a facade with mslinux, then turning stuff into a dick size competition with repo stars and other stats because you know you messed up and don't want to admit it lmfao He is not affiliated with MS at all, but he definitely has done more for the .NET Ecosystem than anyone else in these comments. 3 million downloads"
technical,"There is Qt Core, and somehow Qt Core and .NET Core don't read as similar as ""MAUI"" and ""MAUI"" do.  I presume you mean ""MauiKit"" and  "".NET Multi-platform App UI (MAUI)"" Hi all, the official legal name is .NET Multi-platform App UI and MAUI is an acronym, code name. This has been through legal review.  I don't believe this is the forum for this discussion. If there are ongoing concerns, please email me and we can loop in the appropriate teams."
technical,"Beautiful! You steal name and say that it's OK! Good position! I don't believe this is not the forum for this discussion. If there are ongoing concerns, please email me and we can loop in the appropriate teams.  If you don't believe this is not the forum, why close issue?"
technical,"In the announcement, it is referred to as MAUI most of the time. Obviously, this is how developers are going to reference it, and google it, and mention it in other resources. Maui Project's announcement from 4 days ago refers to apps powered by the technology as ""Maui Apps"". Thank you. In addition to all the other problems ""MAUI"" is not google/bing-able... it brings up the island. ""Xamarin Forms"" is, ""DotNetUI"" would be, ""MAUI"", not so much."
technical,"Why close issue when you ""don't believe this is not the forum""  Isn't that the exact opposite thing that any sane person would do? In the announcement, it is referred to as MAUI most of the time. Obviously, this is how developers are going to reference it, and google it, and mention it in other resources. Maui Project's announcement from 4 days ago refers to apps powered by the technology as ""Maui Apps"". Thank you."
technical,"Let's pretend that this is not a mirror but the official repository (not so).. can a project name be stolen because it has less fame? It's a non issue because things are allowed to have the same or similar names. To be honest, I never heard of the others except for the location.  No matter the name, there is probably someone or something using it."
technical,"They even refer  themselves  to Hawaii. Check, its commit message says, ""Aloha fix"". I don't think they took into account either confusion with the ""Linux project"" *nor with the Hawaiian island*. (By the way, is it seriously necessary to mention that when there's an obvious name conflict with some other free software project in the exactly same domain (UI toolkit)? Come on, get over this ""Linux vs. Windows"" mantra you still seem to follow.) Edit: yes, this was half sarcasm, half serious comment. I think you can figure out which half's which. Let's pretend that this is not a mirror but the official repository (not so).. can a project name be stolen because it has less fame?"
technical,"Nothing about this thread is constructive. Maybe because ""Core"" is not a marketed Microsoft product name, but a component of a marketed product named "".NET""? There is Qt Core, and somehow Qt Core and .NET Core don't read as similar as ""MAUI"" and ""MAUI"" do."
technical,You're obviously not serious. Good constructive argument. Nothing about this thread is constructive.
technical,"so you like this cute little side project... but let's be honest here when it comes to being a serious project it's not one. There isn't major support for MauiKit, the likelihood is that most people probably never heard of it before today, and there is no widespread adoption of it. Can you name 10 large companies that have adopted it? Your attitude is as if Microsoft is being some big evil corporation. The reality is I highly doubt anybody heard of MauiKit, and frankly nobody cares. Ok let's look at kde... there are 10 open issues... are you seriously going to tell me that a serious cross platform framework has 10 open issues... that's not realistic in any shape way or form. So again your complaints are just being ridiculous. Since the issue was opened a mere 60 minutes ago‚"
technical,"And this is not even a confusing  similarity . We are talking about a multi platform framework called MAUI, and a multi platform framework called MAUI. You could've at least googled the name you were giving to your product before doing so, or choose one that's used from a product with a different scope.  KDE is one of the very biggest community in linux. Maui is part of KDE. So much for Microsoft  Linux. Somehow all other cross-platform frameworks I can think of have managed to choose a name that would not conflict with other similar projects. I wonder what's so different here that would prevent Microsoft from finding a scope-unique name."
technical,"This is a tiny amount, big projects have bigger issue count, and this is OK Why you even pointing on this? It is unethical, you are don't have any arguments?  P.S. replying in epic thread That is great, but I did not ask that. If no representative wants to even  talk   about this, so be it. I'll take that as Microsoft and Xamarin's official position regarding this issue."
technical,That's because GitHub stars are not a currency in which you measure a project that is not even developed on GitHub. That's because it's a mirror. The actual project is developed in KDE gitlab instance.
technical,"projects do have value based on the value that they provide. The most notable measure of that is the community around that project. You might not like hearing it but MauiKit doesn't seem to have a community. It looks like a side project. I asked before Can you name 10 large companies that have adopted it? and you didn't or couldn't.  MauiKit whether you want to hear it or not has no fundamentals of what I as an Open Source Author and Maintainer would consider signs of a healthy project. I do not see a community around that project at all. I do not see adoption within the enterprise. What I do see is a bunch of people who want to complain but probably never have used Xamarin and probably never will use Maui regardless of what it's called. There is Qt Core, and somehow Qt Core and .NET Core don't read as similar as ""MAUI"" and ""MAUI"" do.  I presume you mean ""MauiKit"" and  "".NET Multi-platform App UI (MAUI)"""
technical,"There appear to be more passive-aggressive comments towards another project in this thread from you alone than there are from all other people towards this .NET repository. That's not because others really love .NET, but because they try to stay constructive here. I am afraid this is not how most open source maintainers respond in a community. Unless MauiKit has a „ on the name, I can't see how this is at all an issue."
technical,I'm surprised no one complained about Microsoft's use of the name core You're obviously not serious. Good constructive argument.
technical,"We could make sysupgrade=False the default, but then we'd have a lot  more  people complaining about broken systems from unsupported partial upgrades (though, likely with far more tact than you displayed). Please read this article from the ArchWiki so that you can better understand the reason for making sysupgrade=True the default. Have a nice day! ### Description of Issue/Question On Arch Linux running pkg.install vim refresh=true will upgrade the entire OS instead of just installing the latest version of vim. This does not follow the other package managers and becomes unintuitive when dealing with multiple Linux distros.  This behavior is documented. However, It doesn't follow the standard that other pkg modules use for full os upgrade. ## Setup. want vim, get os upgrade."
technical,"Yes, that is correct That should be the defaults on arch, if you refresh, you need to do an upgrade, otherwise you end up with broken so names because pacman does not resolve upgrades to sonames and force other packages to upgrade.  If you want to do only a refresh and install, and not upgrade you can end up with an unbootable system, but you can do this by setting sysupgrade=False on the commandline with the refresh=true Here is the story that archlinux maintainers and people in #archlinux on freenode give to people that -Sy instead of -Syu."
technical,"By not doing what the command line tools do, you are varying from the expected behavior and breaking convention. THIS IS WRONG. Your belief that you're doing anyone a favor by varying from the expected command line tool behavior, you should turn in your commit per.issions and get a job in management where you don't have access to break things AGAIN. We could make sysupgrade=False the default, but then we'd have a lot  more  people complaining about broken systems from unsupported partial upgrades (though, likely with far more tact than you displayed). Please read this article from the ArchWiki so that you can better understand the reason for making sysupgrade=True the default. Have a nice day!"
technical,"By not doing what the command line tools do, you are varying from the expected behavior and breaking convention. THIS IS WRONG. Your belief that you're doing anyone a favor by varying from the expected command line tool behavior, you should turn in your commit per.issions and get a job in management where you don't have access to break things AGAIN. Yes, that is correct That should be the defaults on arch, if you refresh, you need to do an upgrade, otherwise you end up with broken so names because pacman does not resolve upgrades to sonames and force other packages to upgrade.  If you want to do only a refresh and install, and not upgrade you can end up with an unbootable system, but you can do this by setting sysupgrade=False on the commandline with the refresh=true"
technical,"This issue will be auto-closed because there hasn't been any activity for a few months. Feel free to open a new one if you still experience this problem ### Feature Request I know this has been requested many times but the status of it is a bit unclear to me.  1. Is it possible to do this? In #8682 KrauseFx mentioned that:  Mh, I don't think that's gonna work, as all Ruby files are already loaded at that point by bundler. You'd have to run bundle update after importing the file, which is not something fastlane can do on its own as far as I know.  What about fastlane require? It could be used to install a plugin gem - not sure whether it contradicts a  Gemfile  or what other steps are needed.  Anyway, if this is not possible, can we add a note in import from git description with reasoning if any? Maybe it would be great to add a warning when import from git detects a  Pluginfile  to remind that plugins should be installed/updated. I think it would follow  fastlane philosophy .  Or  fastlane require plugin  could be added in order to fail if there is no entry with a provided plugin name in a project  Pluginfile ?  2. If this can be done then maybe someone is already working on it? Maybe  fastlane community  could help? In #7636 lacostej tried to discuss an idea to solve this but with no answer. Any chance to do this in this thread?  #### Motivation Behind Feature  Why should this feature be implemented? What problem does it solve? -- Updating a  shared Fastfile  with usage of a plugin requires an update in a  Pluginfile  in each project importing this  Fastfile  (or also in a  Gemfile  if plugins were not used before). This is a bit inconvenient and not a safe way of introducing changes.  import from git should import a fully configured, standalone  shared Fastfile  with all dependencies defined there and/or other files.  #### Alternatives or Workarounds  Describe alternatives or workarounds you are currently using --  Are there ways to do this with existing actions and plugins? -- proposed an interesting workaround."
technical,"It seems like you have not included the output of fastlane env  To make it easier for us help you resolve this issue, please update the issue to include the output of fastlane env 1. Is it possible to do this? In #8682 KrauseFx mentioned that: Mh, I don't think that's gonna work, as all Ruby files are already loaded at that point by bundler. You'd have to run bundle update after importing the file, which is not something fastlane can do on its own as far as I know.  What about fastlane require? It could be used to install a plugin gem - not sure whether it contradicts a  Gemfile  or what other steps are needed.  I can give some insight on that:  Pluginfile is not similar to Deliverfile and other configuration files: When you add a plugin, some code is added to your Gemfile that includes Pluginfile there. Gemfile (and the included information from your Pluginfile) is used to define which gems to install when you run bundle install on your project.  So to implement a method like the one you want, at runtime in fastlane, you would then have to change both the Gemfile to include the new imported data, and run bundle install. It would change files on the file system and also.   Anyway, if this is not possible, can we add a note in import from git description with reasoning if any?  Documentation of that actions says:  Import another Fastfile from a remote git repository to use its lanes  That is pretty clear that this action is for importing Fastfiles.  Maybe it would be great to add a warning when import from git detects a  Pluginfile  to remind that plugins should be installed/updated. I think it would follow  fastlane philosophy .  What do you mean by that? How should this work?  Or  fastlane require plugin  could be added in order to fail if there is no entry with a provided plugin name in a project  Pluginfile ?  Doesn't this already happen when you use an action of a plugin, that is not installed?  That being said: You can of course create an action in your shared Fastfile that does exactly what I described above. Download some file, edit Pluginfile (or create it if it doesn'T exist) or add it additionally to Gemfile, then run bundle install using the sh() action. That would effectively ""force-install"" a plugin when your imported Fastfile is used. Would that solve your problem?"
technical,"I think the warning could be added somewhere here. Would you be up to create a PR for us to review? Hey, I locked this issue for now. Even though maybe a comment by janpio might have come across as rude, every fastlane contributor here does this in their free time to help other people. Time is limited, and spending time on other developer's issues is usually hard to justify. It's not cool you mentioned and tried to include a random set of other contributors in the hope they'd help out. As you can see,  fastlane  is a highly active, and widely used open source project, and there is no possible way to reply to every single issue. I wrote about the problems of how difficult it is to scale open source communities over here."
technical,Thanks for a great reply! And the issue starts to go somewhere immediately. I think the warning could be added somewhere here. Would you be up to create a PR for us to review?
technical," It seems like you have not included the output of fastlane env  To make it easier for us help you resolve this issue, please update the issue to include the output of fastlane env"
technical,"I posted a reply in the heat of a discussion. Then I quickly deleted it, as I recognized that it was not a helpful reaction and wouldn't help advance the conversation. Now you post a selected part of that deleted comment again. This is highly inappropriate.  Do you really wonder why I don't want to spend any more time helping you?  Mentioning other contributors that have not participated in the discussion is inappropriate as well, by the way. Personally, I understand both sides.  One thinks it would be obvious for plugins to ""just work"" too, while I clearly understand why is not feasible, and I'd argue that it's actually incorrect. You use a dependency manager to install the required dependencies and then run something that uses them, it would be weird for the thing you run to install dependencies for you.  Knowing this, another way to improve this is what Skoti already mentioned, and which I always love: automatically prevent incorrect usages. So the current options seems to be: 1. Warn users (UI.important) when they import from git and a Pluginfile also exists in that remote. 2. Create a separate action (ensure plugins?) that ensures that all the required plugins (defined in the local and in the remote Pluginfiles) are available (could be ""harder"" because of this) And as always, PRs welcome!"
technical,"1. Is it possible to do this? In #8682 KrauseFx mentioned that: Mh, I don't think that's gonna work, as all Ruby files are already loaded at that point by bundler. You'd have to run bundle update after importing the file, which is not something fastlane can do on its own as far as I know.  What about fastlane require? It could be used to install a plugin gem - not sure whether it contradicts a  Gemfile  or what other steps are needed.  I can give some insight on that:  Pluginfile is not similar to Deliverfile and other configuration files: When you add a plugin, some code is added to your Gemfile that includes Pluginfile there. Gemfile (and the included information from your Pluginfile) is used to define which gems to install when you run bundle install on your project.  So to implement a method like the one you want, at runtime in fastlane, you would then have to change both the Gemfile to include the new imported data, and run bundle install. It would change files on the file system and also.   Anyway, if this is not possible, can we add a note in import from git description with reasoning if any?  Documentation of that actions says:  Import another Fastfile from a remote git repository to use its lanes  That is pretty clear that this action is for importing Fastfiles.  Maybe it would be great to add a warning when import from git detects a  Pluginfile  to remind that plugins should be installed/updated. I think it would follow  fastlane philosophy .  What do you mean by that? How should this work?  Or  fastlane require plugin  could be added in order to fail if there is no entry with a provided plugin name in a project  Pluginfile ?  Doesn't this already happen when you use an action of a plugin, that is not installed?  That being said: You can of course create an action in your shared Fastfile that does exactly what I described above. Download some file, edit Pluginfile (or create it if it doesn'T exist) or add it additionally to Gemfile, then run bundle install using the sh() action. That would effectively ""force-install"" a plugin when your imported Fastfile is used. Would that solve your problem? Pluginfile is not similar to Deliverfile and other configuration files: When you add a plugin, some code is added to your Gemfile that includes Pluginfile there. Gemfile (and the included information from your Pluginfile) is used to define which gems to install when you run bundle install on your project.   So to implement a method like the one you want, at runtime in fastlane, you would then have to change both the Gemfile to include the new imported data, and run bundle install. It would change files on the file system and also  Probably that would still not work? When running fastlane command via bundle exec no other gem can be loaded as  bundler  disallows loading gems that are not in the bundle. So when import from git is called in  fastlane  runtime,  bundler  runtime environment has already been created and won't allow any other gems to be loaded.  However, the only solution that comes to my mind, to make it work with  bundler , would be to nest another call to bundle exec but with a temporary Gemfile. This temporary  Gemfile  would include all the entries from a  project   Gemfile / Pluginfile  and all the entries from an  imported   Gemfile / Pluginfile  (as  Gemfile  is actually calling eval gemfile on a  Pluginfile  so it is like those entries would be in a  Gemfile ). No modification of a current  Gemfile  or any other file is needed.  But this is  **not a good** idea ,), and it would only work if import from git was at the beginning of a  Fastfile  - it would be the most convenient to just call bundle exec with the same command and parameters as the original one. One caveat of it is that it nests with each import from git call over and over again... The other one is that if import from git is not placed at the top of a  Fastfile  it means some lanes or anything else might already been executed and would require to somehow reuse this  fastlane  state in a new bundle exec call... Terrible idea that is why I am hiding it in the details ˜  So I guess it **cannot** be done due to usage of  bundler . **But** if one is **not using**  bundler  in a project (and hence both  Gemfile  and  Pluginfile ), then they could just add plugins via fastlane require action in a project  Fastfile  or in an imported  Fastfile . This can be done in the current version of  fastlane  and it works. Anyway, if this is not possible, can we add a note in import from git description with reasoning if any?   Documentation of that actions says:    Import another Fastfile from a remote git repository to use its lanes   That is pretty clear that this action is for importing Fastfiles.  Exactly, that is why I was surprised that it imports all the files of custom actions as well and not only a  Fastfile  ,). It is not even mentioned. And as plugins are just actions this is probably why people think that remote plugins should be imported or actually expect them to be imported implicitly like in a case of custom actions. Besides, searching for issues regarding import from git and  Pluginfile  yields a lot of results so it seems like it is not so obvious for people and they constantly ask to support  Pluginfile  in import from git. Hence, I think an explanation with a good reasoning would be welcomed. Maybe it would be great to add a warning when import from git detects a  Pluginfile  to remind that plugins should be installed/updated. I think it would follow  fastlane philosophy .   What do you mean by that? How should this work?  What I meant is that when import from git detects a  remote   Pluginfile  (meaning in the same directory as the  Fastfile  it tries to import) it could print a warning listing the plugin names and saying that in order to use this  remote   Fastfile  these plugins are required to be in a  local   Pluginfile . It could even compare a  local   Pluginfile  with a remote one and throw an error when it detects that plugins from a remote are not included in the local one. It would be very informative and helpful. Or  fastlane require plugin  could be added in order to fail if there is no entry with a provided plugin name in a project  Pluginfile ?  Doesn't this already happen when you use an action of a plugin, that is not installed?  Yes I know that calling a plugin action would end up with an error but a usage of such an action might be somewhere far down the lanes ˜„  i.e. after a long build or anything else. I think it is better to prevent such a  Fastfile  from running at all as it will almost certainly finish with an error - so why wait for that? This is just another way of prevention I have described above. This time using a fastlane require plugin as an action in a  remote   Fastfile . Instead of adding a  remote   Pluginfile  (like in a case above and handle that  Pluginfile  in import from git) you could specify directly in a  remote   Fastfile  that some plugins are required via fastlane require plugin action - this would work like in a case above - check if these plugins are in a  local   Pluginfile  and then throw an error if missing.  Both of these preventions of course make sense if one is using  bundler ,  Gemfile  and  Pluginfile . Otherwise it is enough to use the current fastlane require action and it will install a particular plugin gem (if necessary) upon running any lane from such a  Fastfile . So now both usages are handled.  That being said: You can of course create an action in your shared Fastfile that does exactly what I described above. Download some file, edit Pluginfile (or create it if it doesn'T exist) or add it additionally to Gemfile, then run bundle install using the sh() action. That would effectively ""force-install"" a plugin when your imported Fastfile is used. Would that solve your problem?  Like I said above, if I use  bundler  it is almost impossible (see details at the top). If I don't use  bundler  (and thus neither  Gemfile  nor  Pluginfile ) I can just require my plugins via fastlane require and they will be installed if necessary."
technical,"Well I didn't feel of this as being a heated discussion. So excuse me if this is how you felt. Anyway, it is nice to know now that you have realized it was a bad move and deleted the comment. However, I quoted the whole part not a selected one as you suggest.  And I do not think that looking for help in other contributors was inappropriate in that case as you clearly didn't want to take care of this issue. And now you make another comment in a similar style ˜ :  Do you really wonder why I don't want to spend any more time helping you? Seriously, have I done something to you? Thanks for a great reply! And the issue starts to go somewhere immediately."
technical,"Hey, I locked this issue for now. Even though maybe a comment by janpio might have come across as rude, every fastlane contributor here does this in their free time to help other people. Time is limited, and spending time on other developer's issues is usually hard to justify. It's not cool you mentioned and tried to include a random set of other contributors in the hope they'd help out. As you can see,  fastlane  is a highly active, and widely used open source project, and there is no possible way to reply to every single issue. I wrote about the problems of how difficult it is to scale open source communities over here. There hasn't been any activity on this issue recently. Due to the high number of incoming GitHub notifications, we have to clean some of the old issues, as many of them have already been resolved with the latest updates. Please make sure to update to the latest fastlane version and check if that solves the issue. Let us know if that works for you by adding a comment"
technical,"There hasn't been any activity on this issue recently. Due to the high number of incoming GitHub notifications, we have to clean some of the old issues, as many of them have already been resolved with the latest updates. Please make sure to update to the latest fastlane version and check if that solves the issue. Let us know if that works for you by adding a comment This issue will be auto-closed because there hasn't been any activity for a few months. Feel free to open a new one if you still experience this problem"
technical,"No. When I locked one thread I didn't mean for others to spawn. ### Is your feature request related to a problem? Please describe. I have been watching/waiting few yrs. patiently, however some recent changes have forced me to request -- Microsoft  revisit  this for their developer community, keeping in mind some of the pricing out there compares to the salaries of 10 to 15 developers in Bangladesh, Vietnam, India, Philippines etc.  **Core & Basic need, yet very complex and critical.**  There's a  void in .NET native drop-in solution, unlike the Java, PHP Stacks etc , where there are  many native drop in Identity Administration & Management Frameworks  options that are leveraged across all their platforms seamlessly by developers, for e.g. the J2EE Pluggable Identity Management Framework or JSR 168 or JSR 286 or JSR-351 or JSR-375.  Why is this important? because using Java or PHP, it has allowed easy, clear, core and basic functionalities in those native stacks. As a result if you look JSR 168 or JSR 286 or JSR-351 or JSR-375[Multi-tenants, Group to roles, or to claims vice versa is so easy vs. NET , mobile devices, portals, they all work seamlessly and cohesively with security fixes managed by either IBM or SalesForce or Oracle or Red Hat etc. This is enables developer to be productive right out of the gate.  In .Net there is void/very limited support, always requiring a combination of 3rd parties as a barrier to entry & adoption of ASP app. This is non-trivial for developers and security vulnerability that requires the attention of Microsoft Experts.  Example: We have private information sharing site non OSS for the community almost free (pay if you want), and when we started with web forms, then Simple Membership, the Identity, Identity 2 ASP MVC we had implement much of it on top of these from scratch, when we moved to .Net Core it was another effort. Besides the development there was a lot of confusion on the internal concepts and how they now meant new things. Roles, Claims, Federation, SAML then SAML 2.0 and then Open ID 2.  ### Describe the solution you'd like  1. A drop-in is extensible solution that supports ASP Identity eco-system, Administration, UI, SSO, Token, Multi-tenant Management 2. A configuration section to turn on-off the various features. 3. Embedded into VS Templates (remove Identity Server etc.)  ### Additional context The current options and the pricing eliminate many of the existing applications from continuing usage on the .NET stack without extensive retooling or completely abandoning the MS framework.  **Its really high time MS address this core gate-keeping feature!!**"
technical, No. When I locked one thread I didn't mean for others to spawn.
technical,"I would have loved to work with you, but since you were not willing to accept any recommendations, I am closing this and we will do any remaining work here. ### What does this PR do? Improves the error message when Python version mismatch detected by providing solutions.  ### What issues does this PR fix or reference? See below.  ### Previous Behavior (Python3 origin - Python2.7 target)  ### New Behavior (Python3 origin - Python2.7 target)  ### Tests written? No ### Commits signed with GPG? No Please review Salt's Contributing Guide for best practices. See GitHub's page on GPG signing for more information about signing commits with GPG."
technical,"I would have loved to work with you, but since you were not willing to accept any recommendations, I am closing this and we will do any remaining work here. actually ""approved"" here means  everyone  agrees, JFYI. And your google result is suggesting exactly what I mean."
technical,"i'm checking first for major.minor, so if we add 3.8 it will go to that first. only if there is not major.minor it will match major.x. i think we are fine here. correct. So that means there **is something** that needs to be shown  instead of  the default message. And if you have none of '8', then you will get the default one. P.S. keep the code, replace the messages. :wink:"
technical,"I would still minimise the text according to the example I showed above. As well as the ""3.x"" thing. having 3.x there means that it matches all the 3 subversions. if I remove the .x someone might think it would only apply to 3 and this would make it more confusing. Adding new line to the text is ignored and I'm not sure I should spend more time on this. Your example seems to remove some of the useful information and has this ""on a Master"" that I don't understand. Is that intended or should it be ""on the Master""? I was using origin and target in order to avoid Master and Minion because none of these machines have salt-master or salt-minion running."
technical,"I would only say that ""target"" is that part between salt and the command... More here. So calling it ""target"" and ""origin"" brings lots of confusion from this POV.  I would definitely keep it Master, especially if SaltSSH is ""to execute salt commands and states over ssh without installing a salt-minion"". Which implies ""Master"" at first place! i am not an expert in salt terminology. I just proposed what I thought it would be appropriate and so far saltstack did not disagree. sure, please propose better messages. the changes were already approved so why should i make more changes to the PR or invest more time into it?"
technical,"actually ""approved"" here means  everyone  agrees, JFYI. And your google result is suggesting exactly what I mean. I have tested my changes on different combinations. It would not make any sense to make additional changes and test the whole thing again with python2.6, 2.7 and 3 and combinations of them just because someone likes it more is some way. (i'm not talking here about the formulation of the messages, that is a valid change that is worth doing). but changing some structure to another structure just because... is not enough reason to redo the whole testing again."
technical,"One note: sometimes, when targeting 1000 machines, maybe 999 are ok but one has an old python. there are two options and we don't know what it is better for the user: - update the origin machine (that works on 999 of the clients) or - update the one client that doesn't work  Other than that, this is open source, so I don't have any problem with adapting my PR to whatever is a better fit for saltstack/salt. I would only say that ""target"" is that part between salt and the command... More here. So calling it ""target"" and ""origin"" brings lots of confusion from this POV.  I would definitely keep it Master, especially if SaltSSH is ""to execute salt commands and states over ssh without installing a salt-minion"". Which implies ""Master"" at first place!"
technical,"i've fixed the lint errors but now, after rebase, it seems to have some unrelated lint errors in the tests. Bo and Daniel already provided an explanation. sorry, for not being clear enough about this in the description. I would still minimise the text according to the example I showed above. As well as the ""3.x"" thing."
technical,"the ""3.x"" is the same as ""3"", as I see it. I also suggested to use integers as in the version info instead of just strings. And so if we have some issue with the 3.8 (e.g.) one would just add a minor version key. Otherwise default should go. But ""3.x"" is more to me looks like a hack. The ""origin"" is something that might not be understood. Personally to me it is very odd terminology here. And you are running ""SaltSSH"" from the Master (or want-to-be-master). i'm checking first for major.minor, so if we add 3.8 it will go to that first. only if there is not major.minor it will match major.x. i think we are fine here."
technical,"I've forgot to change the last error message, which is ""invalid Python"". That just misleads people and they don't understand what is going on, hence the fix. Salt SSH still needs to have both library sets (dependencies) on the Master for the specific Python versions, so the .tar.gz is carrying over those in py<major version subdirectories. NOTE: actually if you are running Salt on a Master from the specific version, you  do not  need to install Salt again for the alternative version, as it is anyway works for both versions. This is just packaging convenience. But in fact you need only couple of version-specific libraries to be installed so they will be picked up by the thin creation procedure. That is, probably we should not say ""install Salt for alternative Python X"" (which implies you will get all the needed dependencies automatically), but ""install Salt  dependencies  only for the alternative Python X"". nope, this is only needed in Fluorine. i've fixed the lint errors but now, after rebase, it seems to have some unrelated lint errors in the tests. Bo and Daniel already provided an explanation. sorry, for not being clear enough about this in the description."
technical,"I've forgot to change the last error message, which is ""invalid Python"". That just misleads people and they don't understand what is going on, hence the fix. Salt SSH still needs to have both library sets (dependencies) on the Master for the specific Python versions, so the .tar.gz is carrying over those in py<major version subdirectories. NOTE: actually if you are running Salt on a Master from the specific version, you  do not  need to install Salt again for the alternative version, as it is anyway works for both versions. This is just packaging convenience. But in fact you need only couple of version-specific libraries to be installed so they will be picked up by the thin creation procedure. That is, probably we should not say ""install Salt for alternative Python X"" (which implies you will get all the needed dependencies automatically), but ""install Salt  dependencies  only for the alternative Python X"". nope, this is only needed in Fluorine. I've forgot to change the last error message, which is ""invalid Python"". That just misleads people and they don't understand what is going on, hence the fix. Salt SSH still needs to have both library sets (dependencies) on the Master for the specific Python versions, so the .tar.gz is carrying over those in py<major version subdirectories. NOTE: actually if you are running Salt on a Master from the specific version, you  do not  need to install Salt again for the alternative version, as it is anyway works for both versions. This is just packaging convenience. But in fact you need only couple of version-specific libraries to be installed so they will be picked up by the thin creation procedure. That is, probably we should not say ""install Salt for alternative Python X"" (which implies you will get all the needed dependencies automatically), but ""install Salt  dependencies  only for the alternative Python X"". nope, this is only needed in Fluorine."
technical,"correct. So that means there **is something** that needs to be shown  instead of  the default message. And if you have none of '8', then you will get the default one. P.S. keep the code, replace the messages. :wink: if there is no 3.8 specific message, 3.x would be shown. is this not enough?"
technical,"you like the proposed error messages and the structure? OK, as per speaking with them it would be also an option to:  - Add all the options into the documentation. - Add short minimally intrusive to the target machine option (i.e. update the SaltSSH side w/o touching the target)  That said, CLI would return something like: The ""my-to-be-minion.greatdomain.com"" machine is running Python 2.6 version. Please install Salt for 2.6 Python on the Salt SSH machine and set it up. or:  The ""my-to-be-minion.greatdomain.com"" machine is running Python 2.7 version. Please install Salt for 2.7 Python on the Salt SSH machine. So such info on the CLI would suggest what to do now, quickly and easiest way. And the following would go to the documentation (rephrased into more extended version): ""Depending on the Python version on the target, you need to install Python2.7 compatible salt on origin to add support for Python2.7 targets or install Python2.6 compatible salt on origin to add support for Python2.6 targets or upgrade to Python==3.x on target"""
technical,"The code I exampled above does this. I am just not in favour of your structure that keeps strings that needs to be parsed instead of just direct map to versions. And your messages are too big and using foreign terminology for Salt-specific domain. And so therefore I would change that. ok. thanks for the suggestion, but if the upstream is fine with my version (changes approved) please merge the PR. thanks!"
technical,"OK, as per speaking with them it would be also an option to:  - Add all the options into the documentation. - Add short minimally intrusive to the target machine option (i.e. update the SaltSSH side w/o touching the target)  That said, CLI would return something like: The ""my-to-be-minion.greatdomain.com"" machine is running Python 2.6 version. Please install Salt for 2.6 Python on the Salt SSH machine and set it up. or:  The ""my-to-be-minion.greatdomain.com"" machine is running Python 2.7 version. Please install Salt for 2.7 Python on the Salt SSH machine. So such info on the CLI would suggest what to do now, quickly and easiest way. And the following would go to the documentation (rephrased into more extended version): ""Depending on the Python version on the target, you need to install Python2.7 compatible salt on origin to add support for Python2.7 targets or install Python2.6 compatible salt on origin to add support for Python2.6 targets or upgrade to Python==3.x on target"" One note: sometimes, when targeting 1000 machines, maybe 999 are ok but one has an old python. there are two options and we don't know what it is better for the user: - update the origin machine (that works on 999 of the clients) or - update the one client that doesn't work  Other than that, this is open source, so I don't have any problem with adapting my PR to whatever is a better fit for saltstack/salt."
technical,"having 3.x there means that it matches all the 3 subversions. if I remove the .x someone might think it would only apply to 3 and this would make it more confusing. Adding new line to the text is ignored and I'm not sure I should spend more time on this. Your example seems to remove some of the useful information and has this ""on a Master"" that I don't understand. Is that intended or should it be ""on the Master""? I was using origin and target in order to avoid Master and Minion because none of these machines have salt-master or salt-minion running. the ""3.x"" is the same as ""3"", as I see it. I also suggested to use integers as in the version info instead of just strings. And so if we have some issue with the 3.8 (e.g.) one would just add a minor version key. Otherwise default should go. But ""3.x"" is more to me looks like a hack. The ""origin"" is something that might not be understood. Personally to me it is very odd terminology here. And you are running ""SaltSSH"" from the Master (or want-to-be-master)."
technical,"if there is no 3.8 specific message, 3.x would be shown. is this not enough? The code I exampled above does this. I am just not in favour of your structure that keeps strings that needs to be parsed instead of just direct map to versions. And your messages are too big and using foreign terminology for Salt-specific domain. And so therefore I would change that."
technical,"I have tested my changes on different combinations. It would not make any sense to make additional changes and test the whole thing again with python2.6, 2.7 and 3 and combinations of them just because someone likes it more is some way. (i'm not talking here about the formulation of the messages, that is a valid change that is worth doing). but changing some structure to another structure just because... is not enough reason to redo the whole testing again. what about target in the context of rosters"
technical,"ok. thanks for the suggestion, but if the upstream is fine with my version (changes approved) please merge the PR. thanks! you like the proposed error messages and the structure?"
technical,"I have written a new proposal, #42201, to replace this one. I will now lock this issue. ### What version of Go are you using (go version)? ### Does this issue reproduce with the latest release? Yes.  ### What operating system and processor architecture are you using (go env)? Applies to all OSes  ### What did you do? 1. Mounted a UNC path as a drive letter. 2. In CMD, switched the current working directory to that drive. 3. Called filepath.Abs on a relative path. 4. Called filepath.Eval Symlinks on the result of that function.  ### What did you expect to see? The same results as calling Get Final Path Name By Handle: a UNC path.  ### What did you see instead? A path using the drive letter instead of the UNC path.  ### Notes This affects any attempt to canonicalize paths using the output of Git in such a situation.  Git produces some paths as absolute and some paths as relative, and uses Get Final Path Name By Handle for canonicalizing absolute paths.  However, Go lacks a function to canonicalize paths in a standard way, so it isn't possible to produce results equivalent to a C program and still write code that works portably across systems.  Go should add a function that is explicitly defined to canonicalize paths in a way equivalent to the underlying operating system, since using filepath.Abs and filepath.Eval Symlinks doesn't work correctly on Windows.  It does work fine on Unix, but Unix paths are much simpler and easier to reason about.  It was determined in #17084 that filepath.Abs and filepath.Eval Symlinks were sufficient in this case, but that doesn't appear to be true.  I expect there are other cases in which those don't work on Windows, but I am insufficiently versed in Windows paths to know what those are.  This was originally reported to the Git LFS project."
technical,"That's **not** what I'm suggesting. I wrote ""If the result of Get Final Path Name By Handle cannot be made relative to the current directory, then the absolute one can be returned."" I did **not** write ""The absolute one can be returned always."" By ""made relative"" I meant transformed to be relative to the current directory by filepath.Rel. Ah, I misinterpreted that. The one issue for a relative transformation is that relative paths have a max length, so some cannot be transformed, tho an app might expect it. did we investigate whether filepath.Rel is healthy on Windows?"
technical,"I disagree. I don't think we should add filepath.Resolve function that behaves like Get Final Path Name By Handle Windows API.  If someone wants to use this API, we can add Get Final Path Name By Handle and they can use it there.  I still don't understand what proposed filepath.Resolve function will do. It will be even harder for others, who are just learning this package, to understand the difference between filepath.Resolve and filepath.EvaluateSymlinks. I would like to see documentation for that function before this proposal is even considered.  filepath.Resolve is not a good name (like utils). It tells you nothing about what the function does. To me it is a good indication that you yourself do not know what this function does.  And what problem will this new function solve? Is filepath.Resolve is supposed to be a replacement (good version) for filepath.EvaluateSymlinks? If yes, let's have someone actually try and replace filepath.EvaluateSymlinks in the current Go tree and see if it still works. If that does not work, then we need to think even harder why we need filepath.Resolve, and not just golang.org/x/sys/windows.Get Final Path Name By Handle.  Thank you for consideration. PS: Sorry I did not comment earlier. But I have been busy with other staff, and missed this thread altogether. all of the questions you raised have been addressed in the comments above. Re missing the thread, you last commented here 7 days ago."
technical,"It would need to keep doing what it's documented to do, namely preserve relative-ness to current directory when possible. That means calling Get Final Path Name By Handle and fixing up the result a little. But we could still build a function around Get Final Path Name By Handle that should handle everything Windows can throw at it. Did you have a specific breakage in mind? Are you proposing that the behavior differ from Get Final Path Name By Handle (on Windows) only when the path name is relative, or when the path name is absolute as well?  The former is fine, I think, and I have no position on it, the latter would be a problem for interoperability with tools written in other languages."
technical,"Relative paths are not thread-safe. Full stop.  Eval Symlinks should not be fixed but replaced. Get Final Path Name By Handle *always* returns an absolute path. Paths with ?\ syntax are always absolute. The string that is returned by this function uses the ""?"" syntax. For more information, see CreateFile.  The CMU guidance obviously is written without considering the arguments against Eval Symlinks that I wrote up - which is based on guidance from Microsoft, so I don't quite agree with your conclusion that resolving file system driver paths is a proper thing to do while canonicalizing. I have just hastily scanned that document from CMU and I don't immediately see they say you should resolve links. They just mention links could be present for awareness.  I have written quite a bit of software and I have never considered writing checks like that. Usually path checks involve checking whether they are in some base path - for which Rel is better than Eval Symlinks. Sure you can ask for a properly cased path and/or fix slashes, but otherwise, why write string handling or comparisons at all? Paths usually come from somewhere. I mean from the working directory, from configuration or perhaps from user input in most cases. Usually those are already trusted without doing any checks. Or they are built from system settings, which are also trusted. That might be circumstantial evidence against your conclusion based on that article but still.  Also about the Git example still I think the application should be in control of which links get resolved. Some of the links that make up a path used by Git could still be system administrator controlled which makes it wrong to resolve them and the result dependant on system configuration. Like for my system - most Git paths ran through Eval Symlinks on my system will (have to) be volume GUID paths (and hence again all always absolute). But Git should never care about that or go that deep. It's wrong.  EDIT: The article ignores case-sensitivity issues by using strncmp. That is wrong as well. Most software on Windows assumes case-insensitivity, although NTFS is case-sensitive. On Linux, FAT32 is case-insensitive. As it's not possible to relativize some results of Get Final Path Name By Handle, that could be a cause of breakage if Eval Symlinks is reimplemented with it."
technical,"FWIW, RealPath is not great because it says Path in it. Real is not great because it is edit-distance-1 from another function that does something similar (Rel). I also am not sure what makes a path ""real"" in the first place.  Resolve is clearer about the action being done. Based on the discussion above, adding Resolve with the semantics described seems like a **likely accept**."
technical,"Hey, Is there any interest in fixing this?  Right now, there is no cross-platform way to canonicalize a path in Go.  We keep running up against additional cases where the existing behavior doesn't canonicalize paths properly, leading to incompatibility with other programs on the system (notably Git).  This necessarily limits the portability of using Go as a cross-platform language. By ""function to canonicalize paths"" do you mean a variation of Eval Symlinks that works on Windows? If so, note that Eval Symlinks is not recommended: #40180 (and probably can't be fixed). Go on Windows has a variety of long-standing filesystem bugs. I suggest using x/sys/windows to call the WinAPI if that solves your problem."
technical,"I gather you didn't read that whole thread. Pathnames over 250ish characters must be absolute. Eval Symlinks doesn't respect this, and correcting it would break callers expecting a relative path. If you want to improve Go on Windows, there are a wide variety of other bugs to tackle. This issue has been resolved. can you provide a link to a real world application or at least a working, compilable code snippet that would break if Eval Symlinks is fixed as originally proposed by Russ?"
technical,"I mentioned #40180 in this to suggest that the issue author reconsider canonicalization of paths. I didn't link it again later, but it documents a long list of problems with Eval Symlinks on Windows (which I've now linked). Re path length bugs, other instances of those have been left alone, see #21782 & #36375. And here's a list of Windows bugs that mention ""filepath"". Canonicalization of paths is required to properly implement any sort of Git support in a project.  More generally, it's required to determine definitively if a path is under a directory, which has a wide variety of general-purpose applications outside of Git.  Whether other people think it is useful in their projects, path canonicalization is commonly used and is almost always provided by the standard library.  Canonicalizing paths is also recommended by CMU's secure coding guidelines, while those are for C, there's no reason to think Go is any different. I agree that users typically don't want to see canonicalized paths and that path canonicalization cannot be used where there's a security-sensitive race condition, but that doesn't mean it lacks applications elsewhere, just that it's unsuitable for some use cases."
technical,"Whether it's OK to return a relative path that's not suitable for external use (e.g. a log) should also be considered. I am not worried about that problem. ""External use"" could mean adding a new path element to a relative directory name, and that might turn a short-enough path into a too-long path. So in the limit the argument would be that we should never use a relative path because of the limits. That's clearly wrong. I filed #41734 for fixing fixLongPath to handle relative paths so that we can stop having discussions about which APIs do and don't accept certain kinds of long paths. did you change any permissions on this thread? If so, could you flip them back? I asked him a Q re filepath.Rel, and he's unable to respond here now."
technical,"That's news to me. A relative path stored as a string inside a Go program should  not  have a max length. If you pass a very long path to os.Open, I thought we did the appropriate rewrite to remove the length restriction. Do we not do that for relative paths? If the Go APIs aren't handling long relative paths correctly, we should fix  that , not introduce strange inconsistencies elsewhere. EDIT: I'm not sure which (if any) of the APIs that take a path can handle a length over the Windows limit for relative paths, but these can't: Whether it's OK to return a relative path that's not suitable for external use (e.g. a log) should also be considered."
technical,"It is not a lot of code. You can, probably, just copy all the code you need from GOROOT directory - just grep for Get Final Path Name By Handle. FWIW, RealPath is not great because it says Path in it. Real is not great because it is edit-distance-1 from another function that does something similar (Rel). I also am not sure what makes a path ""real"" in the first place.  Resolve is clearer about the action being done."
technical,"I mean a function, when given a path, that returns a canonicalized version of that path.  In other words, the equivalent to realpath(3) on Unix or Get Final Path Name By Handle on Windows, and the equivalent to Rust's canonicalize. It isn't helpful to me to call the Windows API because (a) I'm not a Windows programmer and have no clue how to use it, (b) it isn't cross-platform, and (c) this is a function that is generally provided by the standard library. Go has gaps on Windows, I plug them in my code. You've seen the interest this issue evoked :-p. What you need isn't hard. Create a file named yourpkg windows.go, import, define GetCanonicalPath() to call CreateFile(""yourfile"") (to get a handle) then Get Final Path Name By Handle. Create a file yourpkg unix.go with a  +build directive for your unix platforms. Define GetCanonicalPath() with the solution for unix you already know."
technical,"It is also the case that filepath.Eval Symlinks fails to work when canonicalizing paths where there's a junction to a volume that lacks a drive letter (a OneDrive mount is a good example of this).  For example, if Vault is a junction pointing to a OneDrive mount and we try to call this, that will fail with readlink: The system cannot find the path specified. This also works with C-based programs. Hey, Is there any interest in fixing this?  Right now, there is no cross-platform way to canonicalize a path in Go.  We keep running up against additional cases where the existing behavior doesn't canonicalize paths properly, leading to incompatibility with other programs on the system (notably Git).  This necessarily limits the portability of using Go as a cross-platform language."
technical,"I'm sure there are cases where Get Final Path Name By Handle would return a different value, but are there cases where it would return a worse value? I am not aware of any cases with worse values. But I did not spend any time on it, and did not consider any scenarios.  We did not implement filepath.Eval Symlinks in one day - it evolved over time. It was completely broken at the start, and then people complained about their scenarios, and we fixed their scenarios. And so on. But we are not the experts in this area, we made decisions to the best of our abilities at the time. Even Microsoft changed this area in the last 10 years - they added some new features that are used by Docker. And we started with Windows XP and now we have Windows 10 - very different OS in that regard. Perhaps just using Get Final Path Name By Handle would have worked as well. I do not know. But I would not change filepath.Eval Symlinks implementation now. The change will definitely brake some programs."
technical," I can confirm, I had the problem and installing powershell core solved it."
technical,"I can confirm, I had the problem and installing powershell core solved it. I did not try it, but, I suspect, UNC paths wouldn't work in some situations. For example, can you pass UNC path to os.Chdir?"
technical,"I see two problems:  1. Eval Symlinks has Unix-specific behavior as far as returning absolute vs relative paths. If you have a symlink x-y and you Eval Symlinks(""x""), you get ""y"", but if you have a symlink x-/tmp/y, you get ""/tmp/y"". This is correct enough for Unix but much more difficult on Windows, and on both systems sometimes you just want a canonical absolute path. Depending on the symlink is surprising, even on Unix. Note that realpath(3) always returns an absolute path. That's what Resolve will do too.  2. On Windows, we defined ""with a drive letter"" as ""absolute"" but even that's not correct. Drive letters are not absolute in the same way as the Unix root. Instead, Windows has introduced  paths that are a better definition of absolute there. Concretely, supposing that M: and N: map to the same network drive (host\share\), then Eval Symlinks(M:\) = M:\  and Eval Symlinks(N:\) , so that Eval Symlinks(M:\) != Eval Symlinks(N:\) even though they are actually the same. In contrast, in that case. In both cases, Eval Symlinks has behavior that is difficult to change but is less than ideal. Adding Resolve lets us introduce a different operation that behaves like realpath(3) and Get Final Path Name By Handle instead of trying to shoehorn those into Eval Symlinks. Eval Symlinks still does exactly what it says - it evaluates symlinks - and people who want exactly that behavior can still use it for that. But people also sometimes want ""get me a fully resolved, canonical equivalent of this path"". It makes sense to provide that separately. I disagree. I don't think we should add filepath.Resolve function that behaves like Get Final Path Name By Handle Windows API.  If someone wants to use this API, we can add Get Final Path Name By Handle and they can use it there.  I still don't understand what proposed filepath.Resolve function will do. It will be even harder for others, who are just learning this package, to understand the difference between filepath.Resolve and filepath.EvaluateSymlinks. I would like to see documentation for that function before this proposal is even considered.  filepath.Resolve is not a good name (like utils). It tells you nothing about what the function does. To me it is a good indication that you yourself do not know what this function does.  And what problem will this new function solve? Is filepath.Resolve is supposed to be a replacement (good version) for filepath.EvaluateSymlinks? If yes, let's have someone actually try and replace filepath.EvaluateSymlinks in the current Go tree and see if it still works. If that does not work, then we need to think even harder why we need filepath.Resolve, and not just golang.org/x/sys/windows.Get Final Path Name By Handle.  Thank you for consideration. PS: Sorry I did not comment earlier. But I have been busy with other staff, and missed this thread altogether."
technical,"Thanks for the explanation. Would you be willing to unblock ericwj?  I think they have useful information to contribute for moving this proposal forward.  Thanks. I do believe people have the right to decide who they'd like to block, assuming they are not in a government position, and that asking people to unblock others is inappropriate.  That's often an approach people use to engage in continued harassment of women and other folks who tend to be underrepresented in tech.  That's not what's happening here, and I'm sure you had only good intent to move the discussion forward in a productive way, but since we often lack all the context to know people's reasons for blocking others, it's generally best not to ask.  For the limited purpose of advancing this proposal, I have unblocked ericwj for the moment.  I'm fine with Canonical() or Canonicalize() as a name since it's used elsewhere.  That may be more intuitive than Resolve(), but since we'll have sane documentation either way, I'm not very picky about the name."
technical,"I think Alex did give the ""why"" -- the Windows implementation initially tried to mirror the unix one, and evolved from there, as no one questioned that decision, or the lack of a GetCanonicalPath() API. The safest thing to do now is add a new API, and deprecate Eval Symlinks. I do not know / remember why things the way they are. I do not know what exactly would be worse. I wish I could be more useful. And I am sorry that I am not useful. Like I said before, it has been a long process and many changes. I don't remember every decision we made along the way.  I also don't use these features myself. So it is hard for me to judge whether Get Final Path Name By Handle would be worse that what filepath.Eval Symlinks does at this moment. I guess someone actually need to spend some time and investigate every single problem that is reported with filepath.Eval Symlinks and see, if using Get Final Path Name By Handle is a solution to this problem. Unfortunately don't have time to spend on this."
technical,"I did not try it, but, I suspect, UNC paths wouldn't work in some situations. For example, can you pass UNC path to os.Chdir? I don't know for certain, but judging by a quick Google search, it appears to be possible in Ruby, so I assume one can do that in C-based languages. I'm not a Windows developer, so I'm not a good person to ask about the capabilities of Windows.  I'm just a Unix developer trying to make general-purpose software not be terrible on Windows."
technical,"this proposal, as it stands today, is about adding a new API, filepath.Resolve, which still violates everything mentioned in #40180. #40180 is fundamentally a software engineering problem: ericwj is saying that the documentation should mention that the function should be rarely used. The same would apply to filepath.Resolve if it's added. Can you point me to a real-world example? It would be useful while I write the CL. As I said, I plan to go through the history of changes of Eval Symlinks, reconstruct all rationales, make sure there are regression tests, and then attempt a fix. If something is not covered by the existing tests and history of changes, I'd be happy to evaluate it. I gather you didn't read that whole thread. Pathnames over 250ish characters must be absolute. Eval Symlinks doesn't respect this, and correcting it would break callers expecting a relative path. If you want to improve Go on Windows, there are a wide variety of other bugs to tackle. This issue has been resolved."
technical,"No worries. You did not hurt my feelings or anything. I just did not want other people to get impression that they need to convince me more than others. Any proposal here should be judged purely on its merits, not on authority of proposal author or judges. I had gathered that you're recognized as the principle Windows maintainer. I am Go contributor, like many others. I gathered bunch of experience over the years. But that does not make me principle Windows maintainer. I am not responsible for Windows port - Go Team is. No worries. No harm done. I have written a new proposal, #42201, to replace this one. I will now lock this issue."
technical,"By ""function to canonicalize paths"" do you mean a variation of Eval Symlinks that works on Windows? If so, note that Eval Symlinks is not recommended: #40180 (and probably can't be fixed). Go on Windows has a variety of long-standing filesystem bugs. I suggest using x/sys/windows to call the WinAPI if that solves your problem. I mean a function, when given a path, that returns a canonicalized version of that path.  In other words, the equivalent to realpath(3) on Unix or Get Final Path Name By Handle on Windows, and the equivalent to Rust's canonicalize. It isn't helpful to me to call the Windows API because (a) I'm not a Windows programmer and have no clue how to use it, (b) it isn't cross-platform, and (c) this is a function that is generally provided by the standard library."
technical,"you make assertions without being specific about them. I am confused about three of the things you've said related to this issue. - You linked to #40180 which is making a general software engineering argument along the lines of ""you should never actually replace all the symlinks, that's violating the abstractions that have been set up"". I have some sympathy for that, but if it were true, it would apply not just to Eval Symlinks but also this issue as well. If so, then we should just close this very issue (#37113) as a terrible idea. - I see that you mentioned this issue in #40966, which is about some problems with path lengths in Eval Symlinks on Windows. We've had path length problems elsewhere on Windows. Path length issues are usually pretty straightforward to fix. Why would we want to gate a fix to #40966 on a larger design discussion on this issue? - Finally, you said, with no links at all, ""This proposal should probably also deprecate Eval Symlinks, which is seriously broken on Windows."" How is it broken? That comment would be a good place for an issue link. Thanks. I mentioned #40180 in this to suggest that the issue author reconsider canonicalization of paths. I didn't link it again later, but it documents a long list of problems with Eval Symlinks on Windows (which I've now linked). Re path length bugs, other instances of those have been left alone, see #21782 & #36375. And here's a list of Windows bugs that mention ""filepath""."
technical,"can you provide a link to a real world application or at least a working, compilable code snippet that would break if Eval Symlinks is fixed as originally proposed by Russ? I see two problems:  1. Eval Symlinks has Unix-specific behavior as far as returning absolute vs relative paths. If you have a symlink x-y and you Eval Symlinks(""x""), you get ""y"", but if you have a symlink x-/tmp/y, you get ""/tmp/y"". This is correct enough for Unix but much more difficult on Windows, and on both systems sometimes you just want a canonical absolute path. Depending on the symlink is surprising, even on Unix. Note that realpath(3) always returns an absolute path. That's what Resolve will do too.  2. On Windows, we defined ""with a drive letter"" as ""absolute"" but even that's not correct. Drive letters are not absolute in the same way as the Unix root. Instead, Windows has introduced  paths that are a better definition of absolute there. Concretely, supposing that M: and N: map to the same network drive (host\share\), then Eval Symlinks(M:\) = M:\  and Eval Symlinks(N:\) , so that Eval Symlinks(M:\) != Eval Symlinks(N:\) even though they are actually the same. In contrast, in that case. In both cases, Eval Symlinks has behavior that is difficult to change but is less than ideal. Adding Resolve lets us introduce a different operation that behaves like realpath(3) and Get Final Path Name By Handle instead of trying to shoehorn those into Eval Symlinks. Eval Symlinks still does exactly what it says - it evaluates symlinks - and people who want exactly that behavior can still use it for that. But people also sometimes want ""get me a fully resolved, canonical equivalent of this path"". It makes sense to provide that separately."
technical,"you are arguing ""this is the way it's always been"" rather than arguing ""this is why it is the way it is"". That can be a quicker reply, which is understandable. But for what it's worth, explaining why things are the way they are and what exactly would be worse about a change is more useful, because it lets us understand the possible paths forward. I think Alex did give the ""why"" -- the Windows implementation initially tried to mirror the unix one, and evolved from there, as no one questioned that decision, or the lack of a GetCanonicalPath() API. The safest thing to do now is add a new API, and deprecate Eval Symlinks."
technical,"This is what Git does and what other projects do, and it's the semantics we need.  It is the same semantics that other programming languages provide.  I am interested in Go working with other software that is also on the system in a compatible way.  You may argue that Windows and Unix should provide the same semantics for path canonicalization, and perhaps they should.  However, Microsoft was fully aware of Unix path semantics and behavior, and clearly chose deliberately to do things in an incompatible and much more complicated way.  I feel that was a mistake, but we're stuck with it now.  I need to change the directory to the root of the repository to make GIT WORK TREE work.  I also need to be able to resolve any other Git environment variables to their canonical paths so I can set them appropriately before invoking Git, and all of those need to be canonicalized appropriately and compatibly with Git.  This is also the behavior that anyone else working with Git will need to implement.  I also need to be able to determine if a path the user has provided is within the repository and fail if it is not.  I can do that by removing the trailing file component, canonicalizing the directory, and appending the file path.  That works to tell me if the component is in the repository, whether or not the file itself is a symlink.  Repositories are not allowed to span file systems.   I have one question on this. Say that we have network\repo mounted as N:, just like in the referenced bug. Within the repo, there's a committed symlink N:\mylink that points to a file on a local disk C:\file. Would you expect it to return this? If I followed you correctly, you would need the former, but Get Final Path Name By Handle obviously returns the latter.  I don't need to canonicalize file names, only directory names.  However, I do need the standard Get Final Path Name By Handle semantics for that.  Other people may have different use cases and will also need the standard semantics.  I want to be clear that I'm not asking for this just for Git LFS.  I'm asking for this because these are the standard platform semantics and they should be available in a cross-platform way.  I fully realize that they are inconsistent across platforms.  I want the same behavior out of Go that I get out of languages that use the system C library.  I'm not looking for special functionality to handle my special case, I'm looking for generic tools that make Go programs compatible with non-Go programs.  It's fine that Go does not wrap the system C library, but if it chooses not to do that, it needs to be compatible with the semantics of the platform as if it did, and not provide behavior that is materially different.  Trying to sell me on a behavior that is consistent across platforms when that is not practically achievable in a useful way isn't helpful.  That's what we have now with Eval Symlinks, and everyone agrees that it's not the right decision. I want to be clear that while there may be other proposals, such as filepath.SameFile, those are separate and independent proposals and are outside of the scope of this proposal, which is to implement a function which canonicalizes paths.  I don't dispute that they may be valuable and important proposals, but they are not this one, and as such, they should probably be discussed in their own issues, not this one.  I also want to be clear that I'm an experienced developer who understands his problem space well and am confident I know what's best for it, so it's unhelpful (and frankly, insulting) to imply that some other solution would be better for my needs or that I should adopt a different design.  If people disagree with the Git project's decisions to adopt this technique (with which I am implementing compatibility) and feel that they were unwise, those concerns should be addressed to the project's mailing list, although I suspect that in this case such a message will be about as welcome as I have found it.  As I've mentioned, my goal here is to expose a cross-platform way to canonicalize paths as the operating system defines that.  Whether the operating system designers made a provident design decision or whether the operating system has provided desirable semantics is outside the scope of this proposal.  I assume for the purposes of this proposal that the fact that the operating system is successful means that its developers were competent and implemented desirable, useful, and well-thought-out features.  The facts remain that other common, successful languages implement functionally equivalent designs, and that while similar concerns have been expressed during the design of those implementations, nobody has yet proposed a better or more universally desirable solution.  Moreover, Go attempted to provide a solution for this problem in Eval Symlinks which demonstrates that this is a problem that people need solved.  Additionally, canonicalizing paths is considered useful for security purposes by Carnegie Mellon's Software Engineering Institute, which is considered a reputable source of secure coding advice.  Finally, this functionality is implemented in the popular realpath utility, which is widely shipped for scripting across platforms (including Windows).  For these reasons, I believe that the proposal as it stands should be implemented.  I take no position on proposals for other functions, as they are outside my scope."
technical,"Well, I'm sorry for having been slightly edgy due to the total irrelevance of that statement with which he accompanies the news that he unblocked me.  I care to repeat that Brian should argue his case from a position of hands on, mature knowledge rather than having us collect that for him and demanding his proposal be accepted unmodified and that none of us is in a better position than to test and evaluate the suitability of including the proposal of porting canonicalize unmodified into the go standard library for Windows than he himself and the company he works for. Further I can only repeat my previous arguments and fix the oversight of not mentioning the obvious - that Eval Symlinks is implemented with Get Final Path Name By Handle and that some of the problems even mentioned in this issue will be exactly the same with canonicalize for this reason. I'm going to close this issue and I'd like to ask the core team to lock it at their earliest convenience.  I don't feel this discussion is going in a productive way and I don't wish to pursue this issue or proposal further.  If other folks think this a similar proposal will be valuable, I'm happy for them to drive it without my involvement."
technical,"Well, I'm sorry for having been slightly edgy due to the total irrelevance of that statement with which he accompanies the news that he unblocked me.  I care to repeat that Brian should argue his case from a position of hands on, mature knowledge rather than having us collect that for him and demanding his proposal be accepted unmodified and that none of us is in a better position than to test and evaluate the suitability of including the proposal of porting canonicalize unmodified into the go standard library for Windows than he himself and the company he works for. Further I can only repeat my previous arguments and fix the oversight of not mentioning the obvious - that Eval Symlinks is implemented with Get Final Path Name By Handle and that some of the problems even mentioned in this issue will be exactly the same with canonicalize for this reason. I'm sure there are cases where Get Final Path Name By Handle would return a different value, but are there cases where it would return a worse value?"
technical,"That sounds like a great solution.  I'm happy with any of the three proposed names. If the issue fix is to call Get Final Path Name By Handle Windows API, why cannot they just call the API? What would filepath.Resolve documentation say? How will filepath.Resolve differ from filepath.Eval Symlinks? For example, how would you explain to a new Go user on both Linux and Windows when she should use one function instead of the other? Thank you."
technical,"What does ""canonical"" mean, precisely? If there are multiple ways to refer to a filename, the canonical path is the absolute filename which uses no indirections and uses the canonical case (that is, the path component as written to the file system) if the system permits case folding.  On Unix, that's the one that contains no symlinks (and, on macOS, uses canonical case and composition).  On Windows, there are many ways to have indirection in a path: symlinks, junctions, SUBST, etc.  (I don't actually know all of the possible ways, since I almost never use Windows).  The canonical form uses none of those indirections and uses the canonical case. Another way to say this is that assuming no hardlinks exist, a file on Unix should have exactly one canonical name whose components are either directories or non-symlink, non-directory (but possibly special) files."
technical,"the path length bugs are left alone only for lack of time. I don't think there's any objection to fixing them as long as it is done correctly and well. I'm certainly not arguing against this functionality. I'm trying to understand why Eval Symlinks shouldn't be what provides this functionality on Windows.  (I do somewhat object to the name ""canonical"": if ""/home/rsc"" symlinks to some device path like, I have a hard time calling the latter the ""canonical"" one. And for what it's worth there are plenty of people who disagree with you about what ""determine definitively if a path is under a directory"" should mean. I have the bug reports to prove it. :-) But again, I'm not saying we shouldn't do this. I just think Eval Symlinks is probably the answer.) If you change Eval Symlinks to call Get Final Path Name By Handle on Windows, it may break some apps, so I'm pretty sure Alex wouldn't agree."
technical,"all of the questions you raised have been addressed in the comments above. Re missing the thread, you last commented here 7 days ago. is unable to comment on this issue.  As far as we can tell, this is a GitHub bug.  It is not due to any intentional action by the Go team.  This comment is by ericwj, sent via e-mail:  Get Final Path Name By Handle does not canonicalize. I just ran it, trying all flags. The result is different depending on how I configure my system, depending on which path I use to access the file and depending on which flag I provide in each situation. This means canonicalization will break if the machine configuration changes, which can also happen *during* Git operations, and the result may change depending on which path is used to access the same file or directory.   The simplicity of Eval Symlinks is one of its problems. Any function string Foo(string) will have some of the same fundamental, unfixable problems that I documented for Eval Symlinks. Issue #40180 only proposes docs fixes because outright no-op'ing Eval Symlinks on Windows was refused in #40104, but I think the conclusion of the analysis in #40180 is that #40104 was refused because I did not argue the case effectively yet. And this proposal is just too similar, apart from perhaps thread-safety and eventually perhaps looking more often like it works.   Even suppose something like Resolve could be perfect, Go cannot handle the return value. The unfortunate truth is that Go can only handle paths with drive letters and ˜normal' UNC paths in the form server\share (without prefix). I have evaluated about half of all path/filepath API's and the prefixes Get Final Path Name By Handle yields break about every one of them. You cannot depend on stripping the prefixes, either.   This proposal goes wrong the same way path/filepath has, which imho is thinking *nix can be ported unmodified to a fundamentally different operating system without proper attention, expertise involved, or testing. And for letting fundamentals slide for the benefit of a bit of simplicity, offering no alternative besides P/Invoke in cases where this makes things fall apart.   I suggest writing the fundamentals, especially well for Windows, testing and reviewing meticulously, while iterating towards an API suitable for the general public on top of those fundamentals, before proposing inclusion in any standard library. If the problem is just UNC mappings, much more appropriate WNet* functions exist to translate DOS device names. I think object identifiers are the more general solution for Git LFS, in favour of path strings. Wrapped in a suitable, portable abstraction to obtain and compare them."
technical,"all of the questions you raised have been addressed in the comments above. Re missing the thread, you last commented here 7 days ago. It is also the case that filepath.Eval Symlinks fails to work when canonicalizing paths where there's a junction to a volume that lacks a drive letter (a OneDrive mount is a good example of this).  For example, if Vault is a junction pointing to a OneDrive mount and we try to call this, that will fail with readlink: The system cannot find the path specified. This also works with C-based programs."
technical,"We don't want to carry a large amount of non-portable system-specific code.  We are not the Git project, with many regular full-time contributors across a wide variety of platforms.  Our team is mostly two people, neither of whom use Windows on a regular basis, and we rely on Go to make our Windows support more than best effort.  We already have platform-specific code to support NTLM specially on Windows and it's going away because it's broken and nobody wants to volunteer to fix it.  I also happen to feel that requiring every user who wants to support cross-platform path canonicalization to know about and use Get Final Path Name By Handle explicitly and use it is not going to help Windows get better support, especially when filepath.Eval Symlinks is right there and seems to do the right thing in most cases (except it doesn't).  I only happen to know about it because I've actually looked up which function to use for this purpose at some point in the past, in general, a Unix developer won't know about it and won't know to use it.  filepath.Resolve is used in the case when you want to canonicalize a path, and it will always result in an absolute path.  filepath.Eval Symlinks only resolves symlinks, may produce a relative path if its input is relative, and does not handle non-symlinks on Windows.  It is not suitable for canonicalizing a path on Windows and cannot be used successfully for that purpose.  filepath.Resolve should be used unless you are sure you want the behavior of filepath.Eval Symlinks. It is not a lot of code. You can, probably, just copy all the code you need from GOROOT directory - just grep for Get Final Path Name By Handle."
technical,"Based on the discussion above, adding Resolve with the semantics described seems like a **likely accept**. It looks like filepath.Eval Symlinks is broken on Windows, and we are now planning to replace it with a similar function with a different name and slightly different semantics, because it's too hard to touch Eval Symlinks on Windows, where it's already broken and doesn't always work. Nobody can enumerate where it does work or it doesn't, but it seems easier not to touch it anymore rather than fixing it.  This sounds extremely rushed to me. We're adding an API that is basically a duplicate of another existing one because we don't have time to properly fix the existing one, analyzing all issues. Also it looks like there are no volunteers to work on it.  If we can get this proposal on hold for a month or so, I can attempt a thorough analysis on the history of changes of Eval Symlinks on Windows, and attempt a CL as described by Russ in this. This would require some time and some effort to get right, but I think it would be a better long term solution rather than adding a new API. If I fail, we can still fallback to adding a new API, but at least I hope we would get a better understanding around Eval Symlinks and whether it's really inherently broken forever on Windows."
technical,"The two objections to using this seem to be (1) needing to return absolute paths sometimes, and (2) problems with relative paths and threads changing directories.  For (1), Eval Symlinks is  already  defined to return an absolute path when necessary. If the result of Get Final Path Name By Handle cannot be made relative to the current directory, then the absolute one can be returned. That's entirely within the documented behavior.  For (2), there's nothing wrong with relative paths per se provided the process is not calling os.Chdir. It is Chdir (the write operation) that is not ""thread-safe"", not the relative path evaluation (the read operations). If you have a program that uses Chdir, then yes, use absolute paths. Pass an absolute path to Eval Symlinks and you'll get one out. But if Eval Symlinks is passed a relative path, there is no added harm in returning one. It's really looking to me like we should use Get Final Path Name By Handle in Eval Symlinks. Are there other reasons we should not? It may be in-spec to always return an absolute path. But existing apps may rely on the current behavior. It's just conjecture, but the same has torpedoed previous proposals. However I'm not personally opposed to it."
technical,"If you change Eval Symlinks to call Get Final Path Name By Handle on Windows, it may break some apps, so I'm pretty sure Alex wouldn't agree. It would need to keep doing what it's documented to do, namely preserve relative-ness to current directory when possible. That means calling Get Final Path Name By Handle and fixing up the result a little. But we could still build a function around Get Final Path Name By Handle that should handle everything Windows can throw at it. Did you have a specific breakage in mind?"
technical,"the Windows maintainer has objected to this proposal, raising Q's in this. It might help to point out where Russ or Ian have previously addressed those Q's. Looking at how this problem is solved elsewhere, if a new function is to be added, perhaps a better name would be Canonical(). Rust has canonicalize and C++'s filesystem library has canonical, which Microsoft's Standard C++ Library implements using Get Final Path Name By Handle.  Canonical was not immediately obvious to me before I looked at this issue, and I think that's a fair concern. However, looking at modern solutions to this problem, once you know of it, the naming familiarity between languages might be a good thing.   ericwj is unable to comment on this issue. As far as we can tell, this is a GitHub bug. It is not due to any intentional action by the Go team.  ianlancetaylor The original author of this issue has blocked ericwj because they felt insulted by the help they were providing. When you block a user on GitHub, they're unable to comment on issues/pull requests you're the owner of."
technical,"I'm going to close this issue and I'd like to ask the core team to lock it at their earliest convenience.  I don't feel this discussion is going in a productive way and I don't wish to pursue this issue or proposal further.  If other folks think this a similar proposal will be valuable, I'm happy for them to drive it without my involvement. No worries. You did not hurt my feelings or anything. I just did not want other people to get impression that they need to convince me more than others. Any proposal here should be judged purely on its merits, not on authority of proposal author or judges. I had gathered that you're recognized as the principle Windows maintainer. I am Go contributor, like many others. I gathered bunch of experience over the years. But that does not make me principle Windows maintainer. I am not responsible for Windows port - Go Team is. No worries. No harm done."
technical,"That's not true on modern POSIX systems.  You can open a file descriptor to a directory and operate on a path relative to that file descriptor with the *at series of functions.  It may be true on Windows.  I agree with some of your points and I've stated so above.  Users usually are not interested in canonical paths.  It's also wrong to use path canonicalization in a case where the possibility of changing path resolution leads to a security problem or buggy behavior.  That's a well known problem on Unix systems and Windows is no different.  It is irrelevant to me whether the path is created with a drive letter or not, so I have no position on that argument.  I agree that Eval Symlinks, as it exists today, is broken on Windows and does not do anything interesting or useful in most cases.  That's why I opened this issue.  The question is not whether you think this feature is valuable.  You need not use it.  The question is whether Go ought to provide access in a portable way to cross-platform functionality that every operating system and every other major language provides and which is used in many projects for good and valuable reasons, and which, even if used imprudently, is required for compatibility with other software already existing for longer than Go has.  The fact that major organizations like CMU recommend this practice is evidence that this feature is important and valuable, even if you disagree.  Path canonicalization is even more important on Windows than Unix because Windows has case-folding behavior in its file system that depends on attributes related to when the file system was created.  It is therefore impossible without the kernel's help to know the proper name of a file and whether two file names actually refer to the same item on disk, and Windows otherwise lacks the concept of device and inode numbers which are normally used to perform this check on Unix. I have seen first hand how Git broke when it did not canonicalize paths consistently, and I still have a broken repository on my system from that point.  I am a core contributor to Git and the primary driver of the SHA-256 transition.  I've developed in situations where symlinks and path resolution have important and subtle security implications.  I understand the problem space intimately and why Git has the behavior it does.  Please don't try to to tell me that Git's behavior is wrong here, because it is not.  Whether a path is created by the user, the system administrator, or any other actor does not change whether canonicalization is necessary. Obviously that is the same on any operating system if you just use Rel and Join. But Eval Symlinks makes the path relative to the current working directory and it is that fact that makes it not thread-safe. You can use Join but again you need the current working directory as argument, which might have changed in between these two calls. My argument is not against canonicalization, but against using or fixing Eval Symlinks to do it. This whole comment was about using Get Final Path Name By Handle to implement Eval Symlinks. It can't be done properly. There is the concept of object identifiers, but these are an NTFS concept. So yeah sure absolutely, the OS needs to be involved and one of the things that is wrong with path\filepath is that it doesn't involve the OS enough. I don't know what Git does, but I really think it is wrong to resolve all links - even those that are in a parent directory of the Git repo. The issue linked specifically mentions the ability to have files open while these links are being changed and to continue to open files afterwards as long as these links are not in any way, for any length of time, cached. I don't mean to criticize your intimate knowledge and experience with Git development, but the proper way to go is to know which links are part of repository configuration and which ones are system configuration and leave the latter ones forever unresolved."
technical,"I do not know / remember why things the way they are. I do not know what exactly would be worse. I wish I could be more useful. And I am sorry that I am not useful. Like I said before, it has been a long process and many changes. I don't remember every decision we made along the way.  I also don't use these features myself. So it is hard for me to judge whether Get Final Path Name By Handle would be worse that what filepath.Eval Symlinks does at this moment. I guess someone actually need to spend some time and investigate every single problem that is reported with filepath.Eval Symlinks and see, if using Get Final Path Name By Handle is a solution to this problem. Unfortunately don't have time to spend on this. OK, it sounds like maybe it would be best to leave Eval Symlinks alone. That leaves the issue, which is that nothing can turn. If we add something to do that while also resolving symlinks, perhaps filepath.Resolve? Then we get to define a new function, and the result is always as absolute as possible (begins with / on Unix, UNC on Windows avoiding drive letter whenever possible, evaluating out symlinks, and so on). filepath.Resolve would be the same as Unix realpath(3), and the same as Windows Get Final Path Name By Handle. On Unix it would be the same as filepath.Eval Symlinks(filepath.Abs(path)). On Windows, we have no idea what Eval Symlinks does, so it's hard to state in terms of existing API. What do people think of adding filepath.Resolve with this definition?"
technical,"First of all, I appreciate that you're trying to help.  However, I do feel firmly that this functionality should be in the standard library, since it is in almost every other language, and it is in POSIX.  I don't want to carry a lot of platform-specific code in a program because it's difficult to maintain and test, especially when I don't typically develop on Windows.  If Go is known to have known defects on Windows, those should be promptly fixed or clearly documented.  For many purposes, it's fine if code doesn't run or run well on Windows, but there are some cases where it does.  The documentation should clearly and prominently list any limitations with using Go on Windows so that folks can make an informed decision.  Last I checked, the filepath documentation didn't indicate such limitations, and hasn't for some time.  Normally, when I find a bug or missing feature, I would send a patch to implement that functionality.  However, Go has a CLA, and I don't sign CLAs, so any patch I might submit wouldn't be accepted.  If that changes, I'm happy to send a patch to implement this properly if nobody gets to it before me. On Unix systems I think the proposed function is the same as filepath.Eval Symlinks."
technical,"It looks like filepath.Eval Symlinks is broken on Windows, and we are now planning to replace it with a similar function with a different name and slightly different semantics, because it's too hard to touch Eval Symlinks on Windows, where it's already broken and doesn't always work. Nobody can enumerate where it does work or it doesn't, but it seems easier not to touch it anymore rather than fixing it.  This sounds extremely rushed to me. We're adding an API that is basically a duplicate of another existing one because we don't have time to properly fix the existing one, analyzing all issues. Also it looks like there are no volunteers to work on it.  If we can get this proposal on hold for a month or so, I can attempt a thorough analysis on the history of changes of Eval Symlinks on Windows, and attempt a CL as described by Russ in this. This would require some time and some effort to get right, but I think it would be a better long term solution rather than adding a new API. If I fail, we can still fallback to adding a new API, but at least I hope we would get a better understanding around Eval Symlinks and whether it's really inherently broken forever on Windows. please give the folks above some credit for our consideration of this issue. he did a careful analysis of Eval Symlinks on Windows in this (read whole thread) -- which I linked above. [BTW, he was improperly locked out of this thread by an anonymous admin for no stated reason.]  It's clear that we wouldn't add Eval Symlinks today if it didn't exist, so the best long term solution is deprecating Eval Symlinks.  We cannot fix Eval Symlinks because certain changes from relative to absolute paths could break existing Windows programs. Alex will not agree to that, and he's the Windows maintainer."
technical,"what do you think of changing the implementation of filepath.Eval Symlinks to just call Get Final Path Name By Handle? Relative paths are not thread-safe. Full stop.  Eval Symlinks should not be fixed but replaced. Get Final Path Name By Handle *always* returns an absolute path. Paths with ?\ syntax are always absolute. The string that is returned by this function uses the ""?"" syntax. For more information, see CreateFile.  The CMU guidance obviously is written without considering the arguments against Eval Symlinks that I wrote up - which is based on guidance from Microsoft, so I don't quite agree with your conclusion that resolving file system driver paths is a proper thing to do while canonicalizing. I have just hastily scanned that document from CMU and I don't immediately see they say you should resolve links. They just mention links could be present for awareness.  I have written quite a bit of software and I have never considered writing checks like that. Usually path checks involve checking whether they are in some base path - for which Rel is better than Eval Symlinks. Sure you can ask for a properly cased path and/or fix slashes, but otherwise, why write string handling or comparisons at all? Paths usually come from somewhere. I mean from the working directory, from configuration or perhaps from user input in most cases. Usually those are already trusted without doing any checks. Or they are built from system settings, which are also trusted. That might be circumstantial evidence against your conclusion based on that article but still.  Also about the Git example still I think the application should be in control of which links get resolved. Some of the links that make up a path used by Git could still be system administrator controlled which makes it wrong to resolve them and the result dependant on system configuration. Like for my system - most Git paths ran through Eval Symlinks on my system will (have to) be volume GUID paths (and hence again all always absolute). But Git should never care about that or go that deep. It's wrong.  EDIT: The article ignores case-sensitivity issues by using strncmp. That is wrong as well. Most software on Windows assumes case-insensitivity, although NTFS is case-sensitive. On Linux, FAT32 is case-insensitive."
technical,"Yes, I agree that we cannot exactly reproduce this if we switch Eval Symlinks to Get Final Path Name By Handle. Though I'm not fully sure that the clients of Eval Symlinks really do behave on this implementation detail (that is, unfortunately, documented). What if drop that sentence from the documentation, and just say that Eval Symlinks can either return a relative or an absolute path? Do you have an idea on whether clients really do rely on this specific behavior, even on Linux?  Notice that realpath on Linux doesn't follow mountpoints as well:  Is realpath(3) on Linux different than this? I might be missing something here.  So my understanding is that if this proposal is accepted, we end up with:  * Linux: Resolve and Eval Symlinks are basically the same function, with the only difference is that Resolve always returns an absolute path, while Eval Symlinks returns a path constructed following the symlinks, so it might be absolute or relative depending on how the symlinks were created. Neither of them see through mountpoints in any way. * Windows: Eval Symlinks (after bugfixing) would possibly go through all kind of NTFS links (with the same relative/absolute handling than Linux), and it would not follow mountpoints. Resolve instead would also resolve mountpoints.  It looks like the biggest gain here is that we somehow exposes Get Final Path Name By Handle which is a ""common"" function used in other frameworks / languages to canonicalize paths. Since we're missing it, we face interoperability problems. In fact, semantically speaking, we're going to have a Resolve function which behaves differently between Windows and Linux wrt mountpoints. And BTW we're also getting into another corner: if somebody puts in the effort of adding mountpoint resolution to Resolve on Linux, we end up with interoperability problems again, because the rest of the world might still just be calling realpath(3) on Linux.  A slightly different proposal is to add filepath.SameFile(path1, path2 string). This would use Get Final Path Name By Handle on Windows, and realpath on Linux (but can be further improved on Linux by looking at inode number, etc.). It would solve the real-world problem of many softwares without having to define what a canonical path name is, and would also relax the software engineering concern of preferring to keep around the original input path rather than a resolved one. So my understanding is that if this proposal is accepted, we end up with:       * Linux: Resolve and Eval Symlinks are basically the same function, with the only difference is that Resolve always returns an absolute path, while Eval Symlinks returns a path constructed following the symlinks, so it might be absolute or relative depending on how the symlinks were created. Neither of them see through mountpoints in any way.  Yes, because bind mounts on Linux are not identical.  One directory may be read-only while the other may be read-write.  Bind mounts also, by default, don't include submounts and are therefore not necessarily equivalent.       * Windows: Eval Symlinks (after bugfixing) would possibly go through all kind of NTFS links (with the same relative/absolute handling than Linux), and it would not follow mountpoints. Resolve instead would also resolve mountpoints.  As far as I understand it, these mountpoints are functionally identical.  They are more like opaque symlinks rather than the typical Unix mountpoints with different mount options.  This is exactly the behavior that's desired: canonicalizing the path in the way the operating system does that.  Languages that use the system C library don't have this problem because they use the system C library functions for this purpose and implement those functions' behavior.  I would literally define the expected behavior in terms of the system's C library, since that's what other languages (e.g., Rust) do. That doesn't really assist in my use case of finding out whether one path is inside another."
technical,"OK, it sounds like maybe it would be best to leave Eval Symlinks alone. That leaves the issue, which is that nothing can turn. If we add something to do that while also resolving symlinks, perhaps filepath.Resolve? Then we get to define a new function, and the result is always as absolute as possible (begins with / on Unix, UNC on Windows avoiding drive letter whenever possible, evaluating out symlinks, and so on). filepath.Resolve would be the same as Unix realpath(3), and the same as Windows Get Final Path Name By Handle. On Unix it would be the same as filepath.Eval Symlinks(filepath.Abs(path)). On Windows, we have no idea what Eval Symlinks does, so it's hard to state in terms of existing API. What do people think of adding filepath.Resolve with this definition? Sounds good. Maybe should call it Real() or RealPath() since that term is already widely known."
technical,"Looking at how this problem is solved elsewhere, if a new function is to be added, perhaps a better name would be Canonical(). Rust has canonicalize and C++'s filesystem library has canonical, which Microsoft's Standard C++ Library implements using Get Final Path Name By Handle.  Canonical was not immediately obvious to me before I looked at this issue, and I think that's a fair concern. However, looking at modern solutions to this problem, once you know of it, the naming familiarity between languages might be a good thing.   ericwj is unable to comment on this issue. As far as we can tell, this is a GitHub bug. It is not due to any intentional action by the Go team.  ianlancetaylor The original author of this issue has blocked ericwj because they felt insulted by the help they were providing. When you block a user on GitHub, they're unable to comment on issues/pull requests you're the owner of. Thanks for the explanation. Would you be willing to unblock ericwj?  I think they have useful information to contribute for moving this proposal forward.  Thanks."
technical,Sounds good. Maybe should call it Real() or RealPath() since that term is already widely known. That sounds like a great solution.  I'm happy with any of the three proposed names.
technical,"It may be in-spec to always return an absolute path. But existing apps may rely on the current behavior. It's just conjecture, but the same has torpedoed previous proposals. However I'm not personally opposed to it. That's **not** what I'm suggesting. I wrote ""If the result of Get Final Path Name By Handle cannot be made relative to the current directory, then the absolute one can be returned."" I did **not** write ""The absolute one can be returned always."" By ""made relative"" I meant transformed to be relative to the current directory by filepath.Rel."
technical,"Ah, I misinterpreted that. The one issue for a relative transformation is that relative paths have a max length, so some cannot be transformed, tho an app might expect it. did we investigate whether filepath.Rel is healthy on Windows? That's news to me. A relative path stored as a string inside a Go program should  not  have a max length. If you pass a very long path to os.Open, I thought we did the appropriate rewrite to remove the length restriction. Do we not do that for relative paths? If the Go APIs aren't handling long relative paths correctly, we should fix  that , not introduce strange inconsistencies elsewhere."
technical,"As it's not possible to relativize some results of Get Final Path Name By Handle, that could be a cause of breakage if Eval Symlinks is reimplemented with it. That's not true on modern POSIX systems.  You can open a file descriptor to a directory and operate on a path relative to that file descriptor with the *at series of functions.  It may be true on Windows.  I agree with some of your points and I've stated so above.  Users usually are not interested in canonical paths.  It's also wrong to use path canonicalization in a case where the possibility of changing path resolution leads to a security problem or buggy behavior.  That's a well known problem on Unix systems and Windows is no different.  It is irrelevant to me whether the path is created with a drive letter or not, so I have no position on that argument.  I agree that Eval Symlinks, as it exists today, is broken on Windows and does not do anything interesting or useful in most cases.  That's why I opened this issue.  The question is not whether you think this feature is valuable.  You need not use it.  The question is whether Go ought to provide access in a portable way to cross-platform functionality that every operating system and every other major language provides and which is used in many projects for good and valuable reasons, and which, even if used imprudently, is required for compatibility with other software already existing for longer than Go has.  The fact that major organizations like CMU recommend this practice is evidence that this feature is important and valuable, even if you disagree.  Path canonicalization is even more important on Windows than Unix because Windows has case-folding behavior in its file system that depends on attributes related to when the file system was created.  It is therefore impossible without the kernel's help to know the proper name of a file and whether two file names actually refer to the same item on disk, and Windows otherwise lacks the concept of device and inode numbers which are normally used to perform this check on Unix. I have seen first hand how Git broke when it did not canonicalize paths consistently, and I still have a broken repository on my system from that point.  I am a core contributor to Git and the primary driver of the SHA-256 transition.  I've developed in situations where symlinks and path resolution have important and subtle security implications.  I understand the problem space intimately and why Git has the behavior it does.  Please don't try to to tell me that Git's behavior is wrong here, because it is not.  Whether a path is created by the user, the system administrator, or any other actor does not change whether canonicalization is necessary."
technical,"If the issue fix is to call Get Final Path Name By Handle Windows API, why cannot they just call the API? What would filepath.Resolve documentation say? How will filepath.Resolve differ from filepath.Eval Symlinks? For example, how would you explain to a new Go user on both Linux and Windows when she should use one function instead of the other? Thank you. The docs should say… On Windows, Eval Symlinks is broken for many cases. Real/RealPath/Resolve is the only API to ""evaluate"" symlinks. See this. On unix, the RealPath API is equivalent to this. Eval Symlinks should probably be deprecated."
technical,"Canonicalization of paths is required to properly implement any sort of Git support in a project.  More generally, it's required to determine definitively if a path is under a directory, which has a wide variety of general-purpose applications outside of Git.  Whether other people think it is useful in their projects, path canonicalization is commonly used and is almost always provided by the standard library.  Canonicalizing paths is also recommended by CMU's secure coding guidelines, while those are for C, there's no reason to think Go is any different. I agree that users typically don't want to see canonicalized paths and that path canonicalization cannot be used where there's a security-sensitive race condition, but that doesn't mean it lacks applications elsewhere, just that it's unsuitable for some use cases. the path length bugs are left alone only for lack of time. I don't think there's any objection to fixing them as long as it is done correctly and well. I'm certainly not arguing against this functionality. I'm trying to understand why Eval Symlinks shouldn't be what provides this functionality on Windows.  (I do somewhat object to the name ""canonical"": if ""/home/rsc"" symlinks to some device path like, I have a hard time calling the latter the ""canonical"" one. And for what it's worth there are plenty of people who disagree with you about what ""determine definitively if a path is under a directory"" should mean. I have the bug reports to prove it. :-) But again, I'm not saying we shouldn't do this. I just think Eval Symlinks is probably the answer.)"
technical,"Obviously that is the same on any operating system if you just use Rel and Join. But Eval Symlinks makes the path relative to the current working directory and it is that fact that makes it not thread-safe. You can use Join but again you need the current working directory as argument, which might have changed in between these two calls. My argument is not against canonicalization, but against using or fixing Eval Symlinks to do it. This whole comment was about using Get Final Path Name By Handle to implement Eval Symlinks. It can't be done properly. There is the concept of object identifiers, but these are an NTFS concept. So yeah sure absolutely, the OS needs to be involved and one of the things that is wrong with path\filepath is that it doesn't involve the OS enough. I don't know what Git does, but I really think it is wrong to resolve all links - even those that are in a parent directory of the Git repo. The issue linked specifically mentions the ability to have files open while these links are being changed and to continue to open files afterwards as long as these links are not in any way, for any length of time, cached. I don't mean to criticize your intimate knowledge and experience with Git development, but the proper way to go is to know which links are part of repository configuration and which ones are system configuration and leave the latter ones forever unresolved. The two objections to using this seem to be (1) needing to return absolute paths sometimes, and (2) problems with relative paths and threads changing directories.  For (1), Eval Symlinks is  already  defined to return an absolute path when necessary. If the result of Get Final Path Name By Handle cannot be made relative to the current directory, then the absolute one can be returned. That's entirely within the documented behavior.  For (2), there's nothing wrong with relative paths per se provided the process is not calling os.Chdir. It is Chdir (the write operation) that is not ""thread-safe"", not the relative path evaluation (the read operations). If you have a program that uses Chdir, then yes, use absolute paths. Pass an absolute path to Eval Symlinks and you'll get one out. But if Eval Symlinks is passed a relative path, there is no added harm in returning one. It's really looking to me like we should use Get Final Path Name By Handle in Eval Symlinks. Are there other reasons we should not?"
technical,"I want to be clear that while there may be other proposals, such as filepath.SameFile, those are separate and independent proposals and are outside of the scope of this proposal, which is to implement a function which canonicalizes paths.  I don't dispute that they may be valuable and important proposals, but they are not this one, and as such, they should probably be discussed in their own issues, not this one.  I also want to be clear that I'm an experienced developer who understands his problem space well and am confident I know what's best for it, so it's unhelpful (and frankly, insulting) to imply that some other solution would be better for my needs or that I should adopt a different design.  If people disagree with the Git project's decisions to adopt this technique (with which I am implementing compatibility) and feel that they were unwise, those concerns should be addressed to the project's mailing list, although I suspect that in this case such a message will be about as welcome as I have found it.  As I've mentioned, my goal here is to expose a cross-platform way to canonicalize paths as the operating system defines that.  Whether the operating system designers made a provident design decision or whether the operating system has provided desirable semantics is outside the scope of this proposal.  I assume for the purposes of this proposal that the fact that the operating system is successful means that its developers were competent and implemented desirable, useful, and well-thought-out features.  The facts remain that other common, successful languages implement functionally equivalent designs, and that while similar concerns have been expressed during the design of those implementations, nobody has yet proposed a better or more universally desirable solution.  Moreover, Go attempted to provide a solution for this problem in Eval Symlinks which demonstrates that this is a problem that people need solved.  Additionally, canonicalizing paths is considered useful for security purposes by Carnegie Mellon's Software Engineering Institute, which is considered a reputable source of secure coding advice.  Finally, this functionality is implemented in the popular realpath utility, which is widely shipped for scripting across platforms (including Windows).  For these reasons, I believe that the proposal as it stands should be implemented.  I take no position on proposals for other functions, as they are outside my scope. the Windows maintainer has objected to this proposal, raising Q's in this. It might help to point out where Russ or Ian have previously addressed those Q's."
technical,"This should mostly be fixed by my CL. Please test it and let me know.  It also rectifies some Eval Symlinks issues on Windows reported in your #40180 table. I have a followup CL (still not mailed) that fixes another set of issues in Eval Symlinks. NOTE: I'm not touching the semantic of the function as it is implemented now: I'm just trying to fix implementation bugs so that we can see a clearer picture of where we are now. There's clearly new, substantive discussion here. Moving back to Active."
technical,"To be honest, I disagree on this. The two underlying files are indeed the same and in fact os.SameFile returns true. It sounds absolutely surprising that os.SameFile(a,b) differs from filepath.Resolve(a) == filepath.Resolve(b).  Moreover, the same situation happens on Windows. You can easily have C:\FOO and LOCALHOST\BAR\FOO being the same file with different permissions. I'm not sure why on Windows it should be necessary to have filepath.Resolve returning true in that situation, while on Linux it should return false on bind mounts (or any other kind of mount really).  I think the only correct answer here is that the request is not to establish a cross-platform semantic, but just add a function that calls realpath on Linux and Get Final Path Name By Handle on Windows because this is what most open source projects have ended up doing. I sympathize with the goal of interoperability, but I'm just hoping that we can find a better solution. Looking at this, it looked like filepath.SameFile would be enough, to compare if the git root actually matches. From your answer, I understand that you actually need whether a specific file of the repo is within the git repo (with the git root path being read through git and thus canonicalized through Get Final Path Name By Handle)? Can you please elaborate on the use case you are mentioning?  I have one question on this. Say that we have network\repo mounted as N:, just like in the referenced bug. Within the repo, there's a committed symlink N:\mylink that points to a file on a local disk C:\file. Would you expect this to return network\repo\mylink or C:\file? If I followed you correctly, you would need the former, but Get Final Path Name By Handle obviously returns the latter. This is what Git does and what other projects do, and it's the semantics we need.  It is the same semantics that other programming languages provide.  I am interested in Go working with other software that is also on the system in a compatible way.  You may argue that Windows and Unix should provide the same semantics for path canonicalization, and perhaps they should.  However, Microsoft was fully aware of Unix path semantics and behavior, and clearly chose deliberately to do things in an incompatible and much more complicated way.  I feel that was a mistake, but we're stuck with it now.  I need to change the directory to the root of the repository to make GIT WORK TREE work.  I also need to be able to resolve any other Git environment variables to their canonical paths so I can set them appropriately before invoking Git, and all of those need to be canonicalized appropriately and compatibly with Git.  This is also the behavior that anyone else working with Git will need to implement.  I also need to be able to determine if a path the user has provided is within the repository and fail if it is not.  I can do that by removing the trailing file component, canonicalizing the directory, and appending the file path.  That works to tell me if the component is in the repository, whether or not the file itself is a symlink.  Repositories are not allowed to span file systems.   I have one question on this. Say that we have network\repo mounted as N:, just like in the referenced bug. Within the repo, there's a committed symlink N:\mylink that points to a file on a local disk C:\file. Would you expect it to return this? If I followed you correctly, you would need the former, but Get Final Path Name By Handle obviously returns the latter.  I don't need to canonicalize file names, only directory names.  However, I do need the standard Get Final Path Name By Handle semantics for that.  Other people may have different use cases and will also need the standard semantics.  I want to be clear that I'm not asking for this just for Git LFS.  I'm asking for this because these are the standard platform semantics and they should be available in a cross-platform way.  I fully realize that they are inconsistent across platforms.  I want the same behavior out of Go that I get out of languages that use the system C library.  I'm not looking for special functionality to handle my special case, I'm looking for generic tools that make Go programs compatible with non-Go programs.  It's fine that Go does not wrap the system C library, but if it chooses not to do that, it needs to be compatible with the semantics of the platform as if it did, and not provide behavior that is materially different.  Trying to sell me on a behavior that is consistent across platforms when that is not practically achievable in a useful way isn't helpful.  That's what we have now with Eval Symlinks, and everyone agrees that it's not the right decision."
technical,"Yes, I believe that they are identical.  filepath.Eval Symlinks is, as far as I'm aware, equivalent to realpath(3) on Unix and has the semantics I'm looking for. This proposal should probably also deprecate Eval Symlinks, which is seriously broken on Windows, see this."
technical,"please give the folks above some credit for our consideration of this issue. he did a careful analysis of Eval Symlinks on Windows in this (read whole thread) -- which I linked above. [BTW, he was improperly locked out of this thread by an anonymous admin for no stated reason.]  It's clear that we wouldn't add Eval Symlinks today if it didn't exist, so the best long term solution is deprecating Eval Symlinks.  We cannot fix Eval Symlinks because certain changes from relative to absolute paths could break existing Windows programs. Alex will not agree to that, and he's the Windows maintainer. this proposal, as it stands today, is about adding a new API, filepath.Resolve, which still violates everything mentioned in #40180. #40180 is fundamentally a software engineering problem: ericwj is saying that the documentation should mention that the function should be rarely used. The same would apply to filepath.Resolve if it's added. Can you point me to a real-world example? It would be useful while I write the CL. As I said, I plan to go through the history of changes of Eval Symlinks, reconstruct all rationales, make sure there are regression tests, and then attempt a fix. If something is not covered by the existing tests and history of changes, I'd be happy to evaluate it."
technical,"So my understanding is that if this proposal is accepted, we end up with:       * Linux: Resolve and Eval Symlinks are basically the same function, with the only difference is that Resolve always returns an absolute path, while Eval Symlinks returns a path constructed following the symlinks, so it might be absolute or relative depending on how the symlinks were created. Neither of them see through mountpoints in any way.  Yes, because bind mounts on Linux are not identical.  One directory may be read-only while the other may be read-write.  Bind mounts also, by default, don't include submounts and are therefore not necessarily equivalent.       * Windows: Eval Symlinks (after bugfixing) would possibly go through all kind of NTFS links (with the same relative/absolute handling than Linux), and it would not follow mountpoints. Resolve instead would also resolve mountpoints.  As far as I understand it, these mountpoints are functionally identical.  They are more like opaque symlinks rather than the typical Unix mountpoints with different mount options.  This is exactly the behavior that's desired: canonicalizing the path in the way the operating system does that.  Languages that use the system C library don't have this problem because they use the system C library functions for this purpose and implement those functions' behavior.  I would literally define the expected behavior in terms of the system's C library, since that's what other languages (e.g., Rust) do. That doesn't really assist in my use case of finding out whether one path is inside another. This should mostly be fixed by my CL. Please test it and let me know.  It also rectifies some Eval Symlinks issues on Windows reported in your #40180 table. I have a followup CL (still not mailed) that fixes another set of issues in Eval Symlinks. NOTE: I'm not touching the semantic of the function as it is implemented now: I'm just trying to fix implementation bugs so that we can see a clearer picture of where we are now."
technical,"So my understanding is that if this proposal is accepted, we end up with:       * Linux: Resolve and Eval Symlinks are basically the same function, with the only difference is that Resolve always returns an absolute path, while Eval Symlinks returns a path constructed following the symlinks, so it might be absolute or relative depending on how the symlinks were created. Neither of them see through mountpoints in any way.  Yes, because bind mounts on Linux are not identical.  One directory may be read-only while the other may be read-write.  Bind mounts also, by default, don't include submounts and are therefore not necessarily equivalent.       * Windows: Eval Symlinks (after bugfixing) would possibly go through all kind of NTFS links (with the same relative/absolute handling than Linux), and it would not follow mountpoints. Resolve instead would also resolve mountpoints.  As far as I understand it, these mountpoints are functionally identical.  They are more like opaque symlinks rather than the typical Unix mountpoints with different mount options.  This is exactly the behavior that's desired: canonicalizing the path in the way the operating system does that.  Languages that use the system C library don't have this problem because they use the system C library functions for this purpose and implement those functions' behavior.  I would literally define the expected behavior in terms of the system's C library, since that's what other languages (e.g., Rust) do. That doesn't really assist in my use case of finding out whether one path is inside another. To be honest, I disagree on this. The two underlying files are indeed the same and in fact os.SameFile returns true. It sounds absolutely surprising that os.SameFile(a,b) differs from filepath.Resolve(a) == filepath.Resolve(b).  Moreover, the same situation happens on Windows. You can easily have C:\FOO and LOCALHOST\BAR\FOO being the same file with different permissions. I'm not sure why on Windows it should be necessary to have filepath.Resolve returning true in that situation, while on Linux it should return false on bind mounts (or any other kind of mount really).  I think the only correct answer here is that the request is not to establish a cross-platform semantic, but just add a function that calls realpath on Linux and Get Final Path Name By Handle on Windows because this is what most open source projects have ended up doing. I sympathize with the goal of interoperability, but I'm just hoping that we can find a better solution. Looking at this, it looked like filepath.SameFile would be enough, to compare if the git root actually matches. From your answer, I understand that you actually need whether a specific file of the repo is within the git repo (with the git root path being read through git and thus canonicalized through Get Final Path Name By Handle)? Can you please elaborate on the use case you are mentioning?  I have one question on this. Say that we have network\repo mounted as N:, just like in the referenced bug. Within the repo, there's a committed symlink N:\mylink that points to a file on a local disk C:\file. Would you expect this to return network\repo\mylink or C:\file? If I followed you correctly, you would need the former, but Get Final Path Name By Handle obviously returns the latter."
technical,"The docs should say… On Windows, Eval Symlinks is broken for many cases. Real/RealPath/Resolve is the only API to ""evaluate"" symlinks. See this. On unix, the RealPath API is equivalent to this. Eval Symlinks should probably be deprecated. We don't want to carry a large amount of non-portable system-specific code.  We are not the Git project, with many regular full-time contributors across a wide variety of platforms.  Our team is mostly two people, neither of whom use Windows on a regular basis, and we rely on Go to make our Windows support more than best effort.  We already have platform-specific code to support NTLM specially on Windows and it's going away because it's broken and nobody wants to volunteer to fix it.  I also happen to feel that requiring every user who wants to support cross-platform path canonicalization to know about and use Get Final Path Name By Handle explicitly and use it is not going to help Windows get better support, especially when filepath.Eval Symlinks is right there and seems to do the right thing in most cases (except it doesn't).  I only happen to know about it because I've actually looked up which function to use for this purpose at some point in the past, in general, a Unix developer won't know about it and won't know to use it.  filepath.Resolve is used in the case when you want to canonicalize a path, and it will always result in an absolute path.  filepath.Eval Symlinks only resolves symlinks, may produce a relative path if its input is relative, and does not handle non-symlinks on Windows.  It is not suitable for canonicalizing a path on Windows and cannot be used successfully for that purpose.  filepath.Resolve should be used unless you are sure you want the behavior of filepath.Eval Symlinks."
technical,"Are you proposing that the behavior differ from Get Final Path Name By Handle (on Windows) only when the path name is relative, or when the path name is absolute as well?  The former is fine, I think, and I have no position on it, the latter would be a problem for interoperability with tools written in other languages. what do you think of changing the implementation of filepath.Eval Symlinks to just call Get Final Path Name By Handle?"
technical,"This proposal should probably also deprecate Eval Symlinks, which is seriously broken on Windows, see this. What does ""canonical"" mean, precisely?"
technical,"EDIT: I'm not sure which (if any) of the APIs that take a path can handle a length over the Windows limit for relative paths, but these can't: Whether it's OK to return a relative path that's not suitable for external use (e.g. a log) should also be considered. Whether it's OK to return a relative path that's not suitable for external use (e.g. a log) should also be considered. I am not worried about that problem. ""External use"" could mean adding a new path element to a relative directory name, and that might turn a short-enough path into a too-long path. So in the limit the argument would be that we should never use a relative path because of the limits. That's clearly wrong. I filed #41734 for fixing fixLongPath to handle relative paths so that we can stop having discussions about which APIs do and don't accept certain kinds of long paths."
technical,"is unable to comment on this issue.  As far as we can tell, this is a GitHub bug.  It is not due to any intentional action by the Go team.  This comment is by ericwj, sent via e-mail:  Get Final Path Name By Handle does not canonicalize. I just ran it, trying all flags. The result is different depending on how I configure my system, depending on which path I use to access the file and depending on which flag I provide in each situation. This means canonicalization will break if the machine configuration changes, which can also happen *during* Git operations, and the result may change depending on which path is used to access the same file or directory.   The simplicity of Eval Symlinks is one of its problems. Any function string Foo(string) will have some of the same fundamental, unfixable problems that I documented for Eval Symlinks. Issue #40180 only proposes docs fixes because outright no-op'ing Eval Symlinks on Windows was refused in #40104, but I think the conclusion of the analysis in #40180 is that #40104 was refused because I did not argue the case effectively yet. And this proposal is just too similar, apart from perhaps thread-safety and eventually perhaps looking more often like it works.   Even suppose something like Resolve could be perfect, Go cannot handle the return value. The unfortunate truth is that Go can only handle paths with drive letters and ˜normal' UNC paths in the form server\share (without prefix). I have evaluated about half of all path/filepath API's and the prefixes Get Final Path Name By Handle yields break about every one of them. You cannot depend on stripping the prefixes, either.   This proposal goes wrong the same way path/filepath has, which imho is thinking *nix can be ported unmodified to a fundamentally different operating system without proper attention, expertise involved, or testing. And for letting fundamentals slide for the benefit of a bit of simplicity, offering no alternative besides P/Invoke in cases where this makes things fall apart.   I suggest writing the fundamentals, especially well for Windows, testing and reviewing meticulously, while iterating towards an API suitable for the general public on top of those fundamentals, before proposing inclusion in any standard library. If the problem is just UNC mappings, much more appropriate WNet* functions exist to translate DOS device names. I think object identifiers are the more general solution for Git LFS, in favour of path strings. Wrapped in a suitable, portable abstraction to obtain and compare them. Yes, I agree that we cannot exactly reproduce this if we switch Eval Symlinks to Get Final Path Name By Handle. Though I'm not fully sure that the clients of Eval Symlinks really do behave on this implementation detail (that is, unfortunately, documented). What if drop that sentence from the documentation, and just say that Eval Symlinks can either return a relative or an absolute path? Do you have an idea on whether clients really do rely on this specific behavior, even on Linux?  Notice that realpath on Linux doesn't follow mountpoints as well:  Is realpath(3) on Linux different than this? I might be missing something here.  So my understanding is that if this proposal is accepted, we end up with:  * Linux: Resolve and Eval Symlinks are basically the same function, with the only difference is that Resolve always returns an absolute path, while Eval Symlinks returns a path constructed following the symlinks, so it might be absolute or relative depending on how the symlinks were created. Neither of them see through mountpoints in any way. * Windows: Eval Symlinks (after bugfixing) would possibly go through all kind of NTFS links (with the same relative/absolute handling than Linux), and it would not follow mountpoints. Resolve instead would also resolve mountpoints.  It looks like the biggest gain here is that we somehow exposes Get Final Path Name By Handle which is a ""common"" function used in other frameworks / languages to canonicalize paths. Since we're missing it, we face interoperability problems. In fact, semantically speaking, we're going to have a Resolve function which behaves differently between Windows and Linux wrt mountpoints. And BTW we're also getting into another corner: if somebody puts in the effort of adding mountpoint resolution to Resolve on Linux, we end up with interoperability problems again, because the rest of the world might still just be calling realpath(3) on Linux.  A slightly different proposal is to add filepath.SameFile(path1, path2 string). This would use Get Final Path Name By Handle on Windows, and realpath on Linux (but can be further improved on Linux by looking at inode number, etc.). It would solve the real-world problem of many softwares without having to define what a canonical path name is, and would also relax the software engineering concern of preferring to keep around the original input path rather than a resolved one."
technical,"On Unix systems I think the proposed function is the same as filepath.Eval Symlinks. Yes, I believe that they are identical.  filepath.Eval Symlinks is, as far as I'm aware, equivalent to realpath(3) on Unix and has the semantics I'm looking for."
technical,"I am not aware of any cases with worse values. But I did not spend any time on it, and did not consider any scenarios.  We did not implement filepath.Eval Symlinks in one day - it evolved over time. It was completely broken at the start, and then people complained about their scenarios, and we fixed their scenarios. And so on. But we are not the experts in this area, we made decisions to the best of our abilities at the time. Even Microsoft changed this area in the last 10 years - they added some new features that are used by Docker. And we started with Windows XP and now we have Windows 10 - very different OS in that regard. Perhaps just using Get Final Path Name By Handle would have worked as well. I do not know. But I would not change filepath.Eval Symlinks implementation now. The change will definitely brake some programs. you are arguing ""this is the way it's always been"" rather than arguing ""this is why it is the way it is"". That can be a quicker reply, which is understandable. But for what it's worth, explaining why things are the way they are and what exactly would be worse about a change is more useful, because it lets us understand the possible paths forward."
technical,"If there are multiple ways to refer to a filename, the canonical path is the absolute filename which uses no indirections and uses the canonical case (that is, the path component as written to the file system) if the system permits case folding.  On Unix, that's the one that contains no symlinks (and, on macOS, uses canonical case and composition).  On Windows, there are many ways to have indirection in a path: symlinks, junctions, SUBST, etc.  (I don't actually know all of the possible ways, since I almost never use Windows).  The canonical form uses none of those indirections and uses the canonical case. Another way to say this is that assuming no hardlinks exist, a file on Unix should have exactly one canonical name whose components are either directories or non-symlink, non-directory (but possibly special) files. you make assertions without being specific about them. I am confused about three of the things you've said related to this issue. - You linked to #40180 which is making a general software engineering argument along the lines of ""you should never actually replace all the symlinks, that's violating the abstractions that have been set up"". I have some sympathy for that, but if it were true, it would apply not just to Eval Symlinks but also this issue as well. If so, then we should just close this very issue (#37113) as a terrible idea. - I see that you mentioned this issue in #40966, which is about some problems with path lengths in Eval Symlinks on Windows. We've had path length problems elsewhere on Windows. Path length issues are usually pretty straightforward to fix. Why would we want to gate a fix to #40966 on a larger design discussion on this issue? - Finally, you said, with no links at all, ""This proposal should probably also deprecate Eval Symlinks, which is seriously broken on Windows."" How is it broken? That comment would be a good place for an issue link. Thanks."
technical,"I've locked this to contributors for now.  Adding +1 comments is too noisy.  For future reference, add a reaction to the issue body, and don't comment. ##### ISSUE TYPE - Feature Idea  ##### COMPONENT NAME ansible-vault  ##### ANSIBLE VERSION ##### CONFIGURATION ##### OS / ENVIRONMENT ##### SUMMARY ansible-vault decrypt allows the decryption of completely encrypted yaml files, but it will not decrypt vaulted variables in an unencrypted yaml file with encrypted variables.  It would be nice, for CLI purposes, to have decrypt take a partially encrypted file, and give us the decrypted text.  ##### STEPS TO REPRODUCE  ##### EXPECTED RESULTS * Expected plain text output with encrypted variable decrypted.  ##### ACTUAL RESULTS  ERROR! input is not vault encrypted data for test.yml"
technical,"If you just need decryption instead of rekeying, I've written this +1 for this feature"
technical,"same issue here, we need to unencrypt all values and it is a nightmare, this must be common function +1 for this functionality."
technical,"A few expansions to their clever workaround: Instead of a specific var, you can have ansible dump all vars (encrypted and plain) by specifying -a var=""vars"". There will be some noise in the result with a few stock vars, but it's a nice way to see everything at once. You can also specify multiple source files by passing multiple -e ""..."" args. Also, I haven't tested this personally but you can apparently avoid the implicit localhost warnings by setting if needed. Obviously a built-in solution would be much better, but this makes mixed plain/encrypted var files somewhat workable. +1 for this functionality."
technical,I solved this using debug mode. +1 it would be very handy!!!
technical,Works beautifully! No need for --ask-vault-pass if you have the password in a file identified by the ANSIBLE VAULT PASSWORD FILE environment variable: +1 would be super helpful especially as we transition away from fully encrypted ansible-vault files to just files with encrypted variables (for ease of use/readability/ability to modify etc)
technical,"+1 for this functionality. +1, really need it!"
technical,"+1 would be super helpful especially as we transition away from fully encrypted ansible-vault files to just files with encrypted variables (for ease of use/readability/ability to modify etc) A few expansions to their clever workaround: Instead of a specific var, you can have ansible dump all vars (encrypted and plain) by specifying -a var=""vars"". There will be some noise in the result with a few stock vars, but it's a nice way to see everything at once. You can also specify multiple source files by passing multiple -e ""..."" args. Also, I haven't tested this personally but you can apparently avoid the implicit localhost warnings by setting if needed. Obviously a built-in solution would be much better, but this makes mixed plain/encrypted var files somewhat workable."
technical,"Just giving another thumbs up on this, something like the yq solution above works okay and can be scripted, but having the functionality be part of ansible-vault itself would make management and re-keys so much simpler, and require one fewer dependency. A simple but effective solution would be to keep the existing symantecs of ansible vault encrypt, decrypt and view commands, to detect and encrypt and decrypt values of complete files.  For existing users of encrypted files, it would be trivial to convert to the enrypted values.  It could even be considered best practice is to keep encrypted values in files named such as secrets.yml, to make it easier to spot accidently unencrypted secrets.  During the encrypt phase, it would convert any unencrypted values to encrypted values.  This would allow users to very simply add new values just by editing the ""secrets.yml"", test as required, then run the encypt command.   Users would be able to enforce or check encryption by git hooks or similar."
technical,"Yeah definitely that'd be the best option :) Also related, let rekey work on all encrypted variables in a file. There doesn't seem to be a good way to rekey all the encrypted variables, which makes encrypted variables super cumbersome now that we have to rekey (will end up having script this). Even if it just spits it back out to stdout that'd be a huge help instead of modifying the variables in the file directly."
technical,"This issue is waiting for your response. Please respond or the issue will be closed. click here for bot help Any news when this is planned to be implemented in ansible? We have lots of passwords as vaulted variables, hence updating\viewing them is troublesome. I did some script (based on solution, from alikins last post) to at least parse such yml and decrypt every variable to stdout\file to see a decrypted file at once, but this is just a script that is not a complete solution (and it is decrypting only). UPD: I ended up going thru ansible code to understand how it works with encrypted variables and wrote some tiny script that I can use in my automation jobs with Jenkins. I hope it would be useful for anyone who is waiting for this issue to be fixed. It would be nice to have this functionality in ansible out of the box."
technical,"This is still a problem with Ansible 2.8... A solution would be really appreciated! For others looking for a quick solution I created this script.  It uses ruamel.yaml, which preserves ordering, comments etc in the YAML file. Great when depending on decent git diffs etc :)"
technical,"Right now the usability of encrypted variables compared to whole encrypted files is rather poor unfortunately. Especially in cases where I quickly need to access an encrypted variable (e.g. a password) I really don't want to google for solutions like this.  It is also a problem for git diff use cases. Is improving this state still on the roadmap? I didn't find it neither for 2.5 nor 2.6... Hi, so I've figured out a way to do this for checking individual values, using a yaml parser, yq (there's more than one yq project, but I used this). This works with ansible 2.5.2  I have a vars file, with encrypted and unencrypted values, all.yml I have a password file, vault-password  Using yq, I'm able to decrypt the value pretty easily, by selecting the encrypted value and passing it to the decrypt function Decryption successful secretsecret Hope this helps!"
technical, I believe you look after vault
technical,"Any news when this is planned to be implemented in ansible? We have lots of passwords as vaulted variables, hence updating\viewing them is troublesome. I did some script (based on solution, from alikins last post) to at least parse such yml and decrypt every variable to stdout\file to see a decrypted file at once, but this is just a script that is not a complete solution (and it is decrypting only). UPD: I ended up going thru ansible code to understand how it works with encrypted variables and wrote some tiny script that I can use in my automation jobs with Jenkins. I hope it would be useful for anyone who is waiting for this issue to be fixed. It would be nice to have this functionality in ansible out of the box. I do see this in 2.5. ERROR! input is not vault encrypted datavars.yaml is not a vault encrypted file for vars.yaml"
technical,"This issue is waiting for your response. Please respond or the issue will be closed. click here for bot help I poked at this a little yesterday and braindumped some thoughts in code comments. (copied/paraphrased here for discussion) Open a file, figure out if it is all vault or yaml with vault strings, edit.  if yaml with vault strings, parse the yaml with AnsibleYaml and secret. Replace with '!vault-plaintext vault-id' and plaintext. Save, open editor. On save/reencrypt, reparse the file with AnsibleYaml, get the plaintext of the to be reencrypted vaulted string, encrypt it (!vault-plaintext - !vault, AnsibleVaultUnencryptedUnicode - AnsibleVaultEncryptedUnicode). And then, things get complicated... we can't just AnsibleYaml.dumps() the data structure out: 1. Comments and comment placement is not preserved which is kind of annoying 2. AnsibleYaml can loads things into data structures that it can not dumps() out. Ie, we can't serialize a bunch of stuff we can deserialize.  So just AnsibleYaml.dumps'ing the datastructure back to a file will usually either fail or do the wrong thing.  #2 above is unlikely to get fixed soon if ever.  #1 is mostly a limitiation of the PyYaml yaml module ansible uses. Other implementation like Rueyaml can do this, but it is unlikely for ansible to change this any time soon. So, since we can't just serialize to yaml, we likely need to do some string manipulation to replace the '!vault ' blob. We would need to know exactly what the before string looked like and where in the file it is, and what the new vault will look like. But we don't really know what the new  vault-plaintext string will look like.  For that  matter, we don't know if it will be in the same place, or if it will exist at all, or if it will be at the same path in the datastructure after the edit. We could limit edit to only try to work in cases where those aren't changed. We also have no idea what the plaintext will look like.  ideas: - vault-plaintext is a compound yaml type, with fields for the vault id to use, and for the plaintext. Could also possibly include some identifying info for what the vault it replaced looked like. That would give vault-edit enough info to do a reliable job of replacing the previous content. The downside to that approach is that it points out the limitations of the current vault format. It may also be useful to extend vault to support getting a data structure with info in it instead of just the plain text scalar. At the moment, I'm not sure if it could do both but it seems possible.  Or could just call the extended info version of vault  vault-extended or similar.  At that point it might be possible to make !vault-extended the default vault blob format for vaulted files as well. ie, instead of a vaulted file being this. It would be yaml something like this. ie, more or less like this."
technical,"+1, really need it! I solved this using debug mode."
technical,"+1 for this feature I've locked this to contributors for now.  Adding +1 comments is too noisy.  For future reference, add a reaction to the issue body, and don't comment."
technical,"+1 for this functionality. If you just need decryption instead of rekeying, I've written this"
technical,"Why is this issue still assigned to the 2.5 milestone when ansible 2.5 is already release a long time ago ? See #44556 for outdated milestones.  Please reassign to a current milestone, this is a really missing feature imo (especially the lack of rekeying functionality). It would be nice if rekey worked this way as well.  Updating only the encrypted values in a mixed variable file."
technical,"It would be nice if rekey worked this way as well.  Updating only the encrypted values in a mixed variable file. Just giving another thumbs up on this, something like the yq solution above works okay and can be scripted, but having the functionality be part of ansible-vault itself would make management and re-keys so much simpler, and require one fewer dependency."
technical,"For others looking for a quick solution I created this script.  It uses ruamel.yaml, which preserves ordering, comments etc in the YAML file. Great when depending on decent git diffs etc :) maybe a bit unrelated but I like how sops does it."
technical,"This might be something that will get covered in this. As a user, what would you expect the decrypted file to look like? First thought is just to replace the !vault yaml scalar with the decrypted text. That probably makes the most sense for 'view'.  For 'decrypt' and especially 'edit', I'm not sure that will be sufficient. For 'edit', the re-encrypt phrase is going to need to be able to figure out which variable values originally came from a vaulted value. Especially if the file is edited significantly (reordering lines for example, or changing the variable name). So the file presented for editing would need to include some markers indicating the text that was decrypted/should be re-encrypted. A couple of ways to do that:  1) Add comments to mark the text, and doing some text manipulation/regexes to replace it with encrypted text in place. Something like: 2) Add a new yaml type indicating text to be encrypted. Something like: It would be best if we could yaml parse the input, decrypt the value, serialize the yaml to a file for editing, let user edit it, then yaml parse the results, encrypt the value, and serialize to yaml and save.  But... doing that with the available yaml parser would lose comments and ordering of maps.  So likely some in place string/text manipulations will be required. Not going to happen for 2.4, so bumped to 2.5."
technical,Would it be possible for the user to tell you which scalars to decrypt - not try to do the whole file? Right now the usability of encrypted variables compared to whole encrypted files is rather poor unfortunately. Especially in cases where I quickly need to access an encrypted variable (e.g. a password) I really don't want to google for solutions like this.  It is also a problem for git diff use cases. Is improving this state still on the roadmap? I didn't find it neither for 2.5 nor 2.6...
technical,"Running into this issue again and it sucks. Please guys, this issue has been open for almost 2 years now and for people who really use ansible-vault, this is a major pain the butt. same issue here, we need to unencrypt all values and it is a nightmare, this must be common function"
technical,"Hi, so I've figured out a way to do this for checking individual values, using a yaml parser, yq (there's more than one yq project, but I used this). This works with ansible 2.5.2  I have a vars file, with encrypted and unencrypted values, all.yml I have a password file, vault-password  Using yq, I'm able to decrypt the value pretty easily, by selecting the encrypted value and passing it to the decrypt function Decryption successful secretsecret Hope this helps! Thanks, this helps if it is possible to install additional software. I would argue that ansible-vault should also have this functionality built-in."
technical,"A simple but effective solution would be to keep the existing symantecs of ansible vault encrypt, decrypt and view commands, to detect and encrypt and decrypt values of complete files.  For existing users of encrypted files, it would be trivial to convert to the enrypted values.  It could even be considered best practice is to keep encrypted values in files named such as secrets.yml, to make it easier to spot accidently unencrypted secrets.  During the encrypt phase, it would convert any unencrypted values to encrypted values.  This would allow users to very simply add new values just by editing the ""secrets.yml"", test as required, then run the encypt command.   Users would be able to enforce or check encryption by git hooks or similar. That solution would be simple, but likely not enough. For example every variable can be encrypted with a different secret/vault identifier. Also encrypted and unencrypted variables can be mixed.  I'd still like to have a way at least to decrypt all variables belonging to a vault ID transparently using ansible-vault. Seriously, this is a usability problem since Ansible 2.3! This makes it nearly impossible for me to use vaulted variables, since being able to run git diff on changes is important."
technical,"That solution would be simple, but likely not enough. For example every variable can be encrypted with a different secret/vault identifier. Also encrypted and unencrypted variables can be mixed.  I'd still like to have a way at least to decrypt all variables belonging to a vault ID transparently using ansible-vault. Seriously, this is a usability problem since Ansible 2.3! This makes it nearly impossible for me to use vaulted variables, since being able to run git diff on changes is important. This is still a problem with Ansible 2.8... A solution would be really appreciated!"
technical,"Not going to happen for 2.4, so bumped to 2.5. This issue is waiting for your response. Please respond or the issue will be closed. click here for bot help"
technical,I do see this in 2.5. ERROR! input is not vault encrypted datavars.yaml is not a vault encrypted file for vars.yaml This issue is waiting for your response. Please respond or the issue will be closed. click here for bot help
technical,"Would be also good if 'ansible-vault view' worked for such files. This might be something that will get covered in this. As a user, what would you expect the decrypted file to look like? First thought is just to replace the !vault yaml scalar with the decrypted text. That probably makes the most sense for 'view'.  For 'decrypt' and especially 'edit', I'm not sure that will be sufficient. For 'edit', the re-encrypt phrase is going to need to be able to figure out which variable values originally came from a vaulted value. Especially if the file is edited significantly (reordering lines for example, or changing the variable name). So the file presented for editing would need to include some markers indicating the text that was decrypted/should be re-encrypted. A couple of ways to do that:  1) Add comments to mark the text, and doing some text manipulation/regexes to replace it with encrypted text in place. Something like: 2) Add a new yaml type indicating text to be encrypted. Something like: It would be best if we could yaml parse the input, decrypt the value, serialize the yaml to a file for editing, let user edit it, then yaml parse the results, encrypt the value, and serialize to yaml and save.  But... doing that with the available yaml parser would lose comments and ordering of maps.  So likely some in place string/text manipulations will be required."
technical,+1 it would be very handy!!! Works beautifully! No need for --ask-vault-pass if you have the password in a file identified by the ANSIBLE VAULT PASSWORD FILE environment variable:
technical,I believe you look after vault Would be also good if 'ansible-vault view' worked for such files.
technical,"I poked at this a little yesterday and braindumped some thoughts in code comments. (copied/paraphrased here for discussion) Open a file, figure out if it is all vault or yaml with vault strings, edit.  if yaml with vault strings, parse the yaml with AnsibleYaml and secret. Replace with '!vault-plaintext vault-id' and plaintext. Save, open editor. On save/reencrypt, reparse the file with AnsibleYaml, get the plaintext of the to be reencrypted vaulted string, encrypt it (!vault-plaintext - !vault, AnsibleVaultUnencryptedUnicode - AnsibleVaultEncryptedUnicode). And then, things get complicated... we can't just AnsibleYaml.dumps() the data structure out: 1. Comments and comment placement is not preserved which is kind of annoying 2. AnsibleYaml can loads things into data structures that it can not dumps() out. Ie, we can't serialize a bunch of stuff we can deserialize.  So just AnsibleYaml.dumps'ing the datastructure back to a file will usually either fail or do the wrong thing.  #2 above is unlikely to get fixed soon if ever.  #1 is mostly a limitiation of the PyYaml yaml module ansible uses. Other implementation like Rueyaml can do this, but it is unlikely for ansible to change this any time soon. So, since we can't just serialize to yaml, we likely need to do some string manipulation to replace the '!vault ' blob. We would need to know exactly what the before string looked like and where in the file it is, and what the new vault will look like. But we don't really know what the new  vault-plaintext string will look like.  For that  matter, we don't know if it will be in the same place, or if it will exist at all, or if it will be at the same path in the datastructure after the edit. We could limit edit to only try to work in cases where those aren't changed. We also have no idea what the plaintext will look like.  ideas: - vault-plaintext is a compound yaml type, with fields for the vault id to use, and for the plaintext. Could also possibly include some identifying info for what the vault it replaced looked like. That would give vault-edit enough info to do a reliable job of replacing the previous content. The downside to that approach is that it points out the limitations of the current vault format. It may also be useful to extend vault to support getting a data structure with info in it instead of just the plain text scalar. At the moment, I'm not sure if it could do both but it seems possible.  Or could just call the extended info version of vault  vault-extended or similar.  At that point it might be possible to make !vault-extended the default vault blob format for vaulted files as well. ie, instead of a vaulted file being this. It would be yaml something like this. ie, more or less like this. Would it be possible for the user to tell you which scalars to decrypt - not try to do the whole file?"
technical,"Thanks, this helps if it is possible to install additional software. I would argue that ansible-vault should also have this functionality built-in. Yeah definitely that'd be the best option :)"
technical,"Files identified in the description. If these files are incorrect, please update the component name section of the description or use the !component bot command. click here for bot help ##### SUMMARY #67429 (included in Ansible v2.9.6) causes a massive performance penalty compared to Ansible v2.9.5. I have a project making heavy use of Ansible's template module. With v2.9.5 it takes about 15 seconds to render a file. With v2.9.6, it takes about 9 minutes. Reverting the change in #67429 (just changing if cache and only one: back to if cache:) restores the v2.9.5 performance.  ##### ISSUE TYPE - Bug Report  ##### COMPONENT NAME  ##### ANSIBLE VERSION - Paste verbatim output from ""ansible --version"" between quotes.  ##### CONFIGURATION - Paste verbatim output between quotes - paste below ##### OS / ENVIRONMENT - Provide all relevant information below, e.g. target OS versions, network device firmware, etc. -- CentOS 7  ##### STEPS TO REPRODUCE - Describe exactly how to reproduce the problem, using a minimal test-case -- Use large templates with lots of Jinja code. I am rendering configuration files for network devices, so the end resulting rendered files are around 2000 lines or less, but the templates use a lot of Jinja features.  ##### EXPECTED RESULTS - Describe what you expected to happen when running the steps above -- No performance regression.  ##### ACTUAL RESULTS - Describe what actually happened. If possible run with extra verbosity (-vvvv) -- Performance regression."
technical,"Files identified in the description. If these files are incorrect, please update the component name section of the description or use the !component bot command. click here for bot help After reviewing the change, we believe this to be a correct change. However, since you are seeing a performance degradation, I'm going to assume that you have some variable assigned to a lookup, that is being repeatedly re-evaluated now. Can you confirm whether this is the case, and how that variable is being used in these templates? In order to more effectively help you, we ultimately need a reproducer that we can run that replicates the behavior you describe."
technical,"The bug happens here too. Steps are taking up to 20 Minutes with 2.9.7 when they access a software version/location lookup structure that maps a current variable multiple times. For now we have installed an older version (2.9.3) as workaround. As an update to this issue, we will not revert the fix, as the original bug has been reclassified as a CVE and documented here. The 2.9.x series will not see any resolution of this performance degradation, as it will require a new feature to implement. It is also unlikely that such a feature will make it into the 2.10 release. In the meantime, as indicated above, I recommend using an intermediate set fact task to ""finalize"" the value.  If you are experiencing this issue during template file generation due to looping, you can also take advantage of Jinja2 assignments to create an intermediate cached variable. For the time being I am locking this issue to further activity."
technical,"The bug happens here too. Steps are taking up to 20 Minutes with 2.9.7 when they access a software version/location lookup structure that maps a current variable multiple times. For now we have installed an older version (2.9.3) as workaround. Files identified in the description. If these files are incorrect, please update the component name section of the description or use the !component bot command. click here for bot help"
technical,"Well I'm having trouble making a reproducer for this. Unfortunately I can't easily attempt to reproduce this on the same hardware where I first experienced the problem because my company network heavily restricts Internet access from my development environment. So I've been trying to reproduce on my home computer, but am only getting essentially identical results between 2.9.5 and 2.9.6. One difference between my work and home environments is that the work env has HOME mounted over NFS, so I wonder if that has something to do with it. Otherwise, I'm not sure. I can also confirm that I'm seeing this templating caching change have pretty significant performance degradation issues. Before 2.9.6, role execution took anywhere from 3-7 minutes, and when we bump the Ansible minor version to 2.9.6+, it typically takes 45+ minutes to deploy all of our resources and this is always reproducible. When I deploy with more verbose logs, I can see that a task given a jinja2 expression) makes a file lookup call many, many times. In comparison to 2.9.5, this file lookup call was made one or two times, and if I revert the change to if cache and only one, I see the performance issues resolved."
technical,"It's a correct change in 90% of the cases, I agree. Yet there is cases where lookups work with static data and adding an argument to the lookup function would solve 100% of cases. I think what they are saying that while we fixed buggy behaviour as a result, it's not as easy for us  to implement the expected caching behaviour as suggested or previously worked. For now, there is a work-around, using set fact.  Let's use that for now. We are going to introduce a feature in a future release, after 2.10, to re-implement the expected caching behaviour.  That behaviour being, to allow for performance improvement to exist, by having an explicit decision made by the content creator to do the caching."
technical,"It's a correct change in 90% of the cases, I agree. Yet there is cases where lookups work with static data and adding an argument to the lookup function would solve 100% of cases. I'm only using lookup as in somevar, and only in two places out of thousands of lines, and the performance is degraded even when templating files that don't include those lookup calls, so those aren't the problem (not sure if that's what you meant by ""some variable assigned to a lookup""). I'll try to come up with a minimal reproducer soon."
technical,"yes, so in your case, because templating is lazy evaluated, every time meteringconfig default values is used, it's re-evaluated.  The only one var used in templating is described as: Check to see if the string we are trying to render is just referencing a single var. Because meteringconfig default values has a value using lookup it is not just a single var. Right now, to improve performance, you would need to have a set fact task towards the start of the role that ""finalizes"" the variable, so that the result is stored. We've been talking about someway to tell ansible to finalize a variable immediately on definition, like this. But that feature does not exist yet. Another potential future feature, is a way to instruct lookup to cache the value. I'm puzzled why this caching was removed without an optional fallback. lookup() could have dynamic data! Ok, mine don't. Could we have an optional and disabled by default argument ""enable lookup cache""?"
technical,"We fixed valid bugs by making this change.  The change in question is a correct change. However, it exposes that some people were relying on buggy behavior, without realizing it. It's a correct change in 90% of the cases, I agree. Yet there is cases where lookups work with static data and adding an argument to the lookup function would solve 100% of cases."
technical,"I think what they are saying that while we fixed buggy behaviour as a result, it's not as easy for us  to implement the expected caching behaviour as suggested or previously worked. For now, there is a work-around, using set fact.  Let's use that for now. We are going to introduce a feature in a future release, after 2.10, to re-implement the expected caching behaviour.  That behaviour being, to allow for performance improvement to exist, by having an explicit decision made by the content creator to do the caching. No chance to introduce it in 2.9? It seems like a single argument and an if."
technical,"I can also confirm that I'm seeing this templating caching change have pretty significant performance degradation issues. Before 2.9.6, role execution took anywhere from 3-7 minutes, and when we bump the Ansible minor version to 2.9.6+, it typically takes 45+ minutes to deploy all of our resources and this is always reproducible. When I deploy with more verbose logs, I can see that a task given a jinja2 expression) makes a file lookup call many, many times. In comparison to 2.9.5, this file lookup call was made one or two times, and if I revert the change to if cache and only one, I see the performance issues resolved. That would definitely have an impact, if something is doing a filesystem lookup X times, on NFS it might be a 20-30ms delay, whereas on a local SSD it's like 0.01ms... if it's running 100 lookups, that's an extra 3 seconds per task. I've run into similar issues when pulling data from EFS on AWS (not reproducible outside of that environment)."
technical,No chance to introduce it in 2.9? It seems like a single argument and an if. The bug happens here too. Steps are taking up to 20 Minutes with 2.9.7 when they access a software version/location lookup structure that maps a current variable multiple times. For now we have installed an older version (2.9.3) as workaround.
technical,"I'm puzzled why this caching was removed without an optional fallback. lookup() could have dynamic data! Ok, mine don't. Could we have an optional and disabled by default argument ""enable lookup cache""? We fixed valid bugs by making this change.  The change in question is a correct change. However, it exposes that some people were relying on buggy behavior, without realizing it."
technical,"I'm only using lookup as in somevar, and only in two places out of thousands of lines, and the performance is degraded even when templating files that don't include those lookup calls, so those aren't the problem (not sure if that's what you meant by ""some variable assigned to a lookup""). I'll try to come up with a minimal reproducer soon. We use a lot of lookup in variables too and observing a 26% performance degradation in our case on 2.9.6+"
technical,"That would definitely have an impact, if something is doing a filesystem lookup X times, on NFS it might be a 20-30ms delay, whereas on a local SSD it's like 0.01ms... if it's running 100 lookups, that's an extra 3 seconds per task. I've run into similar issues when pulling data from EFS on AWS (not reproducible outside of that environment). yes, so in your case, because templating is lazy evaluated, every time meteringconfig default values is used, it's re-evaluated.  The only one var used in templating is described as: Check to see if the string we are trying to render is just referencing a single var. Because meteringconfig default values has a value using lookup it is not just a single var. Right now, to improve performance, you would need to have a set fact task towards the start of the role that ""finalizes"" the variable, so that the result is stored. We've been talking about someway to tell ansible to finalize a variable immediately on definition, like this. But that feature does not exist yet. Another potential future feature, is a way to instruct lookup to cache the value."
technical,"I've locked this to contributors for now.  Adding +1 comments is too noisy.  For future reference, add a reaction to the issue body, and don't comment. ##### SUMMARY It would be quite useful if you can loop over more than one single tasks. For instance if you have to poll a remote system for some progress and at the same time you want to push this progress to another backend, you could be doing. There are many uses to this. ##### ISSUE TYPE - Feature Idea ##### COMPONENT NAME Core"
technical,"Same as matanbaru with a way to fail after multiple retries. +1 could you please add retry-until in loops ! Absolutely ""Must Have"" feature !"
technical,Linking this here. +1 For implementing this feature
technical,"I tried to use until in a include tasks and it didn't worked. What I did as a workaround was to create a yml file that include the task (loop.yml) and call itself recursively (recursive.yml) while the condition is still not satisfied. In the above file, I included a timeout in the case of taking too long (this is in a role that). Not the ideal solution, but worked for me and was relatively easy to change using until to do the above. +1 for this feature !"
technical, +1 for until loops on blocks
technical,"We do have an open proposal to ""taskify"" includes, which would allow things like until to work on them. I, however, do not believe that blocks should be extended to support this feature. And what would be the reason for not extending the functionality to blocks ? As it would be a natural thing if it would work. (i.e. being able to loop every construction within a play)"
technical,"And what would be the reason for not extending the functionality to blocks ? As it would be a natural thing if it would work. (i.e. being able to loop every construction within a play) blocks are currently 'static' groupings, enabling loops on them (not just having tasks inherit them) would require making them dynamic ... as we saw with include: this has many consequences that are not immediately apparent."
technical," Hmm, I searched for various combination of keywords, and that one did not stick out :-(  That said, I tried using until: with block:, include: and include tasks, but the first one fails, and the 2 others only run the included file once. But apparently looping only works when using loop: ?"
technical,+1 for until loops on blocks I have been searching a way to do until (infinitely) for a success on all modules on the block and I managed to do this with include tasks with rescue. I could not use regular until because the IP is changing over time and had to modify it on the run.
technical,"This is absolutely a shortcoming.  Ansible has been perfect for most of my needs so far, but it seems like expanding the capabilities of blocks would make Ansible a lot better solution. I tried to use until in a include tasks and it didn't worked. What I did as a workaround was to create a yml file that include the task (loop.yml) and call itself recursively (recursive.yml) while the condition is still not satisfied. In the above file, I included a timeout in the case of taking too long (this is in a role that). Not the ideal solution, but worked for me and was relatively easy to change using until to do the above."
technical,"In any case, the documentation does not give any detail, or even does not discuss what is supposed to work and what not. There's no real distinction between ""loops"" and until-loops, not sure how we can make this more clear overal. The expectation is that what works for ""loops"" also works for until-loops. i would do both, allow until/retry loops to work with includes and then clearly document how they work ... so we have something to point at when it does not meet some people's expectations"
technical," I've locked this to contributors for now.  Adding +1 comments is too noisy.  For future reference, add a reaction to the issue body, and don't comment."
technical,"To extend what he mentions, doing so would require us to deprecate block and replace with something like block dynamic and block static. Also,  every  user of ansible utilizes blocks, whether explicit, or our internal implicit use of them.  They are a fundamental building block of how tasks are represented and executed.  Changing such an integral feature is sure to lead to unforeseen issues. In any case, the documentation does not give any detail, or even does not discuss what is supposed to work and what not. There's no real distinction between ""loops"" and until-loops, not sure how we can make this more clear overal. The expectation is that what works for ""loops"" also works for until-loops."
technical,"Yeah, until is not a valid argument for includes In other words, what we need is not possible, neither on blocks or on includes. So I will keep this one open, but changed the title."
technical,"+1 For implementing this feature It is natural that a block should be repeatable. Otherwise, it is very counterintuitive. It confuses. And there seems to be no way to write a playbook with loop with more than one statement. No programming language is so limited."
technical,"It is natural that a block should be repeatable. Otherwise, it is very counterintuitive. It confuses. And there seems to be no way to write a playbook with loop with more than one statement. No programming language is so limited. It looks like in Ansible, if one needs to do something complex, one should write an action plugin. This is what we are going to do. Here one has many examples: Best regards"
technical,"i would do both, allow until/retry loops to work with includes and then clearly document how they work ... so we have something to point at when it does not meet some people's expectations Just to provide a small amount of detail about how includes work, is that dynamic includes are more of an internal trigger, as opposed to something that wraps execution. As such, the task executor short circuits early on an include, indicating to the strategy that it should read a file and insert task blocks into the TQM, that will later be processed by the task executor. Due to this, there is no tracking of state as a roll up to the parent include.  So an until loop, which would rely on some version of a failed when/success scenario, would only refer to whether or not the strategy was told to do as detailed above.  In which case, it should always succeed. In any case, the mode of operation is that we short circuit far before an until conditions are inspected.  If we just ""made it work"" right now, it definitely wouldn't do what a person expects.  To do what people expect, would require ansible/proposals#136 to be implemented."
technical,"The other thread is locked now. But this recent suggestion (sivel comment above) is brand new. And you are saying we might want to leave open the possibility of someone else coming along later, to do a PR for looping over blocks off their own backs. Then at least we could make it crystal clear to them, as to make it as a separate and new block dynamic:, and not touching the traditional static block: intact? Would that not make more sense to everybody ? Can we all agree upon that ahead of time? Because I agree with this idea. For all the same reasons - it's going to help prevent breaking other existing stuff which we rely on. Whilst still allowing the possibility of someone to come along, try making a PR for actually implementing it. Should we really want them to be making the best possible job and such. Then we should at least be clearly specifying this. If we already know that ahead of time. Which seems to be the case now? Linking this here."
technical,I have been searching a way to do until (infinitely) for a success on all modules on the block and I managed to do this with include tasks with rescue. I could not use regular until because the IP is changing over time and had to modify it on the run. Same as matanbaru with a way to fail after multiple retries.
technical,"the until in this case would have to rely on vars set or registered from the included tasks as the registration of the include itself would be useless ... it would still 'work' just not how most other cases do. The other thread is locked now. But this recent suggestion (sivel comment above) is brand new. And you are saying we might want to leave open the possibility of someone else coming along later, to do a PR for looping over blocks off their own backs. Then at least we could make it crystal clear to them, as to make it as a separate and new block dynamic:, and not touching the traditional static block: intact? Would that not make more sense to everybody ? Can we all agree upon that ahead of time? Because I agree with this idea. For all the same reasons - it's going to help prevent breaking other existing stuff which we rely on. Whilst still allowing the possibility of someone to come along, try making a PR for actually implementing it. Should we really want them to be making the best possible job and such. Then we should at least be clearly specifying this. If we already know that ahead of time. Which seems to be the case now?"
technical,"Just to provide a small amount of detail about how includes work, is that dynamic includes are more of an internal trigger, as opposed to something that wraps execution. As such, the task executor short circuits early on an include, indicating to the strategy that it should read a file and insert task blocks into the TQM, that will later be processed by the task executor. Due to this, there is no tracking of state as a roll up to the parent include.  So an until loop, which would rely on some version of a failed when/success scenario, would only refer to whether or not the strategy was told to do as detailed above.  In which case, it should always succeed. In any case, the mode of operation is that we short circuit far before an until conditions are inspected.  If we just ""made it work"" right now, it definitely wouldn't do what a person expects.  To do what people expect, would require ansible/proposals#136 to be implemented. the until in this case would have to rely on vars set or registered from the included tasks as the registration of the include itself would be useless ... it would still 'work' just not how most other cases do."
technical,"It looks like in Ansible, if one needs to do something complex, one should write an action plugin. This is what we are going to do. Here one has many examples: Best regards This is absolutely a shortcoming.  Ansible has been perfect for most of my needs so far, but it seems like expanding the capabilities of blocks would make Ansible a lot better solution."
technical,"blocks are currently 'static' groupings, enabling loops on them (not just having tasks inherit them) would require making them dynamic ... as we saw with include: this has many consequences that are not immediately apparent. To extend what he mentions, doing so would require us to deprecate block and replace with something like block dynamic and block static. Also,  every  user of ansible utilizes blocks, whether explicit, or our internal implicit use of them.  They are a fundamental building block of how tasks are represented and executed.  Changing such an integral feature is sure to lead to unforeseen issues."
technical,"In other words, what we need is not possible, neither on blocks or on includes. So I will keep this one open, but changed the title. We do have an open proposal to ""taskify"" includes, which would allow things like until to work on them. I, however, do not believe that blocks should be extended to support this feature."
technical,"Hmm, I searched for various combination of keywords, and that one did not stick out :-(  That said, I tried using until: with block:, include: and include tasks, but the first one fails, and the 2 others only run the included file once. But apparently looping only works when using loop: ? Whatever I try, using until: does not work with include: and include tasks:."
technical,"Whatever I try, using until: does not work with include: and include tasks:. Yeah, until is not a valid argument for includes"
technical,Bump on this. Ideally I'd love to avoid using something like pipx to install Ansible on Focal. What still needs to be done here? ##### SUMMARY What the title says  ##### ISSUE TYPE - Pick one below and delete the rest  - Bugfix Pull Request  ##### COMPONENT NAME: tests
technical,"Hi, just checking in on this PR--are there plans to merge soon? As more people are upgrading their systems to Ubuntu 20.04, old builds that used Ansible's official PPA are breaking and more people are switching to Pip installs... from his last comment before closing #68645, he mentioned: Packages for new Ubuntu releases are only created during the Ansible release process, and as Focal has yet to be released, a package will not be made available until the next release that is made after Focal has been released. Focal was released in April, and Ansible has had a number of releases since then... is it possible to get this merged soon? Is it held up on tests?"
technical,The test ansible-test sanity --test validate-modules failed with 1 error. click here for bot help Bump on this. Ideally I'd love to avoid using something like pipx to install Ansible on Focal. What still needs to be done here?
technical,"The issue is fixed in the latest  development  release of Ubuntu, but not in 20.04. For those who came here because the PPA does not contain a suitable package, note that Ansible 2.9.6 is included in Ubuntu's standard repositories."
technical," Hi, just checking in on this PR--are there plans to merge soon?"
technical,It's held up by Ubuntu. See this -- there's not much we can do until that is fixed. I see the Ubuntu issue is set to resolved. Are we unblocked here?
technical,"As more people are upgrading their systems to Ubuntu 20.04, old builds that used Ansible's official PPA are breaking and more people are switching to Pip installs... from his last comment before closing #68645, he mentioned: Packages for new Ubuntu releases are only created during the Ansible release process, and as Focal has yet to be released, a package will not be made available until the next release that is made after Focal has been released. Focal was released in April, and Ansible has had a number of releases since then... is it possible to get this merged soon? Is it held up on tests? It's held up by Ubuntu. See this -- there's not much we can do until that is fixed."
technical,"I see the Ubuntu issue is set to resolved. Are we unblocked here? The issue is fixed in the latest  development  release of Ubuntu, but not in 20.04."
technical, The test ansible-test sanity --test validate-modules failed with 1 error. click here for bot help
technical,"I'm sorry I don't have much time for other things beside work and family. I didn't realize a 4 line PR would pull me into a discussion on how to improve it by implementing a new code base and consequently figure why shippable would fail. I understand your request but I'm not sure it is the most efficient. Release fast and refactor after by actors that have an interest (they'll make time) in it (dissecting to list() brings not value to my use case). Look, 24h later, a valid tiny patch is still not merged and we'd need to ""discuss"" why shippable failed on its improvement, not even on the patch itself. It's clearly less effective than merge and refactor later added to the fact I feel my time creating that PR was wasted, so not rewarding, hence I'll think twice before making another PR. There is much more to gain by going fast on smalls steps/iteration and securing what you have asap, that's my way of working anyway so very subjective I'll give you that. Feel free do whatever you see fit, I'm off to other things, good luck & sorry for the noise. ##### SUMMARY when there is a single interface, data['TABLE interface']['ROW interface'] is a dict, not an array (case handled like populate structured neighbors lldp)"
technical,"I'm sorry I don't have much time for other things beside work and family. I didn't realize a 4 line PR would pull me into a discussion on how to improve it by implementing a new code base and consequently figure why shippable would fail. I understand your request but I'm not sure it is the most efficient. Release fast and refactor after by actors that have an interest (they'll make time) in it (dissecting to list() brings not value to my use case). Look, 24h later, a valid tiny patch is still not merged and we'd need to ""discuss"" why shippable failed on its improvement, not even on the patch itself. It's clearly less effective than merge and refactor later added to the fact I feel my time creating that PR was wasted, so not rewarding, hence I'll think twice before making another PR. There is much more to gain by going fast on smalls steps/iteration and securing what you have asap, that's my way of working anyway so very subjective I'll give you that. Feel free do whatever you see fit, I'm off to other things, good luck & sorry for the noise. just so you are aware we have a dedicated Working Group for network. You can find other people interested in this in #ansible-network on Freenode IRC. For more information about communities, meetings and agendas see this."
technical,"somehow your suggestion fails the check. I tried twice via github gui (dont have git installed and no rights to) and I probably shouldn't make any more PR as they will fail. imo, should merge what works and then iterate not the other way around.  it's ok to not merge the original commit, plugins are fine. Please feel free to reopen the pull request so that we can discuss about the CI failure. Your original commit in the PR looked fine and will work, but the review I submitted is how ansible code base does it."
technical,"This would be really nice to have!  I am working on an extension that takes **encrypted** files and **decrypts** them. The problem is I need the **encrypted file path** so that I can decrypt the file.  vscode doesn't let you access the file uri of a binary file because the active editor is undefined.  Alternatively it would be cool to allow the user to choose what encoding to display binary files. I would think the default would be UTF-8. #11247 is about making the tab-model not only readable but also writeable, e.g. open a background-tab which we must consider when implementing this"
technical," A simple list of open editors would be a good starting point  My request (also closed as a dupe of this) was for reliable open/close events. This is required for languages that want to prioritise the open files. From the open/close events you can maintain a list of open editors (and know when they changed), so I think if only one thing was done, events would be better than an API to just get the list (or at least, also an event when they changed)."
technical,"I don't know if this is still blocking you, but I've used a hack here that might work for your depending on your use-case. You can see it in action in the Restore Editors extension. ahhh very nice! Never would have thought to use the workbench.action.nextEditor cmd to loop through all the editors.  Still seems like sort of a ridiculous hack for something that should obviously be accessible in the API."
technical," Also, just wanted to add that this is blocking our vscode Salesforce IDE extension (mavensmate) from reaching feature parity with Sublime and Atom (and thus increasing the risk of it being abandoned by users).  Background:  Salesforce requires that classes be compiled on their servers.  In order to refactor multiple classes, you must send a single compilation request with all classes.  Because compilation is slow, you typically only want to compile a subset of your project (IE only files related to a refactor).  The way existing salesforce IDE's handle this, is by allowing users to compile all open tabs."
technical,Happy to give feedback on the API as well. any news about this feature?  Unfortunately it's a blocker for our extension because we absolutely need to unlock the file after the has finished to editit.
technical,do we have a plan to get a list of open text editors. looping with the command nextEditor is really not an ideal solution. feels sluggish and hacky. Any plan? It has been years.
technical,"Is there any update on this? The tab model issue (that I was pinning my hopes on) has been closed...  This issue has been open for over 2 years? As the fifth highest voted open issue for the API, I hope to see this in the VS Code Roadmap 2019 as the final release of 2018 wraps up in the few weeks."
technical,"This workflow any many others will be supported in the extension that I'm building for VS and VS Code  I want to cover all aspects of ""dev context"" that is created when working at specific PR/task/feature/bug. There is often a need to switch to something else and there is no easy way to turn back and restore lost context. Moreover there will be a way to save and share your contexts with entire team because they will be saved as diffable files (aka mental snapshots). Ready to commit in git. Stay tuned! But how? How do you get the open editors when there's no API for it (hence this issue)?"
technical," Cheers, using your work around for something I'm working on too!  Also noticed I had to copy the way you wait in between executing commands because sometimes the activeEditor wont be set yet? Is that a separate issue worth reporting do you think?"
technical,hope never dies cmon
technical,Finally found this issue after banging my head on the API for hours - this is a crazy oversight and I'm amazed it is 3 years in the waiting with so many interested parties.  Also this issue is not a duplicate. That set of features I also need to be able to deal with webViews and other non-textEditors but this request does not imply that non-textEditor tabs would be exposed in a useful way. do we have a plan to get a list of open text editors. looping with the command nextEditor is really not an ideal solution. feels sluggish and hacky.
technical,"I'm unsure if I can rely on this now though? No do you happen to know what might be triggering it to happen immediately for me today? I'm trying to track down a bug that I think is related to it being called later, but I can't confirm it while it's triggering immediately. Thanks!"
technical,"In my case, would be enough to provide an event when a text document has been hidden, that is, when its tab has been closed. Even the document is still there waiting to be disposed. I just need a notification. Finally found this issue after banging my head on the API for hours - this is a crazy oversight and I'm amazed it is 3 years in the waiting with so many interested parties.  Also this issue is not a duplicate. That set of features I also need to be able to deal with webViews and other non-textEditors but this request does not imply that non-textEditor tabs would be exposed in a useful way."
technical,"good question. When there is no official API it will be harder to implement but I believe possible. In the second comment eamodio mentioned hack that his Restore Editors is using. When you will dig long enough sometimes you find a way.  Visual Studio also doesn't have any explicit API for its internal engine that restores last opened windows and tabs. It has .suo file when it stores them but it's like black box. You could save or restore certain windows state but you have no control how it will be done at window/tab level. Because VS's last opened files engine has many flaws I wrote independent engine that ContextKeeper is using to position VS's windows and restore tabs. It works very well and allows a whole range of possibilities like opening the same mental snapshot when you switch from your desktop box to laptop without manual windows repositioning - it's detects current resolutions of connected monitors and prepare appropriate layout where all windows fit nicely to new environment. The ContextKeeper's engine allows to save mental snapshots to human-friendly diffable files so you could not only switch between different contexts easily but also track what was changed in them via git. Sharing contexts with a team opens another range of possibilities.  Nothing above would be possible when I would give up and tried to use broken and limited "".suo"" API. It took a lot of time but I found a way and created ""alternative"" API. For vscode, it sounds like you'll end up hacking the application code rather than building an extension then, which is a shame. On your website it says you have a commit ready for git - I assume that's for Visual Studio and not vscode?"
technical,"#11247 is about making the tab-model not only readable but also writeable, e.g. open a background-tab which we must consider when implementing this FYI there is some further conversion about this here  any thoughts on that? It doesn't address the need of an API for a lot of other use-cases, but it could provide a nice shortcut for a (decent?) set of scenarios."
technical,"Cheers, using your work around for something I'm working on too!  Also noticed I had to copy the way you wait in between executing commands because sometimes the activeEditor wont be set yet? Is that a separate issue worth reporting do you think? getting an active editor of undefined is to be expected at this point. It gets set to undefined if there are really no editors, but also if the focus switches to a non-editor window"
technical,"But how? How do you get the open editors when there's no API for it (hence this issue)? good question. When there is no official API it will be harder to implement but I believe possible. In the second comment eamodio mentioned hack that his Restore Editors is using. When you will dig long enough sometimes you find a way.  Visual Studio also doesn't have any explicit API for its internal engine that restores last opened windows and tabs. It has .suo file when it stores them but it's like black box. You could save or restore certain windows state but you have no control how it will be done at window/tab level. Because VS's last opened files engine has many flaws I wrote independent engine that ContextKeeper is using to position VS's windows and restore tabs. It works very well and allows a whole range of possibilities like opening the same mental snapshot when you switch from your desktop box to laptop without manual windows repositioning - it's detects current resolutions of connected monitors and prepare appropriate layout where all windows fit nicely to new environment. The ContextKeeper's engine allows to save mental snapshots to human-friendly diffable files so you could not only switch between different contexts easily but also track what was changed in them via git. Sharing contexts with a team opens another range of possibilities.  Nothing above would be possible when I would give up and tried to use broken and limited "".suo"" API. It took a lot of time but I found a way and created ""alternative"" API."
technical,"I've encountered a bunch of extensions that are broken or work poorly due to this specific issue. Haha. Funny you linked that, my comments above were for when I was making that add-on :P  You'll be pleased to know that ""close all saved"" is actually built into VSCode now (thanks to soneymathew)"
technical,Any plan? It has been years. Happy 2020! Still no plan?
technical,please ping me if you have something. I would like to use it in the LSP client libs to only sync documents that are visible in the editor. Happy to give feedback on the API as well.
technical,"I am doing some diffing, so I am relying on onDidOpenTextDocument to get an initial state, then using onDidChangeTextDocument to get the updated state. It seems when the editor restores files, it does not call onDidOpenTextDocument, or my extension is loaded after this happens (activation state is *). Being able to iterate over the open files would really help, especially if I could do that in activate. Thanks! has something changed here recently? In current stable code, I'm seeing onDidCloseTextDocument immediately when I close a tab in the editor. I'm unsure if I can rely on this now though?"
technical,Support the author's request of this extension Has this have any plans about getting the opend files?
technical,Now 4th...  not even a comment on this thread from the devs for nearly 2 years. :disappointed: hope never dies
technical,"Is this planned any time soon? The vim issue is pretty annoying. Hopefully thumbs up attract more attention to this issue. For non-vim users, we cannot effectively edit the same file opened in two different editor groups (e.g. split one file vertically and view/edit different parts in each pane)"
technical,"Has this have any plans about getting the opend files? I am doing some diffing, so I am relying on onDidOpenTextDocument to get an initial state, then using onDidChangeTextDocument to get the updated state. It seems when the editor restores files, it does not call onDidOpenTextDocument, or my extension is loaded after this happens (activation state is *). Being able to iterate over the open files would really help, especially if I could do that in activate. Thanks!"
technical,"It's written badly - there is no such thing as a ""visible text document"".   Yeah, it should a text document in a visible text editor. Which is also a little bogous because all text editors visible by definition... At least we can fix the jsdoc I believe that's wrong and I am sure you'll able to file a separate bug with repro steps where a document that's considered open (e.g loaded) isn't updated anymore  I don't have a repro because in my current testing, onDidcloseTextDocument always fires immediately. Like I said, I'm not sure that's the issue - I was trying to eliminate it. My understanding was that when the editor was visible, the user may be prompted before reloading a changed document, so I figured it might not happen if the editor is gone.  Is there some situation where onDidCloseTextDocument *won't* fire immediately, so I can test it?  I raised this previously."
technical,"thanks for the details.  I'm not specifically trying to know what tabs are visible. It's fine for me if there's a visible tab that doesn't have an open document. It's the opposite that I think may cause issues (an open document without an editor). This is about ensuring language server state doesn't get out of sync with what the user sees. I don't think it's the same issue that this is asking for, but since my issues were all closed I'm not sure where to post it.  Here's an issue I believe *may* exist if onDidCloseTextDocument doesn't fire when the user closes an editor tab, though I can't verify it right now because onDidCloseTextDocument appears to be firing immediately. I'm concerned if that ceases to be the case (which you suggest may happen), this bug may occur. I'm investigating issues where users have reported changing Git branches and seem to have phantom errors left in their Problems view (or new errors that they should not). Here's what I think might be happening:  I'll use the LSP spec for examples here as it's clearest, but I think the issue is the same for non-LSP extensions.  The LSP spec says (about textDocument/didOpen):   The document open notification is sent from the client to the server to signal newly opened text documents. The document's content is now managed by the client **and the server must not try to read the document's content using the document's Uri**  However, if you:  - Open a file in VS Code - Close the editor - At this point, *if* VS Code has not sent didClose, the server believes the state of the document is still managed by VS Code, even though it's not visible to the user - Change Git branch so that the file is modified on disk - I believe VS Code does not tell the server that file has modified on disk, yet the line above states that the server must also not read it from the disk. I believe this results in the server having the old contents for the file and therefore may show/send phantom errors for a file the user cannot see.  If there's some mechanism that prevents this from happening if onDidCloseTextDocument does not fire immediately, that's great - but I'd like to understand what it is so if this issue comes up again I'm able to debug better. I believe VS Code does not tell the server that file has modified on disk,  I believe that's wrong and I am sure you'll able to file a separate bug with repro steps where a document that's considered open (e.g loaded) isn't updated anymore"
technical,"Also, just wanted to add that this is blocking our vscode Salesforce IDE extension (mavensmate) from reaching feature parity with Sublime and Atom (and thus increasing the risk of it being abandoned by users).  Background:  Salesforce requires that classes be compiled on their servers.  In order to refactor multiple classes, you must send a single compilation request with all classes.  Because compilation is slow, you typically only want to compile a subset of your project (IE only files related to a refactor).  The way existing salesforce IDE's handle this, is by allowing users to compile all open tabs. I don't know if this is still blocking you, but I've used a hack here that might work for your depending on your use-case. You can see it in action in the Restore Editors extension."
technical,lol keep on adding it will get done in few smtg I find it sad that this issue is lower priority than something like pinned tabs...
technical,"We have understood the requirements and we can understand the frustration that our priorities aren't always matching expected priorities. I am now locking this issue because we haven't been able to retrieve additional actionable input. We take stable API very serious and we'll take our time to build a sound and solid API. Once that work has started we will report back here. I have a use case where I would like to be able to get a list of all the open editors (basically exactly what is shown here). I understand that Editors are disposed but I just need a list of filenames. Something like this. Long term, it might be nice to have API access to operate on this list (EG: sort open editors pane by last opened, name, etc)."
technical,"If this is going to continue to get bumped each month, could the team outline a fix here and let the community implement it? Need some pointers to which files to look at and change, and what an acceptable behavior would look like. I just cann't understand what prevents this issue from being fixed. It's hard? It has been years."
technical,"I'm starting to wonder if an extension which manually tracks opened editors might be useful... I know that this issue is primarily concerned with exposing a tab/editor API, but since it's somewhat related, is there an existing way to target and close the ""Welcome"" page in code, or would this be tied to the features requested here?"
technical,"do you happen to know what might be triggering it to happen immediately for me today? I'm trying to track down a bug that I think is related to it being called later, but I can't confirm it while it's triggering immediately. Thanks! I stepped through the code, but couldn't figure out any case where it would not call onDidCloseTextDocument. If I understand the code correctly, when the editor is disposed, it disposes the model.  This results in onModelRemoved calling  updateState().  And  updateState uses the models (which no longer contains the document) to do a diff and figure out why documents to fire close events for.  I can't see any obvious way that this *wouldn't* happen. If there aren't any, then this issue may be resolved (which would be good, because I think delaying the close introduces weird bugs for both local extensions and LSP where the language server has invalid state for files when modifying files outside of VS Code, such as a bit branch change)."
technical,"We were waiting for more the two months. Are you able to give us some even ugly workaround? The use case is simple in my opinion. I need a call to free the resource that was locked when it was open in the editor.  It's a blocker for us and unfortunately without this hook we cannot proceed over. I think this issue is taking so long because its request became too broad.  It started with I would like to be able to get a list of all the open editors. I'm a subscriber because when I opened #13639, it was closed being categorized as a  duplicate  of this one (which I didn't buy, tbh). But after all this time, some other issues, which doesn't seems related to the original request, are referencing this one.  You have:  * open/close events * opening in background * extension being able to close documents * middle click on tabs * and so on....  I mean, is it really necessary to have all of this features, just to provide an API to  access the Open Editors ? commented above a lot of features this API could have, but as I stated, the initial request is much less complex. I know you guys take API very seriously, and that's one of the main reasons (IMHO) VS Code is so stable for developers (  for that), but I think, in this particular case, the API could have started a lot simpler, and earlier, evolving with new features/areas on later releases."
technical,"It's currently in the May milestone. I'm confused - my comments have been marked as off-topic in this thread, yet many issues about that subject have been closed as duplicates of this. It's not clear what the expected behaviour is.  All those issues are closed a duplicate because what you and others are trying to achieve is impossible without this API. However, it doesn't mean that using the onDidCloseTextDocument is any good when trying to know what tabs are open.  The jsdoc for onDidCloseTextDocument reads like this. I am not sure if "" To add an event listener when a visible text document is closed, use the TextEditor events in the [window](#window) namespace. "" is written badly or if you didn't see that.  Anyways, what that means that visible editors and closed text documents have very little in common. The only guarantee is that a document that's visible (which an editor is visible) is never closed. It also means that you cannot use the list of text documents as reliable information to know what editor tabs are open.   If onDidCloseTextDocument fires reliably when you close a document  This is a conceptual misunderstanding. You don't not close a document you close an editor or an editor tab - vscode closes the document when it thinks it is time to do so.  Tab-changes cannot be observed and are subject of this issue. In the screen shot below below, you have  1. 1 text editor 1. between 1 and N text documents 1. 1 settings editor (which has no representation in the API) 1. 7 editor tabs We will not change 1, we will not change 2, we will not change 3. We will add an API that allows to observe and mange the list of tabs (some like to refer to this as the open editors (not  text  editors) list).  Goals of the API  * Have an API representation of the tab model (aka open editors) * Have an API to manipulate the tab model (open, close, activate, move tabs and more) * Have events that fire when tabs change  Note that ""tab model != text editors"" e.g an inactive tab has no text editor, an active tab might not show a text editor but something else. It's not yet clear how we associate a tab with editors that the API is aware of (text editors, notebook editors).   Properties that a ""tab-object"" should have  * name, detail * view column * uri (iff applicable) * API editor object (iff applicable) * likely more  Modifiers that should exist  * close, open tab * active tab * move tab within and across view columns * likely more  Potential extension use-cases * language extension validates all tabs for fooLang, not just visible editor and not the whole project * extension to group tabs by folders, file type etc ([foo.html], [foo.js], [foo.css], [bar.html], [bar.js], [bar.css]) * extension to open/close a set of editor tab"
technical,"issues about onDidCloseTextDocument not firing immediately have been closed as dupes of this (for ex #84505). I'm confused - my comments have been marked as off-topic in this thread, yet many issues about that subject have been closed as duplicates of this. It's not clear what the expected behaviour is. If onDidCloseTextDocument fires reliably when you close a document, then many of the people interested in this thread may be unblocked (and also would be resolved). Is it possible someone can clarify the status?"
technical,"Is there any progress or plan for this functionality? It feels really odd that there is still no way for an extension to be able to simply get a list of the currently opened files (even if they are not loaded). I'm hitting this too. I need to provide my language service with a list of ""priority files"" and I was sending what I believed to be:  1. Visible documents 2. Other open (non-visible) documents  However I've discovered that my means for getting open (non-visible) documents (workspace.textDocuments) is flawed and contains documents that had their editors closed, but the documents are still ""open"" according to Code.  This means I'm unable to write tests to ensure that my priority files are working correctly, as I have no way of telling which files a user actually has open (but not visible)."
technical,"Would like to call out here that this kind of hobbles VSCodeVim, and it sure would be nice to have. I'm starting to wonder if an extension which manually tracks opened editors might be useful..."
technical,"has something changed here recently? In current stable code, I'm seeing onDidCloseTextDocument immediately when I close a tab in the editor. I'm unsure if I can rely on this now though? I'm unsure if I can rely on this now though? No"
technical,"As the fifth highest voted open issue for the API, I hope to see this in the VS Code Roadmap 2019 as the final release of 2018 wraps up in the few weeks. I've encountered a bunch of extensions that are broken or work poorly due to this specific issue."
technical,"I'm confused - my comments have been marked as off-topic in this thread, yet many issues about that subject have been closed as duplicates of this. It's not clear what the expected behaviour is. If onDidCloseTextDocument fires reliably when you close a document, then many of the people interested in this thread may be unblocked (and also would be resolved). Is it possible someone can clarify the status? If this is going to continue to get bumped each month, could the team outline a fix here and let the community implement it? Need some pointers to which files to look at and change, and what an acceptable behavior would look like."
technical,"getting an active editor of undefined is to be expected at this point. It gets set to undefined if there are really no editors, but also if the focus switches to a non-editor window Im not getting undefined in these cases, but sometimes the same editor.  I assumed that was why have the 500ms pauses in your code?"
technical,"So it appears that all this information is readily available, but it's not exported to the extension-visible API. Which would mean that we'd need to send a PR that expands that API surface.  I'm not familiar enough with how the VS Code codebase works to make a PR myself, but I was able to find some starting points, which might help if anyone interested in implementing this wants to dive in. You'll have my undying gratitude if you do!  The ""Open Editors"" view is able to access all this data from the Groups Service, and it does so here.  Taking a step back, what we *do* see in the extension API seems to pass through here. And that extension API seems to be populated from the Main Thread via a state machine here (see the class's usage). That state machine seems to interact with the Groups Service only here (which makes sense since it's not really getting that much from the Groups Service, only a notion of what viewColumn it's in).  It seems like the notion of moveable Groups is relatively recently tacked-on to the codebase, and that technical debt may be the only reason nobody's tried to implement this yet. I doubt that performance considerations have anything to do with it - I haven't seen much mention of performance in the above areas. It might be as simple as allowing more information from that service to percolate through the state machine, perhaps sending along a bookkeeping data structure mapping editor IDs to information about where/what tab-order they currently have in the Group system. Feedback Channels is also something I found for those who may just be getting exposed here (like me!).  Looks like at least 200 people really want to see this work, and it's blocking parity with other editors on important functionality! (For my part, I really want to be able to context-switch between Git branches with confidence that I can restore tabs that only make sense on a different branch... but as other commenters indicate, tab restoration is very limited right now.)  Hopefully someone can jump in here! :) In my case, would be enough to provide an event when a text document has been hidden, that is, when its tab has been closed. Even the document is still there waiting to be disposed. I just need a notification."
technical,"any news about this feature?  Unfortunately it's a blocker for our extension because we absolutely need to unlock the file after the has finished to editit. Is there any estimate on when this will actually be worked on? It keeps getting bumped (since March!) and has been open since 2016. I held off on writing some code figuring it would be done in a month, but here we are now in the June 2020 milestone. Should I just unsubscribe from this issue and cite this anytime someone asks me for VSCode support?"
technical,"FYI there is some further conversion about this here  any thoughts on that? It doesn't address the need of an API for a lot of other use-cases, but it could provide a nice shortcut for a (decent?) set of scenarios. Is there any progress or plan for this functionality? It feels really odd that there is still no way for an extension to be able to simply get a list of the currently opened files (even if they are not loaded)."
technical,Needed for a personal extension that keeps me from opening too many files :0 Is there any update on this? The tab model issue (that I was pinning my hopes on) has been closed...  This issue has been open for over 2 years?
technical,We need this for VSCodeVim as well. Our use case is explained here Is this planned any time soon? The vim issue is pretty annoying.
technical,"This issue is about exposing the open editor tab model as API, not about document close events. issues about onDidCloseTextDocument not firing immediately have been closed as dupes of this (for ex #84505)."
technical,"Haha. Funny you linked that, my comments above were for when I was making that add-on :P  You'll be pleased to know that ""close all saved"" is actually built into VSCode now (thanks to soneymathew) it'd be cool to be able to switch between tabs in your workspace on a branch switch."
technical,I just cann't understand what prevents this issue from being fixed. It's hard? It has been years. It's currently in the May milestone.
technical,"I believe VS Code does not tell the server that file has modified on disk,  I believe that's wrong and I am sure you'll able to file a separate bug with repro steps where a document that's considered open (e.g loaded) isn't updated anymore It's written badly - there is no such thing as a ""visible text document"".   Yeah, it should a text document in a visible text editor. Which is also a little bogous because all text editors visible by definition... At least we can fix the jsdoc"
technical,"Is there any estimate on when this will actually be worked on? It keeps getting bumped (since March!) and has been open since 2016. I held off on writing some code figuring it would be done in a month, but here we are now in the June 2020 milestone. Should I just unsubscribe from this issue and cite this anytime someone asks me for VSCode support? lol keep on adding it will get done in few smtg"
technical,"The API feels needlessly restrictive. What's the downside for MS to make this available for extension developers? I can't think of one.  Much as I like vscode the API could really use some love. Not to sound unappreciative, but the quality of extensions has so much potential if only the API would open up a bit more. Needed for a personal extension that keeps me from opening too many files :0"
technical, Now 4th...  not even a comment on this thread from the devs for nearly 2 years. :disappointed:
technical,"sort of -- the timeout is because while the workbench.action.nextEditor will move to the next tab, if you have 2 non-text editors next to each other, the active editor becomes undefined when it switches to the first (and the active editor changed event will fire), but now when it switches to the next, it will still be undefined, but no event will fire, because it went from undefined to undefined -- so the timeout is a safety net for the event not firing Oh okay. Thats different to what i was getting.  I'll try again but i was executing closeActiveEditor then logging the active editors document uri and would sometimes be the same still, until i added the pause. Same with nexrEditor.  Weird."
technical,"I believe that's wrong and I am sure you'll able to file a separate bug with repro steps where a document that's considered open (e.g loaded) isn't updated anymore  I don't have a repro because in my current testing, onDidcloseTextDocument always fires immediately. Like I said, I'm not sure that's the issue - I was trying to eliminate it. My understanding was that when the editor was visible, the user may be prompted before reloading a changed document, so I figured it might not happen if the editor is gone.  Is there some situation where onDidCloseTextDocument *won't* fire immediately, so I can test it?  I raised this previously. please ping me if you have something. I would like to use it in the LSP client libs to only sync documents that are visible in the editor."
technical,"cmon Really miss the zentabs of atom. Although there is one similar extension in vscode market, but its functionality is incomplete.  The main reason is there is no API to manage tabs."
technical,"Really miss the zentabs of atom. Although there is one similar extension in vscode market, but its functionality is incomplete.  The main reason is there is no API to manage tabs. So it appears that all this information is readily available, but it's not exported to the extension-visible API. Which would mean that we'd need to send a PR that expands that API surface.  I'm not familiar enough with how the VS Code codebase works to make a PR myself, but I was able to find some starting points, which might help if anyone interested in implementing this wants to dive in. You'll have my undying gratitude if you do!  The ""Open Editors"" view is able to access all this data from the Groups Service, and it does so here.  Taking a step back, what we *do* see in the extension API seems to pass through here. And that extension API seems to be populated from the Main Thread via a state machine here (see the class's usage). That state machine seems to interact with the Groups Service only here (which makes sense since it's not really getting that much from the Groups Service, only a notion of what viewColumn it's in).  It seems like the notion of moveable Groups is relatively recently tacked-on to the codebase, and that technical debt may be the only reason nobody's tried to implement this yet. I doubt that performance considerations have anything to do with it - I haven't seen much mention of performance in the above areas. It might be as simple as allowing more information from that service to percolate through the state machine, perhaps sending along a bookkeeping data structure mapping editor IDs to information about where/what tab-order they currently have in the Group system. Feedback Channels is also something I found for those who may just be getting exposed here (like me!).  Looks like at least 200 people really want to see this work, and it's blocking parity with other editors on important functionality! (For my part, I really want to be able to context-switch between Git branches with confidence that I can restore tabs that only make sense on a different branch... but as other commenters indicate, tab restoration is very limited right now.)  Hopefully someone can jump in here! :)"
technical,"I find it sad that this issue is lower priority than something like pinned tabs... Sorry all, I've put this on the backlog again for now, since I'm not actively working on it at the moment. I am hoping to still make some progress on the design for the next iteration, but I also don't want to get anyone's hopes up (since I could easily run out of time and not be able to get to it)."
technical,"Im not getting undefined in these cases, but sometimes the same editor.  I assumed that was why have the 500ms pauses in your code? sort of -- the timeout is because while the workbench.action.nextEditor will move to the next tab, if you have 2 non-text editors next to each other, the active editor becomes undefined when it switches to the first (and the active editor changed event will fire), but now when it switches to the next, it will still be undefined, but no event will fire, because it went from undefined to undefined -- so the timeout is a safety net for the event not firing"
technical, Support the author's request of this extension
technical,"I'm confused - my comments have been marked as off-topic in this thread, yet many issues about that subject have been closed as duplicates of this. It's not clear what the expected behaviour is.  All those issues are closed a duplicate because what you and others are trying to achieve is impossible without this API. However, it doesn't mean that using the onDidCloseTextDocument is any good when trying to know what tabs are open.  The jsdoc for onDidCloseTextDocument reads like this. I am not sure if "" To add an event listener when a visible text document is closed, use the TextEditor events in the [window](#window) namespace. "" is written badly or if you didn't see that.  Anyways, what that means that visible editors and closed text documents have very little in common. The only guarantee is that a document that's visible (which an editor is visible) is never closed. It also means that you cannot use the list of text documents as reliable information to know what editor tabs are open.   If onDidCloseTextDocument fires reliably when you close a document  This is a conceptual misunderstanding. You don't not close a document you close an editor or an editor tab - vscode closes the document when it thinks it is time to do so.  Tab-changes cannot be observed and are subject of this issue. In the screen shot below below, you have  1. 1 text editor 1. between 1 and N text documents 1. 1 settings editor (which has no representation in the API) 1. 7 editor tabs We will not change 1, we will not change 2, we will not change 3. We will add an API that allows to observe and mange the list of tabs (some like to refer to this as the open editors (not  text  editors) list).  Goals of the API  * Have an API representation of the tab model (aka open editors) * Have an API to manipulate the tab model (open, close, activate, move tabs and more) * Have events that fire when tabs change  Note that ""tab model != text editors"" e.g an inactive tab has no text editor, an active tab might not show a text editor but something else. It's not yet clear how we associate a tab with editors that the API is aware of (text editors, notebook editors).   Properties that a ""tab-object"" should have  * name, detail * view column * uri (iff applicable) * API editor object (iff applicable) * likely more  Modifiers that should exist  * close, open tab * active tab * move tab within and across view columns * likely more  Potential extension use-cases * language extension validates all tabs for fooLang, not just visible editor and not the whole project * extension to group tabs by folders, file type etc ([foo.html], [foo.js], [foo.css], [bar.html], [bar.js], [bar.css]) * extension to open/close a set of editor tab Thanks for that clarification and update.  I think you've hit the nail on the head:   I am not sure if ""To add an event listener when a visible text document is closed, use the TextEditor events in the window namespace."" is written badly or if you didn't see that.  is not consistent with   You don't not close a document you close an editor or an editor tab - vscode closes the document when it thinks it is time to do so.  It's written badly - there is no such thing as a "" visible text document ""."
technical,"Thanks for that clarification and update.  I think you've hit the nail on the head:   I am not sure if ""To add an event listener when a visible text document is closed, use the TextEditor events in the window namespace."" is written badly or if you didn't see that.  is not consistent with   You don't not close a document you close an editor or an editor tab - vscode closes the document when it thinks it is time to do so.  It's written badly - there is no such thing as a "" visible text document "". thanks for the details.  I'm not specifically trying to know what tabs are visible. It's fine for me if there's a visible tab that doesn't have an open document. It's the opposite that I think may cause issues (an open document without an editor). This is about ensuring language server state doesn't get out of sync with what the user sees. I don't think it's the same issue that this is asking for, but since my issues were all closed I'm not sure where to post it.  Here's an issue I believe *may* exist if onDidCloseTextDocument doesn't fire when the user closes an editor tab, though I can't verify it right now because onDidCloseTextDocument appears to be firing immediately. I'm concerned if that ceases to be the case (which you suggest may happen), this bug may occur. I'm investigating issues where users have reported changing Git branches and seem to have phantom errors left in their Problems view (or new errors that they should not). Here's what I think might be happening:  I'll use the LSP spec for examples here as it's clearest, but I think the issue is the same for non-LSP extensions.  The LSP spec says (about textDocument/didOpen):   The document open notification is sent from the client to the server to signal newly opened text documents. The document's content is now managed by the client **and the server must not try to read the document's content using the document's Uri**  However, if you:  - Open a file in VS Code - Close the editor - At this point, *if* VS Code has not sent didClose, the server believes the state of the document is still managed by VS Code, even though it's not visible to the user - Change Git branch so that the file is modified on disk - I believe VS Code does not tell the server that file has modified on disk, yet the line above states that the server must also not read it from the disk. I believe this results in the server having the old contents for the file and therefore may show/send phantom errors for a file the user cannot see.  If there's some mechanism that prevents this from happening if onDidCloseTextDocument does not fire immediately, that's great - but I'd like to understand what it is so if this issue comes up again I'm able to debug better."
technical,"Hopefully thumbs up attract more attention to this issue. For non-vim users, we cannot effectively edit the same file opened in two different editor groups (e.g. split one file vertically and view/edit different parts in each pane) The API feels needlessly restrictive. What's the downside for MS to make this available for extension developers? I can't think of one.  Much as I like vscode the API could really use some love. Not to sound unappreciative, but the quality of extensions has so much potential if only the API would open up a bit more."
technical,"I stepped through the code, but couldn't figure out any case where it would not call onDidCloseTextDocument. If I understand the code correctly, when the editor is disposed, it disposes the model.  This results in onModelRemoved calling  updateState().  And  updateState uses the models (which no longer contains the document) to do a diff and figure out why documents to fire close events for.  I can't see any obvious way that this *wouldn't* happen. If there aren't any, then this issue may be resolved (which would be good, because I think delaying the close introduces weird bugs for both local extensions and LSP where the language server has invalid state for files when modifying files outside of VS Code, such as a bit branch change). This issue is about exposing the open editor tab model as API, not about document close events."
technical,"it'd be cool to be able to switch between tabs in your workspace on a branch switch. This workflow any many others will be supported in the extension that I'm building for VS and VS Code  I want to cover all aspects of ""dev context"" that is created when working at specific PR/task/feature/bug. There is often a need to switch to something else and there is no easy way to turn back and restore lost context. Moreover there will be a way to save and share your contexts with entire team because they will be saved as diffable files (aka mental snapshots). Ready to commit in git. Stay tuned!"
technical,"Oh okay. Thats different to what i was getting.  I'll try again but i was executing closeActiveEditor then logging the active editors document uri and would sometimes be the same still, until i added the pause. Same with nexrEditor.  Weird. This would be really nice to have!  I am working on an extension that takes **encrypted** files and **decrypts** them. The problem is I need the **encrypted file path** so that I can decrypt the file.  vscode doesn't let you access the file uri of a binary file because the active editor is undefined.  Alternatively it would be cool to allow the user to choose what encoding to display binary files. I would think the default would be UTF-8."
technical,"ahhh very nice! Never would have thought to use the workbench.action.nextEditor cmd to loop through all the editors.  Still seems like sort of a ridiculous hack for something that should obviously be accessible in the API. totally agree, but it was the best I could figure out with the tools available ,)"
technical,"A simple list of open editors would be a good starting point  My request (also closed as a dupe of this) was for reliable open/close events. This is required for languages that want to prioritise the open files. From the open/close events you can maintain a list of open editors (and know when they changed), so I think if only one thing was done, events would be better than an API to just get the list (or at least, also an event when they changed). We have understood the requirements and we can understand the frustration that our priorities aren't always matching expected priorities. I am now locking this issue because we haven't been able to retrieve additional actionable input. We take stable API very serious and we'll take our time to build a sound and solid API. Once that work has started we will report back here."
technical,"I'm hitting this too. I need to provide my language service with a list of ""priority files"" and I was sending what I believed to be:  1. Visible documents 2. Other open (non-visible) documents  However I've discovered that my means for getting open (non-visible) documents (workspace.textDocuments) is flawed and contains documents that had their editors closed, but the documents are still ""open"" according to Code.  This means I'm unable to write tests to ensure that my priority files are working correctly, as I have no way of telling which files a user actually has open (but not visible). We need this for VSCodeVim as well. Our use case is explained here"
technical,"Sorry all, I've put this on the backlog again for now, since I'm not actively working on it at the moment. I am hoping to still make some progress on the design for the next iteration, but I also don't want to get anyone's hopes up (since I could easily run out of time and not be able to get to it). We were waiting for more the two months. Are you able to give us some even ugly workaround? The use case is simple in my opinion. I need a call to free the resource that was locked when it was open in the editor.  It's a blocker for us and unfortunately without this hook we cannot proceed over."
technical,"will be pure extension for both VS Code and VS. I know that are limitations. Some of them discussed in this thread but also others like no support for floating windows in VS Code  #10121 which were always supported in VS. Workaround will be to use grid layout #14909 when you will try to open context from VS in VS Code to simulate the same windows and tabs layout (yes, opening the same context in both VS and VS Code will be possible!). I want to deliver similar experience for both VS and VS Code but knowing the limitations. Well if you find a way to access open editors, please share! :grin:"
technical,"For vscode, it sounds like you'll end up hacking the application code rather than building an extension then, which is a shame. On your website it says you have a commit ready for git - I assume that's for Visual Studio and not vscode? will be pure extension for both VS Code and VS. I know that are limitations. Some of them discussed in this thread but also others like no support for floating windows in VS Code  #10121 which were always supported in VS. Workaround will be to use grid layout #14909 when you will try to open context from VS in VS Code to simulate the same windows and tabs layout (yes, opening the same context in both VS and VS Code will be possible!). I want to deliver similar experience for both VS and VS Code but knowing the limitations."
technical,"Well if you find a way to access open editors, please share! :grin: Would like to call out here that this kind of hobbles VSCodeVim, and it sure would be nice to have."
technical," Yzrsah I have that in the extension already, but what if we want to close it for them to provide a different starting point upon installation?"
technical, #755 has ctrl backspace in the title and probably came up in the suggestion box when you added this issue. Please make sure you search before reporting.
technical,"I hate that these things never actually get fixed or resolved.  WTF? all other terminals (cmd, powershell, pwsh) allow ctrl + backspace to remove the word. CTRL + SHIFT + Arrow should be able to select the word. aka standard hotkeys that we are now used to.  Cheers"
technical,"+1 I think this feature is very interesting to speed up things a bit. :-)  Also an warning/error message should be raised as mentioned before, it took some time debugging why the async has no effect on include statements. +1  in most cases - especially from  re-usability perspective - there is a sequence of tasks runs against an entity. Running those in parallel will be huge gain"
technical,"+1 most handy, esp. for intensive and time taking tasks (Eg: testing multiple Linux kernels in QEMU) +1 I have a perfect use case needs this feature where I need to provision X bare metal nodes for OCP and each of the them takes 45+ min to be fully up, there is no reason that I have to provision them individually in automation and wait 45 * X minutes to do next task."
technical,"it can be inherited, we are planning on an update to make this clearer 'import vs include' +1 I think this feature is very interesting to speed up things a bit. :-)  Also an warning/error message should be raised as mentioned before, it took some time debugging why the async has no effect on include statements."
technical,"i think you are making some assumptions that are not true: - async does not increase paralellization -  imports/includes work by adding new tasks/hosts to the iteration, they don't run on the spot, they are a 'addition to the queue' - parallelization  is based on forks (and limited by serial)  async is about waiting for a task inline or polling for it, if polling is 0 we don't wait for its results ... but these either get ignored or require a follow up task to do async pol. This effectively 'ends the task' from the controller's point of view even if it is still running on the target, this is kind of a 'de facto' increase in parallel tasks, but not from the perspective of the controller.  If the task is about changing the controller loop of tasks .. this HAS to be a locking (serialized) task, so even making it async, would just mean we don't wait for the result but would be LOCKED waiting for the queue being updated before we can proceed. At best 'async' include means we fire up EACH included task in async (w/o increase in forks nor parallelization) and don't wait for results (poll: 0?)  which means we won't know if the tasks succeeded or not (unless you introduce async pol which then gets back to a limited sync to get results). +1 Looks very handy. I need it for my k8s tasks"
technical,"+1 Looks very handy. I need it for my k8s tasks +1 most handy, esp. for intensive and time taking tasks (Eg: testing multiple Linux kernels in QEMU)"
technical,"include is not an actual module, its hardcoded into the engine and as such does not fork and cannot be async. I think it would be good idea to raise warning/error when unsupported parameter is provided."
technical," i think you are making some assumptions that are not true: - async does not increase paralellization -  imports/includes work by adding new tasks/hosts to the iteration, they don't run on the spot, they are a 'addition to the queue' - parallelization  is based on forks (and limited by serial)  async is about waiting for a task inline or polling for it, if polling is 0 we don't wait for its results ... but these either get ignored or require a follow up task to do async pol. This effectively 'ends the task' from the controller's point of view even if it is still running on the target, this is kind of a 'de facto' increase in parallel tasks, but not from the perspective of the controller.  If the task is about changing the controller loop of tasks .. this HAS to be a locking (serialized) task, so even making it async, would just mean we don't wait for the result but would be LOCKED waiting for the queue being updated before we can proceed. At best 'async' include means we fire up EACH included task in async (w/o increase in forks nor parallelization) and don't wait for results (poll: 0?)  which means we won't know if the tasks succeeded or not (unless you introduce async pol which then gets back to a limited sync to get results)."
technical,"+1 I have a perfect use case needs this feature where I need to provision X bare metal nodes for OCP and each of the them takes 45+ min to be fully up, there is no reason that I have to provision them individually in automation and wait 45 * X minutes to do next task. I've locked this to contributors for now.  Adding +1 comments is too noisy.  For future reference, add a reaction to the issue body, and don't comment."
technical, If i'm correct async makes entire task and not each item asynchronous.
technical,"If i'm correct async makes entire task and not each item asynchronous. include is not an actual module, its hardcoded into the engine and as such does not fork and cannot be async."
technical,"I think it would be good idea to raise warning/error when unsupported parameter is provided. it can be inherited, we are planning on an update to make this clearer 'import vs include'"
technical,"I've locked this to contributors for now.  Adding +1 comments is too noisy.  For future reference, add a reaction to the issue body, and don't comment. Verify first that your issue/request is not already reported on GitHub. Also test if the latest release, and master branch are affected too.  ##### ISSUE TYPE - Pick one below and delete the rest: -- - Feature Idea  ##### COMPONENT NAME - Name of the module/plugin/task/feature -- i don't know  ##### ANSIBLE VERSION - Paste verbatim output from ansible --version between quotes below --  2.2.1.0   ##### CONFIGURATION - Mention any settings you have changed/added/removed in ansible.cfg (or using the ANSIBLE * environment variables). --   ##### OS / ENVIRONMENT - Mention the OS you are running Ansible from, and the OS you are managing, or say N/A for anything that is not platform-specific. -- Ubuntu 14.0.4 amd64  ##### SUMMARY - Explain the problem briefly --  ##### STEPS TO REPRODUCE - For bugs, show exactly how to reproduce the problem, using a minimal test-case. For new features, show how the feature would be used. -- I want to play the task list asynchronously for several group vars.  - Paste example playbooks or commands between quotes below -- - You can also paste gist.github.com links for larger files --  ##### EXPECTED RESULTS - What did you expect to happen when running the steps above? -- The task list executed asynchronously.  ##### ACTUAL RESULTS - What actually happened? If possible run with extra verbosity (-vvvv) B3400 The task list is executed in sequence for each variable."
technical,"For exposing service outside the fire, yes, of course. But gain, that's a *very* small part of the concern WCF is used for. IMO, I don't even use WCF for that part because yes, Web API (or now just MVC).  That's focusing on only one tiny thing, which is the problem with this thread as that's pretty much what most of the population knows about WCF, which is a shame.  That's like saying .NET is only about int and string. +1 for locking. It's going nowhere."
technical,"1.      Named pipes 2.      System.Transactions 3.      A transactional queuing programming model 4.      Durable services 5.       Extensibility model 6.       MEX endpoint and MEX framework +1 please provide a way for me to build on WCF and host on linux  Having moved away from windows (an subsequently WCF) here are the things I miss the most and would love to have back.  #1 bindings  named pipes in particular, then tcp #2 security  though i understand this will be difficult without a windows domain #3 extensibility model  I like a number of others out there have done a decent amount with this to make working with WCF easier for engineers in my teams"
technical,"Thank you -- this is really great feedback and much appreciated.  We hear you and are collating this feedback as well as reaching out to other known WCF customers.  Especially useful are the specific features called out (e.g. queuing, transaction, etc.) because it allows us to prioritize and do targeted investigations.  For the record, the ""missing"" features of the full .NET framework's WCF were not deliberately excluded from .NET Core WCF. Rather, the initial goal was to support all the existing Windows Store WCF API's on NET Core (which are all client-facing) before tackling other mission-critical features of WCF.  It might help to know that much of the work of porting WCF features involves re-implementing OS-level libraries that WCF depends on (e.g. socket layer, cryptography, etc.) to allow it to work cross-platform. So lighting up WCF features usually involves replacing OS-level libraries for each platform first.  It might help to think that the ""W"" in WCF is no longer a given in NET Core.  This is one reason why it is so valuable to hear from you which features matter most, because it lets us investigate more deeply questions like ""Which libraries are required to do feature X on Linux? OS X?, etc."".  Please keep those suggestions and specific scenarios coming! 1.      Named pipes 2.      System.Transactions 3.      A transactional queuing programming model 4.      Durable services 5.       Extensibility model 6.       MEX endpoint and MEX framework"
technical,"Unless I missed something, WCF isn't in there. 2 minutes into the presentation there is a slide where WCF is listed. It's on the slide, but not yet in the compatibility pack. Remain filled with hope."
technical,"Thanks for all the feedback on WCF Server top features! Keep it coming!  Providing WCF Server support for .NET Core is on the radar.  As you know, our current POR is to provide the WCF client-side libraries in .NET Core to enable UWP/ ASP.NET Core/.NET Core applications to call .NET Framework based WCF Services.  We are very interested to better understand your scenarios where WCF server side functionality is required. Is this blocking your adoption .NET Core? Do you have active projects on .NET Core for WCF Server scenarios? (We would like to partner closely with you) Would you plan to port over your existing services or is this for needed new development? A specific business case I have is that we have an investment management system with several million lines of code for which the server side is currently hosted on premise on MS Windows Servers at about 200 insurance companies and banks world wide. We have about 100+ service types, of which in the largest installations there are 700 to 800 concurrent service instances at play. Our product is driving important parts of the core businesses.  The expenditure on IT is huge for our customers. This is also the place where we are looking to make major improvements over the coming years. A part of that is to find alternative hosting environments. A favorable choice would be Windows Nano, or the .NET Core on Linux. For being able to adopt .NET Core (or Windows Nano) we are missing the WCF server side.  Since we are very happy with WCF as a programming model, there is no incentive to rewrite our applications other than that WCF server side is unavailable in our future hosting environments. Particular features that we use is long. But to start .NET Core adoption are, these are the important ones: - Self-hosting using ServiceModel.ServiceHost - NetTcp (half-duplex) - Message inspectors for instrumentation and access to SOAP headers - Behaviours - OperationContext - Contract based programming model  Yes. We would continue building WCF services also on .NET Core."
technical,"WCF has been instrumental in delivering quality services that scale and deliver reliability by leveraging transactions across the wire (System.Transactions).  With out support of WCF on .NET Core we would lose many of the ""Free"" benefits we get thru the extensive WCF interceptor chain, including logging, behaviors, and context flow. Absolutely support the idea of having server-side ""WCF"" in .NET Core. We just finished a[nother] fairly large almost entirely server-side processing system (""the user experience is that there isn't any"") Initially we went through a lot of pressure not to use Microsoft/ .NET mostly due to the relative advantages of other (open source) stacks when doing ""microservices-based"" solutions just as traditional web services. But the benefits of WCF such as the enforced contract-based programming model, the availability specifically of Named Pipes binding, flexibility of end points and bindings (yes, declarative approach/configurability can be an advantage), security, extensibility for utilities such as logging, were really key when the system grew and required scale and performance as well as maintainability and having really very little plumbing code. The obvious next step is proper containerization (we have been explicitly waiting for Nano Server) and in general being able to port the system to the next generation of runtime platform(s) without loosing any of the current qualities."
technical,"Maybe not in C#, but with the .NET Framework it is. Unless you want to custom code tons of stuff or pull in many 3rd party libraries. Nor should we have to pull in ASP.NET Core to create even a basic HTTP web service. System.ServiceModel exists for a reason.  (Not to mention we already have the full service implemented that simply needs to be ported...) Actually I agree with jbogard. I go back on my previous comment on the issue, I had no idea about the options for SOA and microservices in .NET core, but now I do, so I'm not bothered about WCF now."
technical,"everything you can do in node without a third party lib is exactly the same as what you can do in asp.net core without a lib.  That's exactly what I said. And since it's all third-party, I'm going to pick the one with the larger and more active community, with the larger corporate backing. Right now that's Node and JS, not .NET Core and C#.  WCF enables best-in-class SOA. It's incredibly powerful. It's just a matter of whether that will be forever tied to Windows and Azure, or be set free. Agreed - ""WCF enables best-in-class SOA. It's incredibly powerful."" The purpose of this list was to gauge interest. Other peopel's lack of interest or ability to ""do it another way using none .NET libraries"" in any way addresses the question that was asked and the reasons for our responses.  Also, the purpose of this was not to educate each other on SOA and all of the thousands of ways it can be achieved.  The question was put out ""Should WCF be added to .NET core."" If I'm trying to get a team to use WCF, the single biggest selling point I have is that it's part of the .NET framework.  Why?  Because I'm an experienced guy and most of the audience is managers, PMs, junior developers, developers, or junior architects.  There are *maybe* 2 or 3 people on any given team that understand anything about SO.  The reason they have guys ""like us"" on the team is to make quality recommendations and using what's right in front of you that is industrial strength, proven, and enterprise ready is a pretty good thing to suggest.  Can SO be done on .NET in other ways?  No one here is asking that question (or at least that's not the purpose of this thread)."
technical,"2 minutes into the presentation there is a slide where WCF is listed. It's on the slide, but not yet in the compatibility pack. Remain filled with hope. Ah my mistake, thanks for that."
technical,"+1 please provide a way for me to build on WCF and host on linux  Having moved away from windows (an subsequently WCF) here are the things I miss the most and would love to have back.  #1 bindings  named pipes in particular, then tcp #2 security  though i understand this will be difficult without a windows domain #3 extensibility model  I like a number of others out there have done a decent amount with this to make working with WCF easier for engineers in my teams All the bindings Security extensibility Mex  I just agree with whatever one else is saying.  WCF is amazing  Rework the config to leverage the new config system. Json config  xml"
technical,"What features do you need in a WCF service that require .NET Core?  Guys, you are not listening at all, is this intentional?  It is not that we desperately need parts of .net core in the 'big' .net framework (4.7.1 and netstandard20 closes that gap).  We need .net core to have WCF. Name it like you want (XCF? ,D), strip it System.Web and Windows OS dependencies, make Windows OS integration optional nuget packages. We do not require it to be 100% compatible with WCF, we can take a cost of migration of whatever we have to that conceptual XCF. But for the God's sake provide service oriented framework in .net core. Because currently there is none and this is pathetic.  We need .net core to make use of docker, server nano, kestrel and all new goodies. To use .net core modularity. And I've seen WCF source code, it is quite modular, you do not have to *rewrite* anything from scratch (as someone here wrongly stated).  Or publish sources and allow community to do its work!  Now you have caged many solutions in a limbo."
technical,"Damianh ad hominem attacks aren't appreciated. And my lack of any public repositories means, what, exactly? That I work for people who choose to not give away their code? That I don't work on many open-source projects?  Kudos on your evidence of... What, exactly? :)"
technical,"I'm curious. From what I understand, Azure itself is written, in large part,  usIng WCF (or at least something that provides all the capabilities of WCF) behind the firewall. That being the case, why would it be excluded from .NET Core in the first place?  Seems odd to exclude such a powerful and foundational toolset. Another use case could be, using WCF on a gateway like device for example in home automation using linux based controllers with an ARM CPU and 512MB memory is not that uncommon. Being able to use .NET core on those types of devices and using WCF to allow creating a SOA like programming model, making use of named pipes and allowing to move around context and create reliable communication could create a whole other way of working than the current C daemons, dbus communicating way of doing things.  Using WCF could also than be used to for example let your app communicate with your WCF service in your house, for offline controlling. It would provide better integration with service bus allowing efficient communication to and from cloud based services."
technical,"I agree that gRPC seems like a fine framework.  And a necessary one for  most of it's implementations because there is nothing in the basic language framework itself.  .NET on the other hand already has WCF.  These two lines of code are just a sample but what's interesting here is that the Binding and the Endpoint schema are components: This is interesting because it means that by swapping those out WCF can communicate over Http/https, UDP, TCP-IP, MSMQ, NetPipes and a bunch of other protocols and endpoint schemes.  I understand that in the recent web many have forgotten about anything other than HTTP, but if your app grows you may find yourself wishing you had the ability to use the exact same code, but pointing to a queue'd endpoint.  Sadly, most other frameworks will leave you re-implementing your system for queueing rather than just repointing it.  Not to mention transactions (also fallen out of favor - UNTIL they are essential) and various forms of authentication and encryption and on and on.  Nothing is wrong with gRPC.  If you're happy with it use it.  All the folks on this list want is the framework functionality of WCF in net core so that we can build solid apps with the code similar to that built into the .net standard.  In other words, in a service oriented http/web enabled world, we see those parts of the framework as ""essential.""  It's not gRPC vs WCF, it's .net core with WCF. Answering your questions: At my current company there's a lot of legacy, PHP-based, UNIX-hosted SOAP web services which need to by rewritten. WCF would be the best solution but the lack of it in ASP.NET Core will probably cause migration to JAVA instead. We would choose .NET stack instead of JAVA..."
technical,"I agree that gRPC seems like a fine framework.  And a necessary one for  most of it's implementations because there is nothing in the basic language framework itself.  .NET on the other hand already has WCF.  These two lines of code are just a sample but what's interesting here is that the Binding and the Endpoint schema are components: This is interesting because it means that by swapping those out WCF can communicate over Http/https, UDP, TCP-IP, MSMQ, NetPipes and a bunch of other protocols and endpoint schemes.  I understand that in the recent web many have forgotten about anything other than HTTP, but if your app grows you may find yourself wishing you had the ability to use the exact same code, but pointing to a queue'd endpoint.  Sadly, most other frameworks will leave you re-implementing your system for queueing rather than just repointing it.  Not to mention transactions (also fallen out of favor - UNTIL they are essential) and various forms of authentication and encryption and on and on.  Nothing is wrong with gRPC.  If you're happy with it use it.  All the folks on this list want is the framework functionality of WCF in net core so that we can build solid apps with the code similar to that built into the .net standard.  In other words, in a service oriented http/web enabled world, we see those parts of the framework as ""essential.""  It's not gRPC vs WCF, it's .net core with WCF. Are you saying that WCF is the only thing you need in Microsoft stack? You've might overlooked some things, really.  P..S. Are you threatening Microsoft and/or the community to leave if won't get WCF Core now? That's hilarious to say the least."
technical,"I'm doing a lot azure. zero WCF on the outside. AFAIK the stuff I'm using is also not using much wcf on the inside. I have no data to proof that, but just one simply argument: everything I use is moving over to core. If these parts of azure would rely on wcf that much, then core would support WCF already. As someone who works as a dev at Microsoft on Azure services and has spent a significant part of the last 18 months building software that helps customers who have SOAP based services, let me offer my perspective with hopefully no judgement on the value of the tech.  I think the folks who are hoping they can convince Microsoft to build WCF style services in .NET Core are wasting their time.  Your time would be better spent working together on an simplified OSS version of WCF.  I'm not from the .net team, so my opinion is just that but when I needed a WSDL parser to do my work, I wrote a new one instead of depending on WCF. My 2c."
technical,"As soon as the WCF is implemented in the .net core, i'm moving away from mono, and using net.core for our existing and future cross platform projects. Mono has so many bugs so.net core looked like a spark of hope. As soon as ***if*** WCF is implemented. We're still waiting on more information 1.5 years later."
technical,"Because we need solid SOAP Framework in .NET Core which will enable us to use plethora of new scenarios. For example docker with nanoserver (yes, servercore image is too large), for example linux. We have big SOA enabled financial system and no one will ever think of moving it to Web API for just moving it to WEB API if SOAP works perfectly for our high profile clients. That has no business value. Having no SOAP framework in .NET Core starts looking pathetic unfortunately :/  I have gut feeling that .NET Framework will lag behind .NET Core more and more in terms of new features and general progress (ie - I am curious when it will make its way to .NET Framewework if ever). In addition way of delivering .NET Framework upgrades is quite heavy and is system wide while for .net core it is side by side... .NET Core is way forward and not having there such crucial component won't provide you buy in from large enterprises. Do you want to have larger adoption of .net core? invest in server side wcf  We would start moving our solutions to .net core As soon as the WCF is implemented in the .net core, i'm moving away from mono, and using net.core for our existing and future cross platform projects. Mono has so many bugs so.net core looked like a spark of hope."
technical,"Wrong place indeed, but you just need to use the #if preprocessor and create a compile flag. So: Note you'll need not just the method calls wrapped, but  all  of the offending code wrapped. ASP.Net Core runs in the full framework just fine.  If you're interested in using the full framework WCF stack (HTTP-basic bindings) through ASP.Net Core, I have made something for it"
technical,"Wrong place indeed, but you just need to use the #if preprocessor and create a compile flag. So: Note you'll need not just the method calls wrapped, but  all  of the offending code wrapped. Basically look at WCF as .NET on steroids, NOT just web services.  This point seems lost on many."
technical,"I have taken the survey. Please share the Road Map for WCF Server side support in .Net Core. That will be very helpful. Because we need solid SOAP Framework in .NET Core which will enable us to use plethora of new scenarios. For example docker with nanoserver (yes, servercore image is too large), for example linux. We have big SOA enabled financial system and no one will ever think of moving it to Web API for just moving it to WEB API if SOAP works perfectly for our high profile clients. That has no business value. Having no SOAP framework in .NET Core starts looking pathetic unfortunately :/  I have gut feeling that .NET Framework will lag behind .NET Core more and more in terms of new features and general progress (ie - I am curious when it will make its way to .NET Framewework if ever). In addition way of delivering .NET Framework upgrades is quite heavy and is system wide while for .net core it is side by side... .NET Core is way forward and not having there such crucial component won't provide you buy in from large enterprises. Do you want to have larger adoption of .net core? invest in server side wcf  We would start moving our solutions to .net core"
technical,Please don't invest in supporting neither VB.NET nor WCF nor WWF. Binding and security please!
technical,No. You can do this with about 15 lines of terraform  Not sure what you mean here. Database transactions? Distributed transactions? Identity Server This and the rest of the above are trivially easy with a pipes and filters style architecture (except for compression and encryption which are baked into HTTP). Best example I can think of is OWIN. Wow. That's incredibly sad. can you explain these transactions because afaik you can't have transactions over http.
technical,"No. You can do this with about 15 lines of terraform  Not sure what you mean here. Database transactions? Distributed transactions? Identity Server This and the rest of the above are trivially easy with a pipes and filters style architecture (except for compression and encryption which are baked into HTTP). Best example I can think of is OWIN. Wow. That's incredibly sad. ccicchitelli Depends on what exact kind of architecture you want to take. Personally my preferred microservices architecture for .NET core, is self contained console apps (or windows services if you want to take a dependency on windows) that communicate via an Enterprise Service Bus. Seems the most loosely coupled option.  But, you don't have to do that via an enterprise service bus, you could equally do it via RPC, or even just make each microservice a REST endpoint. Admittedly, making them REST endpoints I wouldn't advise, as your applications become a bit more coupled to your microservices at that point, but it is still an option.  There's also a wealth of options if you're willing to use Azure, such as Service Fabric, which is clearly really powerful, but obviously not great that you're forced to use Azure. There's also Orleans and Akka.Net  Obviously, most of these options do involve taking a third party dependency, but typically not many, just a couple. Certainly it doesn't require a lot of custom code though."
technical,"We have a customer that require a portable backend/service-solution (Linux-host) based on open source software, serving SOAP. As a .NET-shop, we see a future .NET Core WCF-service as an alternative, as we need to consider Java at this point. cederlof I was working on my own but then I came across ServiceWire which with a very minor teak worked out well for me."
technical,"Would love to see WCF play nicely with emerging Web Service projects that are attempting to be  RMM RESTful service in addition to SOAP.  Such as, but not limited to, - APIs  - OPEN API (Swagger), RAML, WADL - HATEOAS - HAL, JSON API  Would love to see MEX style endpoint that can publish these emerging technologies so we will not have to choose or implement both WebAPI and WCF.  Would love to more reliability consume this new RESTful web service that are ate least RMM Level 3 so that our team can spend more time focus on creating new process and solving problems without focusing time on the plumbing.  Looking to bridge interoperability and integration,some systems and services are now moving these emerging technology.  I would love to see a clearer path to the future.  Is it possible to incorporate WebAPI in  WCF in a self-hosting, Azure or docker style deployment, and have the services published as many useful  technologies as an endpoint to be consumed by other services and systems? Check out this project. it is not WCF, but might be useful to achieve some of backend abilities."
technical,UCF Universal Communications Foundation Communication Foundation*
technical,"Scott, doesn't look like an image went through Cool.   It was a thumbs up and thank you for your comments."
technical,"For HTTP web services, there's a better .NET Core version of it - ASP.NET Core. RPC? gRPC (though be super careful, RPC can break autonomy of services). WS-*? Most of those were bad ideas (transactions, reliable messaging etc.) and didn't actually encourage autonomy. Proxy classes? Stopped using them, even inside WCF because it encouraged RPC-style services. Used service channels directly instead.  TCP? System.Net.Sockets MSMQ? Not on .NET Core since it's not xplat. But you can do Azure Service Bus or RabbitMQ, open protocols with AMQP and client libraries that actually match the protocols and brokers. IPC w/ named pipes? System.IO.Pipes (or if you want, you can always do ZeroMQ if that floats your boat)  To me this reply is equal parts helpful and ridiculous. It's helpful because those things do what you say. It's ridiculous because it ignores that you call do all of them in WCF, WITHOUT CODE CHANGES. Want to offer a method via REST and IPC? Add a binding. Damianh ad hominem attacks aren't appreciated."
technical,"The irony here is that WCF exposes services/contracts using what was supposed to be a platform/technology independent approach.  Therefore anybody was supposed to be able to come along and write compatible services in any tech they wished that implemented those SOAP/WSDL/WS-* contracts.  If the SOA ecosystem promised by WCF (and similar tech from other vendors) had been fulfilled then there would be no problem switching out the WCF stack for some other compliant stack.  The reality is that implementation details leaked out everywhere and the interoperability was often limited to services running the same tech stack behind the scenes.  This is why the vast majority of services being built now choose plain old HTTP.  Because real interoperability is possible.  It's true that HTTP doesn't provide distributed transactions, and good security is taking a long time to come to fruition, and HTTP is mostly a request/response architecture where full duplex can only be simulated.  It's also true that some folks are building distributed systems in closed/controlled ecosystems where this kind of technology coupling is manageable, and they get to take advantage of some out of the box infrastructure.  But eventually there is a price to pay.  In this case the price is due to the creation of a new platform that doesn't support that stack.  It's true there is no uber SOA stack in .net core, but I suspect that is largely because we have learned the lesson that one size fits all tech stacks tend to get massively over complicated.  There are many ""best of breed"" solutions out there that address many of the features that WCF supported.  HTTP provides a substrate that allows many of these solutions to interact.  Yes, there are gaps that still need to be filled.  But in the future we should be able to mix and match the tools that fit the scenarios that are needed, so that never again are we faced with a giant tech stack that is an anchor for our business.  WCF is a technology that will continue to be supported for many years to come and I'm confident many companies will continue to get value from their WCF based systems, but I do believe it is time for companies to move on with new work and adopt technology that can truly achieve the interoperability goals originally promised. darrelmiller Sounds like ""throwing up your hands"" to me. If .NET core doesn't support the previous .NET framework features (even when running exclusively in Windows), then it's not just a matter of ""use HTTP, you'll figure out a workaround"", it's a matter of ""what technology stack will do what we want, and how can we decouple from a .NET ecosystem?"" Adoption is going to suffer, backwards compatibility is going to be lost, and the entire vision of .NET core will go the way of Clippy.  Multiple people in this thread have had good suggestions, whether it's to enable WCF functionality on a platform-level basis (only deployable in Windows until someone implements the same distributed transaction service interface for Linux, etc.) or completely open the stack up to the community. I don't know when the heel-dragging started on this, but from now on I'm going to be investigating Metro as a potential replacement for WCF for a platform-agnostic world. It's too bad, because .NET Core had so much promise."
technical,"Yeah, except that's pretty large corollary to ""use the .NET standards you know and love with the new high performance cross-platform .NET core"": be sure to plan on rewriting all your service architecture using completely different tooling.  Basically, toss the ""Compatible"" subheading, and add the caveat that Microsoft only intends to support their (far less popular) Azure service architectures. If you do go the distance and rewrite your whole system on different service frameworks, be sure to still run a .NET framework component to adapt older clients to the new architecture. Which means you can't actually achieve ""cross platform"" the same time, either. Better throw that heading out too, for anyone who has existing systems they would want to continue to support.  Here's the new tagline for .NET Core: ""You won't even need WCF because this new .NET core is going to be worth re-architecting your entire software stack for"".  Back in 2000 Joel Spolsky wrote something about this, I think Dear community, could somebody please advise what is the current state of server side WCF? According to the discussion above, there's no plan for implementing it in the nearest future, is that correct? Thanks!"
technical,"cederlof I was working on my own but then I came across ServiceWire which with a very minor teak worked out well for me. Does ServiceWire work on .NET Core?  I am not seeing anything about that on their website, but I'm not sure where to look for it.  In the history it says standard .net library."
technical,"Even placing WCF-based application to docker will require to use ServerCore image (which is huge) - not Nano. Don't get too hung up on the WCF that was designed for Windows n number of years ago.  How would you redesign/update ""XCF"" (X comes after W) to work tomorrow in .NET Core?  I think what the community wants is a mature, out-of-the-box, enterprise-grade, service-oriented programming model for inter-process communications regardless of what platform is hosting the process, whether the processes are local or distributed, whether the process is on a server or mobile or IoT device.  Simple enough, right?  It's a tall order for sure, but the WCF team was able to do it in the era of Windows and .NET (so long as the device was running Windows).  Rather than unify disparate technologies that existed previously on the Windows platform, the new challenge is to enable communication across any platform with any device anywhere.  Changes and different approaches will undoubtedly be necessary and perhaps even an improvement.  As developers, why should we concern ourselves with configuration of endpoints or maintaining static proxy code?  Why can't we just connect the dots and let the framework negotiate the best way to communicate?  As long as any POCO class can be converted to a service with interfaces as service contracts, serializable DTOs as data/message contracts, and behavior modification applied AOP-style via attributes or PnP configuration we should be able to cope with whatever the future brings.  This communications channel just needs to work reliably, securely, under every scenario imaginable to be considered enterprise-grade.  The justification for such an endeavor is the same reasons why Microsoft is investing in Azure, to re-position itself for the future.  Streaming data to/from mobile devices on the hyper-loop transit or peer-to-peer autonomous-vehicle networks is not out of the realm of possibilities where .NET Core code might find itself someday.  The question is whether Microsoft will be ready with a solution for those scenarios or scramble to play catch-up and allowing others to fill the void."
technical,"look where the puck is going.   A lot of us were here when they dropped the puck into the game and we've been watching the puck the whole game and we also know where it goes next. I can use my WCF in service fabric. I could use my .net in WCF. I could use my COM in .net. Don't let the marketing confuse you, there is nothing that is coupled to Windows that  must  be coupled to Windows. That's all the name is - marketing."
technical,WCF is still one of the best parts of the .NET Framework and having become part of .NET Core is essential.  Some particular aspects are: 1.      Named pipes 2.      System.Transactions 3.      A transactional queuing programming model 4.      Durable services 5.       Extensibility model 6.       MEX endpoint and MEX framework evelix  at making    CF have a simple convention over configuration baseline.
technical,"Reasons it's incapable and should be considered deprecated by the community, compared to other .NET components: 1.  Not being part of .NET core means it is not ""open source"" and nobody outside Microsoft can rewrite it for .NET core. 2. No cross-platform support, which many cite as a primary reason to use .NET core in any application. 3. It cannot benefit from the .NET Core performance gains (of which there are many). 4. Anyone who wants to port their current WCF services to .NET core is going to have to spend a very significant amount of time redesigning their architectures to include all the features they were dependent on and got for free as part of WCF. This is Microsoft putting the onus on the customer to engineer their way out of a Microsoft-caused problem. You guys should be willing to take the hit in order to make your customers lives easier, instead you're here arguing that your customers are wrong.  SQL server and ASP.NET have migrated to cross-platform capable foundations, apparently avoiding your fatalistic, fallacious arguments.  Being excluded from .NET core is a clear message to developers that there will be less and less support going forward, and to avoid using it. It doesn't matter if you release updates every hour on the hour for WCF in the .NET Framework, if it's not part of Core we all can take that as a sign it has no real future. Just like no new software is being written using Silverlight or Windows Forms, WCF will be done in short order.  .NET Core is not being adopted by my team because the lack of WCF support. And if we're not doing it, there are certainly many others doing the same as us. That means less adoption of your very expensive investment in this new technology. This should be more important to you than the investment required to modify WCF. Even placing WCF-based application to docker will require to use ServerCore image (which is huge) - not Nano."
technical,"That's not really answering Forki 's question. From what I know of NodeJs, everything you can do in node without a third party lib is exactly the same as what you can do in asp.net core without a lib. everything you can do in node without a third party lib is exactly the same as what you can do in asp.net core without a lib.  That's exactly what I said. And since it's all third-party, I'm going to pick the one with the larger and more active community, with the larger corporate backing. Right now that's Node and JS, not .NET Core and C#.  WCF enables best-in-class SOA. It's incredibly powerful. It's just a matter of whether that will be forever tied to Windows and Azure, or be set free."
technical,"Web API is for http services. This is not comparable in any meaningful way to the full WCF. For exposing service outside the fire, yes, of course. But gain, that's a *very* small part of the concern WCF is used for. IMO, I don't even use WCF for that part because yes, Web API (or now just MVC).  That's focusing on only one tiny thing, which is the problem with this thread as that's pretty much what most of the population knows about WCF, which is a shame.  That's like saying .NET is only about int and string."
technical,"Web API is for http services. This is not comparable in any meaningful way to the full WCF. For HTTP web services, there's a better .NET Core version of it - ASP.NET Core. RPC? gRPC (though be super careful, RPC can break autonomy of services). WS-*? Most of those were bad ideas (transactions, reliable messaging etc.) and didn't actually encourage autonomy. Proxy classes? Stopped using them, even inside WCF because it encouraged RPC-style services. Used service channels directly instead.  TCP? System.Net.Sockets MSMQ? Not on .NET Core since it's not xplat. But you can do Azure Service Bus or RabbitMQ, open protocols with AMQP and client libraries that actually match the protocols and brokers. IPC w/ named pipes? System.IO.Pipes (or if you want, you can always do ZeroMQ if that floats your boat)  To me this reply is equal parts helpful and ridiculous. It's helpful because those things do what you say. It's ridiculous because it ignores that you call do all of them in WCF, WITHOUT CODE CHANGES. Want to offer a method via REST and IPC? Add a binding."
technical,"We have a large SOA based healthcare application. We use WCF is two ways, internally (net.tcp and binary serialization) and externally (http and WS-*).  There are a number of RPC frameworks that could replace our use of WCF for internal service communication, but we really need WSHttpBinding support for standards based interoperability scenarios.  We exclusively use ServiceHost and configuration by code, so no loss there. For me, anything public facing is always WebAPI but with everything else, it is WCF simply because you have the various protocol choices via the bindings, throttling, reliable messaging etc. (so basically behind a public facing endpoint) I kind of agree with shawnwildermuth in that having WCF may add to some confusion, but as with anything it depends on the communication and type of support introduced. If WCF has some ""competing"" features with what is currently available (or maybe otherwise available in a future update) thats when confusion will happen. Have the use cases/support distinct enough to circumvent this. To summarise, yes I would want WCF server support specifically for protocol/bindings such as TCP/named pipes, as well as throttling and reliable messaging (dont care about trxs). However if it comes at the cost of mixed messages, confusion, overlapping feature set, then no thanks."
technical,"What claim? How many WCF implementations outside .NET do you know? For those of us who have spent any time in the enterprise it's a bit laughable to see this subject treated so flippantly by some...  Also, I don't think the Microsoft community moved on. But new people joined the community."
technical,"Great thing about microsoft is they virtually never declare anything actually dead. They just move on, stop doing stuff and stop talking about stuff  and eventually everyone ""gets the message that wasn't directly said"".  Except silverlight. They declared that one,.  ... which became the basis of coreclr! undead :) full-featured, singular communication stack into .NET Core so we can host on cheaper metal and smaller devices,   and I will add to that I would like greater reach! I was very disappointed when I sat down years ago to write my first store app, and found out it could not work with my infrastructure, no WCF in it. Same goes for core, UWP, all of it. I can't use any of that. The whole point of those technologies being in service would be to give me the ability to extend my infrastructure across platforms, have other types of clients orchestrated by the infrastructure. This would be huge!"
technical,"Some of them are, some of them aren't. Certainly REST is. All of them are options .net code can make use of though, so what does it matter? Embrace OSS .Net! :D Fundamentally Microsoft needs to ask what they expect for .NET Core if they aren't going to natively support enterprise-level SOA tooling like they do in the desktop framework.  Because I'll tell them right now, they will lose to NodeJS if they haven't already... If I was starting over building microservices at this point .NET Core wouldn't even be a consideration and it would be all Node. That's where the community is - Microsoft has no choice but to support it IMHO."
technical,"I am in a situation in which I am architecting a new solution, a major aspect of which will include SOA.  I cannot find anything that offers what WCF does as noted above.  Also, I have read articles that WCF is dead and hear comes WebAPI (???).  I didn't know they were in competition with one another.  I use WebAPI on the web, client-side but behind our firewall I need a robust solution composed of services developed on our Windows application servers.  Where WCF exceeds my needs, I cannot find anything to recommend to my higher ups as an alternative.  I am having to persuade management that WCF is not dead, won't be a maintenance nightmare in the future when there will only be a scant few who know WCF anymore, that WebApi AND .NET Core are not replacements, etc.  I wish that Microsoft would be more committed to WCF and emphasize that it will be ported or moved along with .NET Core, or a commitment to some roadmap.  How they are lacking a roadmap for such a comprehensive and core technology stack, leaving room for bloggers to post ""WCF is dead ...  love live MVC 6"" is unfortunate.  I am so glad I found this thread and see so many others who find WCF invaluable.  It seems that where there is mention that WCF is dead comments, you will also find comments of how hard WCF is compared to WebApi, etc.  If I may, see pluralsight.com's courses on WCF by Miguel Castro, his most recent being in 2016.  Cheers! Good to see some love for this very alive-and-well technology. My Pluralsight courses have been very successful on this topic. My prediction is a resurgence as true micro service architectures get more mainstream. Many of the things you need to consider in properly architecting micro service systems are built into WCF. You guys all rock!"
technical,WPF and Winforms are a give in since they are EXTREMELY Windows specific. WCF while does have a lot of core Windows underpinning is in fact a viable candidate unlike the rest since a lot of those ties can be broken for Linux use. Great point about MS SQL Server but I am sure you will get crap from him because it is a paid product where .net is not
technical,"People seem to be throwing the word ˜enterprise' around like it helps justify their need for WCF.  If the enterprise already has a WCF SOA, then it does... Great thing about microsoft is they virtually never declare anything actually dead. They just move on, stop doing stuff and stop talking about stuff  and eventually everyone ""gets the message that wasn't directly said"".  Except silverlight. They declared that one,.  ... which became the basis of coreclr! undead :)"
technical,"Binding and security please! gRPC binding would help bring familiar WCF programing model into ""modren"" world of microServices."
technical,"can you explain these transactions because afaik you can't have transactions over http. Handle transactions, er, um, manually?  Because you want to implement auth manually.  You want to implement your own context flow and header management and interception and pipeline intervention and encryption and compression and on and on...  All decent .NET messaging frameworks like MassTransit or NServiceBus perfectly handle this in much more elegant and efficient way."
technical,"We have a strong need to run .NET/WCF on Linux, so please add this to the road map!  We have invested heavily in WCF over the past decade and would hate to give up all the WCF benefits others have listed so well just because this was passed over by .NET Core.  We use most features and benefit greatly from being able to customize/extend the framework to suit our needs, such as custom message broker support (alternatives to MSMQ). Having WCF supported in .NET Core is a great idea and my list of key features are:  Security Behaviors Bindings Instance Management Throttling Metadata Exchange."
technical,"Hi,  I too appreciate WCF, and use it a lot. For me the most important features are: - Security - Extensibllity - Behaviors - Bindings (Especially TCP.Net) - Metadata Exchange - Instance Management - Reliability  I think this could really be a showstopper for .Net core, if it will not have WCF server side support. Helllo,  We also use WCF extensively in our applications. To add to the lists above we also use: - queued services"
technical,"My scenario is pretty much similar to olilanz's, except that my business case is Point of Sales. Like him, we have our application deployed to numerous stores world wide. We are also looking for alternate ways of hosting our application in order to reduce infrastructure costs. As jan-johansson-mr said, agnostic deploying WCF services would be great and would give us a huge flexibility.  WCF plays a major role in our application: it is based on a plug-in architecture where which plug-in is basically a WCF Service, so communication between plug-ins are actually WCF calls. Changing this aspect would mean have to rewrite/rethink a lot of infrastructure code.  In our case, self hosting using instances of  ServiceHost  and the Contract based programming model is crucial. Our plan is not only migrate existing services, but also create new services. Hi, For us the most important features are: - Behaviors. - Bindings. - Transactions. - Headers. - Self-hosting. - Integration with WF. - Queued Services. - MEX. - Binary Serialization (intranet communications). - Callback operations.  Responding to Erica: 1) Yes, at least it is delaying. 2) Yes, we are migrating our application that also uses WF. 3)  For both scenarios.  Thanks!"
technical,"It's not about build infrastructure. It's about provided code and accompanying files. It won't compile because it lacks resources, some dependencies, etc. Someone above posted link to Mono site describing issues with reference source Microsoft  offering. As name implies, it is for reference only and debugging.  What we need is github repo with Microsoft blessing so community could work on porting. Hi, not really a fan of the argumentative tone lots of commenters have taken here, but I would like to give my 2 cents. As I understand it, one of the main scenarios .Net Core is supposed to support are SOA and microservices. Not having WCF server side seems to run counter-intuitive to that. I take your point csharpfritz that WCF is completely coupled to Windows, fair enough. But it does feel like .Net Core really needs a better story for SOA than WebAPI, if you're serious about SOA as a first class supported scenario."
technical,"Honestly before this thread I thought folks had generally moved on. In terms of better replacements, what parts of WCF?  For HTTP web services, there's a better .NET Core version of it - ASP.NET Core. RPC? gRPC (though be super careful, RPC can break autonomy of services). WS-*? Most of those were bad ideas (transactions, reliable messaging etc.) and didn't actually encourage autonomy. Proxy classes? Stopped using them, even inside WCF because it encouraged RPC-style services. Used service channels directly instead.  TCP? System.Net.Sockets. MSMQ? Not on .NET Core since it's not xplat. But you can do Azure Service Bus or RabbitMQ, open protocols with AMQP and client libraries that actually match the protocols and brokers. IPC w/ named pipes? System.IO.Pipes (or if you want, you can always do ZeroMQ if that floats your boat)  I do have banking clients that still use expensive ESBs they purchased with a golf course handshake back in 2004, so I can empathize with the legacy interop story. I've just attacked this by creating a shim between their stuff and mine. Honestly before this thread I thought folks had generally moved on Not at all - not even a remote possibility in the near (5-10 year) future for many enterprises. In fact, from what I see, adoption of WCF has increased - just not using it for *web services* - so it's not as in-your-face.  Many enterprise-scale apps use WCF behind the scenes and would benefit from hosting on cheaper metal - but they can't. The cost to rewrite those apps (*systems)* would be huge - not only in actual manpower lost, but in opportunity loss while doing an entire rewrite. Also: Security costs - we know how to secure WCF, and using WCF for all services/layers provides the same security story throughout. Switching to a more hybrid approach would mean figuring out how to secure, tune, manage, etc a BUNCH of different tools - when one tool does all we need, and does it *very* well.  The problem is that almost everyone fixates on using WCF for web services which is, in reality, is only a very small percentage of how it is actually used. This is because, if you Google for WCF, that's pretty much all you find in the ""how to"" sections. WCF is a communication FRAMEWORK (useful across the entire applications stack from client, business, resource access, etc). It's NOT just about exposing web services to clients - which is what most people who don't use it (and therefore have ZERO actual skin in the game) are aware of. If that's all it was, it would be called something like MSWSF (Microsoft Web Services Framework). The ""Communication Framework"" was intentional."
technical,"Honestly before this thread I thought folks had generally moved on. In terms of better replacements, what parts of WCF?  For HTTP web services, there's a better .NET Core version of it - ASP.NET Core. RPC? gRPC (though be super careful, RPC can break autonomy of services). WS-*? Most of those were bad ideas (transactions, reliable messaging etc.) and didn't actually encourage autonomy. Proxy classes? Stopped using them, even inside WCF because it encouraged RPC-style services. Used service channels directly instead.  TCP? System.Net.Sockets. MSMQ? Not on .NET Core since it's not xplat. But you can do Azure Service Bus or RabbitMQ, open protocols with AMQP and client libraries that actually match the protocols and brokers. IPC w/ named pipes? System.IO.Pipes (or if you want, you can always do ZeroMQ if that floats your boat)  I do have banking clients that still use expensive ESBs they purchased with a golf course handshake back in 2004, so I can empathize with the legacy interop story. I've just attacked this by creating a shim between their stuff and mine. Honestly before this thread I thought folks had generally moved on. In terms of better replacements, what parts of WCF?  For HTTP web services, there's a better .NET Core version of it - ASP.NET Core. RPC? gRPC (though be super careful, RPC can break autonomy of services). WS-*? Most of those were bad ideas (transactions, reliable messaging etc.) and didn't actually encourage autonomy. Proxy classes? Stopped using them, even inside WCF because it encouraged RPC-style services. Used service channels directly instead.  TCP? System.Net.Sockets. MSMQ? Not on .NET Core since it's not xplat. But you can do Azure Service Bus or RabbitMQ, open protocols with AMQP and client libraries that actually match the protocols and brokers. IPC w/ named pipes? System.IO.Pipes (or if you want, you can always do ZeroMQ if that floats your boat)  I do have banking clients that still use expensive ESBs they purchased with a golf course handshake back in 2004, so I can empathize with the legacy interop story. I've just attacked this by creating a shim between their stuff and mine."
technical,"yes you can :-) and props to the team because it is coming along nicely and we will be re-evaluating it in the future as it is not mature enough to even remotely handle our workload but M will continue throwing  at it until it is because they want in on the Linux space. Why people want to fight that on a .net component level is just silly. If we can make it work even with reduced feature set then do it IMHO. How about this Fritz, give the open source community the WCF source, we'll rewrite it for .NET Core and you can take credit for opening up a major feature to the ecosystem. Win/Win?"
technical,"WCF should have been what gRPC is now. A framework for remote procedure calls, that is: session and contract based, extremely fast, and easy to use, with support for duplex and streaming. The exact opposite and complement of HTTP/Rest. If WCF is going to be another way to create/consume HTTP/Rest like services, then it is a waste. I agree that gRPC seems like a fine framework.  And a necessary one for  most of it's implementations because there is nothing in the basic language framework itself.  .NET on the other hand already has WCF.  These two lines of code are just a sample but what's interesting here is that the Binding and the Endpoint schema are components: This is interesting because it means that by swapping those out WCF can communicate over Http/https, UDP, TCP-IP, MSMQ, NetPipes and a bunch of other protocols and endpoint schemes.  I understand that in the recent web many have forgotten about anything other than HTTP, but if your app grows you may find yourself wishing you had the ability to use the exact same code, but pointing to a queue'd endpoint.  Sadly, most other frameworks will leave you re-implementing your system for queueing rather than just repointing it.  Not to mention transactions (also fallen out of favor - UNTIL they are essential) and various forms of authentication and encryption and on and on.  Nothing is wrong with gRPC.  If you're happy with it use it.  All the folks on this list want is the framework functionality of WCF in net core so that we can build solid apps with the code similar to that built into the .net standard.  In other words, in a service oriented http/web enabled world, we see those parts of the framework as ""essential.""  It's not gRPC vs WCF, it's .net core with WCF."
technical,"I have done a lot of projects that leverage the many aspects of WCF. Most of what I leverage in WCF (pretty much everything) is currently missing from .NET Core. Much of this missing capability  would require various 3rd party libraries (or extensive custom code) to fill in the gaps and it's just not worth that level of investment when WCF already works, brilliantly. WCF is, above all else, a great productivity enhancer for my teams.  The biggest piece missing for me, currently, is the extensibility model that WCF provides. Most of my projects leverage the capability to completely decouple cross-cutting concerns from my other components (which are light-weight WCF services). WCF provides a fantastic mechanism for doing this without the need of 3rd party Aspect Oriented Programming libraries. Developers don't know (or even care) and can focus solely on delivering the business value of the features they are concerned with.  We also leverage many other aspects of WCF such as: named pipes, transactional queuing programming model, enforcement (not encouragement) of interface-based design, security capabilities, process isolation, error masking, I could go on.  Without WCF (or equivalent capabilities) in .NET core I would lose way too much productivity and cannot justify the switch. It would be great to have all of these powerful capabilities in a platform that can be used in any type of environment. Imagine, productivity of WCF plus cost-savings of cheaper hosting environments. That is a tremendous business value.  Thanks for tracking this issue, Will I agree with websitewill as I am in that exact situation. I'd also like to point out that WCF in the full .NET Framework is done, finished, complete. If that spec were implemented to the letter in .NET Core I'd be very content and would be able to start transitioning projects.  Thanks, Kenny"
technical,"WCF Server is pretty much legacy at this point. I don't really see why new services would be built on top of it. However having a fairly full fidelity client is important. There are lots of existing services (I'm looking at you, Aussie Government) which are built on WCF and will not be changing any time soon. We can't build systems interfacing with these currently running on .NET Core. I agree with your point about clients but have to take some issue with WCF being legacy.  It's legacy the way ToString() or Int32 is legacy as in it's a part of the framework.  There is no other way in the framework (and in very few other add-in frameworks) to create transactionable, scaleable, secureable, interface based services.  In understand that there are a lot of js / light-client consumers out there, but there are also a lot of security violations and ""leaks"" because if you put it in the client it's not secure.  This is an ENTIRE Wcf service: There's just nothing that is enterprise quality and as simple.  I think WCF has gotten a strangely bad rap because people implemented it in unbelievably complicated ways.  Believe it or not, for .net on both sides of the wire, this is all that's needed.  With the ServiceModelEx framework from iDesign you can even build a proxy on the fly: Seriously, that's 4 lines of code (out of a total 17 with the structural) to create an enterprise service, it's proxy and make the service call. That's not lecagy, that's productivity."
technical,"WCF Server is pretty much legacy at this point. I don't really see why new services would be built on top of it. However having a fairly full fidelity client is important. There are lots of existing services (I'm looking at you, Aussie Government) which are built on WCF and will not be changing any time soon. We can't build systems interfacing with these currently running on .NET Core. I am in a situation in which I am architecting a new solution, a major aspect of which will include SOA.  I cannot find anything that offers what WCF does as noted above.  Also, I have read articles that WCF is dead and hear comes WebAPI (???).  I didn't know they were in competition with one another.  I use WebAPI on the web, client-side but behind our firewall I need a robust solution composed of services developed on our Windows application servers.  Where WCF exceeds my needs, I cannot find anything to recommend to my higher ups as an alternative.  I am having to persuade management that WCF is not dead, won't be a maintenance nightmare in the future when there will only be a scant few who know WCF anymore, that WebApi AND .NET Core are not replacements, etc.  I wish that Microsoft would be more committed to WCF and emphasize that it will be ported or moved along with .NET Core, or a commitment to some roadmap.  How they are lacking a roadmap for such a comprehensive and core technology stack, leaving room for bloggers to post ""WCF is dead ...  love live MVC 6"" is unfortunate.  I am so glad I found this thread and see so many others who find WCF invaluable.  It seems that where there is mention that WCF is dead comments, you will also find comments of how hard WCF is compared to WebApi, etc.  If I may, see pluralsight.com's courses on WCF by Miguel Castro, his most recent being in 2016.  Cheers!"
technical,"Fundamentally Microsoft needs to ask what they expect for .NET Core if they aren't going to natively support enterprise-level SOA tooling like they do in the desktop framework.  Because I'll tell them right now, they will lose to NodeJS if they haven't already... If I was starting over building microservices at this point .NET Core wouldn't even be a consideration and it would be all Node. That's where the community is - Microsoft has no choice but to support it IMHO. I disagree entirely, but each to their own."
technical,"All the bindings Security extensibility Mex  I just agree with whatever one else is saying.  WCF is amazing  Rework the config to leverage the new config system. Json config  xml I do a lot of work on the IoT.  It would greatly facilitate creating cross platform systems if my lightweight WCF services could be hosted anywhere that .NET core could be hosted.  As you can imagine, in IoT (and other systems) discovery is important so MEX and an extensible model.  The ability to debug locally and to support named pipes between services is valuable.  Really, in a nut shell as much of the WCF stack as possible - tranactions, bindings, durability, interoperability with azure and cloud based services (which often requires proper security and metadata)."
technical,"Service Bus binding of WCF is absolutely amazing for building push based reactive applications I do consider a full WCF implementation to be a prerequisite for moving to .NET core. Without it, it will remain a variation of the .NET Framework that cannot be utilized behind the firewall of major enterprise applications that employ Service Orientation. The major features I rely on that haven't been mentioned are: - Net.TCP Binding - Net.Pipe Binding - Interception-based Pipeline - Context - Headers - Contract-based Coding Model  The two bindings are required to enable the efficiency of communication that Service Oriented systems require behind the firewall.  The interception is critical to allow architects to provide the necessary aspects of the system in a way that does not require the developers to do anything except code as they normally would.  Contexts and Headers are required to propagate information that is ubiquitous through the application's stack from the client layer all the way down to the data layer and back up again without affecting the call contracts. Here, I'm thinking about identity information as a universal example.  The contract-based coding model is really necessary to avoid going back to the string-parsing voodoo that had us living in a type-uncertain, data-validity-in-question wild-west back when Microsoft thought passing hash tables to and from SOAP web services was a good idea."
technical,"Service Bus binding of WCF is absolutely amazing for building push based reactive applications I don't believe Microsoft would take such a cavalier attitude to those who have needs, are using the .NET service stack for ESB or application services, and can readily choose to use other technologies out there.  It can be very stressful to be faced with no real good option to move forward when you have the mandate to do so.  I would say not to count out Microsoft and their creativity in bringing together the best of what has worked for us in the past with current standards used in newer technologies.  We are, however, used to using a consistent .NET framework for both client and server side technologies, even though .NET Core could be used for ASP.NET 5 web development and .NET 4.7 for ongoing support of WCF.  As lowly application developers who are often seen to know less than Management or infrastructure teams, our options are often limited.  In any case, I guess I don't find the humor in saying on this thread that you may need to abandon Microsoft in lieu of IBM or JAVA technologies.  Enterprise applications are often made up of a mix of vendors.  My hope is that Microsoft does NOT lose too many WCF/ESB services developers but at the moment, a good case can be made to turn to other technology stacks based on a company's needs.  I love Microsoft!  Please keep your community updated on the progress or roadmap to what we may expect so we can make informed decisions!! Thanks!"
technical,"Service Bus binding of WCF is absolutely amazing for building push based reactive applications I don't think it's a matter of today's hotness and competing standards. WCF offers a collection of things needed to create industrial-strength / enterprise-strength services.  For example, WCF supports transactions and distributed transactions.  This means that architecturally your exposed service can enlist internal services to perform your use case and upon failure the use case service can recover.  When I searched for transactions and gRPC I found this.  It's a working document so things may have changed, but if it's to be believed the notion of a transaction is just to push the failure back to the client. This means that the client needs all of the compensating failure code and POOF, you're right back to the BIG-BALL-OF-MUD design pattern.  Most services tend to be simple (at first) so even pushing things back to the client (caller) works well at first and seems ""easy.""  Time will tell.  In a similar vein WCF's support of authentication and authorization supports configuration via attributes.  It's declarative.  This means that auth is separate from the code (if you want it to be) so you get a natural separation of concerns.  I get it.  Most people don't need this level of nuance and aren't interested in these architectural concerns.  I also know that at enterprise scale, these aspects become issues for everyone.  Take the comments in the link above.  The gRPC team set out to build a REST framework.  But after examining their actual implementations they found they were all RPC in nature.  I've built my fair share of REST apis, but in general I don't use them in my application architectures because what I need are methods, RPC in other words.  The community has gone crazy building REST only to find that they need RPC.  The same evolutionary process happens with services.  You start out just wanting to ""make the call"" but once you get that working you need security.  With security you need auth/auth.  With more sophisticated use cases you need transactions, logging, scaling, fault handling, tracing, etc.  I have no doubt that gRPC will work well ... until it doesn't.  By then, the commitment will be 100%.  Rewrite 2.0 will start, etc.  This can happen with WCF as well, so there's no glass houses here, but there are reasons why teams go down these paths.  Sometimes it's because of what's not in the framework, sometimes it's because we don't even know we need what's missing."
technical,"gRPC binding would help bring familiar WCF programing model into ""modren"" world of microServices. I don't think server side wcf is worth the investment when things like signalr and tooling should take presidence. I've always had such a hard time with wcf client Config  that I dispise it :( please correct me if I'm wrong... But why wouldn't web Api be the way to go... It's lightweight and gives you everything you'd need to build apps that can work from any platform with a simple http call...  Leave security, transactions, pub/sub up to the implementer..."
technical,"Seems like a good enough reason to change the whole .net ecosystem but not WCF... LOL you too funny I don't want to get into a whole back and forth thing here but it's good enough for .net core.  Remember, adding linux support is adding support for dozens of platforms from dozens of vendors.  I would think that after 10 years of being trapped on Windows that old framework would find it quite liberating."
technical,"ASP.Net Core runs in the full framework just fine.  If you're interested in using the full framework WCF stack (HTTP-basic bindings) through ASP.Net Core, I have made something for it I finally caught up on all of the messages above, and I agree with the comments that if the .NET Core team isn't going to do this as part of .NET Core, the best path forward is to release the full WCF source code and let the community replace the pieces that are tied to Windows. It can be released as an extension to .NET Core. And I would guess for many people they can use the initial port once Windows ties are removed without waiting for many of the deeper features to be implemented.  Really the idea that .NET Core can't be used for a powerful SOA is crazy to me. In my own case I built a control system that runs on .NET Framework. Our plan to port this to .NET Core is to change from a hub and spoke architecture to a nodular architecture. Anything that runs .NET Core could be a node - but not if it cannot communicate to the other nodes in a secure and high performance manner. That's what WCF does, so let's make .NET Core equally capable with UCF (Universal Communication Foundation) or MCF (Managed Communication Foundation).  -Chris"
technical,"Hi, For us the most important features are: - Behaviors. - Bindings. - Transactions. - Headers. - Self-hosting. - Integration with WF. - Queued Services. - MEX. - Binary Serialization (intranet communications). - Callback operations.  Responding to Erica: 1) Yes, at least it is delaying. 2) Yes, we are migrating our application that also uses WF. 3)  For both scenarios.  Thanks! I have done a lot of projects that leverage the many aspects of WCF. Most of what I leverage in WCF (pretty much everything) is currently missing from .NET Core. Much of this missing capability  would require various 3rd party libraries (or extensive custom code) to fill in the gaps and it's just not worth that level of investment when WCF already works, brilliantly. WCF is, above all else, a great productivity enhancer for my teams.  The biggest piece missing for me, currently, is the extensibility model that WCF provides. Most of my projects leverage the capability to completely decouple cross-cutting concerns from my other components (which are light-weight WCF services). WCF provides a fantastic mechanism for doing this without the need of 3rd party Aspect Oriented Programming libraries. Developers don't know (or even care) and can focus solely on delivering the business value of the features they are concerned with.  We also leverage many other aspects of WCF such as: named pipes, transactional queuing programming model, enforcement (not encouragement) of interface-based design, security capabilities, process isolation, error masking, I could go on.  Without WCF (or equivalent capabilities) in .NET core I would lose way too much productivity and cannot justify the switch. It would be great to have all of these powerful capabilities in a platform that can be used in any type of environment. Imagine, productivity of WCF plus cost-savings of cheaper hosting environments. That is a tremendous business value.  Thanks for tracking this issue, Will"
technical,"Thanks! I will take a look.  Also, for an interesting exercise, see here :) I have taken the survey. Please share the Road Map for WCF Server side support in .Net Core. That will be very helpful."
technical,"Why sad times? I imagine Phillip says ""sad times"" because he would like WCF to be already migrated to .NET Core. This view is shared by many development companies, but I must admit that the message is hopeful. :-)"
technical,"Putting my thoughts here, WCF is essential for me to look at using .Net core, it's so disappointing that this wasn't prioritized. Having to rewrite dozens of client apps is bad enough, missing support for the server-side puts me in an impossible situation. I personally can't believe the CORE team was allowed to increment major version numbers WITHOUT WCF server-side support. It's probably the single largest bottleneck preventing companies from migrating. Hurry up!"
technical,"Thank you for all your feedback! We have reviewed all the great responses above in regards to WCF Server support in .NET Core.  The WCF feature team is actively working on roadmap plans for WCF functionality in future .NET Core releases.  For next steps, we need your feedback in terms of top scenarios, feature usage and target profiles.  We would appreciate it if you can complete our survey.  ### .NET Core - WCF Support Survey As well, we are looking to engage closely with the WCF Community on specific scenarios, if you are interested in this partnership - please fill out the Contact Information on the final page of the survey. I really like youngsters telling about how fancy is using Web Api for their startups (I do too), however we also have here tons of enterprise grade software hosted on IIS and using WCF as an extremnely flexible communication stack abstracting transport layer from a developer, we do have tons of partner organization integrations using various versions of SOAP protocol. Are you telling me to code my own SOAP stack using web api or OWIN middleware? ,) I do not think so. Having no WCF Server Side support in Core makes it unusable and not even an option for consider thus effectively limiting its adoption, which I think is not MS wants. I would welcome WCF on OWIN more than warmly, then we would be having complete .NET Core offering when it comes to communicating."
technical,"Let's stop with commenting on people in the thread (and that includes the finger pointing that X is old and you need to move to Y) before I lock the issue because it has descended into trolling and flaming. I remember reading some 2-3 years ago some MS document where the **recommended** framework to use for **new web projects** was WebAPi or Asp.Net MVC.  WCF was mentioned in some legacy context. Personally, I think WCF does too much for its own good. Let me choose what I need. Allow me to switch easily some component for another in my app."
technical,"Ok now I'm curious. What SOA stuff can you do with node that you can't with asp.net core? I think the better question is, what can you do with WCF that you can't do with ASP.NET Core (and again, why are we in ASP-land? We're not trying to serve client apps..). Dotnetchris mentions it here."
technical,"This is another good reason to not support WCF. Years of people implementing designs using WCF based on false assumptions or mythical functionality. I understand that WCF provides value to many, many people, but does that mean .NET Core should support it? WCF seems to be at least partially tightly-coupled to Windows--the antithesis of .NET Core. How many shops relying on WCF actually  need  to deploy on Linux? I think that would be a bad architectural decision. If things are running so smoothly now considering the tight-coupling, why change it? Is running on Linux a net positive?"
technical,"Basically look at WCF as .NET on steroids, NOT just web services.  This point seems lost on many. I would like to add, I don't really care. I just use WCF as a marker to avoid an org / project / product etc :)"
technical,"You know, now that I think about it,  it may be a marketing thing.  Reading over the list of features that WCF has and that we all need I believe I stumbled upon a theory.  One of my arguments FOR WCF is that we get all these things in. NET standard. These features are enterprise quality features. One of the difficulties with adopting WCF is developer nativity. Often by the time they know they need it they've passed it by.  So half my theory is that even though most/all SOA apps could benefit from WCF, only a subset of those developers realize it.  The second half of my theory is that while Microsoft is slow/reluctant to build WCF into CORE they have no problem building with it on azure. I'm told that under the hood most of azure is either built on WCF or heavily inspired by it.  So the complete theory is that MS may be reluctant to give us free SO A tech to protect their cloud business. There are a ton of SOA offerings in azure on the cloud.  As teams gain sophistication and outgrow CORE webclient etc, they will have little choice but to buy azure compute time.  I'm not accusing ms of being anything other than a business and to me this makes a hell of a lot more sense than some vague claim of being too closely tied to Windows (while somehow managing just fine with all of CORE).   Others contributed as well, but among these features, support for Transactions ranks high for me. I found ""HangFire"" and this has been a great option in this area and has other features you might find in traditional Windows Services. This is one example of ingenuity that has filled the gap left behind in .NET Core.  The purpose of this thread seems to be a discussion of what others may also see as missing features in .NET Core when leaving WCF behind. Using a stateless protocol for SOA may not be practicable. De-coupling service applications from the underlying OS so they can run on Linux, Unix, iOS, etc. may be the only acceptable way to go if .NET Core is to be positioned as supporting multiple platforms. Does it need to be stateless, though? If not, would this make a difference in bringing forward features WCF has - whether 2, 5 , 10 years old (which is not the issue)?  I am excited to move to .NET Core and have been doing so for client-side applications. REST-based services are awesome, but currently they only compliment WCF. They do not, and cannot, replace them, however. I would like to reference an issue I just opened before noticing this thread: #2378  This isn't .Net Standard. It requires the full framework. But it can fit into the ASP.Net pipeline."
technical,"Our team used WCF to build an entire SOA platform. Each service is hosted in it's own app domain that provides complete isolation for every service. This allowed us to monitor resources, per service, and unload a service and swap it out with the app hot. I know app domains are gone, but what could possibly replace wcf?  Service Bus + Net Messaging Binding Net Pipe binding strongly typed headers and contexts Behaviors and interception MEX  you have a super reliable, and extendable set of tools. wcf provides the best tools for building enterprise applications. Without it, you simply end up rebuilding it.  Bringing the power of wcf to other platforms would be huge.  Linux + sql + wcf + service bus + .net core looks good to me. I'm curious. From what I understand, Azure itself is written, in large part,  usIng WCF (or at least something that provides all the capabilities of WCF) behind the firewall. That being the case, why would it be excluded from .NET Core in the first place?  Seems odd to exclude such a powerful and foundational toolset."
technical,"Don't let the marketing confuse you, there is nothing that is coupled to Windows that  must  be coupled to Windows. That's all the name is - marketing. I'm doing a lot azure. zero WCF on the outside. AFAIK the stuff I'm using is also not using much wcf on the inside. I have no data to proof that, but just one simply argument: everything I use is moving over to core. If these parts of azure would rely on wcf that much, then core would support WCF already."
technical,"For me, anything public facing is always WebAPI but with everything else, it is WCF simply because you have the various protocol choices via the bindings, throttling, reliable messaging etc. (so basically behind a public facing endpoint) I kind of agree with shawnwildermuth in that having WCF may add to some confusion, but as with anything it depends on the communication and type of support introduced. If WCF has some ""competing"" features with what is currently available (or maybe otherwise available in a future update) thats when confusion will happen. Have the use cases/support distinct enough to circumvent this. To summarise, yes I would want WCF server support specifically for protocol/bindings such as TCP/named pipes, as well as throttling and reliable messaging (dont care about trxs). However if it comes at the cost of mixed messages, confusion, overlapping feature set, then no thanks. I'm finding WCF less and less important in a world of Microservices and service bus technology. We've been on Windows Service Bus for a while, hit too many problems, and are migrating to RabbitMQ. Our sysops guys always get a look of surprise when we tell them we don't need IIS installed."
technical,"Ideally what I hope to achieve is to migrate my legacy WCF service with net tcp binding and authorization to .net core without having to switch to a different library or significantly change the code I'm in the same boat, I had two big blockers before I could make my app .NET Core compatible, MEF and WCF. As of December MEF is supported, but I'm sorry to see WCF is still not currently planned.  What is the recommended replacement for a web service via System.ServiceModel that has both WCF and REST endpoints? For reference, my service methods have the following attributes to enable both WCF and REST on each method: Along with the end points, I'm using System.IdentityModel to validate user credentials on inbound requests.  Thanks!!"
technical,"That's not accurate. The github sources are a subset of .net that are licensed under MIT. You can do anything you want with them. Before the Xamarin acquisition, this enabled Mono to copy some of the source to improve their implementation.  I'd like to see server support too, but that is not the current focus of this repo. I'm sure it's a mountain of work, and priorities. I'm not so sure about that. Each source code file clearly states that the source code is copyrighted and all rights are reserved to Microsoft (and that's okay, it's Microsoft source code). The sources have to be reissued with correct license (as in the .NET Core repository) for them to be editable and usable.  I'm all for a project on github, backed and supported by Microsoft, it's a very good solution. However, even if the source codes does not compile and so on, it'll speed up development to start from them and then by steps adapt to .NET Core. A complete re-write, with inspiration from the source code, is another magnitude of project (in my mind)."
technical,"You wrote: Why does the lack of .NET Core support make something deprecated?  For me it goes along the lines like this. Microsoft brings out a complete rewrite of .NET and opens up the door to other OS. It's not something that was a big problem in the past, as our customers knew that when they hired us the got an MS based solution back and it was not a problem for them, because they were running Windows Servers.  But hey, that was something that was not working for 15 years, but know as we are able to do it, then hey it's a great thing. And all along every programming magazine (MS related) was only bringing articles of that new thing.  And all of the blogs just wrote about .NET Core. So now the developers get the feeling this is the new way to go and get the impression that their old tool is just old.  And yes, then was that little incident with Silverlight. All shiny, all great, not a conference without 80% talks about Silverlight and then it vanished in a puff of haze just like that. That was something that stuck with us. When are they pulling such a stung again? Maybe tomorrow everything should be ServiceFabric and then puff, support for WCF is even gone for the Windows world.  You see, the fact that Microsoft is not willing/capable to port WCF to the new world makes us uneasy. For me personally it doesn't have to be a complete full port of all aspects of WCF. It doesn't have to support all bindings. But it should allow us to build SOA applications with transactions, authorization and other good stuff.  In the end my question is: What is .NET Core for?  Why should I start building on the .NET Core platform when even Entity Framework is way behind the EF for the full .NET in regards to functionality? Other stuff like WCF is missing as well. Don't forget not every developer out there is writing bleeding edge, NoSql, NoConstraints, NoRelations, NoSecurtiy software. Some work on systems that evolved for years and are stable and mature as well.  The question is what those developers say to their managers? Developer: Sir you know, MS hat that new shiny thing, but it lacks some of the functionality we need. Manager: But we need to keep up with the technology or our system won't be able to run on Windows Server 2019. Developer: We can do, but it means we have to rewrite it for a big part, and if we do that we might need to evaluate if we do it in Java or not.  Who would be modifying or extending WCF in .NET Framework? The question was from some interested developers asking if they can write a compatible framework for .NET Core...  Why does the lack of .NET Core support make something deprecated? .NET Framework 4.7.1 was released this week, along with a preview announcement of .NET Framework 4.7.2. New Windows servers are planned, and new Windows clients are planned as well. What was deprecated? Who is saying this framework is incapable? These are not messages you are hearing from our teams. I'm still yet to work for a company that isn't desperately trying to remove all traces of EF and WCF. So sounds like core is great got me."
technical,"There are distinct features of .NET that are tightly coupled to Windows: System.Web and WCF as well as WPF and WinForms are four of those frameworks that are not ported to .NET Core for this reason. There are other frameworks and tools in the .NET Ecosystem that can be used for those purposes.  Until .NET Core, there was never a question of being able to run WCF on Linux, and we do not have a compelling reason to enable that.  You can run your services in containers, and we are working to further enable that capability with additional features on Windows with .NET Framework 4.7.1 and later. I'm sure nobody said anything like that when they decided to deploy SQL server to linux, design .NET core to run on linux, decouple other system.web components from IIS, and port 99% of non-UI .NET code to core. As mentioned in dozens of replies in this thread, there is no true replacement for WCF that people can convert their WCF endpoints to in any reasonable amount of time in order to execute in .NET core.  Turns out MVC is able to make the jump, but WCF people are incapable? SQL Server can make the jump to linux, but WCF is too tightly coupled to Windows? Give me a break, you're just making excuses."
technical,"Something that would be really useful is full support for Owin hosting. I've given feedback offline on this topic, but just to keep things consolidated, I'll repeat it here :)  I've helped a couple customers adopt .NET Core who have been hung up on missing WCF features. Items which specifically have been missed include: - Service host with Http and nettcp bindings - Transactions - Discovery"
technical,"Cool.   It was a thumbs up and thank you for your comments. I've had bad and good experiences with WCF. But the bad is not due to the technology but how it was implemented. It can be time consuming to get just right but from my exposure I can definitely see the potential and power of WCF when done right.  I'm a heavy user of asp.net core now but I would/am definitely interested in seeing how WCF is ported across to .net core. I suspect there will be compromises/changes to make since, well, the technology does start with a W! (a cross-platform transaction implementation would be interesting).  So this is a big +1 for me."
technical,Cool.   It was a thumbs up and thank you for your comments. Ideally what I hope to achieve is to migrate my legacy WCF service with net tcp binding and authorization to .net core without having to switch to a different library or significantly change the code
technical,"Outside of pulling ALL of WCF in, which is quite large, has there been any consideration given to just the pieces that don't exist easily? Like a targeted TCP API? That just does those transports, does them well, and doesn't try to build an uber-abstraction that WCF tried so very hard to do?  Most of the other stuff I'm usually only forced to use in legacy scenarios, and have moved on to better, more obvious solutions. I'm OK with isolating parts of my system that need to talk to 2005-era WS-* web services into separate, isolated pieces. If by transports you mean WCF handles location transparency well then yes that is correct. But, there is no uber try. WCF is an interception based execution pipeline. There's no try, only do in that aspect. you say it did/does/will not? Please say more so we may understand.  Legacy scenarios?! Other stuff? You've moved on to better? More obvious and does what WCF does?! Like what? I ask, because I know of one and only one thing that meets that description, and I would like it confirmed please.  And, what does ""2005-era WS-* web services"" have to do with WCF?"
technical,"I disagree entirely, but each to their own. If I have to rely on the community I don't really think it's debatable which has the stronger tooling for SOA. It's not .NET Core. My day job is replacing the entire SOA for a Fortune 50 with Node, so...  The beauty of .NET, the reason it exists at all and why it is so popular in the enterprise, is because it has not only everything built in, but supported by all the weight of Microsoft.  If they aren't willing to do that for .NET Core I agree it's a business decision to prioritize Windows and Azure, but know that those of who are building cross platform SOA will go with Node or something else in droves."
technical,"I disagree entirely, but each to their own. If the premise of .NET Core is to be everywhere, there ought to be a universal mechanism (WCF, or an improved successor to WCF) that connects everything together, don't you think? Why should it matter to your apps what platform your devices are running? Likewise, why should it matter how to get a message from point A to B reliably and securely, handling the serialization/deserialization, over a buffered/streamed/queued transport, supporting transactional rollback of a call chain, with fault exception propagation, etc? WCF came close to offering a universal connection mechanism with an extensible pipeline. Frameworks that do web services with REST over HTTP don't even come close.  The Windows specific part that comes to mind is distributed transaction support that relies on MSDTC. I'll leave it to the Microsofties to figure out how to NuGet and port that to non-Windows platforms."
technical,"A specific business case I have is that we have an investment management system with several million lines of code for which the server side is currently hosted on premise on MS Windows Servers at about 200 insurance companies and banks world wide. We have about 100+ service types, of which in the largest installations there are 700 to 800 concurrent service instances at play. Our product is driving important parts of the core businesses.  The expenditure on IT is huge for our customers. This is also the place where we are looking to make major improvements over the coming years. A part of that is to find alternative hosting environments. A favorable choice would be Windows Nano, or the .NET Core on Linux. For being able to adopt .NET Core (or Windows Nano) we are missing the WCF server side.  Since we are very happy with WCF as a programming model, there is no incentive to rewrite our applications other than that WCF server side is unavailable in our future hosting environments. Particular features that we use is long. But to start .NET Core adoption are, these are the important ones: - Self-hosting using ServiceModel.ServiceHost - NetTcp (half-duplex) - Message inspectors for instrumentation and access to SOAP headers - Behaviours - OperationContext - Contract based programming model  Yes. We would continue building WCF services also on .NET Core. In my case it is needed for new solutions, where WCF services can be agnostic deployed (Windows, Linux, OSX, cloud) on .NET Core."
technical,"A specific business case I have is that we have an investment management system with several million lines of code for which the server side is currently hosted on premise on MS Windows Servers at about 200 insurance companies and banks world wide. We have about 100+ service types, of which in the largest installations there are 700 to 800 concurrent service instances at play. Our product is driving important parts of the core businesses.  The expenditure on IT is huge for our customers. This is also the place where we are looking to make major improvements over the coming years. A part of that is to find alternative hosting environments. A favorable choice would be Windows Nano, or the .NET Core on Linux. For being able to adopt .NET Core (or Windows Nano) we are missing the WCF server side.  Since we are very happy with WCF as a programming model, there is no incentive to rewrite our applications other than that WCF server side is unavailable in our future hosting environments. Particular features that we use is long. But to start .NET Core adoption are, these are the important ones: - Self-hosting using ServiceModel.ServiceHost - NetTcp (half-duplex) - Message inspectors for instrumentation and access to SOAP headers - Behaviours - OperationContext - Contract based programming model  Yes. We would continue building WCF services also on .NET Core. In regards to all the alternatives and possibilities discussed here, I just want to say:  *There is nothing even comparable with Full framework WCF in terms of flexibility and interoperability*  Now, are there better choices for SOA architectures? Yes ... and No. It all depends on how *green* is your field and what are you trying to build. Is REST better choice for an SOA architecture? No ... and Yes, but depends on what you are trying to build. Is an AMQP library the best choice? ... Ok, you're catching my drift ...  I don't think this discussion should be about what's best for an SOA architecture (which lacks requirements or description, by the way, and everyone imagines something different) ...but about the fact that .Net Core lacks a flexible and interoperable communication framework that would allow it to be used in ""not so green fields"" or ""gradual migrations"" completely locking a set of potential projects out (Especially large enterprise migration projects suffer here).  Some final thoughts: -  SOAP is a thing of the past, no question about it. - WS-* is overly complex - There are better, faster, more modern communication solutions  But all of the above don't mean WCF should be discarded, quite the contrary, WCF is *needed* to solve the above issues for it's users."
technical,"Saying that windows based features are a blocker in moving wcf to .net core proves one saying that has little or no experience with it. In fact the only sole windows feature I can think of is automatic integration with DTC which I think is not widely used.  what else can I read there? for example some biased sayings by phillip-haydon. should we ditch swagger? it allows automatic client generation for complex web apis?  just an example. looks like a troll account ,) Is there an official roadmap from MS on porting and supporting WCF Service or Server-side applications to dotnet core?"
technical,"How about this Fritz, give the open source community the WCF source, we'll rewrite it for .NET Core and you can take credit for opening up a major feature to the ecosystem. Win/Win? It might already be in the repo, will check when I have a free minute"
technical,"Nope, my data access code is 100% data access. I don't have to write ANYTHING related to queuing, transactions (across the entire stack across processes and threads), security, concurrency, reliability, *ability, etc. Does your resource access have any of those smells? My data access is exposed as a SERVICE - not a web service, but a service, with all the benefits thereof. Coupling is ALL contracts, all the time - nothing else. It's what a SOA does, and you need good plumbing behind a SOA, otherwise, your dev teams have to be kung-fu masters in a bunch of arbitrary (and constantly changing) environments.  By the way, ""latest and greatest"" aren't what enterprises are interested in. Getting stuff done and producing features/products is.  It is precisely my use of WCF that allows my ecosystems to thrive and iterate. Basically look at WCF as .NET on steroids, NOT just web services. And again, IF you aren't using WCF, then why are you here? This thread is about people who want to get a solid, streamlined, full-featured, singular communication stack into .NET Core so we can host on cheaper metal and smaller devices, NOT about whatever kung-fu you require your teams to perform. It not dead, it's  undead .  This issue is troll bait in fairness.  WCF is not something that should be implemented in .NET Core."
technical,"Opening the source code is not the same as opening the build infrastructure It's not about build infrastructure. It's about provided code and accompanying files. It won't compile because it lacks resources, some dependencies, etc. Someone above posted link to Mono site describing issues with reference source Microsoft  offering. As name implies, it is for reference only and debugging.  What we need is github repo with Microsoft blessing so community could work on porting."
technical,"Good to see some love for this very alive-and-well technology. My Pluralsight courses have been very successful on this topic. My prediction is a resurgence as true micro service architectures get more mainstream. Many of the things you need to consider in properly architecting micro service systems are built into WCF. You guys all rock! johnvndnbrk WCF is solid rock and mature, you won't be disappointed. However this good old Microsoft, create something and abandon."
technical,"SOAP is Not Dead “ It's Undead, a Zombie in the Enterprise Just one detail, if WCF is dead, why is the issue that has the most comments of all? 230 versus 33, which is the next. Almost 10 times  WCF is something that should be implemented in .NET Core."
technical,"I have to throw my support for WCF in .NET Core as well.  My primary client is a fairly large, heterogeneous WAN with intermittent connectivity.  I have dreamed for years of having WCF across their entire network so that I can finally (!) have reliable transactions and durable services spanning their various generations of Linux and Windows.  The thought of bringing all of WCF's many capabilities (as enumerated in this thread) to my client's entire infrastructure would literally be a dream come true. We can spend the vast efforts we've previously put toward plumbing into the services that can streamline everything we do, and take us to the next level.  Please, please make this happen.  -Thomas Let me also mention the value of WCF in .NET core.  WCF gives the best capabailities across all my services. (Perhaps we need a sexier name for it) but in general, I use and wish to continue to use: - **System.Transactions** - **Durable services** - **Named pipes** - **An extensibility model** - **Transactional Queuing** - **MEX framework (and MEX endpoint)**  And when we can run these services on Linux (and eventually IOS), it allows the most solid framework  Paul"
technical,"And my lack of any public repositories means, what, exactly? That I work for people who choose to not give away their code? That I don't work on many open-source projects?  Kudos on your evidence of... What, exactly? :) Let's stop with commenting on people in the thread (and that includes the finger pointing that X is old and you need to move to Y) before I lock the issue because it has descended into trolling and flaming."
technical,that question seems backwards Linux support.
technical,"That's a fair point and the topic of renaming it, IIRC, came up in this thread. About the only part of WCF that is really ""windows-specific"" is the fact that it can very seamlessly interact with MSMQ via an OOTB binding - something that could be made an auxiliary package. If you are targeting MSMQ you aren't going to target .NET Core anyway, so that would be just fine to use regular .net anyway. Everything else has a corollary in non-windows, AFAIK.  On the other hand, if I did target .net core but still wanted to host on windows instances, I want the ability to just configure a binding to leverage MSMQ (along with bindings for other queue techs such as Rabbit, Zero, Azure, SQS, etc). Nothing in my actual code needs to care, at all. The beauty of WCF. look where the puck is going.   A lot of us were here when they dropped the puck into the game and we've been watching the puck the whole game and we also know where it goes next. I can use my WCF in service fabric. I could use my .net in WCF. I could use my COM in .net."
technical,"That's a fair point and the topic of renaming it, IIRC, came up in this thread. About the only part of WCF that is really ""windows-specific"" is the fact that it can very seamlessly interact with MSMQ via an OOTB binding - something that could be made an auxiliary package. If you are targeting MSMQ you aren't going to target .NET Core anyway, so that would be just fine to use regular .net anyway. Everything else has a corollary in non-windows, AFAIK.  On the other hand, if I did target .net core but still wanted to host on windows instances, I want the ability to just configure a binding to leverage MSMQ (along with bindings for other queue techs such as Rabbit, Zero, Azure, SQS, etc). Nothing in my actual code needs to care, at all. The beauty of WCF. Maybe not in C#, but with the .NET Framework it is. Unless you want to custom code tons of stuff or pull in many 3rd party libraries. Nor should we have to pull in ASP.NET Core to create even a basic HTTP web service. System.ServiceModel exists for a reason.  (Not to mention we already have the full service implemented that simply needs to be ported...)"
technical,"I do a lot of work on the IoT.  It would greatly facilitate creating cross platform systems if my lightweight WCF services could be hosted anywhere that .NET core could be hosted.  As you can imagine, in IoT (and other systems) discovery is important so MEX and an extensible model.  The ability to debug locally and to support named pipes between services is valuable.  Really, in a nut shell as much of the WCF stack as possible - tranactions, bindings, durability, interoperability with azure and cloud based services (which often requires proper security and metadata). Messaging between services is important and when doing enterprise backends, REST is not going to work. It lacks too many things like others have mentioned. So certainly Transactions, Queues Messaging, Named Pipes, Extensibility should be supported by .Net Core  So .NET Core has to provide those things in one way or another. If it is called WCF I don't care. Maybe it would be the opportunity to fix some of the weaknesses of WCF like the overy complicated configuration story and replace it with a convention based approach.  Also you should/must support other Messaging frameworks beside MSMQ or Service Bus. In general support of AMQP would be nice, including the various messaging patterns."
technical,"I've had bad and good experiences with WCF. But the bad is not due to the technology but how it was implemented. It can be time consuming to get just right but from my exposure I can definitely see the potential and power of WCF when done right.  I'm a heavy user of asp.net core now but I would/am definitely interested in seeing how WCF is ported across to .net core. I suspect there will be compromises/changes to make since, well, the technology does start with a W! (a cross-platform transaction implementation would be interesting).  So this is a big +1 for me. might I recommend my two Pluralsight courses, WCF End-to-End and WCF Power Topics :)"
technical,There is another start at a WCF-like ServiceCore for .net core: My .NET Solutions rely on configuration service behavior's for certificate authentication. I need MEX support to generate the W.S.D.L. for smart contract binding at run-time. We also generate service proxies at run-time from W.S.D.L. and then use reflection to load and invoke. Also our logging via W.C.F. doesn't perform well enough unless its running using T.C.P. binding.
technical,"We have quants writing server side code in Mono (they need cross platform) who were almost jumping up and down in glee with the introduction of .Net Core ... until I told them that it wouldn't support AppDomains, which they currently rely on heavily.  The look on their faces was one of despair. I have showed them a viable alternative using WCF and named pipes (to give them the isolation they desire without compromising performance). They were even more interested when I showed them that they could then scale these services across machines and across platform (if WCF services were supported on Linux)  So, this is blocking adoption of .NET Core in this scenario and we would be looking to port existing processes/services to .NET Core as well as developing new services using .NET Core. My company (one of the largest financial product company) has huge investment in Linux/Ubuntu. The windows infrastructure is relatively tiny but has tons of WCF services running for business critical applications. Running WCF on .NET Core running in Linux environment is a huge benefit in integrating with other (non-windows) services and platform consolidation point of view."
technical,"Thinking about a service like LogMeIn: I need to maintain a persistent connection from clients to servers so I know they are online and can initiate a remote access from server instantly if they are online, even behind firewalls. And what about big file transfers (big, like, what FASP from Aspera can do)? My company is also waiting for WCF implementation in .NET Core to start moving into .NET Core. Is there any chance that we will be informed of this feature's status?"
technical,"Over the past 6 years, in the solutions I have been involved in (in the .net area), we have always relied on WCF to handle the service layer (behind the firewall & intranet) of the solutions. Nothing out there gives us the same level of flexibility and support for different communication channels like WCF does.  I have been holding back moving to .net core for production environments for the lack of WCF support specifically. Not supporting WCF server on .net core creates a need to rewrite a lot of infrastructure code, which will probably end up mimicking the WCF programming model anyway.  The biggest solution I worked on was used by over 300 health care institutes, rewriting the server layers and functionalities is a big investment to say the least, not to mention high risk. In fact, in that same solution we were looking at a way to unify the programming model between server and embeded devices (linux) for new products. Supporting WCF services on .net core (not just clients) could've been a really big help and cost saver as there would be no need to have 2 development teams, but in stead have a larger singularly focused team. My scenario is pretty much similar to olilanz's, except that my business case is Point of Sales. Like him, we have our application deployed to numerous stores world wide. We are also looking for alternate ways of hosting our application in order to reduce infrastructure costs. As jan-johansson-mr said, agnostic deploying WCF services would be great and would give us a huge flexibility.  WCF plays a major role in our application: it is based on a plug-in architecture where which plug-in is basically a WCF Service, so communication between plug-ins are actually WCF calls. Changing this aspect would mean have to rewrite/rethink a lot of infrastructure code.  In our case, self hosting using instances of  ServiceHost  and the Contract based programming model is crucial. Our plan is not only migrate existing services, but also create new services."
technical,"WCF is not a trivial framework, and is a Windows component... there are plenty of ways to continue running WCF services with Windows and we continue to invest in the .NET framework.  Other products have other business reasons to enhance their platforms. We view WCF as extremely stable, capable, and coupled to Windows.  We have always made source code available Nice try, the source is available but only for reference use, not for modification or extension.  You may view WCF as ""extremely stable"" and capable, but because of the lack of adoption of .NET Core most people should now consider it ""extremely deprecated"" with a one-way ticket to live out the rest of its days with Clippy."
technical,"Sure, because you want to scale manually.  Handle transactions, er, um, manually?  Because you want to implement auth manually.  You want to implement your own context flow and header management and interception and pipeline intervention and encryption and compression and on and on...  I assure you that at the enterprise level these things are happening with wcf more than you know.  One of the challenges here is that enterprise developers tend to be very silent about their efforts because what they do tends to be trade secrets.  While it's common for front end teams to talk about techniques it's rare for an enterprise architect / lead to discuss how the guts of a business are wired together so you don't hear as much about it. (that's not to say they don't write, but it's much less than what's written about the web/http-this-or-that and everything in JS) No. You can do this with about 15 lines of terraform  Not sure what you mean here. Database transactions? Distributed transactions? Identity Server This and the rest of the above are trivially easy with a pipes and filters style architecture (except for compression and encryption which are baked into HTTP). Best example I can think of is OWIN. Wow. That's incredibly sad."
technical,"Just one detail, if WCF is dead, why is the issue that has the most comments of all? 230 versus 33, which is the next. Almost 10 times  WCF is something that should be implemented in .NET Core. Nope, my data access code is 100% data access. I don't have to write ANYTHING related to queuing, transactions (across the entire stack across processes and threads), security, concurrency, reliability, *ability, etc. Does your resource access have any of those smells? My data access is exposed as a SERVICE - not a web service, but a service, with all the benefits thereof. Coupling is ALL contracts, all the time - nothing else. It's what a SOA does, and you need good plumbing behind a SOA, otherwise, your dev teams have to be kung-fu masters in a bunch of arbitrary (and constantly changing) environments.  By the way, ""latest and greatest"" aren't what enterprises are interested in. Getting stuff done and producing features/products is.  It is precisely my use of WCF that allows my ecosystems to thrive and iterate. Basically look at WCF as .NET on steroids, NOT just web services. And again, IF you aren't using WCF, then why are you here? This thread is about people who want to get a solid, streamlined, full-featured, singular communication stack into .NET Core so we can host on cheaper metal and smaller devices, NOT about whatever kung-fu you require your teams to perform."
technical,"If I have to rely on the community I don't really think it's debatable which has the stronger tooling for SOA. It's not .NET Core. My day job is replacing the entire SOA for a Fortune 50 with Node, so...  The beauty of .NET, the reason it exists at all and why it is so popular in the enterprise, is because it has not only everything built in, but supported by all the weight of Microsoft.  If they aren't willing to do that for .NET Core I agree it's a business decision to prioritize Windows and Azure, but know that those of who are building cross platform SOA will go with Node or something else in droves. Ok now I'm curious. What SOA stuff can you do with node that you can't with asp.net core?"
technical,"WCF and soap are entirely orthogonal. thefringeninja for you to post that comment shows you have no idea what you're talking about and you're just trolling this serious thread. On the contrary, I'm being quite serious.  It seems as if there are two major use cases, based on my previous experience and reading through this issue. Feel free to correct me if you think I'm mistaken.  1) You are interoperating with Java or some other language 2) You control both ends of the wire and they are .net  Now, in the case of 1), how many people are actually using any binding other than the SOAP/HTTP binding? Would they even work? In this case I think the link I provided should be sufficient for most use cases. Also, something tells me that the MSMQ binding is never going to work on linux. Of course I would love to be proven wrong here!  In the case of 2), well, you control both ends so is wcf really necessary? As long as your code isn't tightly coupled to WCF here, it shouldn't be too much effort to write an http/json adapter for it."
technical,WCF and soap are entirely orthogonal. thefringeninja for you to post that comment shows you have no idea what you're talking about and you're just trolling this serious thread. Opening the source code is not the same as opening the build infrastructure
technical,"yes it is under servicewire.core pull the src, update the packages and apply my pull request and you should be up and running or simply use my repo"
technical,"The compat pack contains only existing WCF client packages. Outside of pulling ALL of WCF in, which is quite large, has there been any consideration given to just the pieces that don't exist easily? Like a targeted TCP API? That just does those transports, does them well, and doesn't try to build an uber-abstraction that WCF tried so very hard to do?  Most of the other stuff I'm usually only forced to use in legacy scenarios, and have moved on to better, more obvious solutions. I'm OK with isolating parts of my system that need to talk to 2005-era WS-* web services into separate, isolated pieces."
technical,"In my case it is needed for new solutions, where WCF services can be agnostic deployed (Windows, Linux, OSX, cloud) on .NET Core. Over the past 6 years, in the solutions I have been involved in (in the .net area), we have always relied on WCF to handle the service layer (behind the firewall & intranet) of the solutions. Nothing out there gives us the same level of flexibility and support for different communication channels like WCF does.  I have been holding back moving to .net core for production environments for the lack of WCF support specifically. Not supporting WCF server on .net core creates a need to rewrite a lot of infrastructure code, which will probably end up mimicking the WCF programming model anyway.  The biggest solution I worked on was used by over 300 health care institutes, rewriting the server layers and functionalities is a big investment to say the least, not to mention high risk. In fact, in that same solution we were looking at a way to unify the programming model between server and embeded devices (linux) for new products. Supporting WCF services on .net core (not just clients) could've been a really big help and cost saver as there would be no need to have 2 development teams, but in stead have a larger singularly focused team."
technical,"I would like to add, I don't really care. I just use WCF as a marker to avoid an org / project / product etc :) People seem to be throwing the word ˜enterprise' around like it helps justify their need for WCF.  If the enterprise already has a WCF SOA, then it does..."
technical,"There are lots of useful features about the WCF and also would be nice, Built-in **Dependency Injection** (as .NET Core MVC 6) Interceptors support Hosting anywhere (Windows or Linux) Working with AspNet Core Module (Without additionals configurations - httpcfg set urlacl) Please don't invest in supporting neither VB.NET nor WCF nor WWF."
technical,"As soon as ***if*** WCF is implemented. We're still waiting on more information 1.5 years later. Putting my thoughts here, WCF is essential for me to look at using .Net core, it's so disappointing that this wasn't prioritized. Having to rewrite dozens of client apps is bad enough, missing support for the server-side puts me in an impossible situation."
technical,"You may be mis-characterizing history. I have had many conversations over the years about how SO with WCF is the only all-in-one package for services. I've done work in on other platforms where we have to bring together dozens of vendor products to get what .net offers out of the box.  You may not hear people desire it because, in my experience, people are generally a bit cross camp ignorant. I certainly am because its hard enough to keep up with the details of your own camp.  However, as SO matures it becomes clear that what's needed are the features of WCF (hosting, transactions behaviors, auth, interface based, etc etc). The designers of WCF did a unique and amazing job.  I don't really care how tied it is to Windows. That merely sounds like something that needs to be dealt with under the covers.  The question was asked do we want it in .Net core and why and this list contains some of the answers.  We do. Reasons it's incapable and should be considered deprecated by the community, compared to other .NET components: 1.  Not being part of .NET core means it is not ""open source"" and nobody outside Microsoft can rewrite it for .NET core. 2. No cross-platform support, which many cite as a primary reason to use .NET core in any application. 3. It cannot benefit from the .NET Core performance gains (of which there are many). 4. Anyone who wants to port their current WCF services to .NET core is going to have to spend a very significant amount of time redesigning their architectures to include all the features they were dependent on and got for free as part of WCF. This is Microsoft putting the onus on the customer to engineer their way out of a Microsoft-caused problem. You guys should be willing to take the hit in order to make your customers lives easier, instead you're here arguing that your customers are wrong.  SQL server and ASP.NET have migrated to cross-platform capable foundations, apparently avoiding your fatalistic, fallacious arguments.  Being excluded from .NET core is a clear message to developers that there will be less and less support going forward, and to avoid using it. It doesn't matter if you release updates every hour on the hour for WCF in the .NET Framework, if it's not part of Core we all can take that as a sign it has no real future. Just like no new software is being written using Silverlight or Windows Forms, WCF will be done in short order.  .NET Core is not being adopted by my team because the lack of WCF support. And if we're not doing it, there are certainly many others doing the same as us. That means less adoption of your very expensive investment in this new technology. This should be more important to you than the investment required to modify WCF."
technical,"There very much is a WCF team, and we are working on features to ensure compatibility with Windows 10, Windows Server, and Windows Containers.  We had a very successful presentation at Build where we showed the first of our Windows Containers.  Work is continuing in that space, and we plan to continue in these investments.  WCF for .NET Core is very much on our radar, and we are working with the ASP.NET Core to prioritize this work.  We will share more details when they are available to broadcast publicly. Sad times."
technical,"The main usage scenario for our company would be using ChannelFactory<T to generate clients that use a SHARED(server/client) contract For communication between a web-layer (client facing) WebApi and a Backend service layer WebApi (exposed on the internal network only).  There is no easy way to make a client that uses the same contracts on the server and client without a lot of boilerplate.  We have tried using swagger clients, and hand coding http proxies to hit a WebApi, but there is way to much boilerplate.  If you could make ChannelFactory simply work with an existing WebApi (properly using get/put/post etc) and share the contracts, I would be happy.  If anyone has any other ways to do this with minimal boilerplate, I would be happy to hear how you are doing it. Saying that windows based features are a blocker in moving wcf to .net core proves one saying that has little or no experience with it. In fact the only sole windows feature I can think of is automatic integration with DTC which I think is not widely used.  what else can I read there? for example some biased sayings by phillip-haydon. should we ditch swagger? it allows automatic client generation for complex web apis?  just an example. looks like a troll account ,)"
technical,"The main usage scenario for our company would be using ChannelFactory<T to generate clients that use a SHARED(server/client) contract For communication between a web-layer (client facing) WebApi and a Backend service layer WebApi (exposed on the internal network only).  There is no easy way to make a client that uses the same contracts on the server and client without a lot of boilerplate.  We have tried using swagger clients, and hand coding http proxies to hit a WebApi, but there is way to much boilerplate.  If you could make ChannelFactory simply work with an existing WebApi (properly using get/put/post etc) and share the contracts, I would be happy.  If anyone has any other ways to do this with minimal boilerplate, I would be happy to hear how you are doing it. Scott, doesn't look like an image went through"
technical,That's not a good enough answer to justify an investment to re-write a 10 year old framework that has significant coupling to the Windows platform. Linux alone is not good enough… Seems like a good enough reason to change the whole .net ecosystem but not WCF... LOL you too funny
technical,"Throwing in my 2 cents here: I work in a heavily Microsoft-oriented shop, and having WCF baked into .NET Core would open up a lot of new operation for our department and our company. I and the other developers I work with would love to see this. similar with mentioned above, in my case, i'm just refactoring all my app using .net core, and there're many services using wcf which called by other apps that belong to another team. my team is the first one to upgrade tech from .netframework to .netcore in my company, so the more convenient and stronger the .net core is the ealier .netcore can be used in other teams. btw, the load balance solution based on linux is considered by the operation team, after we changing to .netcore, it will be very flexiblelow cost to integrate with the lb solution"
technical,"+1 for locking. It's going nowhere. since you have no skin in the game and are being the most divisive here, I'd suggest banning you before locking those of us with business needs for WCF out of the WCF thread..."
technical,"For those of us who have spent any time in the enterprise it's a bit laughable to see this subject treated so flippantly by some...  Also, I don't think the Microsoft community moved on. But new people joined the community. SOAP is Not Dead “ It's Undead, a Zombie in the Enterprise"
technical,"Those options you list aren't in the .NET Framework... Some of them are, some of them aren't. Certainly REST is. All of them are options .net code can make use of though, so what does it matter? Embrace OSS .Net! :D"
technical,"I don't think server side wcf is worth the investment when things like signalr and tooling should take presidence. I've always had such a hard time with wcf client Config  that I dispise it :( please correct me if I'm wrong... But why wouldn't web Api be the way to go... It's lightweight and gives you everything you'd need to build apps that can work from any platform with a simple http call...  Leave security, transactions, pub/sub up to the implementer... Something that would be really useful is full support for Owin hosting."
technical,"On the contrary, I'm being quite serious.  It seems as if there are two major use cases, based on my previous experience and reading through this issue. Feel free to correct me if you think I'm mistaken.  1) You are interoperating with Java or some other language 2) You control both ends of the wire and they are .net  Now, in the case of 1), how many people are actually using any binding other than the SOAP/HTTP binding? Would they even work? In this case I think the link I provided should be sufficient for most use cases. Also, something tells me that the MSMQ binding is never going to work on linux. Of course I would love to be proven wrong here!  In the case of 2), well, you control both ends so is wcf really necessary? As long as your code isn't tightly coupled to WCF here, it shouldn't be too much effort to write an http/json adapter for it. Sure, because you want to scale manually.  Handle transactions, er, um, manually?  Because you want to implement auth manually.  You want to implement your own context flow and header management and interception and pipeline intervention and encryption and compression and on and on...  I assure you that at the enterprise level these things are happening with wcf more than you know.  One of the challenges here is that enterprise developers tend to be very silent about their efforts because what they do tends to be trade secrets.  While it's common for front end teams to talk about techniques it's rare for an enterprise architect / lead to discuss how the guts of a business are wired together so you don't hear as much about it. (that's not to say they don't write, but it's much less than what's written about the web/http-this-or-that and everything in JS)"
technical,"Then, I can change the binding to use whatever I want, without having to change my code. You presume to know of some other technology that can do that? Please share.  The fact that this doesn't exist elsewhere renders the argument about WCF being outdated moot IMHO. To say nothing of all the other benefits of WCF... surinam because this ALWAYS happens in such threads. People demanding a feature and if they don't get it people discuss if things are dead or not. But nobody actually offers their help - it's ""Microsoft needs to do it"", ""people who don't need it, don't understand it"", ""you did not work in the enterprise like me"" and all that stuff."
technical,"The last two comments here hit the nail on the head and I couldn't agree more. It is called ""Communication Foundation"" for a reason. ""Moved on""? Putting that out there is essentially telling every customer currently using WCF (in whatever capacity) that they have always been doing it wrong and they should get with the program - very unfair thing to say. It seems to me that a common pattern in the certain parts of the Microsoft community is that the minute there is another way of doing something, the previous way is now labeled wrong or idiotic and therefore dead. if it ain't what the cool kids are using then you must be a moron. Call me old-school, but for my customers it's about solving a problem and providing value. WCF did and does that very well. I agree with Will. It's way more than doing web services. But the fact that it can do them too is still great. Why go through a list of ten products that can each offer an alternative to a single feature of WCF when I  know I can do it using WCF? I use WCF in so many other capacities and in conjunction with Web API and ASP Core. I've set up solutions using WCF's discovery and content-based routing, and that shit rocks. There isn't anything else out there that can do it as well or as easily. Tbf it's not the .NET Community that moved on. It's the rest of the industry. And it's hard to justify SOA things if that means .NET to .NET communication only. But of course you want to maintain legacy stuff - and you can. Full framework won't go away. We just got a new version."
technical,"My company (one of the largest financial product company) has huge investment in Linux/Ubuntu. The windows infrastructure is relatively tiny but has tons of WCF services running for business critical applications. Running WCF on .NET Core running in Linux environment is a huge benefit in integrating with other (non-windows) services and platform consolidation point of view. Thank you -- this is really great feedback and much appreciated.  We hear you and are collating this feedback as well as reaching out to other known WCF customers.  Especially useful are the specific features called out (e.g. queuing, transaction, etc.) because it allows us to prioritize and do targeted investigations.  For the record, the ""missing"" features of the full .NET framework's WCF were not deliberately excluded from .NET Core WCF. Rather, the initial goal was to support all the existing Windows Store WCF API's on NET Core (which are all client-facing) before tackling other mission-critical features of WCF.  It might help to know that much of the work of porting WCF features involves re-implementing OS-level libraries that WCF depends on (e.g. socket layer, cryptography, etc.) to allow it to work cross-platform. So lighting up WCF features usually involves replacing OS-level libraries for each platform first.  It might help to think that the ""W"" in WCF is no longer a given in NET Core.  This is one reason why it is so valuable to hear from you which features matter most, because it lets us investigate more deeply questions like ""Which libraries are required to do feature X on Linux? OS X?, etc."".  Please keep those suggestions and specific scenarios coming!"
technical,"Yes to standardizing the plumbing in Core to do what WCF did for .NET in pursuit of interoperability: where anything can talk to anything.  From inter-process, across machines, IoT devices, to the cloud, async prcessing queues, etc. without having to reinvent the wheel with proprietary tech just to get non-functional requirements like transactions, security, reliability, etc.  Give developers the tools to focus on delivering business value.  Standardizing the communications framework opens possibilities for inter-connecting software systems faster than ever before.  Governance and versioning of APIs will be even more critical for dependencies, perhaps dynamic proxy with roslyn and operation re-mapping.  Do away with static configuration in favor of self-optimizing statistics instead (countless times where timeouts or buffer sizes caused problems during peak load).  DDoS attacks can be addressed more intelligently.  Our use-cases today include native protocol bindings like net.pipe, net.tcp, msmq with transaction support via msdtc on-premise.  As we migrate to Azure, off-line queuing and reliable sessions would be more critical to our system design. Thank you for all your feedback! We have reviewed all the great responses above in regards to WCF Server support in .NET Core.  The WCF feature team is actively working on roadmap plans for WCF functionality in future .NET Core releases.  For next steps, we need your feedback in terms of top scenarios, feature usage and target profiles.  We would appreciate it if you can complete our survey.  ### .NET Core - WCF Support Survey As well, we are looking to engage closely with the WCF Community on specific scenarios, if you are interested in this partnership - please fill out the Contact Information on the final page of the survey."
technical,"If you don't like it, don't use it!!! Why try and stop others from using it? and most importantly why do so with absolutely no information just flaming? sorry but show me, tell me why?  I have been using WCF on some major back end infrastructure since 2013 and have never had a problem.  I am currently refactoring/rewriting a lot of WCF code to use on Linux which will use what I have dubbed UCF (Universal Communications Foundation). So far it is working great and while it is not full WCF it does use the same structuring and tags.  So, if this way of doing things is so terrible tell me why, don't just talk smack. Facts, Proof, Scientific Method... I will even take an article written by some random guy who knows nothing but javascript written on Wikipedia, just show me something.  Many companies, both large and small, use WCF! The ability to run .NET Core on Linux is huge and is being widely adopted. The idea of write once run anywhere that took off with Java is now moving to .NET. This is in part because of the language offerings of .NET (F#, C#, VB, C++...). Another big factor is the way Oracle treats the open source community (very poorly as of late) and the fact that Microsoft is doing the exact opposite by opening code not closing it off. C# is also a huge draw as it is very popular with both novice and expert programmers which allows many companies to hire lower level developers.  I personally and I am sure many others agree, that having higher developers create consumable nearly foolproof services and feeding lower level developers the basics of how it works is both a time and money saver. The only real reason I can see anyone truly hating WCF and want to see it gone is that the either, don't have much experience with it, they are a senior developer that feels like creating positions for lower levels may cause job insecurity.  **I ask kindly post something informative and factual or just stop trolling.** Thanks for the comment and not just because I'm part of the choir.  If they don't move WCF to .NET Core then there are a host of other things that can't be built.  Yet, if they do build it there will be little to no impact on the dissenters. So on one hand, it's a big deal to those of us that want it and can use it and it's of no importance to those that don't so I don't care at all for the negative attitude. Hopefully reason will prevail."
technical,"The source is available (MIT license) Thanks for the link to the reference source repository! It included lots more than ServiceModel (with support for server side WCF).  Though, in the header of the source code for (e.g. UriTemplate.cs in the ServiceModel catalog), we have the header of"
technical,"or simply use my repo thanks! However it doesn't seem like ServiceWire can  serve  SOAP? We get WSDL-files from the customer to implement, for other clients to make calls to us using SOAP. All the best!"
technical,"might I recommend my two Pluralsight courses, WCF End-to-End and WCF Power Topics :) Thanks! I will take a look.  Also, for an interesting exercise, see here :)"
technical,"might I recommend my two Pluralsight courses, WCF End-to-End and WCF Power Topics :) that question seems backwards"
technical,"yes!  And, that has been a continuing issue. Like any other large organization, we can expect that MS will not always have their message completely aligned with reality. Remember way back when WCF came out? MS marketed it as being 'all about web services'. Which, in fact, they had invented something completely different. WCF had almost nothing to do with web services. And yet, that fact remain lost even today. Look how many references to web services and SOAP have been made in this thread alone! WCF is actually an extensibility model, an extensible interception based pipeline. the first component oriented platform.  I have had some interesting conversation with 'thought leaders' that originally said WebAPI was the wave of the future and that WCF / Service Fabric / etc was not needed. Those are also some of the same people that delivered addresses this year at conferences you all know of on those very topics: WCF / service fabric etc. It's not immediately obvious to everyone that not everything can be done over raw http interactions.  So, yes, you have been given recommendations. But, you have to decide for your scenarios what is best. And I do use WebAPI, but, for a very specific purpose. Same can be said for WCF, Service Fabric, asp.net, etc. They're complimentary to each other but are very different things used for very different purposes and not really equatable or 100% comparable to each other in terms of feature sets or usage scenarios. That's a fair point and the topic of renaming it, IIRC, came up in this thread. About the only part of WCF that is really ""windows-specific"" is the fact that it can very seamlessly interact with MSMQ via an OOTB binding - something that could be made an auxiliary package. If you are targeting MSMQ you aren't going to target .NET Core anyway, so that would be just fine to use regular .net anyway. Everything else has a corollary in non-windows, AFAIK.  On the other hand, if I did target .net core but still wanted to host on windows instances, I want the ability to just configure a binding to leverage MSMQ (along with bindings for other queue techs such as Rabbit, Zero, Azure, SQS, etc). Nothing in my actual code needs to care, at all. The beauty of WCF."
technical,"yes!  And, that has been a continuing issue. Like any other large organization, we can expect that MS will not always have their message completely aligned with reality. Remember way back when WCF came out? MS marketed it as being 'all about web services'. Which, in fact, they had invented something completely different. WCF had almost nothing to do with web services. And yet, that fact remain lost even today. Look how many references to web services and SOAP have been made in this thread alone! WCF is actually an extensibility model, an extensible interception based pipeline. the first component oriented platform.  I have had some interesting conversation with 'thought leaders' that originally said WebAPI was the wave of the future and that WCF / Service Fabric / etc was not needed. Those are also some of the same people that delivered addresses this year at conferences you all know of on those very topics: WCF / service fabric etc. It's not immediately obvious to everyone that not everything can be done over raw http interactions.  So, yes, you have been given recommendations. But, you have to decide for your scenarios what is best. And I do use WebAPI, but, for a very specific purpose. Same can be said for WCF, Service Fabric, asp.net, etc. They're complimentary to each other but are very different things used for very different purposes and not really equatable or 100% comparable to each other in terms of feature sets or usage scenarios. That's not a good enough answer to justify an investment to re-write a 10 year old framework that has significant coupling to the Windows platform. Linux alone is not good enough…"
technical,"those sources are for reference only and  a) won't build b) do not produce official output (ie in form of nuget package) c) you have no license and right to extract parts of them and build own solution  I'd like to see it as a Microsoft backed and supported project driven with community support, using github as a collaborative tool, like dotnet core, ef core, aspnet core and others producing nuget packages as an output. That's not accurate. The github sources are a subset of .net that are licensed under MIT. You can do anything you want with them. Before the Xamarin acquisition, this enabled Mono to copy some of the source to improve their implementation.  I'd like to see server support too, but that is not the current focus of this repo. I'm sure it's a mountain of work, and priorities."
technical,"I think the better question is, what can you do with WCF that you can't do with ASP.NET Core (and again, why are we in ASP-land? We're not trying to serve client apps..). Dotnetchris mentions it here. That's not really answering Forki 's question. From what I know of NodeJs, everything you can do in node without a third party lib is exactly the same as what you can do in asp.net core without a lib."
technical,"This is about cross-platform WCF, as it is still available on the full fx, correct?  It does not read as an indictment of WCF, its utility, programming model, or popularity.  There is a valid question of whether the effort to de-Windows WCF is a sound investment. Given that it has the **W** right in the name, my instinct says it's not. The business needs have been expressed and the alternatives + market direction have been presented.  I have lots of skin in the .net game. I strongly feel that WindowsCF has no place in .net core for it to be competitive in the cloud landscape and for the type of community it need to (re-)attract.  Its available and supported (but not actively developed) on .net desktop. Continue with that.  Final thing I can say is - look where the puck is going."
technical,"Ah my mistake, thanks for that. The compat pack contains only existing WCF client packages."
technical,"Tbf it's not the .NET Community that moved on. It's the rest of the industry. And it's hard to justify SOA things if that means .NET to .NET communication only. But of course you want to maintain legacy stuff - and you can. Full framework won't go away. We just got a new version. The cost to rewrite those apps (*systems)* would be huge - not only in actual manpower lost, but in opportunity loss while doing an entire rewrite.  Hate to be that guy, but how exactly is your business supposed to respond to evolving market conditions if this is the case? Is your database access code coupled to WCF in some way?"
technical,"It might already be in the repo, will check when I have a free minute The first step to decoupling WCF from Windows would be to drop the W. Could call it CF Core ,)"
technical,"If by transports you mean WCF handles location transparency well then yes that is correct. But, there is no uber try. WCF is an interception based execution pipeline. There's no try, only do in that aspect. you say it did/does/will not? Please say more so we may understand.  Legacy scenarios?! Other stuff? You've moved on to better? More obvious and does what WCF does?! Like what? I ask, because I know of one and only one thing that meets that description, and I would like it confirmed please.  And, what does ""2005-era WS-* web services"" have to do with WCF? The irony here is that WCF exposes services/contracts using what was supposed to be a platform/technology independent approach.  Therefore anybody was supposed to be able to come along and write compatible services in any tech they wished that implemented those SOAP/WSDL/WS-* contracts.  If the SOA ecosystem promised by WCF (and similar tech from other vendors) had been fulfilled then there would be no problem switching out the WCF stack for some other compliant stack.  The reality is that implementation details leaked out everywhere and the interoperability was often limited to services running the same tech stack behind the scenes.  This is why the vast majority of services being built now choose plain old HTTP.  Because real interoperability is possible.  It's true that HTTP doesn't provide distributed transactions, and good security is taking a long time to come to fruition, and HTTP is mostly a request/response architecture where full duplex can only be simulated.  It's also true that some folks are building distributed systems in closed/controlled ecosystems where this kind of technology coupling is manageable, and they get to take advantage of some out of the box infrastructure.  But eventually there is a price to pay.  In this case the price is due to the creation of a new platform that doesn't support that stack.  It's true there is no uber SOA stack in .net core, but I suspect that is largely because we have learned the lesson that one size fits all tech stacks tend to get massively over complicated.  There are many ""best of breed"" solutions out there that address many of the features that WCF supported.  HTTP provides a substrate that allows many of these solutions to interact.  Yes, there are gaps that still need to be filled.  But in the future we should be able to mix and match the tools that fit the scenarios that are needed, so that never again are we faced with a giant tech stack that is an anchor for our business.  WCF is a technology that will continue to be supported for many years to come and I'm confident many companies will continue to get value from their WCF based systems, but I do believe it is time for companies to move on with new work and adopt technology that can truly achieve the interoperability goals originally promised."
technical,"Yes, because WCF is and has always been all about web services!  RPC?! There's no RPC in WCF. It is stackless, which is a key aspect that makes it superior in every respect to anything else available that you know about.  TCP?! Um, so, it seems you do not understand that WCF features location transparency. I do not have to write pipe code, or tcp code, or msmq code, or amqp code, or service bus code. I write my code. Then, I can change the binding to use whatever I want, without having to change my code. You presume to know of some other technology that can do that? Please share.  Of the legitimate needs listed here, reliability, transactions, throttling, bindings, etc, etc, etc you will not find any comparable replacement, none, because it does not exist. So, for those of us that actually need those features we have not moved on. Some of us are able to move on to a true service oriented platform (Service Fabric) but for infrastructure, we still very much need WCF.  It is clear that many that have commented here have absolutely no idea what WCF is or what it is for or how many of us use it. I'm not sure it's fair for those voices to remain in the conversation. If you think that WCF is all about web services and there's better stuff out there then you really do not have any idea what some of us are talking about and your time might be better spent in another conversation. I know my time here would be better spent having an actual conversation about the subject with actual practitioners who actually have experience with these systems. The last two comments here hit the nail on the head and I couldn't agree more. It is called ""Communication Foundation"" for a reason. ""Moved on""? Putting that out there is essentially telling every customer currently using WCF (in whatever capacity) that they have always been doing it wrong and they should get with the program - very unfair thing to say. It seems to me that a common pattern in the certain parts of the Microsoft community is that the minute there is another way of doing something, the previous way is now labeled wrong or idiotic and therefore dead. if it ain't what the cool kids are using then you must be a moron. Call me old-school, but for my customers it's about solving a problem and providing value. WCF did and does that very well. I agree with Will. It's way more than doing web services. But the fact that it can do them too is still great. Why go through a list of ten products that can each offer an alternative to a single feature of WCF when I  know I can do it using WCF? I use WCF in so many other capacities and in conjunction with Web API and ASP Core. I've set up solutions using WCF's discovery and content-based routing, and that shit rocks. There isn't anything else out there that can do it as well or as easily."
technical,"thanks! However it doesn't seem like ServiceWire can  serve  SOAP? We get WSDL-files from the customer to implement, for other clients to make calls to us using SOAP. All the best! The main usage scenario for our company would be using ChannelFactory<T to generate clients that use a SHARED(server/client) contract For communication between a web-layer (client facing) WebApi and a Backend service layer WebApi (exposed on the internal network only).  There is no easy way to make a client that uses the same contracts on the server and client without a lot of boilerplate.  We have tried using swagger clients, and hand coding http proxies to hit a WebApi, but there is way to much boilerplate.  If you could make ChannelFactory simply work with an existing WebApi (properly using get/put/post etc) and share the contracts, I would be happy.  If anyone has any other ways to do this with minimal boilerplate, I would be happy to hear how you are doing it."
technical,"And I've seen WCF source code, it is quite modular, you do not have to *rewrite* anything from scratch (as someone here wrongly stated).  Or publish sources and allow community to do its work!  Now you have caged many solutions in a limbo. The source is available (MIT license)"
technical,"And I've seen WCF source code, it is quite modular, you do not have to *rewrite* anything from scratch (as someone here wrongly stated).  Or publish sources and allow community to do its work!  Now you have caged many solutions in a limbo. The title of this thread is server side WCF.  On the server WCF supports transactions that flow through and down through the interaction.  So if your server side component wants / needs to begin a transaction and enlist other WCF services it all gets maintained and handled.  To use transactions you have to have transactional resources.  Unless your company/team has done enterprise level infrastructure it's probably unlikely you have transactional resources, but none the less it is possible.  Now on the client side, you may be right.  The typical http client is the browser (obviously that's a gross statement) and it's unlikely you'll start a transaction there.  But if you have a .net client, even over http, you can use transactions."
technical,"It not dead, it's  undead .  This issue is troll bait in fairness.  WCF is not something that should be implemented in .NET Core. Then, I can change the binding to use whatever I want, without having to change my code. You presume to know of some other technology that can do that? Please share.  The fact that this doesn't exist elsewhere renders the argument about WCF being outdated moot IMHO. To say nothing of all the other benefits of WCF..."
technical,"I don't want to get into a whole back and forth thing here but it's good enough for .net core.  Remember, adding linux support is adding support for dozens of platforms from dozens of vendors.  I would think that after 10 years of being trapped on Windows that old framework would find it quite liberating. There are distinct features of .NET that are tightly coupled to Windows: System.Web and WCF as well as WPF and WinForms are four of those frameworks that are not ported to .NET Core for this reason. There are other frameworks and tools in the .NET Ecosystem that can be used for those purposes.  Until .NET Core, there was never a question of being able to run WCF on Linux, and we do not have a compelling reason to enable that.  You can run your services in containers, and we are working to further enable that capability with additional features on Windows with .NET Framework 4.7.1 and later."
technical,"I'm finding WCF less and less important in a world of Microservices and service bus technology. We've been on Windows Service Bus for a while, hit too many problems, and are migrating to RabbitMQ. Our sysops guys always get a look of surprise when we tell them we don't need IIS installed. There are lots of useful features about the WCF and also would be nice, Built-in **Dependency Injection** (as .NET Core MVC 6) Interceptors support Hosting anywhere (Windows or Linux) Working with AspNet Core Module (Without additionals configurations - httpcfg set urlacl)"
technical,"Check out this project. it is not WCF, but might be useful to achieve some of backend abilities. There is another start at a WCF-like ServiceCore for .net core:"
technical,"Check out this project. it is not WCF, but might be useful to achieve some of backend abilities. There very much is a WCF team, and we are working on features to ensure compatibility with Windows 10, Windows Server, and Windows Containers.  We had a very successful presentation at Build where we showed the first of our Windows Containers.  Work is continuing in that space, and we plan to continue in these investments.  WCF for .NET Core is very much on our radar, and we are working with the ASP.NET Core to prioritize this work.  We will share more details when they are available to broadcast publicly."
technical,"WCF has been abandoned in exactly the same way that the socket library has been abandoned... It has been so thoroughly tested and tuned that further *development* is not needed. What's there just works. So, no, they don't have a WCF team anymore. But it is because it isn't needed, not because WCF is being abandoned. Thinking about a service like LogMeIn: I need to maintain a persistent connection from clients to servers so I know they are online and can initiate a remote access from server instantly if they are online, even behind firewalls. And what about big file transfers (big, like, what FASP from Aspera can do)?"
technical,Another vote for WC on .net core. I have an entire infrastructure that is being converted to netstandard and core however we are currently deliberating what to do about not having WCF available. WCF for us means that developers can easily consume a complex api with almost no knowledge of it. Without WCF we will either have to abandon a lot of the plans or suffer a decrease in productivity.  :-( This couldn't be any better of a reason to NOT support WCF.
technical,"I imagine Phillip says ""sad times"" because he would like WCF to be already migrated to .NET Core. This view is shared by many development companies, but I must admit that the message is hopeful. :-) This insecurity we feel in our continued investment in developing with WCF is compounded by which does not even mention WCF in a design document for networking in dotnet.  I know that server side WCF on core won't be here for a while due to resource allocations.  Sorry, this sounded a bit too negative.  I am looking forward to your communications about the future of WCF and dotnet's networking stack."
technical,"since you have no skin in the game and are being the most divisive here, I'd suggest banning you before locking those of us with business needs for WCF out of the WCF thread... This is about cross-platform WCF, as it is still available on the full fx, correct?  It does not read as an indictment of WCF, its utility, programming model, or popularity.  There is a valid question of whether the effort to de-Windows WCF is a sound investment. Given that it has the **W** right in the name, my instinct says it's not."
technical,"I'm in the same boat, I had two big blockers before I could make my app .NET Core compatible, MEF and WCF. As of December MEF is supported, but I'm sorry to see WCF is still not currently planned.  What is the recommended replacement for a web service via System.ServiceModel that has both WCF and REST endpoints? For reference, my service methods have the following attributes to enable both WCF and REST on each method: Along with the end points, I'm using System.IdentityModel to validate user credentials on inbound requests.  Thanks!! This might be the wrong place to ask, but would it be possible to create a class library in an ASP.NET Core application solution that uses the full .NET Framework as long as none of the ASP.NET Core code actually references it? I mean, could I create a library that supports server-side WCF that's meant to be packaged with a set of libraries that otherwise uses .NET Core only with the knowledge that the WCF Server class library only be used by an application that runs only on Windows?"
technical,"full-featured, singular communication stack into .NET Core so we can host on cheaper metal and smaller devices,   and I will add to that I would like greater reach! I was very disappointed when I sat down years ago to write my first store app, and found out it could not work with my infrastructure, no WCF in it. Same goes for core, UWP, all of it. I can't use any of that. The whole point of those technologies being in service would be to give me the ability to extend my infrastructure across platforms, have other types of clients orchestrated by the infrastructure. This would be huge! This says much more about you than it does anything about any technology. We are not here to educate those who insist upon remaining ignorant of good tools. You can continue to drive screws with your hammer if you choose. Move along."
technical,"Don't get too hung up on the WCF that was designed for Windows n number of years ago.  How would you redesign/update ""XCF"" (X comes after W) to work tomorrow in .NET Core?  I think what the community wants is a mature, out-of-the-box, enterprise-grade, service-oriented programming model for inter-process communications regardless of what platform is hosting the process, whether the processes are local or distributed, whether the process is on a server or mobile or IoT device.  Simple enough, right?  It's a tall order for sure, but the WCF team was able to do it in the era of Windows and .NET (so long as the device was running Windows).  Rather than unify disparate technologies that existed previously on the Windows platform, the new challenge is to enable communication across any platform with any device anywhere.  Changes and different approaches will undoubtedly be necessary and perhaps even an improvement.  As developers, why should we concern ourselves with configuration of endpoints or maintaining static proxy code?  Why can't we just connect the dots and let the framework negotiate the best way to communicate?  As long as any POCO class can be converted to a service with interfaces as service contracts, serializable DTOs as data/message contracts, and behavior modification applied AOP-style via attributes or PnP configuration we should be able to cope with whatever the future brings.  This communications channel just needs to work reliably, securely, under every scenario imaginable to be considered enterprise-grade.  The justification for such an endeavor is the same reasons why Microsoft is investing in Azure, to re-position itself for the future.  Streaming data to/from mobile devices on the hyper-loop transit or peer-to-peer autonomous-vehicle networks is not out of the realm of possibilities where .NET Core code might find itself someday.  The question is whether Microsoft will be ready with a solution for those scenarios or scramble to play catch-up and allowing others to fill the void. This thread was opened 23rd of May, 2016, about 1.5 years ago, with the subject line ""Server side WCF"". The amazing responses, from majority of writers, have been of high quality. The participants in the thread have replied several times about why and what, and also with several business cases. There was a survey (with a link) given, bundled with the message (16th of July, 2016):  ""The WCF feature team is actively working on roadmap plans for WCF functionality in future .NET Core releases. For next steps, we need your feedback in terms of top scenarios, feature usage and target profiles.""  True, the message did not specify if the roadmap was for the client side .NET Core, or for a future server side. I've never seen any result of the survey.  The recent posts suggest that there are no plans for ""Server side WCF"", and that it will never happen, because of this or that.  Is this the official message from Microsoft? (it's good to have a clear and official message)  To refer to a ""read-only"" repository is, in my opinion, rather low point, especially to all these fantastic contributors of this thread, many from what I understand have many years of experience using and developing for WCF on .NET Framework.  The .NET Standard and it's implementation in .NET Core is truly amazing! The giant leap from version 1.x to 2.0 was incredible. The community has chipped in with high quality code, increased performance on all levels, even with basic primitives (sorry I can't give you a link, but I've read on several sites about the performance increase and that some was going to be merged back to .NET Framework from all this work). The suggestion of ""releasing"" (the full) WCF to the community is maybe not such a bad idea..."
technical,"ccicchitelli Depends on what exact kind of architecture you want to take. Personally my preferred microservices architecture for .NET core, is self contained console apps (or windows services if you want to take a dependency on windows) that communicate via an Enterprise Service Bus. Seems the most loosely coupled option.  But, you don't have to do that via an enterprise service bus, you could equally do it via RPC, or even just make each microservice a REST endpoint. Admittedly, making them REST endpoints I wouldn't advise, as your applications become a bit more coupled to your microservices at that point, but it is still an option.  There's also a wealth of options if you're willing to use Azure, such as Service Fabric, which is clearly really powerful, but obviously not great that you're forced to use Azure. There's also Orleans and Akka.Net  Obviously, most of these options do involve taking a third party dependency, but typically not many, just a couple. Certainly it doesn't require a lot of custom code though. Those options you list aren't in the .NET Framework..."
technical,"Thanks for the link to the reference source repository! It included lots more than ServiceModel (with support for server side WCF).  Though, in the header of the source code for (e.g. UriTemplate.cs in the ServiceModel catalog), we have the header of those sources are for reference only and  a) won't build b) do not produce official output (ie in form of nuget package) c) you have no license and right to extract parts of them and build own solution  I'd like to see it as a Microsoft backed and supported project driven with community support, using github as a collaborative tool, like dotnet core, ef core, aspnet core and others producing nuget packages as an output."
technical,"Absolutely support the idea of having server-side ""WCF"" in .NET Core. We just finished a[nother] fairly large almost entirely server-side processing system (""the user experience is that there isn't any"") Initially we went through a lot of pressure not to use Microsoft/ .NET mostly due to the relative advantages of other (open source) stacks when doing ""microservices-based"" solutions just as traditional web services. But the benefits of WCF such as the enforced contract-based programming model, the availability specifically of Named Pipes binding, flexibility of end points and bindings (yes, declarative approach/configurability can be an advantage), security, extensibility for utilities such as logging, were really key when the system grew and required scale and performance as well as maintainability and having really very little plumbing code. The obvious next step is proper containerization (we have been explicitly waiting for Nano Server) and in general being able to port the system to the next generation of runtime platform(s) without loosing any of the current qualities. Throwing in my 2 cents here: I work in a heavily Microsoft-oriented shop, and having WCF baked into .NET Core would open up a lot of new operation for our department and our company. I and the other developers I work with would love to see this."
technical,"I personally can't believe the CORE team was allowed to increment major version numbers WITHOUT WCF server-side support. It's probably the single largest bottleneck preventing companies from migrating. Hurry up! To me I see .NET Core as a huge effort to cut away from System.Web and to update immediately relevant technologies such as Entity Framework, etc.  Having said that, it would still be nice to know if and what role WCF will play in future development.  I was overridden on a choice to use WCF because the Manager wanted to use RESTful services and JSON.  I thought at first ""You are joking."" I couldn't see hosting the entire application services in IIS.  How would I maintain A.C.I.D. rules of data or what is to prevent IIS from recycling when I am in the middle of what would or should be a transaction.  I see both sides of the argument for turning to Web Api 2 and staying with the feature rich and mature SOAP based WCF services.  These services I am referring to exist behind a firewall and are separate from any client-side frameworks, such as ASP.NET Core/MVC 5, etc.  In my case, having to develop new services I did not have to concern myself with converting from WCF to Web Api 2.  I was able to turn to HangFire, which was very helpful in using the best aspects of REST based services and being able to persist data in a stateless http environment.  This will not help those who are looking to upgrade from WCF to say, WCF Core, for example.  There are several use cases which make using Web Api 2 sufficient, which makes using Web Api 2 not possible or not even relevant and cases in which both WCF and Web Api 2 complement each other.  From what I am reading in these posts, it may not be sufficient to simply use Web Api 2 in lieu of WCF and mentioning the ease of use, etc. are not relevant in a true SOA environment.  So prior to saying there is no need for WCF with Web Api 2, that may be true for some, but for most of us it is not.  For those of us who do not have the luxury of arguing to the point to our peers or Management, see if Hangfire will help you resolve some potential issues you are anticipating when moving away from WCF to Web Api.  Good luck!"
technical,"Unfortunately I don't think it's that simple. Despite all the warts, net standard 1.x basically worked - all the apis it said it implemented were implemented.  WCF is something else entirely. While some of the bindings are probably easy to implement - HTTP(s)/SOAP - some are going to be impossible - MSMQ.  What is everyone's expectation here? Putting  on my non technical hat here, if I heard that WCF was getting ported to dotnet core then I would expect *all* of it to work. Which isn't happening. Unless they decide to support a different set of bindings on linux. That might have merit. They'd have to call it something else though...  At the end of the day, these communication frameworks will come and go. 20 years ago we had DCOM and CORBA, last decade it was WCF, today's new hotness is gRPC, tomorrow ??? BTW does anyone remember what happened to .net remoting? To take my non technical hat on, in response to ""thefringeninja"" :-) ...  There is already an answer to ""not supported"" on .NET Core today, the exception ""Not supported platform"", the same can go with anything that might be ported to .NET Core.  That said, the thread already addressed that several technical aspects may not be possible, and that's fine, because WCF is more than a technology set, WCF supports a (service oriented) programming model. Maybe not everything can be translated to .NET Core, but I'm sure a lot of things can be.  I don't think this is so much of a technical issue, it's a business decision. If Microsoft do not prioritize WCF on .NET Core, it's their decision. But maybe the community can chip in, if Microsoft allows (licensing).  As Scott says, there are many alternatives and viable options. However, if you have invested years of knowledge into .NET ecosystem, then I guess you'd be happy to apply your skills on the same ecosystem."
technical,"I would like to reference an issue I just opened before noticing this thread: #2378  This isn't .Net Standard. It requires the full framework. But it can fit into the ASP.Net pipeline. to the rescue, at least on windows?"
technical,"The first step to decoupling WCF from Windows would be to drop the W. Could call it CF Core ,) UCF Universal Communications Foundation"
technical,"The first step to decoupling WCF from Windows would be to drop the W. Could call it CF Core ,) Unfortunately I don't think it's that simple. Despite all the warts, net standard 1.x basically worked - all the apis it said it implemented were implemented.  WCF is something else entirely. While some of the bindings are probably easy to implement - HTTP(s)/SOAP - some are going to be impossible - MSMQ.  What is everyone's expectation here? Putting  on my non technical hat here, if I heard that WCF was getting ported to dotnet core then I would expect *all* of it to work. Which isn't happening. Unless they decide to support a different set of bindings on linux. That might have merit. They'd have to call it something else though...  At the end of the day, these communication frameworks will come and go. 20 years ago we had DCOM and CORBA, last decade it was WCF, today's new hotness is gRPC, tomorrow ??? BTW does anyone remember what happened to .net remoting?"
technical,"to the rescue, at least on windows? Unless I missed something, WCF isn't in there."
technical,"to the rescue, at least on windows? WCF and soap are entirely orthogonal. thefringeninja for you to post that comment shows you have no idea what you're talking about and you're just trolling this serious thread."
technical,"Helllo,  We also use WCF extensively in our applications. To add to the lists above we also use: - queued services WCF applications on Core would be a huge boon.  The only thing I would explicitly like to add to the OP's features are all of the behavior types (not just the ones available in Service Fabric) - Operation Behaviors - Service Behaviors - Contract Behaviors - Endpoint Behaviors  I distributed transactions are a prohibitive factor, they can be thrown out the window though."
technical,"johnvndnbrk WCF is solid rock and mature, you won't be disappointed. However this good old Microsoft, create something and abandon. WCF has been abandoned in exactly the same way that the socket library has been abandoned... It has been so thoroughly tested and tuned that further *development* is not needed. What's there just works. So, no, they don't have a WCF team anymore. But it is because it isn't needed, not because WCF is being abandoned."
technical,"evelix  at making    CF have a simple convention over configuration baseline. WCF has been instrumental in delivering quality services that scale and deliver reliability by leveraging transactions across the wire (System.Transactions).  With out support of WCF on .NET Core we would lose many of the ""Free"" benefits we get thru the extensive WCF interceptor chain, including logging, behaviors, and context flow."
technical,"Communication Foundation* WCF is not a trivial framework, and is a Windows component... there are plenty of ways to continue running WCF services with Windows and we continue to invest in the .NET framework.  Other products have other business reasons to enhance their platforms. We view WCF as extremely stable, capable, and coupled to Windows.  We have always made source code available"
technical,"Messaging between services is important and when doing enterprise backends, REST is not going to work. It lacks too many things like others have mentioned. So certainly Transactions, Queues Messaging, Named Pipes, Extensibility should be supported by .Net Core  So .NET Core has to provide those things in one way or another. If it is called WCF I don't care. Maybe it would be the opportunity to fix some of the weaknesses of WCF like the overy complicated configuration story and replace it with a convention based approach.  Also you should/must support other Messaging frameworks beside MSMQ or Service Bus. In general support of AMQP would be nice, including the various messaging patterns. WCF is still one of the best parts of the .NET Framework and having become part of .NET Core is essential.  Some particular aspects are: 1.      Named pipes 2.      System.Transactions 3.      A transactional queuing programming model 4.      Durable services 5.       Extensibility model 6.       MEX endpoint and MEX framework"
technical,My .NET Solutions rely on configuration service behavior's for certificate authentication. I need MEX support to generate the W.S.D.L. for smart contract binding at run-time. We also generate service proxies at run-time from W.S.D.L. and then use reflection to load and invoke. Also our logging via W.C.F. doesn't perform well enough unless its running using T.C.P. binding. WCF on .NET core will be critical for out migration and adoption of .NET core.  We're leveraging many of the capabilities of WCF and without a clear migration path core won't be as compelling. These are requirements.  1. Transactions 2. A transactional queuing programming model 3. Extensibility model
technical,"This insecurity we feel in our continued investment in developing with WCF is compounded by which does not even mention WCF in a design document for networking in dotnet.  I know that server side WCF on core won't be here for a while due to resource allocations.  Sorry, this sounded a bit too negative.  I am looking forward to your communications about the future of WCF and dotnet's networking stack. WCF Server is pretty much legacy at this point. I don't really see why new services would be built on top of it. However having a fairly full fidelity client is important. There are lots of existing services (I'm looking at you, Aussie Government) which are built on WCF and will not be changing any time soon. We can't build systems interfacing with these currently running on .NET Core."
technical,"This insecurity we feel in our continued investment in developing with WCF is compounded by which does not even mention WCF in a design document for networking in dotnet.  I know that server side WCF on core won't be here for a while due to resource allocations.  Sorry, this sounded a bit too negative.  I am looking forward to your communications about the future of WCF and dotnet's networking stack. WCF should have been what gRPC is now. A framework for remote procedure calls, that is: session and contract based, extremely fast, and easy to use, with support for duplex and streaming. The exact opposite and complement of HTTP/Rest. If WCF is going to be another way to create/consume HTTP/Rest like services, then it is a waste."
technical,"This insecurity we feel in our continued investment in developing with WCF is compounded by which does not even mention WCF in a design document for networking in dotnet.  I know that server side WCF on core won't be here for a while due to resource allocations.  Sorry, this sounded a bit too negative.  I am looking forward to your communications about the future of WCF and dotnet's networking stack. We have a customer that require a portable backend/service-solution (Linux-host) based on open source software, serving SOAP. As a .NET-shop, we see a future .NET Core WCF-service as an alternative, as we need to consider Java at this point."
technical,"While having to stacks is likely going to continue the confusion about which is recommended, I think the feedback I get from the course/talks I do is almost always about TCP connectivity. The rest of the stack is interesting, but it feels like WCF as network service versus web service to me, but I've been wrong before. We have a large SOA based healthcare application. We use WCF is two ways, internally (net.tcp and binary serialization) and externally (http and WS-*).  There are a number of RPC frameworks that could replace our use of WCF for internal service communication, but we really need WSHttpBinding support for standards based interoperability scenarios.  We exclusively use ServiceHost and configuration by code, so no loss there."
technical,"Another use case could be, using WCF on a gateway like device for example in home automation using linux based controllers with an ARM CPU and 512MB memory is not that uncommon. Being able to use .NET core on those types of devices and using WCF to allow creating a SOA like programming model, making use of named pipes and allowing to move around context and create reliable communication could create a whole other way of working than the current C daemons, dbus communicating way of doing things.  Using WCF could also than be used to for example let your app communicate with your WCF service in your house, for offline controlling. It would provide better integration with service bus allowing efficient communication to and from cloud based services. We have quants writing server side code in Mono (they need cross platform) who were almost jumping up and down in glee with the introduction of .Net Core ... until I told them that it wouldn't support AppDomains, which they currently rely on heavily.  The look on their faces was one of despair. I have showed them a viable alternative using WCF and named pipes (to give them the isolation they desire without compromising performance). They were even more interested when I showed them that they could then scale these services across machines and across platform (if WCF services were supported on Linux)  So, this is blocking adoption of .NET Core in this scenario and we would be looking to port existing processes/services to .NET Core as well as developing new services using .NET Core."
technical,"I remember reading some 2-3 years ago some MS document where the **recommended** framework to use for **new web projects** was WebAPi or Asp.Net MVC.  WCF was mentioned in some legacy context. Personally, I think WCF does too much for its own good. Let me choose what I need. Allow me to switch easily some component for another in my app. Web API is for http services. This is not comparable in any meaningful way to the full WCF."
technical,"Hi, not really a fan of the argumentative tone lots of commenters have taken here, but I would like to give my 2 cents. As I understand it, one of the main scenarios .Net Core is supposed to support are SOA and microservices. Not having WCF server side seems to run counter-intuitive to that. I take your point csharpfritz that WCF is completely coupled to Windows, fair enough. But it does feel like .Net Core really needs a better story for SOA than WebAPI, if you're serious about SOA as a first class supported scenario. Well said! I took csharpfritz's comments re WCF, WPF as statements of fact.  In the original post by jan-johansson-mr he has list of features he likes in WCF **""Here is a list of some of the WCF features (that comes to my mind):** - **Throttling** **- Reliability** **- Ordered Messages** **- Bindings** **- Instance Management** **- Behaviors** **- Transactions** **- Security** **- Discovery** **- Metadata Exchange** **- Extensibility**""  Others contributed as well, but among these features, support for Transactions ranks high for me.  I found ""HangFire"" and this has been a great option in this area and has other features you might find in traditional Windows Services.  This is one example of ingenuity that has filled the gap left behind in .NET Core.  The purpose of this thread seems to be a discussion of what others may also see as missing features in .NET Core when leaving WCF behind.  Using a stateless protocol for SOA may not be practicable.  De-coupling service applications from the underlying OS so they can run on Linux, Unix, iOS, etc. may be the only acceptable way to go if .NET Core is to be positioned as supporting multiple platforms.  Does it need to be stateless, though?  If not, would this make a difference in bringing forward features WCF has - whether 2, 5 , 10 years old (which is not the issue)?  I am excited to move to .NET Core and have been doing so for client-side applications.  REST-based services are awesome, but currently they only compliment WCF.  They do not,  and cannot, replace them, however."
technical,"Hi, not really a fan of the argumentative tone lots of commenters have taken here, but I would like to give my 2 cents. As I understand it, one of the main scenarios .Net Core is supposed to support are SOA and microservices. Not having WCF server side seems to run counter-intuitive to that. I take your point csharpfritz that WCF is completely coupled to Windows, fair enough. But it does feel like .Net Core really needs a better story for SOA than WebAPI, if you're serious about SOA as a first class supported scenario. Well, I've bitten my tongue long enough here guys. Scott, I'm totally with you. I've been doing WCF for years and it is still the superior framework for enterprise level service development. The fact that some are calling it legacy is just plain wrong. Yeah, many will argue that Web API is just easier to use. Is it really? Compare apples to apples, where the client is NOT javascript (in such a case, yes to Web API) and you'll find that Web API has ALL of the same components are WCF. Physically, the only one that is missing is the service contract and that's there conceptually in the form of the resource definition. In fact, a .NET client in WCF is simpler to write than one in Web API. OK, so you need configuration. Big deal. I can't tell you how many times having that 30K foot few of the service scenario has helped me. Am I bias? Perhaps, but I also believe in #1, using the right tool for the right job, and #2, that there is no one-size-fits-all. Jeff Fritz already mentioned that it is in the plans to have it in .NET Core. I'm with most of you as I'd like to see this happen sooner rather than later, but since none of my clients are going to rush to jump to .NET Core any time soon, I'll have patience. There is a team at MS and they actively address WCF so anyone that says it's a dead product is just hung up on the shiny new coin and not on providing maximum customer value. Because the fact is that WCF is FAR more powerful than Web API. Many of you have noticed the momentum that Micro Service architecture has had. WCF can provide solutions for many of the requirements a true Micro Service implementation will have, and it can do it without third party products. Solutions for callbacks, discovery, routing, detailed fault handling, and even monitoring can be built using WCF with greater ease and power since the infrastructure for them is already built into the product. Furthermore on the topic of architecture, it pains me to see that many who look to Web API for ""simplicity"" do so at the sacrifice of certain architecture principles like layering, or concern separation, etc. But that is a separate argument. I have my own views on that and my own techniques and styles for achieving maximum scalability and reusability for my customer. So I know this thread started as a ""server WCF on .NET core"" thread, but it seemed to slightly pivot to where I thought WCF needed a little advocacy. For a while there seemed to be a rumor going around that WPF is dead and now we all see it's just been kinda rebranded since XAML is alive and well. Many seem to confuse Microsoft's lack of ""pushing"" or ""talking"" about a technology as the death of that technology. Such is not the case. So endeth the sermon."
technical,"Hi, not really a fan of the argumentative tone lots of commenters have taken here, but I would like to give my 2 cents. As I understand it, one of the main scenarios .Net Core is supposed to support are SOA and microservices. Not having WCF server side seems to run counter-intuitive to that. I take your point csharpfritz that WCF is completely coupled to Windows, fair enough. But it does feel like .Net Core really needs a better story for SOA than WebAPI, if you're serious about SOA as a first class supported scenario. What claim? How many WCF implementations outside .NET do you know?"
technical,Sad times. What does working with ASP.NET Core mean? Working on at least a subset of WCF server-side features that runs on .NET Core?
technical,"Yes, and how's that working out for them. I agree about EF, but what are they replacing WCF with?  You have to understand the power in almost no lines of code.  There is just nothing else that can do that.  Here is a complete service *and* hosting in 4 lines of code.  It's not just that the above is possible, it's that the above is ENTERPRISE quality.  You can just change a config file or attributes to change the authentication or authorization.  You can add transactions.  You can modify and tweak the failure modes.  I've shared your experience with going into places and having them desperate to ditch WCF.  I concur with you that this is a common story.  Where you and I probably differ is that I don't see them changing to something better.  I see them having dramatically over complicated something that can be very very simple.  The complexity I've seen is enough to bring NASA to it's knees.  Worse, these companies don't even know they're doing it.  When I show them the lines above (particularly the InProcFactory and other things from iDesign) they literally don't believe it.  It can't be true because they've struggled with it for so long.  I'll admit that this is a hard problem.  What we're talking about is software engineering and most people can barely program.  As someone said, ""Programming is easy, software development is hard.""  COM and COM+ used to be difficult.  Now every single class in .NET is a COM object.  Most developers aren't even aware of this fact.  In ServiceFabric every class is a Service.  Do developers (and companies) know what to do with that power?  No.  But that's not reason to throw it out because some of us do use it.  Removing the infrastructure (attributes and classes) from the code above and you have literally 4 lines of code: 1 line in the method, one in the interface definition and one to host the service with InProcFactory, and one to call the service and display the results.  Other frameworks can do this in as few lines of code, but NONE of them bring the power of the entire WCF framework with them at the same time.  This is the genius of WCF.  The understanding of this power is lost on almost everyone.  For those of us that use it, it's heartbreaking. What features do you need in a WCF service that require .NET Core?  Guys, you are not listening at all, is this intentional?  It is not that we desperately need parts of .net core in the 'big' .net framework (4.7.1 and netstandard20 closes that gap).  We need .net core to have WCF. Name it like you want (XCF? ,D), strip it System.Web and Windows OS dependencies, make Windows OS integration optional nuget packages. We do not require it to be 100% compatible with WCF, we can take a cost of migration of whatever we have to that conceptual XCF. But for the God's sake provide service oriented framework in .net core. Because currently there is none and this is pathetic.  We need .net core to make use of docker, server nano, kestrel and all new goodies. To use .net core modularity."
technical,"I understand that WCF provides value to many, many people, but does that mean .NET Core should support it? WCF seems to be at least partially tightly-coupled to Windows--the antithesis of .NET Core. How many shops relying on WCF actually  need  to deploy on Linux? I think that would be a bad architectural decision. If things are running so smoothly now considering the tight-coupling, why change it? Is running on Linux a net positive? Which parts are you talking about?  There are some protocols that WCF supports that are Windows specific but I'm not aware of WCF being windows specific.  And yes, there is a ton of code that it would be advantageous to deploy on Linux for the same reasons that any .NET core code should be run on Linux.  I'm surprised at the anti-WCF bias here.  Where is this coming from?  It seems...in appropriate.  Not all services are best built as web services without a proxy."
technical,"similar with mentioned above, in my case, i'm just refactoring all my app using .net core, and there're many services using wcf which called by other apps that belong to another team. my team is the first one to upgrade tech from .netframework to .netcore in my company, so the more convenient and stronger the .net core is the ealier .netcore can be used in other teams. btw, the load balance solution based on linux is considered by the operation team, after we changing to .netcore, it will be very flexiblelow cost to integrate with the lb solution While having to stacks is likely going to continue the confusion about which is recommended, I think the feedback I get from the course/talks I do is almost always about TCP connectivity. The rest of the stack is interesting, but it feels like WCF as network service versus web service to me, but I've been wrong before."
technical,"Nice try, the source is available but only for reference use, not for modification or extension.  You may view WCF as ""extremely stable"" and capable, but because of the lack of adoption of .NET Core most people should now consider it ""extremely deprecated"" with a one-way ticket to live out the rest of its days with Clippy. Who would be modifying or extending WCF in .NET Framework?  The question was from some interested developers asking if they can write a compatible framework for .NET Core...  Why does the lack of .NET Core support make something deprecated?  .NET Framework 4.7.1 was released this week, along with a preview announcement of .NET Framework 4.7.2.  New Windows servers are planned, and new Windows clients are planned as well.  What was deprecated?  Who is saying this framework is incapable?  These are not messages you are hearing from our teams."
technical,What does working with ASP.NET Core mean? Working on at least a subset of WCF server-side features that runs on .NET Core? Why sad times?
technical,"This is the worst piece of advice anyone can give. Security should never be left to the implementer, it's the source of most atrocious security bugs ever ....  There are really few security experts or developers capable of implementing security details properly, this is a job for less than 1% of the programmers out there. Would love to see WCF play nicely with emerging Web Service projects that are attempting to be  RMM RESTful service in addition to SOAP.  Such as, but not limited to, - APIs  - OPEN API (Swagger), RAML, WADL - HATEOAS - HAL, JSON API  Would love to see MEX style endpoint that can publish these emerging technologies so we will not have to choose or implement both WebAPI and WCF.  Would love to more reliability consume this new RESTful web service that are ate least RMM Level 3 so that our team can spend more time focus on creating new process and solving problems without focusing time on the plumbing.  Looking to bridge interoperability and integration,some systems and services are now moving these emerging technology.  I would love to see a clearer path to the future.  Is it possible to incorporate WebAPI in  WCF in a self-hosting, Azure or docker style deployment, and have the services published as many useful  technologies as an endpoint to be consumed by other services and systems?"
technical,"I'm sure nobody said anything like that when they decided to deploy SQL server to linux, design .NET core to run on linux, decouple other system.web components from IIS, and port 99% of non-UI .NET code to core. As mentioned in dozens of replies in this thread, there is no true replacement for WCF that people can convert their WCF endpoints to in any reasonable amount of time in order to execute in .NET core.  Turns out MVC is able to make the jump, but WCF people are incapable? SQL Server can make the jump to linux, but WCF is too tightly coupled to Windows? Give me a break, you're just making excuses. WPF and Winforms are a give in since they are EXTREMELY Windows specific. WCF while does have a lot of core Windows underpinning is in fact a viable candidate unlike the rest since a lot of those ties can be broken for Linux use."
technical,"This might be the wrong place to ask, but would it be possible to create a class library in an ASP.NET Core application solution that uses the full .NET Framework as long as none of the ASP.NET Core code actually references it? I mean, could I create a library that supports server-side WCF that's meant to be packaged with a set of libraries that otherwise uses .NET Core only with the knowledge that the WCF Server class library only be used by an application that runs only on Windows? Wrong place indeed, but you just need to use the #if preprocessor and create a compile flag. So: Note you'll need not just the method calls wrapped, but  all  of the offending code wrapped."
technical,"This might be the wrong place to ask, but would it be possible to create a class library in an ASP.NET Core application solution that uses the full .NET Framework as long as none of the ASP.NET Core code actually references it? I mean, could I create a library that supports server-side WCF that's meant to be packaged with a set of libraries that otherwise uses .NET Core only with the knowledge that the WCF Server class library only be used by an application that runs only on Windows? Yeah, except that's pretty large corollary to ""use the .NET standards you know and love with the new high performance cross-platform .NET core"": be sure to plan on rewriting all your service architecture using completely different tooling.  Basically, toss the ""Compatible"" subheading, and add the caveat that Microsoft only intends to support their (far less popular) Azure service architectures. If you do go the distance and rewrite your whole system on different service frameworks, be sure to still run a .NET framework component to adapt older clients to the new architecture. Which means you can't actually achieve ""cross platform"" the same time, either. Better throw that heading out too, for anyone who has existing systems they would want to continue to support.  Here's the new tagline for .NET Core: ""You won't even need WCF because this new .NET core is going to be worth re-architecting your entire software stack for"".  Back in 2000 Joel Spolsky wrote something about this, I think"
technical,"Does ServiceWire work on .NET Core?  I am not seeing anything about that on their website, but I'm not sure where to look for it.  In the history it says standard .net library. yes it is under servicewire.core pull the src, update the packages and apply my pull request and you should be up and running"
technical,"Handle transactions, er, um, manually?  Because you want to implement auth manually.  You want to implement your own context flow and header management and interception and pipeline intervention and encryption and compression and on and on...  All decent .NET messaging frameworks like MassTransit or NServiceBus perfectly handle this in much more elegant and efficient way. Yes there are a gazillion frameworks, large and small, more enterprisey or more 1337 h4x0r oriented, with different options and patterns, some rigid and some flexible ... and  ... this is exactly the problem ... there's no base line.  WCF should help to get rid of all these disparate options and provide an unified and abstracted communication framework that is a base line for .Net. development.  You want binary protocolX instead of http, sure, you plug that in.  You want Oauth2 instead of windows auth, sure, configure that one, you want to use NServiceBus plug in an NServiceBus binding. You want to somehow transfer contexts like a Transaction scope over the wire, you can plug that in too. The application programming model stays the same.  RPC in itself isn't a complicated thing from an .Net application point of view, you call a method or receive a call. What makes it complicated is the multitude of ways and frameworks, with security, format, feature and other options that you can implement it in.  This is where WCF should come in, and that's the point of WCF."
technical,"I've given feedback offline on this topic, but just to keep things consolidated, I'll repeat it here :)  I've helped a couple customers adopt .NET Core who have been hung up on missing WCF features. Items which specifically have been missed include: - Service host with Http and nettcp bindings - Transactions - Discovery Yes to standardizing the plumbing in Core to do what WCF did for .NET in pursuit of interoperability: where anything can talk to anything.  From inter-process, across machines, IoT devices, to the cloud, async prcessing queues, etc. without having to reinvent the wheel with proprietary tech just to get non-functional requirements like transactions, security, reliability, etc.  Give developers the tools to focus on delivering business value.  Standardizing the communications framework opens possibilities for inter-connecting software systems faster than ever before.  Governance and versioning of APIs will be even more critical for dependencies, perhaps dynamic proxy with roslyn and operation re-mapping.  Do away with static configuration in favor of self-optimizing statistics instead (countless times where timeouts or buffer sizes caused problems during peak load).  DDoS attacks can be addressed more intelligently.  Our use-cases today include native protocol bindings like net.pipe, net.tcp, msmq with transaction support via msdtc on-premise.  As we migrate to Azure, off-line queuing and reliable sessions would be more critical to our system design."
technical,you can run an express edition on linux. yes you can :-) and props to the team because it is coming along nicely and we will be re-evaluating it in the future as it is not mature enough to even remotely handle our workload but M will continue throwing  at it until it is because they want in on the Linux space. Why people want to fight that on a .net component level is just silly. If we can make it work even with reduced feature set then do it IMHO.
technical,"I'm still yet to work for a company that isn't desperately trying to remove all traces of EF and WCF. So sounds like core is great got me. Yes, and how's that working out for them. I agree about EF, but what are they replacing WCF with?  You have to understand the power in almost no lines of code.  There is just nothing else that can do that.  Here is a complete service *and* hosting in 4 lines of code.  It's not just that the above is possible, it's that the above is ENTERPRISE quality.  You can just change a config file or attributes to change the authentication or authorization.  You can add transactions.  You can modify and tweak the failure modes.  I've shared your experience with going into places and having them desperate to ditch WCF.  I concur with you that this is a common story.  Where you and I probably differ is that I don't see them changing to something better.  I see them having dramatically over complicated something that can be very very simple.  The complexity I've seen is enough to bring NASA to it's knees.  Worse, these companies don't even know they're doing it.  When I show them the lines above (particularly the InProcFactory and other things from iDesign) they literally don't believe it.  It can't be true because they've struggled with it for so long.  I'll admit that this is a hard problem.  What we're talking about is software engineering and most people can barely program.  As someone said, ""Programming is easy, software development is hard.""  COM and COM+ used to be difficult.  Now every single class in .NET is a COM object.  Most developers aren't even aware of this fact.  In ServiceFabric every class is a Service.  Do developers (and companies) know what to do with that power?  No.  But that's not reason to throw it out because some of us do use it.  Removing the infrastructure (attributes and classes) from the code above and you have literally 4 lines of code: 1 line in the method, one in the interface definition and one to host the service with InProcFactory, and one to call the service and display the results.  Other frameworks can do this in as few lines of code, but NONE of them bring the power of the entire WCF framework with them at the same time.  This is the genius of WCF.  The understanding of this power is lost on almost everyone.  For those of us that use it, it's heartbreaking."
technical,"Honestly before this thread I thought folks had generally moved on Not at all - not even a remote possibility in the near (5-10 year) future for many enterprises. In fact, from what I see, adoption of WCF has increased - just not using it for *web services* - so it's not as in-your-face.  Many enterprise-scale apps use WCF behind the scenes and would benefit from hosting on cheaper metal - but they can't. The cost to rewrite those apps (*systems)* would be huge - not only in actual manpower lost, but in opportunity loss while doing an entire rewrite. Also: Security costs - we know how to secure WCF, and using WCF for all services/layers provides the same security story throughout. Switching to a more hybrid approach would mean figuring out how to secure, tune, manage, etc a BUNCH of different tools - when one tool does all we need, and does it *very* well.  The problem is that almost everyone fixates on using WCF for web services which is, in reality, is only a very small percentage of how it is actually used. This is because, if you Google for WCF, that's pretty much all you find in the ""how to"" sections. WCF is a communication FRAMEWORK (useful across the entire applications stack from client, business, resource access, etc). It's NOT just about exposing web services to clients - which is what most people who don't use it (and therefore have ZERO actual skin in the game) are aware of. If that's all it was, it would be called something like MSWSF (Microsoft Web Services Framework). The ""Communication Framework"" was intentional. Yes, because WCF is and has always been all about web services!  RPC?! There's no RPC in WCF. It is stackless, which is a key aspect that makes it superior in every respect to anything else available that you know about.  TCP?! Um, so, it seems you do not understand that WCF features location transparency. I do not have to write pipe code, or tcp code, or msmq code, or amqp code, or service bus code. I write my code. Then, I can change the binding to use whatever I want, without having to change my code. You presume to know of some other technology that can do that? Please share.  Of the legitimate needs listed here, reliability, transactions, throttling, bindings, etc, etc, etc you will not find any comparable replacement, none, because it does not exist. So, for those of us that actually need those features we have not moved on. Some of us are able to move on to a true service oriented platform (Service Fabric) but for infrastructure, we still very much need WCF.  It is clear that many that have commented here have absolutely no idea what WCF is or what it is for or how many of us use it. I'm not sure it's fair for those voices to remain in the conversation. If you think that WCF is all about web services and there's better stuff out there then you really do not have any idea what some of us are talking about and your time might be better spent in another conversation. I know my time here would be better spent having an actual conversation about the subject with actual practitioners who actually have experience with these systems."
technical,"The business needs have been expressed and the alternatives + market direction have been presented.  I have lots of skin in the .net game. I strongly feel that WindowsCF has no place in .net core for it to be competitive in the cloud landscape and for the type of community it need to (re-)attract.  Its available and supported (but not actively developed) on .net desktop. Continue with that.  Final thing I can say is - look where the puck is going. yes!  And, that has been a continuing issue. Like any other large organization, we can expect that MS will not always have their message completely aligned with reality. Remember way back when WCF came out? MS marketed it as being 'all about web services'. Which, in fact, they had invented something completely different. WCF had almost nothing to do with web services. And yet, that fact remain lost even today. Look how many references to web services and SOAP have been made in this thread alone! WCF is actually an extensibility model, an extensible interception based pipeline. the first component oriented platform.  I have had some interesting conversation with 'thought leaders' that originally said WebAPI was the wave of the future and that WCF / Service Fabric / etc was not needed. Those are also some of the same people that delivered addresses this year at conferences you all know of on those very topics: WCF / service fabric etc. It's not immediately obvious to everyone that not everything can be done over raw http interactions.  So, yes, you have been given recommendations. But, you have to decide for your scenarios what is best. And I do use WebAPI, but, for a very specific purpose. Same can be said for WCF, Service Fabric, asp.net, etc. They're complimentary to each other but are very different things used for very different purposes and not really equatable or 100% comparable to each other in terms of feature sets or usage scenarios."
technical,Great point about MS SQL Server but I am sure you will get crap from him because it is a paid product where .net is not you can run an express edition on linux.
technical,"Your time would be better spent working together on an simplified OSS version of WCF.  I could do without SOAP if need be, but I still would like a communication framework with automatic services + automatic proxies suited for RPC, which is configurable and supports a wide array of security and transport options and which comes out of the box in .Net Core. You do know it's the first week of 2018, right?"
technical,"Well said! I took csharpfritz's comments re WCF, WPF as statements of fact.  In the original post by jan-johansson-mr he has list of features he likes in WCF **""Here is a list of some of the WCF features (that comes to my mind):** - **Throttling** **- Reliability** **- Ordered Messages** **- Bindings** **- Instance Management** **- Behaviors** **- Transactions** **- Security** **- Discovery** **- Metadata Exchange** **- Extensibility**""  Others contributed as well, but among these features, support for Transactions ranks high for me.  I found ""HangFire"" and this has been a great option in this area and has other features you might find in traditional Windows Services.  This is one example of ingenuity that has filled the gap left behind in .NET Core.  The purpose of this thread seems to be a discussion of what others may also see as missing features in .NET Core when leaving WCF behind.  Using a stateless protocol for SOA may not be practicable.  De-coupling service applications from the underlying OS so they can run on Linux, Unix, iOS, etc. may be the only acceptable way to go if .NET Core is to be positioned as supporting multiple platforms.  Does it need to be stateless, though?  If not, would this make a difference in bringing forward features WCF has - whether 2, 5 , 10 years old (which is not the issue)?  I am excited to move to .NET Core and have been doing so for client-side applications.  REST-based services are awesome, but currently they only compliment WCF.  They do not,  and cannot, replace them, however. You know, now that I think about it,  it may be a marketing thing.  Reading over the list of features that WCF has and that we all need I believe I stumbled upon a theory.  One of my arguments FOR WCF is that we get all these things in. NET standard. These features are enterprise quality features. One of the difficulties with adopting WCF is developer nativity. Often by the time they know they need it they've passed it by.  So half my theory is that even though most/all SOA apps could benefit from WCF, only a subset of those developers realize it.  The second half of my theory is that while Microsoft is slow/reluctant to build WCF into CORE they have no problem building with it on azure. I'm told that under the hood most of azure is either built on WCF or heavily inspired by it.  So the complete theory is that MS may be reluctant to give us free SO A tech to protect their cloud business. There are a ton of SOA offerings in azure on the cloud.  As teams gain sophistication and outgrow CORE webclient etc, they will have little choice but to buy azure compute time.  I'm not accusing ms of being anything other than a business and to me this makes a hell of a lot more sense than some vague claim of being too closely tied to Windows (while somehow managing just fine with all of CORE).   Others contributed as well, but among these features, support for Transactions ranks high for me. I found ""HangFire"" and this has been a great option in this area and has other features you might find in traditional Windows Services. This is one example of ingenuity that has filled the gap left behind in .NET Core.  The purpose of this thread seems to be a discussion of what others may also see as missing features in .NET Core when leaving WCF behind. Using a stateless protocol for SOA may not be practicable. De-coupling service applications from the underlying OS so they can run on Linux, Unix, iOS, etc. may be the only acceptable way to go if .NET Core is to be positioned as supporting multiple platforms. Does it need to be stateless, though? If not, would this make a difference in bringing forward features WCF has - whether 2, 5 , 10 years old (which is not the issue)?  I am excited to move to .NET Core and have been doing so for client-side applications. REST-based services are awesome, but currently they only compliment WCF. They do not, and cannot, replace them, however."
technical,"Who would be modifying or extending WCF in .NET Framework?  The question was from some interested developers asking if they can write a compatible framework for .NET Core...  Why does the lack of .NET Core support make something deprecated?  .NET Framework 4.7.1 was released this week, along with a preview announcement of .NET Framework 4.7.2.  New Windows servers are planned, and new Windows clients are planned as well.  What was deprecated?  Who is saying this framework is incapable?  These are not messages you are hearing from our teams. You may be mis-characterizing history. I have had many conversations over the years about how SO with WCF is the only all-in-one package for services. I've done work in on other platforms where we have to bring together dozens of vendor products to get what .net offers out of the box.  You may not hear people desire it because, in my experience, people are generally a bit cross camp ignorant. I certainly am because its hard enough to keep up with the details of your own camp.  However, as SO matures it becomes clear that what's needed are the features of WCF (hosting, transactions behaviors, auth, interface based, etc etc). The designers of WCF did a unique and amazing job.  I don't really care how tied it is to Windows. That merely sounds like something that needs to be dealt with under the covers.  The question was asked do we want it in .Net core and why and this list contains some of the answers.  We do."
technical,"I don't think it's a matter of today's hotness and competing standards. WCF offers a collection of things needed to create industrial-strength / enterprise-strength services.  For example, WCF supports transactions and distributed transactions.  This means that architecturally your exposed service can enlist internal services to perform your use case and upon failure the use case service can recover.  When I searched for transactions and gRPC I found this.  It's a working document so things may have changed, but if it's to be believed the notion of a transaction is just to push the failure back to the client. This means that the client needs all of the compensating failure code and POOF, you're right back to the BIG-BALL-OF-MUD design pattern.  Most services tend to be simple (at first) so even pushing things back to the client (caller) works well at first and seems ""easy.""  Time will tell.  In a similar vein WCF's support of authentication and authorization supports configuration via attributes.  It's declarative.  This means that auth is separate from the code (if you want it to be) so you get a natural separation of concerns.  I get it.  Most people don't need this level of nuance and aren't interested in these architectural concerns.  I also know that at enterprise scale, these aspects become issues for everyone.  Take the comments in the link above.  The gRPC team set out to build a REST framework.  But after examining their actual implementations they found they were all RPC in nature.  I've built my fair share of REST apis, but in general I don't use them in my application architectures because what I need are methods, RPC in other words.  The community has gone crazy building REST only to find that they need RPC.  The same evolutionary process happens with services.  You start out just wanting to ""make the call"" but once you get that working you need security.  With security you need auth/auth.  With more sophisticated use cases you need transactions, logging, scaling, fault handling, tracing, etc.  I have no doubt that gRPC will work well ... until it doesn't.  By then, the commitment will be 100%.  Rewrite 2.0 will start, etc.  This can happen with WCF as well, so there's no glass houses here, but there are reasons why teams go down these paths.  Sometimes it's because of what's not in the framework, sometimes it's because we don't even know we need what's missing. You wrote: Why does the lack of .NET Core support make something deprecated?  For me it goes along the lines like this. Microsoft brings out a complete rewrite of .NET and opens up the door to other OS. It's not something that was a big problem in the past, as our customers knew that when they hired us the got an MS based solution back and it was not a problem for them, because they were running Windows Servers.  But hey, that was something that was not working for 15 years, but know as we are able to do it, then hey it's a great thing. And all along every programming magazine (MS related) was only bringing articles of that new thing.  And all of the blogs just wrote about .NET Core. So now the developers get the feeling this is the new way to go and get the impression that their old tool is just old.  And yes, then was that little incident with Silverlight. All shiny, all great, not a conference without 80% talks about Silverlight and then it vanished in a puff of haze just like that. That was something that stuck with us. When are they pulling such a stung again? Maybe tomorrow everything should be ServiceFabric and then puff, support for WCF is even gone for the Windows world.  You see, the fact that Microsoft is not willing/capable to port WCF to the new world makes us uneasy. For me personally it doesn't have to be a complete full port of all aspects of WCF. It doesn't have to support all bindings. But it should allow us to build SOA applications with transactions, authorization and other good stuff.  In the end my question is: What is .NET Core for?  Why should I start building on the .NET Core platform when even Entity Framework is way behind the EF for the full .NET in regards to functionality? Other stuff like WCF is missing as well. Don't forget not every developer out there is writing bleeding edge, NoSql, NoConstraints, NoRelations, NoSecurtiy software. Some work on systems that evolved for years and are stable and mature as well.  The question is what those developers say to their managers? Developer: Sir you know, MS hat that new shiny thing, but it lacks some of the functionality we need. Manager: But we need to keep up with the technology or our system won't be able to run on Windows Server 2019. Developer: We can do, but it means we have to rewrite it for a big part, and if we do that we might need to evaluate if we do it in Java or not.  Who would be modifying or extending WCF in .NET Framework? The question was from some interested developers asking if they can write a compatible framework for .NET Core...  Why does the lack of .NET Core support make something deprecated? .NET Framework 4.7.1 was released this week, along with a preview announcement of .NET Framework 4.7.2. New Windows servers are planned, and new Windows clients are planned as well. What was deprecated? Who is saying this framework is incapable? These are not messages you are hearing from our teams."
technical,"Your time would be better spent working together on an simplified OSS version of WCF.  If by that you mean writing it from scratch, then that defeats the entire purpose of asking for this.  If instead you mean the community should maintain WCF going forward, then by all means, send me a link to the compilable source code and I'll have the GitHub project spun up in an hour, and a version running without MSMQ on .NET Core an hour after that ,) Your time would be better spent working together on an simplified OSS version of WCF.  I could do without SOAP if need be, but I still would like a communication framework with automatic services + automatic proxies suited for RPC, which is configurable and supports a wide array of security and transport options and which comes out of the box in .Net Core."
technical,"As someone who works as a dev at Microsoft on Azure services and has spent a significant part of the last 18 months building software that helps customers who have SOAP based services, let me offer my perspective with hopefully no judgement on the value of the tech.  I think the folks who are hoping they can convince Microsoft to build WCF style services in .NET Core are wasting their time.  Your time would be better spent working together on an simplified OSS version of WCF.  I'm not from the .net team, so my opinion is just that but when I needed a WSDL parser to do my work, I wrote a new one instead of depending on WCF. My 2c. Your time would be better spent working together on an simplified OSS version of WCF.  If by that you mean writing it from scratch, then that defeats the entire purpose of asking for this.  If instead you mean the community should maintain WCF going forward, then by all means, send me a link to the compilable source code and I'll have the GitHub project spun up in an hour, and a version running without MSMQ on .NET Core an hour after that ,)"
technical,"Well this thread was a good hilarious read to start my week. Thanks for that.  I know that this is 90% a meme post from twitter, but this is also our actual work tracker, so I've gotta be at least 5% serious here:  I don't believe the Terminal is the place to implement most of this. * We've already got a commandline ecosystem problem of users just executing random scripts that they download with curl - if we're letting users install scripts that they saw on someone's story, then i can guarantee you there'll be scripts like ""Run Graphical Applications in WSL!!!"" that actually just rm -rf /mnt/c. I don't want to be held responsible for contributing to that problem. * I think GitHub gists are already better suited for a lot of this - they've already got syntax highlighting for different languages, they've already got accounts set up, and authentication, and hey, they've already got a web-scale website that can host all this, and frankly I'm not prepared, nor is anyone on the team, to build that ourselves. * We've also already got #469 on the backlog for recording Terminal output to the asciinema format, for sharing. Again, that's another site that's probably better suited for hosting recordings of terminal sessions, rather than us standing this up on our own.  So those are reasons why  I  would reject this feature request. However, this thread is definitely cinnamon-msft's responsibility, so I'll let them handle it. As far as the following is concerned:   there could even be a way to share a gist/link to your config file to share your specific terminal preferences and font/color information.  I believe there's been discussion about that in the past, and that I'm on board with. ""Stories"", not so much. +1 this is a vital feature. pls ship cinnamon-msft '"
technical,"Please lock this thread Alright yea, I thought we could all play along and enjoy a nice, lighthearted joke thread, but I guess I was wrong. I'm not gonna take lightly to comments disparaging any of our colleagues. Perhaps having a sense of humor is a desirable trait in an employee, something some participants in this thread are clearly lacking.  Obviously, we're not adding Stories to the Terminal. Anyone can  read the thread  and figure that out, if that's not asking too much."
technical,Please lock this thread Are you talking about me?
technical,Please lock this thread Btw can we call this Stories for Terminal E5 X?
technical,No. Just no. This isn't social media this is for code but is it really ?
technical,This needs a kickstarter campaign Can this be integrated with #469?
technical,On the other hand a native support for twitter could prevent us from missing the meme post Command + Story = Cory?
technical,"It's fairly frustrating to see my words being repeatedly ignored. Perhaps consider that the idea may have been tossed around *a few times already* (including on this very repository, and in this very thread) and that there's a reason why the Yarn core team is doing a PR adding Yarn 1 (which acts as a jumper for both v1 and v2 users), and not Yarn 2 (which does not). If we are to work together for the sake of our users I'd expect to see trust go both ways. Could you maybe engage who ever is doing the visual studio code stories   Having something that is uniform across Microsoft development platforms will have a far better chance of being more than just a gimmick and have a solid base to grow. I thought you were kidding, but 11,819 installs..."
technical,"It's fairly frustrating to see my words being repeatedly ignored. Perhaps consider that the idea may have been tossed around *a few times already* (including on this very repository, and in this very thread) and that there's a reason why the Yarn core team is doing a PR adding Yarn 1 (which acts as a jumper for both v1 and v2 users), and not Yarn 2 (which does not). If we are to work together for the sake of our users I'd expect to see trust go both ways. Could you maybe engage who ever is doing the visual studio code stories.  Having something that is uniform across Microsoft development platforms will have a far better chance of being more than just a gimmick and have a solid base to grow."
technical,Are you talking about me? Doesn't it look cool?
technical,Are you talking about me? Even the Windows Kernel says NO!
technical,"Alright yea, I thought we could all play along and enjoy a nice, lighthearted joke thread, but I guess I was wrong. I'm not gonna take lightly to comments disparaging any of our colleagues. Perhaps having a sense of humor is a desirable trait in an employee, something some participants in this thread are clearly lacking.  Obviously, we're not adding Stories to the Terminal. Anyone can  read the thread  and figure that out, if that's not asking too much. I ACKNOWLEDGE THE FOLLOWING BEFORE PROCEEDING: 1. If I delete this entire template and go my own path, the core team may close my issue without further explanation or engagement. 2. If I list multiple bugs/concerns in this one issue, the core team may close my issue without further explanation or engagement. 3. If I write an issue that has many duplicates, the core team may close my issue without further explanation or engagement (and without necessarily spending time to find the exact duplicate ID number). 4. If I leave the title incomplete when filing the issue, the core team may close my issue without further explanation or engagement. 5. If I file something completely blank in the body, the core team may close my issue without further explanation or engagement.  All good? Then proceed! --  # Add a Stories Feature to Windows Terminal Stories are the new hotness. Snapchat invented Stories but the paradigm of ephemeral updates has since been adopted by Instagram, WhatsApp, Facebook, LinkedIn, Twitter, and most recently, Spotify. There is even a VS Code extension that adds this feature.  Why should LinkedIn get all the fun?! Windows Terminal might be a singular experience (but with user profiles, is it really), but that doesn't mean we shouldn't create an ephemeral social layer into it.  Windows Terminal Stories could be in gist form (a la GitHub) or scripts in your favorite shell language (PWSH or BASH or ZSH, in da clurb we all fam), they could also be print screens of your current terminal session. Because who doesn't want to show off their latest Neofetch/winfetch ASCII art!  As a longterm goal, there could even be a way to share a gist/link to your config file to share your specific terminal preferences and font/color information.  # Proposed technical implementation details (optional) In order to see Stories from others (assuming we don't want to focus just on local users), we would need to run some sort of web server that ties into some type of social network for followers/displays. GitHub would probably be ideal for that.  Having said all this, as good of a feature as I believe Stories would be, it would probably require a lot of resources and overhead, not to mention possible performance concerns. Perhaps this is a better request for Hyper, since it is Electron and basically running a web browser anyway. (No shade to Hyper ” Hyper is great and beautiful and I'm just teasing out of love!)  In fact, the more I write about this. Maybe it's a bad idea."
technical,We need this. I guess probably something could be hacked together with Cowsay and Fortune. This would get closer to feature parity with a real Linux machine. ˜
technical,We need this. I haven't seen any feature request in this repo having this much reactions (and upvotes) in such short amount of time.
technical,I think someone could implement this as an extension e I seriously hope this is just a joke post
technical,Btw can we call this Stories for Terminal E5 X? I think someone could implement this as an extension e
technical,I seriously hope this is just a joke post I use Terminal to access Vim on WSL as my main editor
technical,This has my full support I would love to see this happen.
technical,"It's a fun-take and I wanted to upvote it for the fun part, but then I thought it may send the wrong message so I had to downvote it, just to not confuse newcomers. ,)  Many people want super-simple stories - the terminal story. They want a terminal, that's the story! But if people want pink ribbon hipster dancing marquee tags in the terminal, well, the terminal can always be as customizable as possible. Perhaps make the themes epic as much as possible. But for me personally? Simplicity. (It's ok to have more options). And the simplest story is to keep it so simple that stories are not necessary. USING the terminal is already the best story. ,)  (Admittedly I am actually more likely to use e. g. KDE Konsole + WSL on windows these days than windows Terminal BUT this is not always possible, and in these cases, then, Windows Terminal is so much nicer than oldschool cmd.exe ...) It's fairly frustrating to see my words being repeatedly ignored. Perhaps consider that the idea may have been tossed around *a few times already* (including on this very repository, and in this very thread) and that there's a reason why the Yarn core team is doing a PR adding Yarn 1 (which acts as a jumper for both v1 and v2 users), and not Yarn 2 (which does not). If we are to work together for the sake of our users I'd expect to see trust go both ways."
technical,"Can this be integrated with #469? most recently, Twitter. I think Spotify was more recent"
technical,Can this be integrated with #469? no thanks
technical,Command + Story = Cory? No. Just no. This isn't social media this is for code
technical,"This definitely was a fun read, but I agree with zadjii-msft that this doesn't really make sense for Terminal. Kudos to filmgirl for writing such an entertaining feature request, however I will have to close this issue since we aren't planning to implement it. ˜„ On the other hand a native support for twitter could prevent us from missing the meme post"
technical,"This definitely was a fun read, but I agree with zadjii-msft that this doesn't really make sense for Terminal. Kudos to filmgirl for writing such an entertaining feature request, however I will have to close this issue since we aren't planning to implement it. ˜„ Please don't fill the issue tracker with jokes."
technical,Please don't fill the issue tracker with jokes. Please lock this thread
technical,"You use the terminal for coding? Terminal? Coding? You must be insane, that is impossible! The terminal exists only for cmatrix"
technical,"+1 this is a vital feature. pls ship cinnamon-msft ' This definitely was a fun read, but I agree with zadjii-msft that this doesn't really make sense for Terminal. Kudos to filmgirl for writing such an entertaining feature request, however I will have to close this issue since we aren't planning to implement it. ˜„"
technical,I haven't seen any feature request in this repo having this much reactions (and upvotes) in such short amount of time. This has my full support
technical,Knowing that this is possible my life will not be the same without it We need this.
technical,"Doesn't it look cool? Well this thread was a good hilarious read to start my week. Thanks for that.  I know that this is 90% a meme post from twitter, but this is also our actual work tracker, so I've gotta be at least 5% serious here:  I don't believe the Terminal is the place to implement most of this. * We've already got a commandline ecosystem problem of users just executing random scripts that they download with curl - if we're letting users install scripts that they saw on someone's story, then i can guarantee you there'll be scripts like ""Run Graphical Applications in WSL!!!"" that actually just rm -rf /mnt/c. I don't want to be held responsible for contributing to that problem. * I think GitHub gists are already better suited for a lot of this - they've already got syntax highlighting for different languages, they've already got accounts set up, and authentication, and hey, they've already got a web-scale website that can host all this, and frankly I'm not prepared, nor is anyone on the team, to build that ourselves. * We've also already got #469 on the backlog for recording Terminal output to the asciinema format, for sharing. Again, that's another site that's probably better suited for hosting recordings of terminal sessions, rather than us standing this up on our own.  So those are reasons why  I  would reject this feature request. However, this thread is definitely cinnamon-msft's responsibility, so I'll let them handle it. As far as the following is concerned:   there could even be a way to share a gist/link to your config file to share your specific terminal preferences and font/color information.  I believe there's been discussion about that in the past, and that I'm on board with. ""Stories"", not so much."
technical,"Terminal? Coding? You must be insane, that is impossible! The terminal exists only for cmatrix you should try it. I recently switched to Terminal from VS 2019 and am really happy since then: same level of code completion and refactoring with almost twice less memory!"
technical,but is it really ? You use the terminal for coding?
technical,"Regardless of origin, allow/deny are simply clearer terms that does not require tracing the history of black/white as representations of that meaning. We can simply use the meaning directly. 1. etymology is quite important: in the end, we might consider plain words black and white racist and enter the realms of newspeak which i figure you especially, are familiar with. 2. allow/deny are simply clearer terms ” now that's an actual, technically useful argument. 3. can we please stop jumping onto political bandwagons? i am here for the sanity."
technical,"I think the question here should be: is replacing whitelist and blacklist with allowlist and denylist a better option? If presented with the word blacklist and denylist, which is most likely self-explanatory as to the action to be performed? A quick search and replace in the codebase shows that all entries for those words were already changed. I'll close this issue."
technical,"Good intentions, but I doubt there's any relation of the origin of the terms blacklist/whitelist to race. There are many idioms and phrases in the English language that make use of colours without any racial backstories. I haven't met any black person (myself included) who was ever offended by the use of ""blacklist"". Frankly, a good number find it patronising to make this kind of change. At least one source from a search suggests the word had its origins around union members."
technical," Good intentions, but I doubt there's any relation of the origin of the terms blacklist/whitelist to race. There are many idioms and phrases in the English language that make use of colours without any racial backstories. I haven't met any black person (myself included) who was ever offended by the use of ""blacklist"". Frankly, a good number find it patronising to make this kind of change."
technical," I respectfully disagree with your opinion that your (or my) future colleagues would impose that kind of cultural bias on words that exist in the English language as well as are well-defined on Wikipedia, of which blacklist has far more far reaching disciplines than just comp-sci. That said and since I think this issue will garner overwhelming support in this community, I do support a compromise suggested by rafaelfranca's review to replace the actual words with their definition or some other phrase that is more fluid than a single term."
technical,"I respectfully disagree with your opinion that your (or my) future colleagues would impose that kind of cultural bias on words that exist in the English language as well as are well-defined on Wikipedia, of which blacklist has far more far reaching disciplines than just comp-sci. That said and since I think this issue will garner overwhelming support in this community, I do support a compromise suggested by rafaelfranca's review to replace the actual words with their definition or some other phrase that is more fluid than a single term. I think the question here should be: is replacing whitelist and blacklist with allowlist and denylist a better option? If presented with the word blacklist and denylist, which is most likely self-explanatory as to the action to be performed?"
technical,The terms **Blocklist** and **Clearlist** are sometimes used in place of Blacklist and Whitelist. I'm gonna go ahead and get started on this…
technical,"I'm gonna go ahead and get started on this… One could also argue that in color theory, black is the absence of color (photons which make up the spectrum) and white is the accumulation of all colors. Thus a blacklist is a list which contains elements that are to be absent and a whitelist to be allowed..."
technical,"A quick search and replace in the codebase shows that all entries for those words were already changed. I'll close this issue. Per this, I'd like for Rails to set a good example and tone by using better terminology when we can. An easy fix would be to replace our use of whitelist with allowlist and blacklist with denylist. We can even just use them as verbs directly, as we do with the former terms. So something is allowlisted or denylisted. I took a quick look and it seems like this change is mostly about docs. We only have one piece of the code that I could find on a search that uses the term whitelist with enforce raw sql whitelist. Need to consider whether we need an alias and a deprecation for that."
technical,"At least one source from a search suggests the word had its origins around union members. Regardless of origin, allow/deny are simply clearer terms that does not require tracing the history of black/white as representations of that meaning. We can simply use the meaning directly."
technical,"1. etymology is quite important: in the end, we might consider plain words black and white racist and enter the realms of newspeak which i figure you especially, are familiar with. 2. allow/deny are simply clearer terms ” now that's an actual, technically useful argument. 3. can we please stop jumping onto political bandwagons? i am here for the sanity. The terms **Blocklist** and **Clearlist** are sometimes used in place of Blacklist and Whitelist."
technical,"We also need this 2 years later and google still ignores this need.  i have several payment gateways in order to process users purchases in my apps. i use firebase functions as backend to process payment status changes (ipn) once the payment is approved i would like very much to log an event of 'purchase'  the current analytics archtecture forces me to log a purchase in the client side, before it has been approved or create a frankenstein of services in order to push notification back to user and then log the event"
technical,"Please, consider adding support for Firebase Analytics in the server A pretty good work around here would be to have an intermediate Firebase cloud function that will route all the events to your data pipeline system like kafka or another rpc. This way you can get the event (append client side firebase token in your events list) on your backend and then trigger push notifications to that client."
technical,Is there any solution here? Is there some way to use BigQuery perhaps? Also interested in this. Any updates?
technical,"I'm not able to track revenue because it's not bound to any user action on the client, it's actually an event by the negation of an action. It's not bound by a specific schedule either (it's not subscription model). The only way i could sort of maybe kind of do this is with silent push notifications that trigger a Firebase event, but this error prone. The only thing i can think of is using user properties to track accumulating revenue which will at least provide some context. Any updates on this feature, or documentation on doing it ourselves (via REST API for example)? I'm rather surprised this isn't a use-case covered by the core library. In server-authoritative applications, there are many use-cases where analytics events are emitted by the server, and not available to the client. In order to have a complete data-set we need to be able to emit analytics events server-to-server."
technical,"Just to clarify, A user does something which you log on your server, you are looking for a way for the server to log the event *as the user* (would you be passing user specific event attributes? (e.g. user-agent)). is that it? avishalom the scenario you are describing is exactly valid. You can consider our server as an agent."
technical,I am a little surprised that this also was not ported over from old analytics - absolutely need this for server side event logging out of things happening in the database into user lifecycles... For us especially it would also be crucial in order to get some events as conversions into ads... Can anyone share what kind of workflow (alternatives) they have to handle this missing feature?
technical,A pretty good work around here would be to have an intermediate Firebase cloud function that will route all the events to your data pipeline system like kafka or another rpc. This way you can get the event (append client side firebase token in your events list) on your backend and then trigger push notifications to that client. Client sides analytics are being blocked more and more nowadays. All our users using Brave will simply block everything and Brave is one of the most used browsers in our business niche.
technical,"ToddKerpelman, Is there any update on this? Does that mean this FR can now be implemented?  Looks like it has some limitations. Do you recommend for general logging from Firebase functions?"
technical,"Please upvote issue as well For what it's worth, I don't see this happening anytime soon. Google simply doesn't support server side analytics, firebase analytics is exclusively for apps.  Right now I've imported my data into bigquery and am working on adding server side integration with my bigquery table. With this, I should be able to use bq to query data and later use datastudio to display it. I found this video helpful:"
technical,"2 years later and google still ignores this need.  i have several payment gateways in order to process users purchases in my apps. i use firebase functions as backend to process payment status changes (ipn) once the payment is approved i would like very much to log an event of 'purchase'  the current analytics archtecture forces me to log a purchase in the client side, before it has been approved or create a frankenstein of services in order to push notification back to user and then log the event Google Analytics is an integration. Firebase doesn't build or control the functionality. When server APIs become available for Analytics, they can be added to the SDKs. The feature request has been communicated (not ignored) and will be prioritized relative to other major feature work by the Analytics team. 2 years isn't a bad timeline for major feature development on platforms operating at scale."
technical,"I'll add in that a SaaS analytics product that doesn't support a server-to-server flow today is extremely rare. Google's own ""measurement protocol"" for Google Analytics is widely adopted. And of course players like Adobe, Amplitude and Mixpanel support it. The value and use-cases for server-side APIs are well enumerated over hundreds of other tools.  That being said - opening up a server-to-server flow is more complicated than just documenting an API - since you have to support all of the ""interesting"" things folks will use it for. *Supporting* server data means that customers will try to send in web data, and offline data, and OTT data, etc. How do you accurately link up client-side and server-side identity?  By way of a guess - perhaps Google is in the process of combining GA and Firebase Analytics (frankly the branding is already getting conflated - they're calling it ""Google Analytics for Firebase"" as of this writing...), in which case it makes sense to hold off on releasing a true server-to-server flow. More conspiratorially, maybe they're deliberately limiting the use-cases for Firebase such that it's basically unusable for the enterprise businesses, who absolutely need a server-to-server flow, but who already are paying for GA 360.  Only time will tell! Either they'll come up with a solution or, in my opinion, Firebase will be relegated as a 2nd-tier analytics offering. Hey, folks. I'll just chime in to say this is a feature request the Analytics team has certainly heard from customers, and it's something they're looking into, but as usual I can't share any specific plans or roadmap or anything.  Right now, probably your best option would be to import your analytics data from Google Analytics for Firebase into BigQuery and then combine that with any other data you might be generating server-side, kinda like what namanyayg mentioned."
technical,We also need this. I am a little surprised that this also was not ported over from old analytics - absolutely need this for server side event logging out of things happening in the database into user lifecycles... For us especially it would also be crucial in order to get some events as conversions into ads...
technical,"Does that mean this FR can now be implemented?  Looks like it has some limitations. Do you recommend for general logging from Firebase functions? I consulting Firebase analytics with the clients and its common problem :/ I was thinking about solution and maybe create few standard events (or it will be a new type of standard events) which can have permission to connect with server-side. Two cases that I have in my mind are 1. mobile app with subscription model 2. mobile app like a uber where clients don't need to have an open app after they finished their trip. I'm not programmer but marketer so sry if my thoughts are totally wrong ,)"
technical,"What features would you like to see in a server-side analytics API? Client SDKs primarily support logging analytics events. I don't think there's a use case to support that server-side. I would like to use firebase analytics as a marketing automation tool. My primary purpose is forwarding all events in our system to firebase and using its audience feature. I want to do this in server side because it has major advntages. 1. Instead of implementing firebase in multiple client, I can implement in a single server and maintain it. 2. Accuracy of data in client sides are usually problematic. I believe that it has great potential in server side as well."
technical,"Is still there an option to add Firebase Analytics on the server side? I'll add in that a SaaS analytics product that doesn't support a server-to-server flow today is extremely rare. Google's own ""measurement protocol"" for Google Analytics is widely adopted. And of course players like Adobe, Amplitude and Mixpanel support it. The value and use-cases for server-side APIs are well enumerated over hundreds of other tools.  That being said - opening up a server-to-server flow is more complicated than just documenting an API - since you have to support all of the ""interesting"" things folks will use it for. *Supporting* server data means that customers will try to send in web data, and offline data, and OTT data, etc. How do you accurately link up client-side and server-side identity?  By way of a guess - perhaps Google is in the process of combining GA and Firebase Analytics (frankly the branding is already getting conflated - they're calling it ""Google Analytics for Firebase"" as of this writing...), in which case it makes sense to hold off on releasing a true server-to-server flow. More conspiratorially, maybe they're deliberately limiting the use-cases for Firebase such that it's basically unusable for the enterprise businesses, who absolutely need a server-to-server flow, but who already are paying for GA 360.  Only time will tell! Either they'll come up with a solution or, in my opinion, Firebase will be relegated as a 2nd-tier analytics offering."
technical,"This appears to be exactly what we need to send data from Firebase to BigQuery using Cloud Functions: From Firestore to BigQuery with Firebase Functions I'm not able to track revenue because it's not bound to any user action on the client, it's actually an event by the negation of an action. It's not bound by a specific schedule either (it's not subscription model). The only way i could sort of maybe kind of do this is with silent push notifications that trigger a Firebase event, but this error prone. The only thing i can think of is using user properties to track accumulating revenue which will at least provide some context."
technical,"For what it's worth, I don't see this happening anytime soon. Google simply doesn't support server side analytics, firebase analytics is exclusively for apps.  Right now I've imported my data into bigquery and am working on adding server side integration with my bigquery table. With this, I should be able to use bq to query data and later use datastudio to display it. I found this video helpful: Is still there an option to add Firebase Analytics on the server side?"
technical,Can anyone share what kind of workflow (alternatives) they have to handle this missing feature? Is there any news on this?
technical,"Any updates on this feature, or documentation on doing it ourselves (via REST API for example)? I'm rather surprised this isn't a use-case covered by the core library. In server-authoritative applications, there are many use-cases where analytics events are emitted by the server, and not available to the client. In order to have a complete data-set we need to be able to emit analytics events server-to-server. Is there any solution here? Is there some way to use BigQuery perhaps?"
technical,"This is the only solution to implement subscription tracking since there is no automatic subscription tracking provided by firebase Is there any update on this or a link to a custom solution? Ideally I want to be able to update user properties in firebase analytics from server side, so I can filter by them later when using other firebase services e.g. FCM"
technical,"This is crucial for our solution as well. Please consider a fix ,-) It is now possible to add an Analytics label to messages. Does that mean this FR can now be implemented?"
technical,"I would like to use firebase analytics as a marketing automation tool. My primary purpose is forwarding all events in our system to firebase and using its audience feature. I want to do this in server side because it has major advntages. 1. Instead of implementing firebase in multiple client, I can implement in a single server and maintain it. 2. Accuracy of data in client sides are usually problematic. I believe that it has great potential in server side as well. Just to clarify, A user does something which you log on your server, you are looking for a way for the server to log the event *as the user* (would you be passing user specific event attributes? (e.g. user-agent)). is that it?"
technical,"I consulting Firebase analytics with the clients and its common problem :/ I was thinking about solution and maybe create few standard events (or it will be a new type of standard events) which can have permission to connect with server-side. Two cases that I have in my mind are 1. mobile app with subscription model 2. mobile app like a uber where clients don't need to have an open app after they finished their trip. I'm not programmer but marketer so sry if my thoughts are totally wrong ,) Looks like there're server-side reporting already which is used to report app store * events. They are not exported to BigQuery and do not have any standard properties (user id, device, ...) comparing to any other events. Here is my SO question about it."
technical,"There is an important use case for games with IAP as any client-side information is going to be unreliable. Having the event logged in server-side make it a bit more reliable. Many things happen server-side (e.g. events happening when the user is offline). Without such a simple feature, it's impossible to target the right users. This fact forces us think towards other solutions on the market :("
technical,"Google Analytics is an integration. Firebase doesn't build or control the functionality. When server APIs become available for Analytics, they can be added to the SDKs. The feature request has been communicated (not ignored) and will be prioritized relative to other major feature work by the Analytics team. 2 years isn't a bad timeline for major feature development on platforms operating at scale. My Environment:  * Operating System version: Any * Firebase SDK version: Any * Library version: Latest * Firebase Product: analytics  Please provide Firebase analytics Api for server side. We would like to give a decision right now and being limited on client side is unfortunately not acceptable."
technical,"Is there any news on this? Not a solution, but for a pseudo (UDP analytics version) alternative, I literally send silent push notifications (iOS) packaged with event data that is pushed back to firebase from the background using client SDK. If this is a mobile first/product/foreground only philosophy, it can easily be abused. With the merger of GA/Firebase, this should theoretically be supported now, they would just be events from a different platform. The only thing I can think that might be blocking this feature is internal firbebase ML predictions. Events not generated by an active session may pollute learning models. Also, just generally, REST apis are usually the lowest priority items, because it's cannibalising the services they are selling. We're supposed to be using Firestore/cloud functions, not RDS."
technical,Also interested in this. Any updates? Please upvote issue as well
technical,"Updates about this feature? This would be extremely helpful if you want gather all your analytics in one place. Please, consider adding support for Firebase Analytics in the server"
technical,"Client sides analytics are being blocked more and more nowadays. All our users using Brave will simply block everything and Brave is one of the most used browsers in our business niche. Same here, it's been a feature we've been looking for for years. Server side logging is more reliable and the server posseses info that you want to include in the logging that is not available on the client side."
technical,We also need this. There is an important use case for games with IAP as any client-side information is going to be unreliable. Having the event logged in server-side make it a bit more reliable.
technical,We have the same problem in cloud functions. The cloud function reacts to the client doing something and needs to log an event to analytics on behalf of that user. We don't want to give the client access to the data necessary to log such an event though.  It's the same use cases and motivate: This appears to be exactly what we need to send data from Firebase to BigQuery using Cloud Functions: From Firestore to BigQuery with Firebase Functions
technical,"Many things happen server-side (e.g. events happening when the user is offline). Without such a simple feature, it's impossible to target the right users. This fact forces us think towards other solutions on the market :( This is crucial for our solution as well. Please consider a fix ,-)"
technical,"Hey, folks. I'll just chime in to say this is a feature request the Analytics team has certainly heard from customers, and it's something they're looking into, but as usual I can't share any specific plans or roadmap or anything.  Right now, probably your best option would be to import your analytics data from Google Analytics for Firebase into BigQuery and then combine that with any other data you might be generating server-side, kinda like what namanyayg mentioned. This is great. I think that this API is already implemented for mobile devices. In order to send events by mobile devices there must be an API for this purpose. I feel that the API needs to be officially documented and publicly available for server side."
technical,This is great. I think that this API is already implemented for mobile devices. In order to send events by mobile devices there must be an API for this purpose. I feel that the API needs to be officially documented and publicly available for server side. This is the only solution to implement subscription tracking since there is no automatic subscription tracking provided by firebase
technical,"It is now possible to add an Analytics label to messages. Does that mean this FR can now be implemented? ToddKerpelman, Is there any update on this?"
technical,"While the idea is nice, background notifications are ""unreliable"" at best in our (extensive) experience.  Even Apple says so in the docs - ""The system treats background notifications as low-priority"" Updates about this feature? This would be extremely helpful if you want gather all your analytics in one place."
technical,"Same here, it's been a feature we've been looking for for years. Server side logging is more reliable and the server posseses info that you want to include in the logging that is not available on the client side. We also need this"
technical,"Is there any update on this or a link to a custom solution? Ideally I want to be able to update user properties in firebase analytics from server side, so I can filter by them later when using other firebase services e.g. FCM We also need this."
technical,"Oh, you gotta be joking. There's in-app subscription events happening, which is related directly to client app, but can only be reliably detected on server side. Such as users being charged on monthly basis, trial periods, introductional periods, etc. So there is no way (apart from using a billion other tools) to directly send these events to Firebase? All analytics tools support this... We also need this."
technical,avishalom the scenario you are describing is exactly valid. You can consider our server as an agent. We have the same problem in cloud functions. The cloud function reacts to the client doing something and needs to log an event to analytics on behalf of that user. We don't want to give the client access to the data necessary to log such an event though.  It's the same use cases and motivate:
technical, What features would you like to see in a server-side analytics API? Client SDKs primarily support logging analytics events. I don't think there's a use case to support that server-side.
technical,"Not a solution, but for a pseudo (UDP analytics version) alternative, I literally send silent push notifications (iOS) packaged with event data that is pushed back to firebase from the background using client SDK. If this is a mobile first/product/foreground only philosophy, it can easily be abused. With the merger of GA/Firebase, this should theoretically be supported now, they would just be events from a different platform. The only thing I can think that might be blocking this feature is internal firbebase ML predictions. Events not generated by an active session may pollute learning models. Also, just generally, REST apis are usually the lowest priority items, because it's cannibalising the services they are selling. We're supposed to be using Firestore/cloud functions, not RDS. While the idea is nice, background notifications are ""unreliable"" at best in our (extensive) experience.  Even Apple says so in the docs - ""The system treats background notifications as low-priority"""
technical,"Because we can undo it is not a good reason to present bad statistics. Data should not only be technically accurate, but meaningful and not lead readers to incorrect assumptions. fCC should strive to follow sound statistical principals and not teach bad habits by example. I think we understand each other's opinions on the matter. Let's let others chime in. 52/130 (40%) is not inacurrate. That is 100% accurate as  a gauge of percentage of challenges completed .  Maybe it's important to clarify that this feature is not an estimate of time remaining, but simply progress through a module."
technical,"To be fair, the 300hrs estimate was quite demotivating for me when I started. Adding UX clutter should have a purpose. What is the *actionable information* this would give the user? Why is it worth adding the clutter? Reported statistics must help the recipient make better decisions. That's the purpose of statistics. If we 'all know' (bad assumption) this statistic is not useful, then why add it?"
technical,"52/130 (40%) is not inacurrate. That is 100% accurate as  a gauge of percentage of challenges completed .  Maybe it's important to clarify that this feature is not an estimate of time remaining, but simply progress through a module. All the percentage or ratio reveals is the percent/ratio of challenges completed. As I said, the data is *technically accurate* but not *meaningful* and *may lead readers to incorrect assumptions*.  The percent of challenges completed is, at best, only roughly correlated with the total time remaining for the user. Given the wide range of learning styles, processes, challenge difficulty, etc, this statistic provides zero *actionable information* to the user. It is just clutter, and possibly demotivating clutter to users that do not understand the limited and misleading statistic we'd be putting on the front page. Reporting statistics that distill information and strip contextual factors is a statistical bad practice, and we should not show statistical bad practices."
technical,"Hi everyone, I am going ahead and closing this thread for now. Please continue on the forum here. The staff and the contributors are very much active on the forum and will be happy to answer open-ended queries there. We really want to limit the GitHub tracker for dev work. Please do not take this  as a rejection or endorsement of anything. Thanks for your understanding again. As a user, I constantly return to the module level (multiple courses) to see how far I have progressed in relation to the entire module. It would be nice if I didn't have to go back to this view and scroll up and down it to get an indication of how much I have left.  For me, I think a feature like this would keep me more engaged and therefore reduce the likelihood of me not finishing a module. **Describe the solution you'd like**  **Describe alternatives you've considered** I think there are many ways this could be done, here a few I could think of: - Percentage of completion - Ratio - Approximation of remaining time. Given FCC has data on average time to completion on each challenge.  Just curious, where does the 300hr estimation come from?   **Additional context** There is some useful discussion that has already happened on the FCC forum here.  I think it's fair to say that this is a simple feature to experiment with. If users say it's helpful, and it increases engagement, I say keep it. If not, then toss it. I remember being surprised it wasn't included when I first started though."
technical,"The one thing I really want to touch on is that freeCodeCamp operates on a budget of limited funds from our generous donors. As such, our approach does not allow for a ""try it and see if it works"" mindset - every feature we implement needs to be deliberate, planned, and well thought out to avoid sinking our finite development resources into something that does not pan out. Hi everyone, I am going ahead and closing this thread for now. Please continue on the forum here. The staff and the contributors are very much active on the forum and will be happy to answer open-ended queries there. We really want to limit the GitHub tracker for dev work. Please do not take this  as a rejection or endorsement of anything. Thanks for your understanding again."
technical,"Thanks for your candid and honest feedback. We have been thinking about this and in fact we are not against the idea considering it does exist in a form here.  One of the ways, that users of the curriculum keep coming back is gamification. Does it keep users motivated? Yes - The #100DaysOfCode challenge is one of those initiatives.  I am 100% in agreement with you that we can never design a program that will give exact report of where a learner stands in terms of knowledge gained. An 80% completion does not mean a user has 80% mastery on the topic. Let's keep this thread constructive towards an actual implementation. You should continue to deliberate on the forum thread for pros-cons. Deliberation is a good thing to vet out a feature.  Meanwhile let me check with the team and get back to you, after discussing this more objectively. Thanks for your patience and comments everyone. I am fine with the fact that we have progress bars for each course. The variance in task difficulty is closer in the individual courses than it is across the entire module, so the statistic is not as bad as an aggregation across all courses in a module. The user can gain actionable information from the percentage of challenges completed in a course - specifically, a reasonable estimate of the time and effort required to complete the course. A challenge from Basic JavaScript and one from Intermediate Algorithm Scripting are not remotely comparable. Mixing disparate objects and obscuring differences is bad statistics, and we shouldn't show bad statistics. The user cannot derive actionable information from this statistic.  I understand that the *feature* feels useful, but we all seem to be in agreement that the *statistic* is meaningless. I'm trying to use specific words with intentionality to be clear. Please ask for clarification as needed. I really am reading and responding to what others write here."
technical,"Thank you all for taking time out of your day to discuss this. Especially, jeremylt. This  has  been pretty constructive in my opinion.  Maybe it would be helpful to step back and discuss the role that metrics play within the app, because I don't necessarily agree that a ratio of completed modules is meaningless.  As a user, from the start I'm assuming variance of time-to-completion within challenges. But maybe the value in a metric like this is that it simply re-inforces that progress is being made. Re-orienting the learner in a way. And maybe, like video games, it's safe to assume users are expecting challenges to get harder or take longer as they go? Anyways, I'll let the pro's take it from here ˜ I think has made good points! It feels like we're at a bit of a stalemate though.  From experience, when stalemates like this are reached on an idea, the ""Fail fast"" approach can provide value. By trying something, then, at least we can say with greater confidence (and data) if it provided value or not."
technical,"I am fine with the fact that we have progress bars for each course. The variance in task difficulty is closer in the individual courses than it is across the entire module, so the statistic is not as bad as an aggregation across all courses in a module. The user can gain actionable information from the percentage of challenges completed in a course - specifically, a reasonable estimate of the time and effort required to complete the course. A challenge from Basic JavaScript and one from Intermediate Algorithm Scripting are not remotely comparable. Mixing disparate objects and obscuring differences is bad statistics, and we shouldn't show bad statistics. The user cannot derive actionable information from this statistic.  I understand that the *feature* feels useful, but we all seem to be in agreement that the *statistic* is meaningless. I'm trying to use specific words with intentionality to be clear. Please ask for clarification as needed. I really am reading and responding to what others write here. I understand the goal should be able to make the UX more actionable, and like I said: Let's keep this thread constructive towards an actual implementation. We are not inclined in anyway right now.That could either be proceed with some form of the proposal by the OP or an objective rejection. I would urge to continue the deliberation on the forum thread instead. Thanks for your understanding."
technical,"All the percentage or ratio reveals is the percent/ratio of challenges completed. As I said, the data is *technically accurate* but not *meaningful* and *may lead readers to incorrect assumptions*.  The percent of challenges completed is, at best, only roughly correlated with the total time remaining for the user. Given the wide range of learning styles, processes, challenge difficulty, etc, this statistic provides zero *actionable information* to the user. It is just clutter, and possibly demotivating clutter to users that do not understand the limited and misleading statistic we'd be putting on the front page. Reporting statistics that distill information and strip contextual factors is a statistical bad practice, and we should not show statistical bad practices. I'm not super passionate about it but I agree that this would be a useful feature, as long as it's in the format ""number completed/number total"". It'll be obvious to everyone who goes through the curriculum that the difficulty and the time you need to invest into a challenge increases. Just like it's obvious to everyone who's ever played a video game that the boss level won't be as easy as level 1-10."
technical,"Adding UX clutter should have a purpose. What is the *actionable information* this would give the user? Why is it worth adding the clutter? Reported statistics must help the recipient make better decisions. That's the purpose of statistics. If we 'all know' (bad assumption) this statistic is not useful, then why add it? just said ""I agree that this would be a useful feature"""
technical,"See additional discussion here.  Personally, given the wide range of difficulty between challenges, I think this would be a bad statistic that makes it easy for users to infer incorrect information about their progress, as percent of challenges completed in a certification module is not related to amount of effort or time remaining in such a simple manner.  This would especially be a bad piece of information to display next to the *very* rough estimate of 300 hours, and it would be worse to 'estimate' the time remaining for a user. The variance in time to competition between users is already pretty high and we already have to tell a decent number of users that its ok if they go 'too fast' or 'too slow' wrt the 300 hours.  It is really tempting to provide numbers because of the assumption that more numbers = more information = more better. But aggregating data that is too dissimilar is a common statistical sin that we shouldn't be showing our users.  Also, I think this would add clutter to the simpler UX. Luckily, this is software, it's always changing. If we try it, and it doesn't work, atleast then we have data to re-inforce that the feature doesn't improve the app."
technical," See additional discussion here.  Personally, given the wide range of difficulty between challenges, I think this would be a bad statistic that makes it easy for users to infer incorrect information about their progress, as percent of challenges completed in a certification module is not related to amount of effort or time remaining in such a simple manner.  This would especially be a bad piece of information to display next to the *very* rough estimate of 300 hours, and it would be worse to 'estimate' the time remaining for a user. The variance in time to competition between users is already pretty high and we already have to tell a decent number of users that its ok if they go 'too fast' or 'too slow' wrt the 300 hours.  It is really tempting to provide numbers because of the assumption that more numbers = more information = more better. But aggregating data that is too dissimilar is a common statistical sin that we shouldn't be showing our users.  Also, I think this would add clutter to the simpler UX."
technical,"I understand the goal should be able to make the UX more actionable, and like I said: Let's keep this thread constructive towards an actual implementation. We are not inclined in anyway right now.That could either be proceed with some form of the proposal by the OP or an objective rejection. I would urge to continue the deliberation on the forum thread instead. Thanks for your understanding. Thank you all for taking time out of your day to discuss this. Especially, jeremylt. This  has  been pretty constructive in my opinion.  Maybe it would be helpful to step back and discuss the role that metrics play within the app, because I don't necessarily agree that a ratio of completed modules is meaningless.  As a user, from the start I'm assuming variance of time-to-completion within challenges. But maybe the value in a metric like this is that it simply re-inforces that progress is being made. Re-orienting the learner in a way. And maybe, like video games, it's safe to assume users are expecting challenges to get harder or take longer as they go? Anyways, I'll let the pro's take it from here ˜"
technical,"I think has made good points! It feels like we're at a bit of a stalemate though.  From experience, when stalemates like this are reached on an idea, the ""Fail fast"" approach can provide value. By trying something, then, at least we can say with greater confidence (and data) if it provided value or not. Thank you, for opening this feature request. We have spent a lot of time going back-and-forth between ideas. One of the main discussions occurred here:  Ultimately, the decision was: It looks cool, but there are a few things that prevent visual representation from working well here:  For each certification, the lessons only constitute about 16% of the cert. The other 84% is the certification projects. This means that on the first certification, a camper could make it through a hundred or more lessons and only be 5 or 6% of the way done with the certification. So all they would see would be a tiny slice of the pie for their first 50 hours of coding. This is why I recommended using the (300 hours - 1.03% complete) text. Because we really need that level of granularity in the numbers. Otherwise progress would be virtually invisible from one coding session to the next. Also, people may wonder ""what is this thing over on the right hand margin""? The ""X.XX% complete"" text is unambiguous. Thus, if you want to implement a text-based version of this, we would be very interested in this.  So, there is still scope to implement such a feature, but keeping in mind the discussion had in this issue, and the linked PR.  We welcome PoCs (proof of concepts) - as you have done, or more, visual communication is appreciated. However, it is in your and our best interest to read through the current/past discussion so as to not end up spending hours on something which ultimately might not be used. Hope this adds to the discussion."
technical,"just said ""I agree that this would be a useful feature"" Thanks for your candid and honest feedback. We have been thinking about this and in fact we are not against the idea considering it does exist in a form here.  One of the ways, that users of the curriculum keep coming back is gamification. Does it keep users motivated? Yes - The #100DaysOfCode challenge is one of those initiatives.  I am 100% in agreement with you that we can never design a program that will give exact report of where a learner stands in terms of knowledge gained. An 80% completion does not mean a user has 80% mastery on the topic. Let's keep this thread constructive towards an actual implementation. You should continue to deliberate on the forum thread for pros-cons. Deliberation is a good thing to vet out a feature.  Meanwhile let me check with the team and get back to you, after discussing this more objectively. Thanks for your patience and comments everyone."
technical,"Thank you, for opening this feature request. We have spent a lot of time going back-and-forth between ideas. One of the main discussions occurred here:  Ultimately, the decision was: It looks cool, but there are a few things that prevent visual representation from working well here:  For each certification, the lessons only constitute about 16% of the cert. The other 84% is the certification projects. This means that on the first certification, a camper could make it through a hundred or more lessons and only be 5 or 6% of the way done with the certification. So all they would see would be a tiny slice of the pie for their first 50 hours of coding. This is why I recommended using the (300 hours - 1.03% complete) text. Because we really need that level of granularity in the numbers. Otherwise progress would be virtually invisible from one coding session to the next. Also, people may wonder ""what is this thing over on the right hand margin""? The ""X.XX% complete"" text is unambiguous. Thus, if you want to implement a text-based version of this, we would be very interested in this.  So, there is still scope to implement such a feature, but keeping in mind the discussion had in this issue, and the linked PR.  We welcome PoCs (proof of concepts) - as you have done, or more, visual communication is appreciated. However, it is in your and our best interest to read through the current/past discussion so as to not end up spending hours on something which ultimately might not be used. Hope this adds to the discussion. The estimate of ""300 hours"" is already very nebulous statistics, so I see little harm in adding much more descriptive information that only shows the number of challenges completed vs. total challenges. I'm thumbs up for the feature because it saves the user the effort of having to click on each section to get that information."
technical,"The estimate of ""300 hours"" is already very nebulous statistics, so I see little harm in adding much more descriptive information that only shows the number of challenges completed vs. total challenges. I'm thumbs up for the feature because it saves the user the effort of having to click on each section to get that information. The one thing I really want to touch on is that freeCodeCamp operates on a budget of limited funds from our generous donors. As such, our approach does not allow for a ""try it and see if it works"" mindset - every feature we implement needs to be deliberate, planned, and well thought out to avoid sinking our finite development resources into something that does not pan out."
technical,i gave up a year ago..download to slow.. problems after down load.. have to download for 3 hours again.... Vagrant will not fix the problem that has been there for several years now.. you would think that after 3 or 4 years of this problem they would address the issue..... 8 hours to download! I hate you all!
technical,"Your question is totally unrelated to this thread. The box documentation should help you with your question. 99% of the time, this is what I'm dealing with. On a stable fibre connection. No WiFi. Every now and again I get a short burst of speed that pushes progress up around 5%, but overall, I'm left waiting several hours. This is insane. People have proposed work arounds and mitigations but those aren't solutions. When this issue was first closed, it was implied that curl might be to blame. Perhaps this is true, and if it is, then it should be switched out for a different solution."
technical,"Sign me upp here, 100mb symmetric connection (Fiber) sloooow as shit, doing 150kb/s after looking at the years of complaints of slow download with no effort of resolving the issue,,, i think its time to start emailing Laravel to stop endorsing homestead until the issue is resolved..... maybe that will get their attention!!!! this is a real problem... 15 retries and then 4.6 hours to download a file is irresponsible on their part........"
technical,"Speeds ok from the UK:   Bringing machine 'default' up with 'virtualbox' provider...  default: Box could not be found. Attempting to find and install... default: Box Provider: virtualbox default: Box Version: = 0  default: Loading metadata for box 'bento/centos-7.2' default: URL:   default: Adding box  (v2.2.6) for provider: virtualbox default: Downloading:  default: Progress: 11% (Rate: 7728k/s, Estimated time remaining: 0:01:19)   What is your location? Also, URL's are delivered from Amazon Web Services  so I doubt they are tight for bandwidth ,-)  Are the slow downloads being made from locations a long distance away from the AWS us-east-1 DC, perhaps thats the root cause of the issue?  Maybe the AWS CDN could be used to cache files around the world?"
technical,"See relevant discussion at #2765. Any ISP, in any state I've traveled to in the last couple years.  This problem is squarely on whatever Hashicorp is doing for hosting, and that's where it needs to be fixed.  If they're unable or unwilling to fix it, a torrent solution would certainly help without taking their resources aside from development for adding a torrent client to the tool.  If all this stuff is on S3 anyway, AWS provides torrent seeding out of the box."
technical,"DeadlySystem We have the same experience, when I download the same box (url) using curl from the commandline (during the vagrant up) Any update on this, fetching box from Hashicorp is painfully slow."
technical,"Definitely still an issue! Are there any trusted torrent links available? I am trying to download bentoo/ubuntu16.04 and i am getting 12 kbps and upto 15kbps when i try to do wget, i am sure that my connection is fine!"
technical,Still an issue. bradisbell The real solution is probably a docker-based development environment. Just waiting for that ecosystem to settle down .
technical,"Typical issue, been around for ages, everybody's moaning about it, and nothing is being done. Came here via Google and this might help some people:  When I run vagrant init [box] && vagrant up, it loads super slow. But when I run vagrant box add [box], it's faster.  Going from 120 kb/s to 1000 kb/s."
technical,"I am trying to download boxes more quickly, and following the advice of some of the above comments, I used aria2 through homebrew to download much faster in parallel. Sample command (url was from original box add attempt from Vagrant):  My speed was around 100k/s with Vagrant's download, up to 1 MB/s with aria2.  Then when you finish downloading that, you can add the box using: I think you can remove the downloaded box after this since it will be copied to [the standard box storage path after adding it. Hope this helps save someone else some time. comment above doesn't quite work if a Vagrantfile expects a specific version of a box (that command will add a box without a version). Instead, you can do the below.  # Steps to get a specific box at a specific version without using vagrant to perform the download  I'm using laravel/homestead in this example because it's what I was trying to get.  1. Download the box using a better download client (e.g. your browser, curl, wget, whatever).  2. Create a new json file (anywhere), add this to it (note, tweak the name, version and url keys to match the box you want, don't worry about the checksum key yet. For url, use the path to the file you just downloaded, example below):  3. In your Vagrantfile, add the following lines vm.box and vm.box url keys):  4. Run vagrant up as normal.  5. vagrant will complain that the sha hash doesn't match (The checksum of the downloaded box did not match...). Take the string that appears next to 'Expected', copy and paste that into the checksum key of the json file you created in step 2.  6. Run vagrant up again, this time it should load from the local file, store it as the correct name and version, and successfully run it.  Hopefully someone finds these steps useful... funnily enough I have done all this research (having never used vagrant before), have tested it a bunch of times, and the original vagrant box add laravel/homestead command that I started running 3 hours ago is still only 8% complete, even though in that time I've downloaded the box file 8 times outside of vagrant.  The rest of this is just my experience, no need to read further ,)  # My experience (aka. why is vagrant so slow to download boxes)  Was getting 420b/sec (yes, that's bytes per second) on a gigabit connection downloading. I downloaded the same file via browser, curl and wget with speeds varying between 12 and 27MB/sec. I then tried doing both at the same time - I was able to download via Chrome, Firefox, curl and wget before vagrant box add laravel/homestead had downloaded 1%.  The URL I got in the browser was this. I don't know what the problem here is, but I can think of a couple: * Whatever UA vagrant uses is bad, and AWS severely limits it * Whatever vagrant uses to download isn't following redirects, or somehow never ends up downloading from AWS infrastructure, instead using some other terribly overloaded server/proxy"
technical,"from the previous comments, this is a solved issue: it's purely the speed of ubuntu's servers. The solution is to use some other org's images. Confirming that when I used instead of ubuntu/xenial64 (why?), I got pretty good download speeds. The box downloaded in less than a minute, on a 300 Mbps fibre optic connection. (Unfortunately I didn't think to copy and paste the log.)"
technical,"Sooooooooooooooo slowly:( create a new issue, this has been closed for ages"
technical,"While I am unsure of the origin of the problem, I really do wish that Hashicorp would get back to its unrelenting focus on user experience with this one.  **Muli-hour downloads (that should take 1-10 minutes)==bad ux.** Currently downloading an image for the 5th time (13Xk/s, even with wget). Keep disconnecting me while around 50-90%. But it ALWAYS downloads at full speed either early morning / late night EST.  Assuming it is a traffic  issue, but regardless very bad UX.   box: Progress: 47% (Rate: 106k/s, Estimated time remaining: 0:14:50)"
technical,"Same here, I have a 30Mbps connection   default: Adding box  for provider: virtualbox default: Downloading:  default: Box download is resuming from prior download progress default: Progress: 0% (Rate: 80568/s, Estimated time remaining: 1:26:22) DeadlySystem We have the same experience, when I download the same box (url) using curl from the commandline (during the vagrant up)"
technical,"Perhaps it's time to fork.  This project is core to a lot of development environments, leaving us all subject to the whims of HashiCorp... and on this issue we seemingly cannot even get a reasonable official response.  Vagrant is MIT-licensed.  Boxes could easily be distributed via torrent, or we can even just specify URLs in our Vagrantfile.  We don't need the Vagrant Cloud dependency if some basic enhancements around box handling are made.  Are there any maintained forks already in existence?  Any thoughts from others? Definitely still an issue!"
technical,"I'm trying to download the scotch/box and current download speeds using vagrant are less than 10kbps.  default: Progress: 0% (Rate: 2603/s, Estimated time remaining: 33:17:38)  However just as bad using wget. ditto, some popular boxes are very slow to download - i'm updating ubuntu/trusty64 as we speak and it's dropping below 1Kb/s. Been seeing this for a couple wks now."
technical,"Frankly, I don't see how your commentary on docker has anything to do with the issue at hand.  Please stop taking this thread off-topic. Docker is not an alternative to Vagrant. They work completely differently and are intended for different environments. While I'm all for a discussion on the merits of when each should be used, this thread is not really the place for it.  Edit: I love both and use them both in development and production environments."
technical,"It could be an idea for Hashicorp to move the images for download into S3 and use that for downloads... that would save on running instances specifically for downloads. Downloading boxes used to be quick, now it's so slow it makes vagrant a no-go for quick and simple developer environments."
technical,"I promised I'd report back on my transition to Docker, so here goes. I spent a ton of time creating a config that delivers the one-liner vagrant up experience. It's docker-compose up. And there's only one dependency, Docker. Not two (VM  plus  Vagrant).  My post about performance of a Docker dev environment on Mac and Linux: medium.com  Repos with my configuration, tailored for Ruby on Rails and Phoenix development. Very very similar. Can be tailored for any language, I'd imagine. Frankly, I don't see how your commentary on docker has anything to do with the issue at hand.  Please stop taking this thread off-topic."
technical,Today I've tried aria2 with 16 simultaneous connections to download laravel/homestead managed to get up to 9.6MiB From last two days i'm trying to download vigrant box but every time i try to download it gives me error and downloading spedd is very slow. I also tried to install it manually but it also didn't worked.For newbie to Laravel it pretty frustrating and i'm still not able to download it.Don't know what should i do now.
technical,"Ignoring such persistent issue means something on Hashicorp's end, I believe. from the previous comments, this is a solved issue: it's purely the speed of ubuntu's servers. The solution is to use some other org's images."
technical,"Ignoring such persistent issue means something on Hashicorp's end, I believe. Gonna try again tonight to see if the migration helped. Anyway I noted that with a different ISP the download rates were fine, so I guess it's not a problem with Vagrant itself (like many users already reported)."
technical,"Lol, I hate you too  :) Guys why is this issue closed? This is still an outstanding issue and needs to be addressed ASAP. I am experiencing the same issue."
technical,"Docker is not an alternative to Vagrant. They work completely differently and are intended for different environments. While I'm all for a discussion on the merits of when each should be used, this thread is not really the place for it.  Edit: I love both and use them both in development and production environments. Haven't read the full thread, but it was a significant difference between using PowerShell and Git Bash."
technical,"I'm experience the same slow experience. Anyone can try aria It's seems a little bit faster, but, man, you can set this up using default vagrant download mechanism and take a walk or make yourself a sandwich. Get way from screen for a little bit. Having the same problem here: 1. Upload a box manually to atlas 2. Create a new Vagrantfile with just vm cfg.vm.box url = <user/box-name 3. vagrant up - box downloads slowly 4. wget box url from atlas (see vagrant up output) - box downloads lightening fast"
technical,The download is extremely slow on my end too. I'm trying vagrant for the very first time. Might ditch this software and go back to my native apache2 instea Help. I have same problem. I can't wait 3 hours! Very slow! Stupid!
technical,"Any update on this, fetching box from Hashicorp is painfully slow. Hey, quick thought:  If this uses curl (not libcurl) through some sort of ruby-controlled, bash-mediated process, why not just remove curl for one of:  * ipfs * aria2  Both would do the job better than curl."
technical," Hi   Some boxes are hosted on Atlas and sometimes Atlas is just acting as a proxy to a user-hosted box. If you give more information on the specific box(es) you're downloading, we can do some research."
technical,"I was getting painfully slow speeds and so I decided to see if upgrading Vagrant would change anything. Before I updated I was getting speeds of around 50-200kbps. After the update I was using the full 70mbps of my connection.  So for those of you that have slow speeds, try updating to v1.9.4 if you aren't already running it. Hi, I'm on the latest Vagrant 1.9.4 and the download speed is so slow that keeps disconnect. No way to download latest laravel homestead 2.1.0 box... When I downloaded the 2.0 with Vagrant 1.9.3 (or previous, can't remember) it was flawless"
technical,"comment above doesn't quite work if a Vagrantfile expects a specific version of a box (that command will add a box without a version). Instead, you can do the below.  # Steps to get a specific box at a specific version without using vagrant to perform the download  I'm using laravel/homestead in this example because it's what I was trying to get.  1. Download the box using a better download client (e.g. your browser, curl, wget, whatever).  2. Create a new json file (anywhere), add this to it (note, tweak the name, version and url keys to match the box you want, don't worry about the checksum key yet. For url, use the path to the file you just downloaded, example below):  3. In your Vagrantfile, add the following lines vm.box and vm.box url keys):  4. Run vagrant up as normal.  5. vagrant will complain that the sha hash doesn't match (The checksum of the downloaded box did not match...). Take the string that appears next to 'Expected', copy and paste that into the checksum key of the json file you created in step 2.  6. Run vagrant up again, this time it should load from the local file, store it as the correct name and version, and successfully run it.  Hopefully someone finds these steps useful... funnily enough I have done all this research (having never used vagrant before), have tested it a bunch of times, and the original vagrant box add laravel/homestead command that I started running 3 hours ago is still only 8% complete, even though in that time I've downloaded the box file 8 times outside of vagrant.  The rest of this is just my experience, no need to read further ,)  # My experience (aka. why is vagrant so slow to download boxes)  Was getting 420b/sec (yes, that's bytes per second) on a gigabit connection downloading. I downloaded the same file via browser, curl and wget with speeds varying between 12 and 27MB/sec. I then tried doing both at the same time - I was able to download via Chrome, Firefox, curl and wget before vagrant box add laravel/homestead had downloaded 1%.  The URL I got in the browser was this. I don't know what the problem here is, but I can think of a couple: * Whatever UA vagrant uses is bad, and AWS severely limits it * Whatever vagrant uses to download isn't following redirects, or somehow never ends up downloading from AWS infrastructure, instead using some other terribly overloaded server/proxy how is this closed? still a major issue"
technical,"I'm located in Vermont, which is pretty us-east-1 last I checked :dart: I am in Brazil.... got it to download.... 10m connection here took 4.6 hours!!!!! my wife just gave birth to our 8th little girl.. It only took her 40 minutes !!!!! lol There is a big problem with there download!!!!!"
technical,"Using terminal to add a box is extremely slow, more than 1 hour with 100kb/s. If I use a browser to directly download the box it is much faster, just a few minutes with 600kb/s. What's the reason for having such a huge difference? Seems as adding the box through terminal won't use all the available bandwidth. I am trying to download boxes more quickly, and following the advice of some of the above comments, I used aria2 through homebrew to download much faster in parallel. Sample command (url was from original box add attempt from Vagrant):  My speed was around 100k/s with Vagrant's download, up to 1 MB/s with aria2.  Then when you finish downloading that, you can add the box using: I think you can remove the downloaded box after this since it will be copied to [the standard box storage path after adding it. Hope this helps save someone else some time."
technical,"after looking at the years of complaints of slow download with no effort of resolving the issue,,, i think its time to start emailing Laravel to stop endorsing homestead until the issue is resolved..... maybe that will get their attention!!!! this is a real problem... 15 retries and then 4.6 hours to download a file is irresponsible on their part........ I bet it gets closed, but if you don't ask you don't get:"
technical,"I contacted support about this around the same time I chimed in here initially. Their response is that Vagrant uses curl to download things so they don't see this as a Vagrant problem. IMO that's an unprofessional cop-out because they chose to use curl, know that there are problems and aren't considering swapping out with an alternative to eliminate the problem for their users. I can confirm that this is still an issue. All my peers also report times of 1h, while the connection here for other connections is around 200MB/s.   vagrant up Bringing machine 'default' up with 'virtualbox' provider...  default: Box could not be found. Attempting to find and install... default: Box Provider: virtualbox default: Box Version:  0  default: Loading metadata for box ' default: URL:   default: Adding box 'ubuntu/trusty32' (v20160406.0.0) for provider: virtualbox default: Downloading: default: Progress: 11% (Rate: 43801/s, Estimated time remaining: 1:36:50)"
technical,"It's not just an Atlas issue. I have boxes and metadata.json on S3, with a Fastly CDN in front and regularly have the exact same issue: sometimes vagrant downloads at 100kbps and sometimes it downloads at  5mbps. You can cancel a slow download and half the time a retry gets you the faster speeds. I contacted support about this around the same time I chimed in here initially. Their response is that Vagrant uses curl to download things so they don't see this as a Vagrant problem. IMO that's an unprofessional cop-out because they chose to use curl, know that there are problems and aren't considering swapping out with an alternative to eliminate the problem for their users."
technical,"I'm just going to leave these here for any poor sod waiting for this download. I found a root cause and a solution for my situation. I increased my download speed 100x:  Root cause: Slow speed of ubuntu.com cloud box servers Solution: Switch to another trusted source for Ubuntu boxes, Puppet Labs.  I've been using ubuntu/xenial64 for developing apps that must deploy to Ubuntu 16. (Heroku) And download speeds have become ridiculously slow, or failed entirely. I used wget and watched it follow the redirects and saw that it's actually hitting ubuntu.com. I tried several ways to download from Ubuntu, but they all resulted in speeds around 50 KB/s.  So I went back to the Vagrant box listing to see who else might have a Xenial 64 box, and if I'd trust them. Turns out Puppet does, and I trust them as much as (or even more than) Ubuntu. My Vagrantfile now has the config line: And it downloaded in just a few seconds, as opposed to possibly an hour or never."
technical,Help. I have same problem. I can't wait 3 hours! Very slow! Stupid! i gave up a year ago..download to slow.. problems after down load.. have to download for 3 hours again.... Vagrant will not fix the problem that has been there for several years now.. you would think that after 3 or 4 years of this problem they would address the issue.....
technical,"Is there any way to use something like axel to stream downloads in quicker? I guess there's nothing preventing people from sharing boxes via torrent.  For example, below is a magnet link for the heroku-cedar-14 box: Anyone know a good website where one can search for torrents of vagrant boxes?"
technical,"Currently downloading an image for the 5th time (13Xk/s, even with wget). Keep disconnecting me while around 50-90%. But it ALWAYS downloads at full speed either early morning / late night EST.  Assuming it is a traffic  issue, but regardless very bad UX.   box: Progress: 47% (Rate: 106k/s, Estimated time remaining: 0:14:50) I have been trying for 2 day's now and still can not get it to download... its a shame.. it is really not impressing new comers to  laravel .. i can only get 34ks speed........."
technical,"create a new issue, this has been closed for ages i just downloaded latest scotch-box vagrant box, spent ~1hour for that, and then i found an issue which could be fixed only by downgrading to previous version of box, so i'm here again downloading second box for next hour of my working time. So.. it's good since i got per-time payment, but on the other hand probably i'll have some problems explaining why i spen last day downloading stuff from the internet"
technical,"create a new issue, this has been closed for ages I just tried asking Vagrant to download ubuntu/trusty64, and was getting speeds of <= 5 KiB/sec. I killed it and tried again using the exact same command, and got 29 MiB/sec.  I think mitchellh is correct in that this doesn't really seem like a Vagrant issue. If anything, it seems more like an Atlas issue (so possibly the ELB and/or whatever's sitting behind it). I highly doubt it has anything to do with the routes or hops between end-users and the ELB VIPs -- you wouldn't typically see such a polarizing set of speeds in that case, especially considering both VIPs terminate in us-east-1.  If for no other reason, it'd be highly desirable to see these made available through a CDN rather than a centrally-located ELB. Then again, I'm just one guy (who isn't paying for this service), so take that for what it's worth. Pretty thankful it's there either way."
technical,"I'm on a 150Mbps line.  vagrant up = HOURS vagrant box add = HOURS browser download /wget = HOURS  May not be a vagrant issue per se, BUT IT IS.   If your infrastructure can't handle it then your product is broken.  BAD UX I opened a ticket about customizing the download tool, but it got rejected as too complicated to impliment ,-(  That said, my recent download speeds have been ok from the UK for a while. This example downloaded just now:   default: Box Provider: virtualbox default: Box Version: = 0  default: Loading metadata for box  default: URL:   default: Adding box 'bento/ubuntu-16.04' (v2.3.4) for provider: virtualbox default: Downloading: default: Progress: 12% (Rate: 8940k/s, Estimated time remaining: 0:01:06)   Maybe you guys are just too far from their AWS instances for a good download speed? All their download servers look to be in New York with no CDN to distribute content.  Maybe its worth the people getting slow downloads detailing what ISP they are using? Maybe you are all on an ISP with a high contention ratio?"
technical,"I guess there's nothing preventing people from sharing boxes via torrent.  For example, below is a magnet link for the heroku-cedar-14 box: Anyone know a good website where one can search for torrents of vagrant boxes? I personally don't know at the moment about any torrent sites - but for me I wouldn't want to trust torrent links as the source for my infrastructure testing. It's a possible option but security is also important. For me official vagrant boxes from folks like puppetlabs hosted on Atlas are so slow to download at times that I wish this issue could be resolved. For internal vagrant boxes that I build for my company we have the option to host on S3 or Artifactory or private Atlas org. yes - curl is just as slow (for me). I don't think it is a Vagrant issue - but a backed server hosting issue. Granted - not a Vagrant issue per se."
technical,"Confirming that when I used instead of ubuntu/xenial64 (why?), I got pretty good download speeds. The box downloaded in less than a minute, on a 300 Mbps fibre optic connection. (Unfortunately I didn't think to copy and paste the log.) I promised I'd report back on my transition to Docker, so here goes. I spent a ton of time creating a config that delivers the one-liner vagrant up experience. It's docker-compose up. And there's only one dependency, Docker. Not two (VM  plus  Vagrant).  My post about performance of a Docker dev environment on Mac and Linux: medium.com  Repos with my configuration, tailored for Ruby on Rails and Phoenix development. Very very similar. Can be tailored for any language, I'd imagine."
technical,"sorry - I misread your original issue.  I would suspect (and maybe mitchellh could elaborate more) a few things: 1. Ruby is slow and somehow throttling the subprocess 2. Wget is faster than curl (which is what Vagrant is using) 3. Vagrant is also allocating time to unpack the box 4. Wget is allowing for some type of compressed download  It would be helpful if you could benchmark this with curl for reference. I really can't explain this. Vagrant doesn't do anything during the subprocess Ruby-wise: it subprocesses to curl. It doesn't even do the download in Ruby. Perhaps wget is using multiple connections to download multiple parts? I really don't know, but unless we get more information I have to assume that Vagrant is fine here.  Is curl just as slow? Vagrant is just subprocessing to curl until it completes."
technical,"99% of the time, this is what I'm dealing with. On a stable fibre connection. No WiFi. Every now and again I get a short burst of speed that pushes progress up around 5%, but overall, I'm left waiting several hours. This is insane. People have proposed work arounds and mitigations but those aren't solutions. When this issue was first closed, it was implied that curl might be to blame. Perhaps this is true, and if it is, then it should be switched out for a different solution. I suffer the same problem also  My connection on speedtest.net 6.6Mbps while the download takes only 0.6 Mbps of my bandwidth"
technical,"Any ISP, in any state I've traveled to in the last couple years.  This problem is squarely on whatever Hashicorp is doing for hosting, and that's where it needs to be fixed.  If they're unable or unwilling to fix it, a torrent solution would certainly help without taking their resources aside from development for adding a torrent client to the tool.  If all this stuff is on S3 anyway, AWS provides torrent seeding out of the box. I was getting painfully slow speeds and so I decided to see if upgrading Vagrant would change anything. Before I updated I was getting speeds of around 50-200kbps. After the update I was using the full 70mbps of my connection.  So for those of you that have slow speeds, try updating to v1.9.4 if you aren't already running it."
technical,"surely there must be a way for a website to publish the hash of their box along with a torrent link? I wish in general, there was a way to have incremental images, like docker images, with vagrant boxes.  For the provisioners, which bootstrap (cfengine, chef, salt, puppet, docker, etc) by downloading their platform, I wish there was a way to download a packaged up installer, so that other fresh images that use that provisioner, e.g. ubuntu + docker, would not need to download the goods again.  Box updates and provisioner downloads were already painful, but recently, have been beyond notoriously slow."
technical,Having the same problem here: 1. Upload a box manually to atlas 2. Create a new Vagrantfile with just vm cfg.vm.box url = <user/box-name 3. vagrant up - box downloads slowly 4. wget box url from atlas (see vagrant up output) - box downloads lightening fast I wish there was just a +1 for this. Me too. Same connection for all 3 attempts. VPN turned off. - vagrant up took 25+ minutes. - wget took 3 minutes. - curl took 4 minutes.
technical,"Well, I cant download a single box tonite.  I'm in the UK.  The box could not be found or could not be accessed in the remote catalog. If this is a private box on HashiCorp's Atlas, please verify you're logged in via vagrant login. Also, please double-check the name. The expanded URL and error message are shown below: Error: The requested URL returned error: 404 Not Found none of those work.  BUT if you download the box manually via wget, all works fine... could not be accessed in the remote catalog. If this is a private box on HashiCorp's Atlas, please verify you're logged in via vagrant login. Also, please double-check the name. The expanded URL and error message are shown below: Error: The requested URL returned error: 404 Not Found  A direct box add works too I would like to know how can I manually add the .box file in an offline manner. It is not clear in the documents on how to do that. I think the correct place for modification is Vagrantfile. However, I can not figure out which parameter should I change."
technical,"i just downloaded latest scotch-box vagrant box, spent ~1hour for that, and then i found an issue which could be fixed only by downgrading to previous version of box, so i'm here again downloading second box for next hour of my working time. So.. it's good since i got per-time payment, but on the other hand probably i'll have some problems explaining why i spen last day downloading stuff from the internet I'm also facing the slow download issue."
technical,"I really can't explain this. Vagrant doesn't do anything during the subprocess Ruby-wise: it subprocesses to curl. It doesn't even do the download in Ruby. Perhaps wget is using multiple connections to download multiple parts? I really don't know, but unless we get more information I have to assume that Vagrant is fine here.  Is curl just as slow? Vagrant is just subprocessing to curl until it completes. I'm experience the same slow experience. Anyone can try aria It's seems a little bit faster, but, man, you can set this up using default vagrant download mechanism and take a walk or make yourself a sandwich. Get way from screen for a little bit."
technical,"Now it's fast again... really depends on time of day and luck (as stated above...). I'm having this same problem when trying to download Bento boxes. I'm using Vagrant 1.9.5 with 5.1.22. I've tried several times, during various days. My normal download speed is 1.7mb~ but when dowloading boxes I can't get pass 40kb.  If I try downloading with wget I get the same low speed."
technical,bradisbell The real solution is probably a docker-based development environment. Just waiting for that ecosystem to settle down . I'm just going to leave these here for any poor sod waiting for this download.
technical,"Also, URL's are delivered from Amazon Web Services  so I doubt they are tight for bandwidth ,-)  Are the slow downloads being made from locations a long distance away from the AWS us-east-1 DC, perhaps thats the root cause of the issue?  Maybe the AWS CDN could be used to cache files around the world? I'm located in Vermont, which is pretty us-east-1 last I checked :dart:"
technical,"Also, URL's are delivered from Amazon Web Services  so I doubt they are tight for bandwidth ,-)  Are the slow downloads being made from locations a long distance away from the AWS us-east-1 DC, perhaps thats the root cause of the issue?  Maybe the AWS CDN could be used to cache files around the world? I'm on a 150Mbps line.  vagrant up = HOURS vagrant box add = HOURS browser download /wget = HOURS  May not be a vagrant issue per se, BUT IT IS.   If your infrastructure can't handle it then your product is broken.  BAD UX"
technical,"From last two days i'm trying to download vigrant box but every time i try to download it gives me error and downloading spedd is very slow. I also tried to install it manually but it also didn't worked.For newbie to Laravel it pretty frustrating and i'm still not able to download it.Don't know what should i do now. I'm seeing the same problem. Running latest Vagrant 2.0.0. When downloading a box (configured using external URL on Vagrant Cloud, so it's actually redirecting to Rackspace Cloud Files) it's painfully slow.  If I use cURL or Chrome to download the same URL from the same machine it only takes a couple of seconds to complete the download. What could be the issue here?"
technical,"Ubuntu vivid64 is downloading at ~56kbps. I'm on a 100mbit symmetric connection. edit: it timed out before it could finish. edit2: I can confirm that downloads dramatically faster over wget than via ""vagrant up"". I'm trying to download the scotch/box and current download speeds using vagrant are less than 10kbps.  default: Progress: 0% (Rate: 2603/s, Estimated time remaining: 33:17:38)  However just as bad using wget."
technical,"Same here, I thought this is a personal problem lol. Even update box still get really poor download speed, hashicorp please fix :(    default: Updating with provider 'virtualbox' from version  default: '20171221.0.0' to '20180122.0.0'...  default: Loading metadata for box   default: Adding box (v20180122.0.0) for provider: virtualbox default: Downloading:  default: Box download is resuming from prior download progress default: Progress: 13% (Rate: 52999/s, Estimated time remaining: 0:31:46) Ignoring such persistent issue means something on Hashicorp's end, I believe."
technical,"This is painful to do anything on any more - On 100mbps synchronous connection and getting 168kb, either overloaded servers or throttling In looking to debug curl being slow -- I found a stackoverflow post makes your curl go at the speed you'd expect.  For me, I'm trying to download CentosOS 7and here are my results:  With trace-ascii the first time: Does anyone else see the same behavior?"
technical,"same here   vagrant box update  default: Checking for updates  default: Latest installed version: 0.4.1 default: Version constraints: = 0 default: Provider: vmware desktop  default: Updating with provider 'vmware desktop' from version  default: '0.4.1' to '0.4.2'...  default: Loading metadata for box   default: Adding box (v0.4.2) for provider: vmware desktop default: Downloading default: Progress: 0% (Rate: 42210/s, Estimated time remaining: 6:10:54)) Is there any way to use something like axel to stream downloads in quicker?"
technical,"These errant speed symptoms from hashicorp's servers could be indicative of bumping up against AWS's IOPS credits for GP2 filesystems.   We had some testing infrastructure on drupal.org that would run fine for a long time, then suddenly drop to a crawl because we had ""spent"" all of our IO credits.  It could be possible that hashicorps' servers are bumping up against the same limit.  sar -b could give some insight as to whether or not this explains the random performance drops. It could be an idea for Hashicorp to move the images for download into S3 and use that for downloads... that would save on running instances specifically for downloads."
technical,"I just tried asking Vagrant to download ubuntu/trusty64, and was getting speeds of <= 5 KiB/sec. I killed it and tried again using the exact same command, and got 29 MiB/sec.  I think mitchellh is correct in that this doesn't really seem like a Vagrant issue. If anything, it seems more like an Atlas issue (so possibly the ELB and/or whatever's sitting behind it). I highly doubt it has anything to do with the routes or hops between end-users and the ELB VIPs -- you wouldn't typically see such a polarizing set of speeds in that case, especially considering both VIPs terminate in us-east-1.  If for no other reason, it'd be highly desirable to see these made available through a CDN rather than a centrally-located ELB. Then again, I'm just one guy (who isn't paying for this service), so take that for what it's worth. Pretty thankful it's there either way. It's not just an Atlas issue. I have boxes and metadata.json on S3, with a Fastly CDN in front and regularly have the exact same issue: sometimes vagrant downloads at 100kbps and sometimes it downloads at  5mbps. You can cancel a slow download and half the time a retry gets you the faster speeds."
technical,"I bet it gets closed, but if you don't ask you don't get: Just snagged a box at 85mbit.  You all fix something recently?  Much better than it used to be."
technical,"I wish in general, there was a way to have incremental images, like docker images, with vagrant boxes.  For the provisioners, which bootstrap (cfengine, chef, salt, puppet, docker, etc) by downloading their platform, I wish there was a way to download a packaged up installer, so that other fresh images that use that provisioner, e.g. ubuntu + docker, would not need to download the goods again.  Box updates and provisioner downloads were already painful, but recently, have been beyond notoriously slow. Just went to update my box for the first time (trusty64 - noticed the warning on my vagrant up command output), and it's going to take my 1.5 hours on a 150MBps connection - pathetic. It's 2016 - I don't know the specifics of what's going on here, but surely we can fix this, like, by the end of next week? The tech that goes into modern technologies like vagrant is amazing, something this basic should be overcome in mere hours."
technical,"8 hours to download! I hate you all! Lol, I hate you too  :)"
technical,"same issue here, very slow download rates Rate: 35033/s (keeps jumping between 20000 and 60000...) Estimated time remaining: 9:26:13 :( (on vagrant 1.9.5) Now it's fast again... really depends on time of day and luck (as stated above...)."
technical,"Pain. This is a pain. I'll never be able to download the 5.0.1 I guess On a 100mbps connection in Manila, with Vagrant 2.0.1, I was trying to download:  ...and I was getting speeds of anywhere from 1k-150k via vagrant up or even wget. Total download time: **12 hours!**  Then I did one thing: **I VPN'd to California.**  Suddenly, my download took only 3 mins. So that's something worth trying possibly."
technical,"Yep, very different! But for my use case, they're equivalent and commodified: Ways I can create and launch an isolated dev environment with just one or two commands.  Docker Compose works great and is far simpler than Vagrant (note, this not just ""Docker"" per se, which only launches containers one by one) and I'd need to hear about a compelling reason to try a Vagrant+Docker solution, which sounds pretty damn complex. ,-) Pain. This is a pain. I'll never be able to download the 5.0.1 I guess"
technical,"how is this closed? still a major issue Perhaps it's time to fork.  This project is core to a lot of development environments, leaving us all subject to the whims of HashiCorp... and on this issue we seemingly cannot even get a reasonable official response.  Vagrant is MIT-licensed.  Boxes could easily be distributed via torrent, or we can even just specify URLs in our Vagrantfile.  We don't need the Vagrant Cloud dependency if some basic enhancements around box handling are made.  Are there any maintained forks already in existence?  Any thoughts from others?"
technical,"Same here:    vagrant box add lazygray/heroku-cedar-14 box: Loading metadata for box 'box: URL box: Adding box for provider: virtualbox box: Downloading:  box: Box download is resuming from prior download progress box: Progress: 3% (Rate: 281k/s, ... same here   vagrant box update  default: Checking for updates  default: Latest installed version: 0.4.1 default: Version constraints: = 0 default: Provider: vmware desktop  default: Updating with provider 'vmware desktop' from version  default: '0.4.1' to '0.4.2'...  default: Loading metadata for box   default: Adding box (v0.4.2) for provider: vmware desktop default: Downloading default: Progress: 0% (Rate: 42210/s, Estimated time remaining: 6:10:54))"
technical,"use latest devops tools to speed things up spend days watching max 420k/s download speeds Same here, I have a 30Mbps connection   default: Adding box  for provider: virtualbox default: Downloading:  default: Box download is resuming from prior download progress default: Progress: 0% (Rate: 80568/s, Estimated time remaining: 1:26:22)"
technical,"I am in Brazil.... got it to download.... 10m connection here took 4.6 hours!!!!! my wife just gave birth to our 8th little girl.. It only took her 40 minutes !!!!! lol There is a big problem with there download!!!!! Same here, I have a 50Mbps connection...   default: Progress: 44% (Rate: 102k/s, Estimated time remaining: 0:15:01))"
technical,"On a 100mbps connection in Manila, with Vagrant 2.0.1, I was trying to download:  ...and I was getting speeds of anywhere from 1k-150k via vagrant up or even wget. Total download time: **12 hours!**  Then I did one thing: **I VPN'd to California.**  Suddenly, my download took only 3 mins. So that's something worth trying possibly. Same here, I thought this is a personal problem lol. Even update box still get really poor download speed, hashicorp please fix :(    default: Updating with provider 'virtualbox' from version  default: '20171221.0.0' to '20180122.0.0'...  default: Loading metadata for box   default: Adding box (v20180122.0.0) for provider: virtualbox default: Downloading:  default: Box download is resuming from prior download progress default: Progress: 13% (Rate: 52999/s, Estimated time remaining: 0:31:46)"
technical,"On a 100mbps connection in Manila, with Vagrant 2.0.1, I was trying to download:  ...and I was getting speeds of anywhere from 1k-150k via vagrant up or even wget. Total download time: **12 hours!**  Then I did one thing: **I VPN'd to California.**  Suddenly, my download took only 3 mins. So that's something worth trying possibly. Same here:    vagrant box add lazygray/heroku-cedar-14 box: Loading metadata for box 'box: URL box: Adding box for provider: virtualbox box: Downloading:  box: Box download is resuming from prior download progress box: Progress: 3% (Rate: 281k/s, ..."
technical,"Hi, I'm on the latest Vagrant 1.9.4 and the download speed is so slow that keeps disconnect. No way to download latest laravel homestead 2.1.0 box... When I downloaded the 2.0 with Vagrant 1.9.3 (or previous, can't remember) it was flawless same issue here, very slow download rates Rate: 35033/s (keeps jumping between 20000 and 60000...) Estimated time remaining: 9:26:13 :( (on vagrant 1.9.5)"
technical,"I opened a ticket about customizing the download tool, but it got rejected as too complicated to impliment ,-(  That said, my recent download speeds have been ok from the UK for a while. This example downloaded just now:   default: Box Provider: virtualbox default: Box Version: = 0  default: Loading metadata for box  default: URL:   default: Adding box 'bento/ubuntu-16.04' (v2.3.4) for provider: virtualbox default: Downloading: default: Progress: 12% (Rate: 8940k/s, Estimated time remaining: 0:01:06)   Maybe you guys are just too far from their AWS instances for a good download speed? All their download servers look to be in New York with no CDN to distribute content.  Maybe its worth the people getting slow downloads detailing what ISP they are using? Maybe you are all on an ISP with a high contention ratio? See relevant discussion at #2765."
technical,"Same here, I have a 50Mbps connection...   default: Progress: 44% (Rate: 102k/s, Estimated time remaining: 0:15:01)) Sign me upp here, 100mb symmetric connection (Fiber) sloooow as shit, doing 150kb/s"
technical,"I have been trying for 2 day's now and still can not get it to download... its a shame.. it is really not impressing new comers to  laravel .. i can only get 34ks speed......... Speeds ok from the UK:   Bringing machine 'default' up with 'virtualbox' provider...  default: Box could not be found. Attempting to find and install... default: Box Provider: virtualbox default: Box Version: = 0  default: Loading metadata for box 'bento/centos-7.2' default: URL:   default: Adding box  (v2.2.6) for provider: virtualbox default: Downloading:  default: Progress: 11% (Rate: 7728k/s, Estimated time remaining: 0:01:19)   What is your location?"
technical,"Are there any trusted torrent links available? I am trying to download bentoo/ubuntu16.04 and i am getting 12 kbps and upto 15kbps when i try to do wget, i am sure that my connection is fine! Still an issue here, have to update my box at ~150k/s, not able to do a lot of useful work in the meanwhile.."
technical,"Still an issue here, have to update my box at ~150k/s, not able to do a lot of useful work in the meanwhile.. Still an issue."
technical,"Yes, this is because curl can only use one of my 3 connections at the same time.  No, that's not the connection's rated speed.  The rated speed is 45mbps.  Yes, bittorrent does perform better.  Just sayin-- your rationale for not supporting bittorrent is kinda thin here. surely there must be a way for a website to publish the hash of their box along with a torrent link?"
technical,"Came here via Google and this might help some people:  When I run vagrant init [box] && vagrant up, it loads super slow. But when I run vagrant box add [box], it's faster.  Going from 120 kb/s to 1000 kb/s. Thanks for the tip. At least it worked for me."
technical,"I found a root cause and a solution for my situation. I increased my download speed 100x:  Root cause: Slow speed of ubuntu.com cloud box servers Solution: Switch to another trusted source for Ubuntu boxes, Puppet Labs.  I've been using ubuntu/xenial64 for developing apps that must deploy to Ubuntu 16. (Heroku) And download speeds have become ridiculously slow, or failed entirely. I used wget and watched it follow the redirects and saw that it's actually hitting ubuntu.com. I tried several ways to download from Ubuntu, but they all resulted in speeds around 50 KB/s.  So I went back to the Vagrant box listing to see who else might have a Xenial 64 box, and if I'd trust them. Turns out Puppet does, and I trust them as much as (or even more than) Ubuntu. My Vagrantfile now has the config line: And it downloaded in just a few seconds, as opposed to possibly an hour or never. Thanks for this! My download time estimate went from 3 hours to 3 minutes when I switched to a box from the list"
technical,"In looking to debug curl being slow -- I found a stackoverflow post makes your curl go at the speed you'd expect.  For me, I'm trying to download CentosOS 7and here are my results:  With trace-ascii the first time: Does anyone else see the same behavior? The download is extremely slow on my end too. I'm trying vagrant for the very first time. Might ditch this software and go back to my native apache2 instea"
technical,"I think it really has to do with time of day, traffic, alignment of the planets, etc. I haven't had slow speeds in a while. It seems very hit or miss. In fact, if it is slow you and start and stop it with the chance of getting a better connection. I'm not sure this is really the fault of the vagrant framework as much as it is the nature of large bottlenecked downloads. These errant speed symptoms from hashicorp's servers could be indicative of bumping up against AWS's IOPS credits for GP2 filesystems.   We had some testing infrastructure on drupal.org that would run fine for a long time, then suddenly drop to a crawl because we had ""spent"" all of our IO credits.  It could be possible that hashicorps' servers are bumping up against the same limit.  sar -b could give some insight as to whether or not this explains the random performance drops."
technical,"Hi   Some boxes are hosted on Atlas and sometimes Atlas is just acting as a proxy to a user-hosted box. If you give more information on the specific box(es) you're downloading, we can do some research. This box is actually a box I built using packer and it is hosted on a remote server. I'm trying to understand why the download in significantly slower using vagrant then using wget to the exact same URL."
technical,"Just snagged a box at 85mbit.  You all fix something recently?  Much better than it used to be. This is painful to do anything on any more - On 100mbps synchronous connection and getting 168kb, either overloaded servers or throttling"
technical,Thanks for the tip. At least it worked for me. Today I've tried aria2 with 16 simultaneous connections to download laravel/homestead managed to get up to 9.6MiB
technical,"I'm also facing the slow download issue. Try downloading from a wired connection, it's exponentially faster!"
technical,"Downloading boxes used to be quick, now it's so slow it makes vagrant a no-go for quick and simple developer environments. Trying to download ubuntu/xenial64. Download speed maxes out at 150 KB/s on a 1 Gbps symmetrical fiber connection. WTF. Remaining time 1 hour? I could probably download the ISO, read the guide on how to set up my own box, and finish earlier.  EDIT: Interestingly, speed went up by factor 10 when I tried to download the same box in the browser simultaneously."
technical,"Try downloading from a wired connection, it's exponentially faster! Typical issue, been around for ages, everybody's moaning about it, and nothing is being done."
technical,"I wish there was just a +1 for this. Me too. Same connection for all 3 attempts. VPN turned off. - vagrant up took 25+ minutes. - wget took 3 minutes. - curl took 4 minutes. Ubuntu vivid64 is downloading at ~56kbps. I'm on a 100mbit symmetric connection. edit: it timed out before it could finish. edit2: I can confirm that downloads dramatically faster over wget than via ""vagrant up""."
technical,"Trying to download ubuntu/xenial64. Download speed maxes out at 150 KB/s on a 1 Gbps symmetrical fiber connection. WTF. Remaining time 1 hour? I could probably download the ISO, read the guide on how to set up my own box, and finish earlier.  EDIT: Interestingly, speed went up by factor 10 when I tried to download the same box in the browser simultaneously. use latest devops tools to speed things up spend days watching max 420k/s download speeds"
technical,"I'm seeing the same problem. Running latest Vagrant 2.0.0. When downloading a box (configured using external URL on Vagrant Cloud, so it's actually redirecting to Rackspace Cloud Files) it's painfully slow.  If I use cURL or Chrome to download the same URL from the same machine it only takes a couple of seconds to complete the download. What could be the issue here? Using terminal to add a box is extremely slow, more than 1 hour with 100kb/s. If I use a browser to directly download the box it is much faster, just a few minutes with 600kb/s. What's the reason for having such a huge difference? Seems as adding the box through terminal won't use all the available bandwidth."
technical,"You're welcome!  Ironically: In the past week I switched to Docker Compose, and it's **AMAZING**. I can't believe I've waited this long to use it. I used this Quickstart for Rails doc.  * Configuration is 1/10 the size and complexity of Vagrant for my typical Rails app * Build and Boot times are much faster * On my Linux desktop in particular, there is a 10x speed increase vs. running on Mac. * There's a huge reduction in RAM required. * I'm going to save a ton of money on development computers. I.e., 8GB RAM is plenty for a Docker-based dev machine. (Not so good for Vagrant+VirtualBox though.) And a top-tier i7 isn't necessary anymore either to get good speeds.  I was able to setup a Docker-based dev environment on four computers in a fraction of the time it takes me to create a single Vagrant setup from scratch.  I'm going to write up a blog with detailed metrics - I'll post it here. Vagrant and Docker seems similar but they are different things. Also, you can use a Docker provider within Vagrant."
technical,"Haven't read the full thread, but it was a significant difference between using PowerShell and Git Bash. Well, I cant download a single box tonite.  I'm in the UK.  The box could not be found or could not be accessed in the remote catalog. If this is a private box on HashiCorp's Atlas, please verify you're logged in via vagrant login. Also, please double-check the name. The expanded URL and error message are shown below: Error: The requested URL returned error: 404 Not Found none of those work.  BUT if you download the box manually via wget, all works fine... could not be accessed in the remote catalog. If this is a private box on HashiCorp's Atlas, please verify you're logged in via vagrant login. Also, please double-check the name. The expanded URL and error message are shown below: Error: The requested URL returned error: 404 Not Found  A direct box add works too"
technical,"Gonna try again tonight to see if the migration helped. Anyway I noted that with a different ISP the download rates were fine, so I guess it's not a problem with Vagrant itself (like many users already reported). Well, nothing has changed, it still downloads at a snail's pace given that i am on a 125 Mbps connection!"
technical,"Hi everyone,  This issue overall is very complicated in that it is not easily reproducible and is very dependent on location, network health, and the actual download location of the target box. In many cases the box being downloaded is located on a third party system. One feature that was introduced in Vagrant a couple releases ago was the notification of redirect to show when boxes are being downloaded from a third party server.  There have also been claims that the embedded curl is downloading files slower than the system curl. I have not been successful in validating this claim and do not see a difference in download speed between my system curl and the embedded curl. However, as of Vagrant 2.0.4, the VAGRANT PREFER SYSTEM BINS environment variable is available for all platforms. When enabled, Vagrant will use local system binaries if available instead of the embedded binaries. If you observe faster download speeds with your local curl binary, this provides a switch to easily enable Vagrant to use the local binary.  My apologies for the frustration this issue has caused. It is something I have investigated multiple times, but the number of variables involved makes this extremely difficult to get a valid reproduction. I have updated Vagrant's functionality to make the underlying cause more easily understandable where available (redirect notifications) or easier to work around. I am continuing to investigate our options for box downloads from Vagrant Cloud in different regions of the world (which have occasionally presented with issues). However, with box downloads from third party locations, there is little I am able to do to control their available bandwidth or any kind of throttling they may impose.  Cheers! When relying on vagrant to download a box I frequently see connection speeds like this:  Yet when I use wget to the same URL: This particular example was pulled when on Wifi and connected to an IPSEC VPN."
technical,"I can confirm that this is still an issue. All my peers also report times of 1h, while the connection here for other connections is around 200MB/s.   vagrant up Bringing machine 'default' up with 'virtualbox' provider...  default: Box could not be found. Attempting to find and install... default: Box Provider: virtualbox default: Box Version:  0  default: Loading metadata for box ' default: URL:   default: Adding box 'ubuntu/trusty32' (v20160406.0.0) for provider: virtualbox default: Downloading: default: Progress: 11% (Rate: 43801/s, Estimated time remaining: 1:36:50) While I am unsure of the origin of the problem, I really do wish that Hashicorp would get back to its unrelenting focus on user experience with this one.  **Muli-hour downloads (that should take 1-10 minutes)==bad ux.**"
technical,Guys why is this issue closed? This is still an outstanding issue and needs to be addressed ASAP. I am experiencing the same issue. Wow! Downloading boxes is painful please fix this. PLEASE?
technical,"Vagrant and Docker seems similar but they are different things. Also, you can use a Docker provider within Vagrant. Yep, very different! But for my use case, they're equivalent and commodified: Ways I can create and launch an isolated dev environment with just one or two commands.  Docker Compose works great and is far simpler than Vagrant (note, this not just ""Docker"" per se, which only launches containers one by one) and I'd need to hear about a compelling reason to try a Vagrant+Docker solution, which sounds pretty damn complex. ,-)"
technical,"I personally don't know at the moment about any torrent sites - but for me I wouldn't want to trust torrent links as the source for my infrastructure testing. It's a possible option but security is also important. For me official vagrant boxes from folks like puppetlabs hosted on Atlas are so slow to download at times that I wish this issue could be resolved. For internal vagrant boxes that I build for my company we have the option to host on S3 or Artifactory or private Atlas org. yes - curl is just as slow (for me). I don't think it is a Vagrant issue - but a backed server hosting issue. Granted - not a Vagrant issue per se. Yes, this is because curl can only use one of my 3 connections at the same time.  No, that's not the connection's rated speed.  The rated speed is 45mbps.  Yes, bittorrent does perform better.  Just sayin-- your rationale for not supporting bittorrent is kinda thin here."
technical,"Thanks for this! My download time estimate went from 3 hours to 3 minutes when I switched to a box from the list You're welcome!  Ironically: In the past week I switched to Docker Compose, and it's **AMAZING**. I can't believe I've waited this long to use it. I used this Quickstart for Rails doc.  * Configuration is 1/10 the size and complexity of Vagrant for my typical Rails app * Build and Boot times are much faster * On my Linux desktop in particular, there is a 10x speed increase vs. running on Mac. * There's a huge reduction in RAM required. * I'm going to save a ton of money on development computers. I.e., 8GB RAM is plenty for a Docker-based dev machine. (Not so good for Vagrant+VirtualBox though.) And a top-tier i7 isn't necessary anymore either to get good speeds.  I was able to setup a Docker-based dev environment on four computers in a fraction of the time it takes me to create a single Vagrant setup from scratch.  I'm going to write up a blog with detailed metrics - I'll post it here."
technical,"I would like to know how can I manually add the .box file in an offline manner. It is not clear in the documents on how to do that. I think the correct place for modification is Vagrantfile. However, I can not figure out which parameter should I change. Your question is totally unrelated to this thread. The box documentation should help you with your question."
technical,"If I see a tutorial on an external website it soon or later will become obsolete. If I see a tutorial on main site I think that must be ok, tested, and updated. Simply. Close this issue. A company that does not understand the need of a full doc do not will offer a serious support in the long time.  Bye bye. A company that does not understand the need of a full doc do not will offer a serious support in the long time  Jekyll is an open-source project **freely maintained by volunteers**, it has been around for 10 years now and powers hundred of thousands of websites around the world.  Jekyll has a great community and docs are continuously improved with the help of the contributors.  Jekyll themes  are  documented. I'll see if I can add some links to point to theming at the end of the step-by-step guide."
technical,"Sorry but ... To rest in topic... A simple step added to main step by step intro is so wrong? I don't see what is wrong with referring to a tutorial on another site? Yes the tutorial was paid for, but tutorials of that size take time to create. Maybe we could add some more detail on the points people are getting stuck on in the docs?"
technical,"Thanks, but it's better have an official doc, and not links to external resources. IMHO If it's validation you're looking for then I can 100% endorse these tutorials . However if you want this set in stone then maybe we could link to these tutorials in the official docs? Maybe in this. What do you say?"
technical,"Oooh, I like their idea. IMHO, Documentation is one thing and a (sponsored) community blog is an entirely different league altogether. They should not be intermingled. Documentation is translating ""the code base"" into layman lingo. (*Point-of-View: Maintainers - Users*) Community blog is about sharing experience with the code base. (*Point-of-View: One User - Other Users*) Bringing the sponsors into the mix, is just complicating things, unnecessarily........."
technical," On how to start theming for Jekyll, I'd recommend daviddarnes articles:"
technical,"If it's validation you're looking for then I can 100% endorse these tutorials . However if you want this set in stone then maybe we could link to these tutorials in the official docs? Maybe in this. What do you say? Oooh, I like their idea."
technical,"IMHO, Documentation is one thing and a (sponsored) community blog is an entirely different league altogether. They should not be intermingled. Documentation is translating ""the code base"" into layman lingo. (*Point-of-View: Maintainers - Users*) Community blog is about sharing experience with the code base. (*Point-of-View: One User - Other Users*) Bringing the sponsors into the mix, is just complicating things, unnecessarily......... Sorry but ... To rest in topic... A simple step added to main step by step intro is so wrong?"
technical,"On how to start theming for Jekyll, I'd recommend daviddarnes articles: Thanks, but it's better have an official doc, and not links to external resources. IMHO"
technical,"Welcome to hacking into a game that was never built to support mods. OnlyIn exists only to support vanilla, for the record. SubscribeEvent is superceded recently by DeferredRegister, and your argument is overall invalid.  Also, yes, it is a pattern. This is standard procedure... A few things: * OnlyIn is used to mark vanilla methods that are missing, it doesn't cause them to be missing. The OnlyIn transformer is a hack, and only exists for specific exceptional situations, it's not a convenience and will never be endorsed as one. * Mod and SubscribeEvent are API markers, they are used to identify features, it's very much not the same as what you are proposing. * IExtensibleEnum isn't an annotation? It's a hack needed by modders.  To use client-only code correctly you just need, at most, one (1) helper class, which has some forwarders and keeps all client-only references all hidden from the shared logic. You just need to follow two simple rules: 1. This helper class shouldn't have any outward references to the client-only code (no client-only extends/implement, params or fields) 2. The methods in this class should avoid ""implicit casts"" (eg, assigning Minecraft#player to a PlayerEntity variable), which cause the jvm to have to classload the types so it can verify they are compatible.  That's all. One class and some basic defensive coding."
technical,"I concur. Sided safe referencing is a Java pattern, and it is generally encouraged to separate code that will be used to build two different releases of the same program. In this case, client and server of a game.  Your solution is no better than just using OnlyIn, which is known to break a lot of things: see #7440. Closed by the dictators who made Mod, Mod.EventBusSubscriber, SubscribeEvent, OnlyIn, IExtensibleEnum, ... neither are they java patterns in this case"
technical,"I don't think we should be adding more ASM based hackyness because you are too lazy to write a separate class. I concur. Sided safe referencing is a Java pattern, and it is generally encouraged to separate code that will be used to build two different releases of the same program. In this case, client and server of a game.  Your solution is no better than just using OnlyIn, which is known to break a lot of things: see #7440."
technical, I don't think we should be adding more ASM based hackyness because you are too lazy to write a separate class.
technical,"A few things: * OnlyIn is used to mark vanilla methods that are missing, it doesn't cause them to be missing. The OnlyIn transformer is a hack, and only exists for specific exceptional situations, it's not a convenience and will never be endorsed as one. * Mod and SubscribeEvent are API markers, they are used to identify features, it's very much not the same as what you are proposing. * IExtensibleEnum isn't an annotation? It's a hack needed by modders.  To use client-only code correctly you just need, at most, one (1) helper class, which has some forwarders and keeps all client-only references all hidden from the shared logic. You just need to follow two simple rules: 1. This helper class shouldn't have any outward references to the client-only code (no client-only extends/implement, params or fields) 2. The methods in this class should avoid ""implicit casts"" (eg, assigning Minecraft#player to a PlayerEntity variable), which cause the jvm to have to classload the types so it can verify they are compatible.  That's all. One class and some basic defensive coding. Today I ran into the annoying coding style issue that you have to create a complete new class just for calling a client-only method if available. Because this makes code very very ugly I created a different approach:  A new ImplementOn annotation can specify a method in the same class to be overwritten by another method when on a specific Dist. For example:  Then, upon loading of this class, an ASM transformer (net.minecraftforge.common.asm.RuntimeDistImplementor to be precise) injects either myMethod client or myMethod server into the method myMethod and strips unused ImplementOn methods so that they won't get loaded. The above example will, on Dist.CLIENT become: Note how the method is directly delegated: this keeps line numbers and debug info understandable in stack traces. The system works for methods with or without return values, static and virtual methods, and any set of arguments. For ImplementOn to work properly, the following conditions are checked, with the intentions specified:  - The return types of the target method and the injected method must match **exactly**. This is because it finds the target method based on the name specified and the descriptor (including return type) of the injector method. - The argument lists of the target method and the injected method must match **exactly**. This is because it finds the target method based on the name specified and the descriptor of the injector method. - The static modifier must either be specified on both the target and injected method, or must be omitted on both methods. When one is static and the other isn't, the injection fails. This is because a static method can not reference an instance method. An instance method can access a static method but for the semantics of 'replacing the method' I forced the static modifiers to match exactly. - The injected method **must** be private. This prevents overriding which can cause strange issues as unused injected methods are stripped (subclasses might be unable to load if they call super). This also prevents them being called from within another class (even inner classes can't call them - this goes via a bridge method). - The injected method may never be called (only checked in a development environment, since it checks all instructions in a class and might drop performance a lot). This filters the cases where a method of the same class accesses the injector (including bridge methods), which must not happen either. - There must be, for each separate Dist, only one injected method: you can specify injections for the same method but for different dists. This rule is obvious: you should not inject into a method twice because you will then overwrite your own injection.  Note that this works with method overloading: an ImplementOn annotated method will be injected into the method with the name specified in the annotation's method parameter and the descriptor of the injected method. Hence this will work:  What's also notable here is that an ImplementOn method is only injected when the dist parameter matches the Dist of the environment. When the Dists don't match, the target method is left untouched and the injected method is stripped from the class, if the Dists  do  match, the contents of the target method are stripped **completely** and are overwritten by the necessary bytecode.  ---  As an alternative suggestion to the semantics I've implemented in the current pull request, it's also possible to make a DelegateOn annotation which is put above the target method and using the OnlyIn annotations for stripping sided methods. It would then look like this: Drawback is here: the myMethod client and myMethod server render as unused and don't have an annotation indicating they are dynamically being injected, nor is there any visual reason not to call them. I think this is a well-working alternative to DistExecutor.safeCallWhenOn, since it doesn't require the targeted method to be in a separate class, it allows the use of arguments and it works with return types. Things that might still be necessary and which are missing now:  - Checking the signatures of the methods (so that generic types and throws clauses do match). This is only a semantical issue though, in bytecode it's perfectly safe if a checked exception is thrown without having it in the throws clause and due to type erasure generics will work."
technical,"Closed by the dictators who made Mod, Mod.EventBusSubscriber, SubscribeEvent, OnlyIn, IExtensibleEnum, ... neither are they java patterns in this case Welcome to hacking into a game that was never built to support mods. OnlyIn exists only to support vanilla, for the record. SubscribeEvent is superceded recently by DeferredRegister, and your argument is overall invalid.  Also, yes, it is a pattern. This is standard procedure..."
technical,"Sorry, this is not going to work this way. You have to edit the base yaml files directly (and not add new files). A lot of time is being wasted doing what I've already done. I've tested my changes in hundreds of  matches with great feedback. The only negative opinions I get are from trolls or people who need an excuse for why they suck at the game.  This attitude runs counter to our code of conduct and I have received a complaint about this PR under that code.  This PR is one of many examples of this user's behavior across GitHub, the forum, and IRC.  Participating in an open community requires both the ability to come up with good ideas and the interpersonal skills to effectively communicate them.  I am sorry, but ""I am better than you, so stop wasting your time and do what I say"" is a completely inappropriate base to start a pull request from."
technical," Sorry, this is not going to work this way. You have to edit the base yaml files directly (and not add new files)."
technical,Cool story A polite and hopefully unnecessary reminder that when the license change is released it should be a major version bump. I'm imagining the fall out that would occur if this were released as a patch version and it wouldn't be pretty.
technical,"Once upon a time I thought I had to go work for those big corporations that I hate in order to do the kind of open source work I want to do. That turned out to be incredibly false. So I fucked off and told them to eat shit. As long as you're maintaining the repository and represent it in the face of other contributors and open source community members - no, you did not. Maintainer is a part of a project."
technical,"As long as you're maintaining the repository and represent it in the face of other contributors and open source community members - no, you did not. Maintainer is a part of a project. Cool story"
technical,"i would personally appreciate some certainty around this, because if you're likely to release this under a patch version I now need to go through all my clients' repos to make sure they're using lock files, fixed versions or a fork. Not because my clients are Microsoft et al but because they have contractually approved lists of licenses that we can use and this license will not qualify.  The intent is pretty clear, but the unfortunate side effect is that it also screws with many developers who use this tool. I know this wasn't your intention but it sucks if I have to spend my evening checking a bunch of old repos because you'll  maybe  release this as a patch. I left the Lerna project a long time ago, I've gone as far as to replace Lerna with a new tool called Bolt. All technology is political, open source is especially political. It would not exist if not for political reasons. Open sourcing something is in itself a political act. We'll release it as major"
technical, same.
technical,"You're going to introduce a major license change, refuse to change the license name and do it in a minor version bump? What the actual hell is your goal here? To screw with companies that support ICE, was that not clear?"
technical,"You  can  expose them, so they spawn outside of the flatpak. See this. Anyway, this is getting off-topic.  See no reason for a lock here. The issue is not really fixed or so or Fedora has to fix it (then one needs to notify them), whatever A small trick that is not a solution but probably do it for most. Install the permanent-delete plugin and use shift+del. It is made to bypass the trash can but it's a quick work around."
technical,"It changes nothing :thinking: And if you just use gio trash from the command line it works fine?   Also, are you getting the exact same error message you mentioned (""When trying, the following message appears: Is gvfs-trash installed?"") after you set the ELECTRON TRASH environment variable to gio?"
technical,No. Not selinux. It has to do with how trash folders work. My rather verbose explanation gets in to the nitty gritty for where atom's behavior is deficient.  The short answer is that atom only asks to move files to trash and does not delete files. If a trash folder cannot be created (multiple causes all related to permissions) than it fails. Atom should inquire with user if they should permanently delete the files instead of failing. As yodatak also mentioned that Fedora 29 fresh install has this problem since gvfs-trash has finally after many deprecation notices been removed  and replaced with  gio. So as rsese mentioned you need to set e.g. add it to your ~/.bash profile.  logout and login or source ~/.bash profile and trashing will work
technical,"your comments were deleted as a violation of the Atom Code of Conduct as they were insulting or derogatory. Atom is using an electron API to move items to the trash, this API used to default to gvfs-trash but this got changed in electron v3 to default to gio.  We're planning to upgrade to Electron v3 very soon (WIP PR), so this issue will get resolved quite soon."
technical,"This solution works perfectly on Fedora 28/29. In strict terms, I would argue that this is not a bug intrinsically related to Atom (i.e., this is not an error in the source) ” if much, the ""is gvfs-trash installed"" error message is misleading to Fedora users and perhaps could be changed into something more generic ” but given that there is a workaround shouldn't this issue be closed? BTW for anyone on Fedora who wants an easy ""fix"": The flatpak version of Atom works fine :smiley:"
technical,I got the same problem on feodra 29 on a fresh install can we reopen this bug ? can reopen this
technical,"however the error message it kept returning me was quite misleading.  Yeah, that's the default error message.  Unfortunately, Electron's moveToTrash method doesn't say  why  it failed, just that it did.  We've found that the most common reason is that gvfs-trash isn't installed, which is why we decided to include it in the error message. Devligue This is a legit bug. The  tldr answer  is that  g file trash may not be supported. In order for  g file trash to succeed, the following conditions must be met:  - The file being moved to trash is on the same partition as your home folder **OR** - A trash folder already exists or can be created at the mount point and, - The trash folder is considered 'safe/secure' by verifying UID and restrictive permissions  **How this should probably be fixed...**  Atom does not have a fallback mechanism like offering the ability to permanently remove a file instead. This would address bring atom in line with gnome's graceful behaviour. The attached image show how  Gnome Files (aka Nautilus) prompt the user to permanently delete the file (when trash not supported)  **Impact: Users with multiple disks and partitions**  Fedora/Redhat/etc:  Impacted  due to default partition scheme that separates / and /home in to partitions. Files outside of the /home partition cannot be moved to trash.  Ubuntu: Less likely due to partition layout (/home is part of the / partition)  Conditions where users will be impacted: - File systems with unix permissions: -  Impacted with workaround  due to typical restrictive top-level directory permissions (root) - FAT/non-unix file systems: - User Session Mount (i.e. /run/media/USER/disk-label):  No Impact  - Fstab:  Impacted  even with permissive umask. The default uid/gid is root. It will be unable to satisfy the requirements of a trash folder without additional options (uid/gid/umask). - Network shared folders:  Unknown/Did not test.   **Notes/Testing the Root Cause**  Note: Though I am confident that my analysis is decent enough, much of the code I was referencing was unfamiliar/new to me.  Initially I encountered this issue when I put files in a certain location like the reporter above. I created a delete-me file test as suggested above in the relevant directory, /opt/cupenv. I ran the command  strace gio trash delete-me.txt. A abridged version of the output is here:  That's unexpected. The API documentation for g file trash lacks some level of specificity.   Sends file to the ""Trashcan"", if possible. This is similar to deleting it, but the user can recover it before emptying the trashcan. Not all file systems support trashing, so this call can return the G IO ERROR NOT SUPPORTED error.  One might assume (as I did): Given a path, if the user has permission to modify/delete the file, then GIO's g file trash API should succeed at removing the file in some manner. Perhaps, if trash functionality is not available, then there might be a fallback mechanism. In the case of gio trash I expected there to be a -f force option that would prioritize trash over permanently deleting the file. (gio trash -f only ignores files that don't exist)  Glib appears to implement GIO local file access using glocalfile.c. The trash algorithm looks like this:  - Is the path on the same partition as the user's home directory? If so, move the file to the home trash folder if possible and exit. - Given the path, locate the mount point top level directory (denoted as topdir in source) - In my case (the primary partition) - A more typical case: - If topdir is found, pick one of folder. - If topdir/.Trash-UID and topdir/.Trash/UID exists: Validate proper UID and file permission (or fail) - Try to create folder. Validate proper UID and file permission (or fail)  At the end of the function we find G IO ERROR NOT SUPPORTED is returned when such a folder is not found and cannot be created.  I confirmed my understanding by creating a top level trash folder and using gio trash command.  Files located on the same partition as your home directory can always be moved to your user trash folder. On my operating system, the paths / and /home are separate partitions. I created a file /home/tmp/x where tmp and x are owned by my user. In this case, the files get moved to your HOME/.local/share/Trash folder.  But when mounting disks (external or internal) using fstab or mount commands (as opposed to session-based mounting) issues can arise. I tested a FAT file system with umask=0000 and uid/gid set to root.  You can see gio trash going through the motions. The operations succeeds but the command still reports an error.  GLib/Gio will not write to a trash directory with incorrect permissions/ownership. (Security?) Without unix permissions, trash will always fail on these mount points. Modifying fstab to appear similar to options used during session mounts (UID/GID set to user vs root, umask is set appropriately). Conveniently the source code has a comment that seems to strongly imply this is a known/expected.    Ensure that the created dir has the right uid etc. This might fail on e.g. a FAT dir   Most of this was unnecessary, but I figured I'd show my work."
technical,"I have now.. I got as far as the first person saying to try .bash profile, oh well Electron introduced the fix already in October 2018 How difficult is it to get it fixed?"
technical,As yodatak also mentioned that Fedora 29 fresh install has this problem since gvfs-trash has finally after many deprecation notices been removed  and replaced with  gio. So as rsese mentioned you need to set e.g. add it to your ~/.bash profile.  logout and login or source ~/.bash profile and trashing will work Gio operates the same way. I used the Gio command line to manually step through this...
technical,"Yes, gio is available and installed by default Hmmm, not sure why it's not working then - any difference if you specifically set an ELECTRON TRASH environment variable to gio and then restart Atom?"
technical,"The echo hello  delete-me.txt, gio trash delete-me.txt works just fine normally but crashes on the ntfs partition. As I see this, it is not an Atom bug, however the error message it kept returning me was quite misleading.  I didn't solve the problem yet, but I am closing this since it is not related to Atom. Sorry for trouble! however the error message it kept returning me was quite misleading.  Yeah, that's the default error message.  Unfortunately, Electron's moveToTrash method doesn't say  why  it failed, just that it did.  We've found that the most common reason is that gvfs-trash isn't installed, which is why we decided to include it in the error message."
technical,"There was quite a long thread so myself and others are skipping through, would be helpful to change the original post to point to your comments I added relevant cutephoton comments to the original question."
technical,"I may have no additional information about the issue, but I am sure cutephoton provided enough insight. There are also other people who have similar unsolved problems. I am reopening this. I got no problem with the flatpak version by the way"
technical,"Devligue This is a legit bug. The  tldr answer  is that  g file trash may not be supported. In order for  g file trash to succeed, the following conditions must be met:  - The file being moved to trash is on the same partition as your home folder **OR** - A trash folder already exists or can be created at the mount point and, - The trash folder is considered 'safe/secure' by verifying UID and restrictive permissions  **How this should probably be fixed...**  Atom does not have a fallback mechanism like offering the ability to permanently remove a file instead. This would address bring atom in line with gnome's graceful behaviour. The attached image show how  Gnome Files (aka Nautilus) prompt the user to permanently delete the file (when trash not supported)  **Impact: Users with multiple disks and partitions**  Fedora/Redhat/etc:  Impacted  due to default partition scheme that separates / and /home in to partitions. Files outside of the /home partition cannot be moved to trash.  Ubuntu: Less likely due to partition layout (/home is part of the / partition)  Conditions where users will be impacted: - File systems with unix permissions: -  Impacted with workaround  due to typical restrictive top-level directory permissions (root) - FAT/non-unix file systems: - User Session Mount (i.e. /run/media/USER/disk-label):  No Impact  - Fstab:  Impacted  even with permissive umask. The default uid/gid is root. It will be unable to satisfy the requirements of a trash folder without additional options (uid/gid/umask). - Network shared folders:  Unknown/Did not test.   **Notes/Testing the Root Cause**  Note: Though I am confident that my analysis is decent enough, much of the code I was referencing was unfamiliar/new to me.  Initially I encountered this issue when I put files in a certain location like the reporter above. I created a delete-me file test as suggested above in the relevant directory, /opt/cupenv. I ran the command  strace gio trash delete-me.txt. A abridged version of the output is here:  That's unexpected. The API documentation for g file trash lacks some level of specificity.   Sends file to the ""Trashcan"", if possible. This is similar to deleting it, but the user can recover it before emptying the trashcan. Not all file systems support trashing, so this call can return the G IO ERROR NOT SUPPORTED error.  One might assume (as I did): Given a path, if the user has permission to modify/delete the file, then GIO's g file trash API should succeed at removing the file in some manner. Perhaps, if trash functionality is not available, then there might be a fallback mechanism. In the case of gio trash I expected there to be a -f force option that would prioritize trash over permanently deleting the file. (gio trash -f only ignores files that don't exist)  Glib appears to implement GIO local file access using glocalfile.c. The trash algorithm looks like this:  - Is the path on the same partition as the user's home directory? If so, move the file to the home trash folder if possible and exit. - Given the path, locate the mount point top level directory (denoted as topdir in source) - In my case (the primary partition) - A more typical case: - If topdir is found, pick one of folder. - If topdir/.Trash-UID and topdir/.Trash/UID exists: Validate proper UID and file permission (or fail) - Try to create folder. Validate proper UID and file permission (or fail)  At the end of the function we find G IO ERROR NOT SUPPORTED is returned when such a folder is not found and cannot be created.  I confirmed my understanding by creating a top level trash folder and using gio trash command.  Files located on the same partition as your home directory can always be moved to your user trash folder. On my operating system, the paths / and /home are separate partitions. I created a file /home/tmp/x where tmp and x are owned by my user. In this case, the files get moved to your HOME/.local/share/Trash folder.  But when mounting disks (external or internal) using fstab or mount commands (as opposed to session-based mounting) issues can arise. I tested a FAT file system with umask=0000 and uid/gid set to root.  You can see gio trash going through the motions. The operations succeeds but the command still reports an error.  GLib/Gio will not write to a trash directory with incorrect permissions/ownership. (Security?) Without unix permissions, trash will always fail on these mount points. Modifying fstab to appear similar to options used during session mounts (UID/GID set to user vs root, umask is set appropriately). Conveniently the source code has a comment that seems to strongly imply this is a known/expected.    Ensure that the created dir has the right uid etc. This might fail on e.g. a FAT dir   Most of this was unnecessary, but I figured I'd show my work. I got the same problem on feodra 29 on a fresh install can we reopen this bug ?"
technical,"Did you actually read the thread? I have now.. I got as far as the first person saying to try .bash profile, oh well"
technical,"This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further. I may have no additional information about the issue, but I am sure cutephoton provided enough insight. There are also other people who have similar unsolved problems. I am reopening this."
technical,I got no problem with the flatpak version by the way I reproduced on the latest version still. **shrug** I don't see how flatpak and rpm/deb would differ. It only fails when the preconditions listed above are met -- a partition where you don't have permission to create a trash folder under your user ID.
technical,"Without sudo powers, you can place the script anywhere which is in your PATH (for example, ~/bin). On Fedora 29, the real gfvs-trash has been removed and is not meant to reappear, so you can have a script with that name without worrying about any conflict. I think it's bad user experience having to experience the bug, then google it, find this thread, add a workaround, restart the terminal, restart the session, restart Atom..."
technical,"same here with fedora 29 and atom 1.33.0x64 If I explicitly set an environment variable like this within my bashrc This did work for me, in Fedora 29."
technical,"Honest question: does anybody read what I've been posting about this issue? There is a bug here and posting that this is fixed is misleading. Maybe I should just submit new issue request specific to the limitations of the the ""delete"" functionality. If the above fixes don't work try this (worked on Fedora 29) add: export to the end of your bashrc.  You will need to open a new terminal or log out/in"
technical,"BTW for anyone on Fedora who wants an easy ""fix"": The flatpak version of Atom works fine :smiley: Installing the flatpak version worked for me as well (Fedora 29). It is a solution, although installing via flatpak means giving up CLI tools."
technical,"running fedora 29, i have installed atom from packages, since recently I faced the same error as described here. I think this bug deserves an action in order to improve the package installation, or the runtime, to detect gio/gvfs and make this work out of the box.  This really is an ugly bug for the end user experience. Is there any workaround to delete files with Atom on Fedora 29?"
technical,"Hmmm, not sure why it's not working then - any difference if you specifically set an ELECTRON TRASH environment variable to gio and then restart Atom? It changes nothing :thinking:"
technical,"I guess the solution is to use gio when gvfs-trash is not present, because it's been deprecated for quite some time. Part of the solution would be to make the error message relevant. Some workarounds have been provided and they work. So, the solution could consider them.  The problem is nbot the ability to permanently remove files, please the first message of the issue... It's explicit. It explicitly states in the updated comments added to the first message:  Atom does not have a fallback mechanism like offering the ability to permanently remove a file instead.  Other than that, I didn't see a proposed solution. So that's why I asked."
technical,"If I explicitly set an environment variable like this within my bashrc This did work for me, in Fedora 29. It worked for me too, on Fedora 29, but not undefinitely: after a moment, Atom could not delete files anymore, just like the variable was no more defined. Had to restart atom from a terminal, which was not an acceptable solution. On the contrary, the workaround quoted by den-is works perfectly."
technical,With flatpak i got no error but it don't get delete sorry about this misinformation. I try to disable selinux it don't change. No. Not selinux. It has to do with how trash folders work. My rather verbose explanation gets in to the nitty gritty for where atom's behavior is deficient.  The short answer is that atom only asks to move files to trash and does not delete files. If a trash folder cannot be created (multiple causes all related to permissions) than it fails. Atom should inquire with user if they should permanently delete the files instead of failing.
technical,"Gio operates the same way. I used the Gio command line to manually step through this... running fedora 29, i have installed atom from packages, since recently I faced the same error as described here. I think this bug deserves an action in order to improve the package installation, or the runtime, to detect gio/gvfs and make this work out of the box.  This really is an ugly bug for the end user experience."
technical,same here with fedora 29 and atom 1.34.0  that workaround has helped me same here with fedora 29 and atom 1.33.0x64
technical,same here with fedora 29 and atom 1.34.0  that workaround has helped me same here with fedora 29 and atom 1.34.0  that workaround has helped me
technical, Thanks for opening a new issue - is gio available on Fedora 28?
technical,Works on my Fedora 29 -- had to restart terminal and restart atom from that terminal. That is a not a fix. There are preconditions as documented above to reproduce the bug.
technical,"And if you just use gio trash from the command line it works fine?   Also, are you getting the exact same error message you mentioned (""When trying, the following message appears: Is gvfs-trash installed?"") after you set the ELECTRON TRASH environment variable to gio? The echo hello  delete-me.txt, gio trash delete-me.txt works just fine normally but crashes on the ntfs partition. As I see this, it is not an Atom bug, however the error message it kept returning me was quite misleading.  I didn't solve the problem yet, but I am closing this since it is not related to Atom. Sorry for trouble!"
technical,"This is not working on fedora 29, even with gio installed and ELECTRON TRASH variable set to gio. There are no workarounds."
technical,"This is a different issue as I have noted a couple times. Unfortunately, this unbelievably dumb error message has caused lots of confusion. There was quite a long thread so myself and others are skipping through, would be helpful to change the original post to point to your comments"
technical,"Would love for this to be fixed (on my Fedora 29 installed via the official RPM). This bug is also affecting me on a fresh install of Fedora 29 and Atom 1.33.1.  If I try to delete a file within atom, I get the following error: If I use gio on the command line:  no issue and it deletes as expected.  My system partitions are broken up like so: If I explicitly set an environment variable like this within my **~/.bashrc**:  it does delete within atom correctly.  Having to set this environment variable is less than ideal. I read through cutephoton's research, testing, and analysis and I believe mine works because **/home** and **/** are on different mounts, but the same partition. Perhaps LVM versus standard partitions?  Either way, I would imagine the handling of ""gvfs"" or ""gio"" would be done by atom and set correctly during start time of the application. I haven't read through the atom code so there may have been a reason to couple that functionality to ""gvfs"" rather than dynamically choosing a suitable trash program and application startup."
technical,"Electron introduced the fix already in October 2018 How difficult is it to get it fixed? This is a different issue as I have noted a couple times. Unfortunately, this unbelievably dumb error message has caused lots of confusion."
technical,"Is there any workaround to delete files with Atom on Fedora 29? This is not working on fedora 29, even with gio installed and ELECTRON TRASH variable set to gio."
technical,"It explicitly states in the updated comments added to the first message:  Atom does not have a fallback mechanism like offering the ability to permanently remove a file instead.  Other than that, I didn't see a proposed solution. So that's why I asked. This is the standard approach used by most software when confronted with the ""trash unavailability"" problem. If you want to be absolutely sure, put a delay on the dialog or a check box ""I really want to delete this file and understand the risks."
technical,"can reopen this This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further."
technical,"Yes, *one* solution could be to have a fallback mechanism, *like* offering the ability to permanently remove a file. You say it's not gonna happen, very well. It was just a suggestion.  Electron does support gio, Atom does not. *Another* solution could be to support gio, since gvfs-trash has been deprecated for quite a long time on Fedora (for example).  One possible solution would be to have a configuration option to define the delete command. By default it could be gvfs-trash, and user could define it to gio trash. But it **requires** the error message to be explicit.  You assumed the most common error was because gvfs-trash was not installed. This thread is the proof it's no more the case. Please, update. This solution works perfectly on Fedora 28/29. In strict terms, I would argue that this is not a bug intrinsically related to Atom (i.e., this is not an error in the source) ” if much, the ""is gvfs-trash installed"" error message is misleading to Fedora users and perhaps could be changed into something more generic ” but given that there is a workaround shouldn't this issue be closed?"
technical,I reproduced on the latest version still. **shrug** I don't see how flatpak and rpm/deb would differ. It only fails when the preconditions listed above are met -- a partition where you don't have permission to create a trash folder under your user ID. With flatpak i got no error but it don't get delete sorry about this misinformation. I try to disable selinux it don't change.
technical,"Hmm... I'm not experienced that, but I'll use it for a while to see what happens.  I would like to keep it simple and avoid ""cheat"" Atom that is using the ""real"" gvfs-trash. I don't know if usage changes or etc... But ***especially*** I want to avoid in the case that I use a computer without sudo power.  So, I hope don't have to. Thanks for your info. I'll keep an eye on it. Without sudo powers, you can place the script anywhere which is in your PATH (for example, ~/bin). On Fedora 29, the real gfvs-trash has been removed and is not meant to reappear, so you can have a script with that name without worrying about any conflict."
technical,There are no workarounds. Works on my Fedora 29 -- had to restart terminal and restart atom from that terminal.
technical,That is a not a fix. There are preconditions as documented above to reproduce the bug. Would love for this to be fixed (on my Fedora 29 installed via the official RPM).
technical,"I think it's bad user experience having to experience the bug, then google it, find this thread, add a workaround, restart the terminal, restart the session, restart Atom... Yes, as reported, the ELECTRON TRASH environment variable is needed. This is the issue referenced #15949 bug. The issue reported here is not related to #15949. The error message related to gvfs-trash is a blunt tool (even access denied errors produce the same error message) so I understand why people are responding here.  The reporter clarifies one of the key symptoms here: And that led to my deep dive. My deep dive details the specific cases where trash becomes unavailable (i.e. when a trash folder cannot be safely created). **I just revalidated my findings and unsurprisingly gio trash still fails.**  Anyhow, I wanted to clarify this so it doesn't get lost when/if atom devs look in to this issue."
technical,"Thanks for opening a new issue - is gio available on Fedora 28? Yes, gio is available and installed by default"
technical,"I would direct people to actually read the bug report before commenting. Somebody please lock down this issue. You  can  expose them, so they spawn outside of the flatpak. See this. Anyway, this is getting off-topic.  See no reason for a lock here. The issue is not really fixed or so or Fedora has to fix it (then one needs to notify them), whatever"
technical,I would direct people to actually read the bug report before commenting. Somebody please lock down this issue. your comments were deleted as a violation of the Atom Code of Conduct as they were insulting or derogatory.
technical,"I wouldn't call an underscore prefixed method ""fully"" public because the intention is private. But... well they didn't use a symbol or # to make it completely private ,) a symbol is also fully public, because it's also accessible from the outside :-) only closed-over variables and private fields are actually ""private"" in javascript."
technical,"I mean you can make the react test renderer support effects if you override one of its ""private"" (underscore prefixed) methods and then flush the hooks after each call to render. ah, sure, if hooking into fully public methods can be a reliable mechanism for enzyme, that would suffice pending the proper PRs being merged and released :-)"
technical,"I mean you can make the react test renderer support effects if you override one of its ""private"" (underscore prefixed) methods and then flush the hooks after each call to render. Any updates on this idea of creating a react-shallow-renderer package (and merging this PR into it) that can be maintained in isolation or within Enzyme?"
technical,Any updates on this idea of creating a react-shallow-renderer package (and merging this PR into it) that can be maintained in isolation or within Enzyme? btw I'm happy to donate the package name when you guys are ready. I'd also like to talk to someone about contributing to the project in some way - any pointers?
technical,"Lol. I need this in my testing life. Closed by accident, sorry."
technical,"I'm going to lock this PR because the discussion has turned in an unconstructive direction. Additionally,  said he can't comment on the PR for some reason (maybe a GitHub bug).  I opened a new issue to discuss this. I welcome everyone from this thread (but please let's keep this one civil). Fixes the effects related part of #15275 by allowing the user to tell the shallow renderer to call effect hooks.  ~It could do with a couple more tests but I wanted to get feedback on the approach and was also worried about putting any more time into it given that the maintainers have not responded to the corresponding issue in the months it's been open.~  Added a full set of tests."
technical,I'd also love to keep using enzyme to test my stateful functional components Following up on this on a weekly basis to make sure things don't go stale. Any updates?
technical,"This addition would help immensely when testing hooks with enzyme. Hopefully this gets added soon. what do you mean by hooking into private methods?  Right now I am trying to find a good way to test private functions under a functional component and I'm not having any luck with it. Functional components can't have ""methods"", that's a term that applies only to classes. If you mean, functions created inside the function, or only accessible via closure, you can't and shouldn't directly test those."
technical,"I believe part of the mismatch here is that Facebook largely/historically doesn't do serverside rendering, which is where enzyme (and the shallow renderer) is critical. I'm not aware of any scalable alternatives. How about just renaming  createDispatcher to createDispatcher. By making it part of the public interface of the class, we can hook into it and implement whatever effects we want from outside?  In the long term, to extract the shallow renderer, couldn't the shared package just be published?  But please consider implementing my first proposal. In the react-native world the shallow renderer is very popular, not being able to use (test) hooks makes us very sad."
technical,"I recognize you're frustrated with the lack of movement on this but doing personal attacks on people working on this repository is not acceptable. As I said in my earlier comment: At this point I think it would be better if we could move out the shallow renderer out of this repository.  There is literally nothing that would prevent you from doing so and collaborating with e.g. Enzyme on it. The shallow renderer can be copy pasted, published in its own package, and be developed there. I'm sorry that you were waiting for us to merge this PR, but I thought we set the expectation clear right above by saying we don't intend to invest more into shallow renderer and expanding its API surface. I don't see what kind of action you were expecting on our side. I believe part of the mismatch here is that Facebook largely/historically doesn't do serverside rendering, which is where enzyme (and the shallow renderer) is critical.  This is not correct btw, we do rely heavily on server rendering for the new Facebook."
technical,"I recognize you're frustrated with the lack of movement on this but doing personal attacks on people working on this repository is not acceptable. As I said in my earlier comment: At this point I think it would be better if we could move out the shallow renderer out of this repository.  There is literally nothing that would prevent you from doing so and collaborating with e.g. Enzyme on it. The shallow renderer can be copy pasted, published in its own package, and be developed there. I'm sorry that you were waiting for us to merge this PR, but I thought we set the expectation clear right above by saying we don't intend to invest more into shallow renderer and expanding its API surface. I don't see what kind of action you were expecting on our side. I believe part of the mismatch here is that Facebook largely/historically doesn't do serverside rendering, which is where enzyme (and the shallow renderer) is critical. I'm not aware of any scalable alternatives."
technical,"yea i think ohjames was referring to private methods on the shallow renderer itself I mean you can make the react test renderer support effects if you override one of its ""private"" (underscore prefixed) methods and then flush the hooks after each call to render."
technical,"to be honest, this response is borderline unacceptable and I don't even know what to reply. Wow.  Can someone reopen the issue? I recognize you're frustrated with the lack of movement on this but doing personal attacks on people working on this repository is not acceptable. As I said in my earlier comment: At this point I think it would be better if we could move out the shallow renderer out of this repository.  There is literally nothing that would prevent you from doing so and collaborating with e.g. Enzyme on it. The shallow renderer can be copy pasted, published in its own package, and be developed there. I'm sorry that you were waiting for us to merge this PR, but I thought we set the expectation clear right above by saying we don't intend to invest more into shallow renderer and expanding its API surface. I don't see what kind of action you were expecting on our side."
technical,"thanks for figuring this out! If you want to copy the logic for componentDidMount and componentDidUpdate, you can see it at #15589 I thought it would be good to handle the life-cycle events in a different PR. Since all my code is using hooks now, I'm not so interested in them. Plus it's easy to call them from outside if you need them, I see that enzyme already does that."
technical,"ah, sure, if hooking into fully public methods can be a reliable mechanism for enzyme, that would suffice pending the proper PRs being merged and released :-) I wouldn't call an underscore prefixed method ""fully"" public because the intention is private. But... well they didn't use a symbol or # to make it completely private ,)"
technical,This will make unit tests with React.16 easier I'd also love to keep using enzyme to test my stateful functional components
technical,"I believe part of the mismatch here is that Facebook largely/historically doesn't do serverside rendering, which is where enzyme (and the shallow renderer) is critical.  This is not correct btw, we do rely heavily on server rendering for the new Facebook. I'm going to lock this PR because the discussion has turned in an unconstructive direction. Additionally,  said he can't comment on the PR for some reason (maybe a GitHub bug).  I opened a new issue to discuss this. I welcome everyone from this thread (but please let's keep this one civil)."
technical,Following up on this on a weekly basis to make sure things don't go stale. Any updates? I'm late to the party but would like to say testing is important hence this pull request is important. Please merge. Thx
technical,"I thought it would be good to handle the life-cycle events in a different PR. Since all my code is using hooks now, I'm not so interested in them. Plus it's easy to call them from outside if you need them, I see that enzyme already does that. Lol. I need this in my testing life."
technical,"The shallow renderer uses the following modules from ""shared"" that I don't think are externally reachable Note that I've previously manually extracted shallowEqual, and getComponentName - but it'd be *much* better if I could deprecate both of those to use official React packages, or alternatively, if React could use those instead of its current implementation.  Also, is exists as this (same thing tho, it'd be ideal if React just used that package)."
technical,"Is there someone I should be -ing from the react team to get some  on this? Seems to be quite a lot of interest, but not a single issue on this topic has received a comment from a maintainer in 5 months. Maybe they would have dropped in to tell us it won't be accepted and not to waste any effort on it?  Just wanted to chime in and say I strongly feel the same way - it would be at least good to get a ""hey we won't be considering this"". Sorry I haven't replied earlier!  Generally we've been using shallow renderer less and less at Facebook because it encourages brittle tests that verify implementation detail. So components that relied on it a lot introduced a lot of refactoring friction, and didn't catch valuable regressions anyway for us. (Your experience might differ.)  The main value proposition of shallow rendering is that you mock out the inner components. We see the value in this, but in our experience doing this at the module system level gives you more leeway when refactoring. For example, you can always jest.mock('Button', () = 'button') to replace a child Button component with a mock. We've mostly been happy with this approach and found it more flexible.  Since we don't use the shallow renderer much anymore, we don't feel like we would be good stewards of its API, or even can recommend it. Additionally, Enzyme has been adding different behavior to it (such as calling class commit phase lifecycles) that we didn't originally plan. At least some of it was due to a misunderstanding (for example, React intends to guarantee refs are resolved in the commit phase, but running code with shallow renderer forces you to handle null gracefully). But there is also some mismatch between how Enzyme uses it internally, and how we initially thought it would be used.  **At this point I think it would be better if we could move out the shallow renderer out of this repository.** It's stagnating here, getting fixes very late, and causes frustration like in this PR. By design, it doesn't use almost any of the React code, so it should be easy to move. On the other hand, Enzyme is in active development and might as well want to take ownership of this piece.  If davidmarkclements were kind to offer the react-shallow-renderer package name, we could externalize it there, and maybe it could be maintained in the Enzyme repository or elsewhere instead.  Would anyone like to propose a plan? Thanks."
technical,"Sorry I haven't replied earlier!  Generally we've been using shallow renderer less and less at Facebook because it encourages brittle tests that verify implementation detail. So components that relied on it a lot introduced a lot of refactoring friction, and didn't catch valuable regressions anyway for us. (Your experience might differ.)  The main value proposition of shallow rendering is that you mock out the inner components. We see the value in this, but in our experience doing this at the module system level gives you more leeway when refactoring. For example, you can always jest.mock('Button', () = 'button') to replace a child Button component with a mock. We've mostly been happy with this approach and found it more flexible.  Since we don't use the shallow renderer much anymore, we don't feel like we would be good stewards of its API, or even can recommend it. Additionally, Enzyme has been adding different behavior to it (such as calling class commit phase lifecycles) that we didn't originally plan. At least some of it was due to a misunderstanding (for example, React intends to guarantee refs are resolved in the commit phase, but running code with shallow renderer forces you to handle null gracefully). But there is also some mismatch between how Enzyme uses it internally, and how we initially thought it would be used.  **At this point I think it would be better if we could move out the shallow renderer out of this repository.** It's stagnating here, getting fixes very late, and causes frustration like in this PR. By design, it doesn't use almost any of the React code, so it should be easy to move. On the other hand, Enzyme is in active development and might as well want to take ownership of this piece.  If davidmarkclements were kind to offer the react-shallow-renderer package name, we could externalize it there, and maybe it could be maintained in the Enzyme repository or elsewhere instead.  Would anyone like to propose a plan? Thanks. That sounds like a great plan, as long as the react team can provide some kind of commitment to expose API as needed, and ideally include tests as part of core - so that enzyme (and the shallow renderer) doesn't unintentionally break with every release."
technical,"We'd be happy to run some tests against a separate package fwiw. Similar to how we have integration tests with create-react-class. The shallow renderer uses the following modules from ""shared"" that I don't think are externally reachable"
technical,"Yeah, it'll take a while. With a draftjs change I was following, took a decent amount of time. But it happened!  For the hooking into private methods, could you link a gist or some examples of how to do that you could add to your PR comment? I think that helped with this draftjs PR” provide a way for people to rest in the wild and confirm? Or post to the Enzyme issue related to this. If it's not too difficult? This addition would help immensely when testing hooks with enzyme. Hopefully this gets added soon. what do you mean by hooking into private methods?  Right now I am trying to find a good way to test private functions under a functional component and I'm not having any luck with it."
technical,btw I'm happy to donate the package name when you guys are ready. I'd also like to talk to someone about contributing to the project in some way - any pointers? This will make unit tests with React.16 easier
technical,"How about just renaming  createDispatcher to createDispatcher. By making it part of the public interface of the class, we can hook into it and implement whatever effects we want from outside?  In the long term, to extract the shallow renderer, couldn't the shared package just be published?  But please consider implementing my first proposal. In the react-native world the shallow renderer is very popular, not being able to use (test) hooks makes us very sad. This would be quite useful to me and to other experienced TDD practitioners who work with react.  In addition to being on the order of 100x faster to run as compared to tests that require a stubbed DOM (i.e. JSDOM), shallow rendering allows for test-driving the designs of components much more efficiently/directly than the equivalent virtual DOM tests. Certain classes of components (for example: Providers that encapsulate state machines and provide to their children via context) are much, much easier to test-drive with shallow approaches.  Please consider merging this or otherwise exposing hooks to easily integrate effects into shallow test tools like enzyme in a way that is reliable and relatively future-proof."
technical,What API do you expect to need? Shallow renderer doesn't depend on any other code so I don't expect it to need private APIs. The only one I can think of is the way Hooks Dispatcher is exposed from the react package ” but that isn't Enzyme-specific. We'd be happy to run some tests against a separate package fwiw. Similar to how we have integration tests with create-react-class.
technical,"That sounds like a great plan, as long as the react team can provide some kind of commitment to expose API as needed, and ideally include tests as part of core - so that enzyme (and the shallow renderer) doesn't unintentionally break with every release. What API do you expect to need? Shallow renderer doesn't depend on any other code so I don't expect it to need private APIs. The only one I can think of is the way Hooks Dispatcher is exposed from the react package ” but that isn't Enzyme-specific."
technical,"Functional components can't have ""methods"", that's a term that applies only to classes. If you mean, functions created inside the function, or only accessible via closure, you can't and shouldn't directly test those. yea i think ohjames was referring to private methods on the shallow renderer itself"
technical,"Nothing more we can really do for this until the maintainers have a look. They haven't commented on any of the related issues and they've been open for months. It's possible to get this working in your own extension of this library by hooking into one private method on ReactShallowRenderer, that's what I'm doing for now. Enzyme could possibly do the same. Yeah, it'll take a while. With a draftjs change I was following, took a decent amount of time. But it happened!  For the hooking into private methods, could you link a gist or some examples of how to do that you could add to your PR comment? I think that helped with this draftjs PR” provide a way for people to rest in the wild and confirm? Or post to the Enzyme issue related to this. If it's not too difficult?"
technical,"Women can also be a construction worker Absolutely, why would you even think otherwise? In this case, I am the one doing the construction work and I happen to be a man... I think."
technical,"I guess the same reason why GitHub defaults to 'main' master branch name now. Good old PC BS. Apparently word 'master' has a bad stigma attached to it now '‚ Guys & gals, I'm sorry. Nothing bad happened here, but I'm just going to lock this thread right now before it does. As a team, we've decided to go ahead with this rename, because reasons. This issue isn't the place to discuss the merits and drawbacks of the rename. If you feel like venting, I recommend you do so through your own personal mechanisms. Mine is running 10k out in the cold. "
technical,May I ask why is this necessary? Did I miss something? I guess the same reason why GitHub defaults to 'main' master branch name now. Good old PC BS. Apparently word 'master' has a bad stigma attached to it now '‚
technical,"Absolutely, why would you even think otherwise? In this case, I am the one doing the construction work and I happen to be a man... I think. May I ask why is this necessary? Did I miss something?"
technical,"Guys & gals, I'm sorry. Nothing bad happened here, but I'm just going to lock this thread right now before it does. As a team, we've decided to go ahead with this rename, because reasons. This issue isn't the place to discuss the merits and drawbacks of the rename. If you feel like venting, I recommend you do so through your own personal mechanisms. Mine is running 10k out in the cold.  On **Feb 14, 2021, 8PM UTC** the microsoft/vscode repository will rename its master branch to main.   **Call to Action**   If you have a clone of microsoft/vscode locally, you'll need to run the following commands ** after ** the rename happens"
technical,"Can you give indications on where the changes should happen and other indications? I am a bit lost in the source code   We do plan to work on this issue but we don't have an ETA for when we'll  be able to get to it. If people would like to help this happen sooner than we can get to it, we would be very interested in a well-written pull request. abumalick I would start with the code governing selections and drill in from there"
technical,"I'm agree with you . But in this situation the only thing that you can do is this :) Although Majid-Kaffash solution isn't perfect, but it works until there's a better solution. Is there anyway I can make a keybind that toggles this edit in the stylesheet?"
technical,Any updates on this issue? Any update for RTL ?
technical,"Hello my friends, any updates regarding RTL languages in Atom? Any update on this?!"
technical,"To devs, consider reading  and 2 for getting some context. Any updates on this issue?"
technical,"iamsoorena since this is happening in text editors, I would start by looking at text-editor-component.js Any updates on this issue?"
technical,"Brackets works too with RTL language. I think atom and brackets are based on electron ? Maybe it can help developers : I really want this feature in atom  Brackets is based on CodeMirror. CodeMirror has bidi support with mild bugs, the main constraint is all lines are left-aligned with LTR base direction: There was a GSoC leading to experimental branch improving things, including per-line direction, but it hasn't landed.  Atom does not use CodeMirror, IIUC atom implemented their own editor using electron."
technical,Microsoft's Visual Studio Code works perfectly in this case. Maybe Developers can inspire from its opensource codes? Brackets works too with RTL language. I think atom and brackets are based on electron ? Maybe it can help developers : I really want this feature in atom 
technical,just use vs code Did VS code support RTL label now?  But I didn't  find the way in it's home  page
technical,"I stated above, if people want to help speed up the process we would definitely welcome the help. The reality is that our resources are finite and there will be things that we can't get to as quickly as some would want. The awesome part about open source is that everyone can work on the bits of functionality that are important to them and the whole system gets better. does anyone know which source files are related to this issue? If someone wants to help, he/she will need some context."
technical,"Not a solution for ""actual"" RTL support (it probably even makes it very hard to read RTL text), but this way at least bidirectionality doesn't destroy the text (and selection) flow, so it's at least possible to work with files with bidirectional text: **Edit 2017-01-12:** Small update, because of the shadow DOM changes in 1.13.0, the following code should be added to the atom stylesheet instead: Every update is making it even worse. I was solving this problem in a tricking way by inverse selecting words. For example, previously to select, I double-clicked on  and  was being selected.  But now in latest version 1.12.7, even this trick isn't working."
technical,"I switched to vscode  because of this issue. It supports arabic very well, is open source,  have a lot of plugins too and is very fast. I did love atom but I really need arabic Fortunately version 1.15 optimized it. Now updated to 1.16 and this bug still is annoying. I'm really thinking to switch to VSCode. Is there any plan to work on this issue?"
technical,"I won't complain and I agree with lee-dohm . If we RTLers want to get it better then we have to contribute. unfortunatly I don't have the knwledge about developing IDEs and Editors. Hello my friends, any updates regarding RTL languages in Atom?"
technical,"Fortunately version 1.15 optimized it. Now updated to 1.16 and this bug still is annoying. I'm really thinking to switch to VSCode. Is there any plan to work on this issue? I can ask around and try to get back to you. (If I don't respond in a week, ping me again)"
technical,"Thanks, at least we can edit lines containing Arabic text when adding your code to atom stylesheet I have the same problem when I try to write in Arabic, it's really confusing for highlighted text. Any solutions?"
technical,Any updates on this issue? I Installed yesterday and problem still exists.I think its not a good idea to change the Editors entire direction for every time you want to edit a none English word or sentence. even with text-align:left
technical,"We need this to be fixed, coding shouldn't be just for ltr languages. I'm just surprised that this is overlooked still after 4 years. I stated above, if people want to help speed up the process we would definitely welcome the help. The reality is that our resources are finite and there will be things that we can't get to as quickly as some would want. The awesome part about open source is that everyone can work on the bits of functionality that are important to them and the whole system gets better."
technical,"I have the same problem when I try to write in Arabic, it's really confusing for highlighted text. Any solutions? I switched to vscode  because of this issue. It supports arabic very well, is open source,  have a lot of plugins too and is very fast. I did love atom but I really need arabic"
technical,"Sorry, no updates - the team hasn't had time to work on this and Lee's earlier comments still apply. I won't complain and I agree with lee-dohm . If we RTLers want to get it better then we have to contribute. unfortunatly I don't have the knwledge about developing IDEs and Editors."
technical,I Installed yesterday and problem still exists.I think its not a good idea to change the Editors entire direction for every time you want to edit a none English word or sentence. even with text-align:left I'm agree with you . But in this situation the only thing that you can do is this :)
technical,"does anyone know which source files are related to this issue? If someone wants to help, he/she will need some context. iamsoorena since this is happening in text editors, I would start by looking at text-editor-component.js"
technical,"vscode is good but i think its ftp plugins are not efficient in the comparison of atom... In general, we don't mind the odd comment about other tools that might be better for some people's uses. The back-and-forth conversation about things that aren't Atom is off-topic though and should be taken elsewhere."
technical,please fix the problemmm just use vs code
technical,"Although Majid-Kaffash solution isn't perfect, but it works until there's a better solution. Is there anyway I can make a keybind that toggles this edit in the stylesheet? Microsoft's Visual Studio Code works perfectly in this case. Maybe Developers can inspire from its opensource codes?"
technical,"Brackets is based on CodeMirror. CodeMirror has bidi support with mild bugs, the main constraint is all lines are left-aligned with LTR base direction: There was a GSoC leading to experimental branch improving things, including per-line direction, but it hasn't landed.  Atom does not use CodeMirror, IIUC atom implemented their own editor using electron. Not a solution for ""actual"" RTL support (it probably even makes it very hard to read RTL text), but this way at least bidirectionality doesn't destroy the text (and selection) flow, so it's at least possible to work with files with bidirectional text: **Edit 2017-01-12:** Small update, because of the shadow DOM changes in 1.13.0, the following code should be added to the atom stylesheet instead:"
technical,abumalick I would start with the code governing selections and drill in from there please fix the problemmm
technical,"Any update for RTL ? Sorry, no updates - the team hasn't had time to work on this and Lee's earlier comments still apply."
technical,"Why atom team don't like to fix this issue? I think, you have many professional developers and why this F... RTL problem is not important for you? how a developer can develop a Multilanguage learning app without RTL? Thanks everyone for your feedback. We understand and agree that this is an important issue, that's why it is still open. On the other hand, we don't have an ETA for when we'll be able to get to it. If people would like to help this happen sooner than we can get to it, we would be very interested in a well-written pull request.  Because the above message keeps getting lost in the chatter and since there is nothing new being said here, I'm going to lock the issue for further comments."
technical,"Every update is making it even worse. I was solving this problem in a tricking way by inverse selecting words. For example, previously to select, I double-clicked on  and  was being selected.  But now in latest version 1.12.7, even this trick isn't working. Thanks, at least we can edit lines containing Arabic text when adding your code to atom stylesheet"
technical,Any update on this?! This issue should be fixed.
technical," To devs, consider reading  and 2 for getting some context."
technical,"We used in VS code in past 6 months and now I can say It's better than atom even if Microsoft developing it. It's have way way more efficiency about typescript and tslint and as I can tell uses less resource in my ubuntu in compare to atom. And finally as a Persian we have much better experience with VS code.  I hope atom's team don't mind about my comment, they have good product and I used it over a year but It's good for themselves (LTR languages runs on computers with giant resources) not for me! vscode is good but i think its ftp plugins are not efficient in the comparison of atom..."
technical,"We used in VS code in past 6 months and now I can say It's better than atom even if Microsoft developing it. It's have way way more efficiency about typescript and tslint and as I can tell uses less resource in my ubuntu in compare to atom. And finally as a Persian we have much better experience with VS code.  I hope atom's team don't mind about my comment, they have good product and I used it over a year but It's good for themselves (LTR languages runs on computers with giant resources) not for me! We do plan to work on this issue but we don't have an ETA for when we'll be able to get to it. If people would like to help this happen sooner than we can get to it, we would be very interested in a well-written pull request."
technical,"When this ability will be added to Atom? We need this to be fixed, coding shouldn't be just for ltr languages. I'm just surprised that this is overlooked still after 4 years."
technical,"Did VS code support RTL label now?  But I didn't  find the way in it's home  page We used in VS code in past 6 months and now I can say It's better than atom even if Microsoft developing it. It's have way way more efficiency about typescript and tslint and as I can tell uses less resource in my ubuntu in compare to atom. And finally as a Persian we have much better experience with VS code.  I hope atom's team don't mind about my comment, they have good product and I used it over a year but It's good for themselves (LTR languages runs on computers with giant resources) not for me!"
technical,"In general, we don't mind the odd comment about other tools that might be better for some people's uses. The back-and-forth conversation about things that aren't Atom is off-topic though and should be taken elsewhere. When this ability will be added to Atom?"
technical,"Thanks everyone for your feedback. We understand and agree that this is an important issue, that's why it is still open. On the other hand, we don't have an ETA for when we'll be able to get to it. If people would like to help this happen sooner than we can get to it, we would be very interested in a well-written pull request.  Because the above message keeps getting lost in the chatter and since there is nothing new being said here, I'm going to lock the issue for further comments. When trying to select a RTL substring, the highlighted text seems to be current but if you just press delete button, It will delete another part of the text instead of highlighted.  also double clicking on the text will highlight another word/part of text.  It behaves like the position of the mouse is horizontally invert on the RTL text.  Simulate it: First Test: Select  With mouse down and mouse up and Press Delete button, and it will delete  instead of : Second Test: Double click on  will be Highlighted but if you press delete,  will be deleted."
technical,"Please don't! We'll just have to close it as a duplicate of a feature that made it into v0.3 actually Ctrl+C works fine in powershell in windows terminal, the real problem is the support to wsl, when I use wsl2 in windows terminal, for example, a ubuntu18.04, I can't use Ctrl+C or Ctrl+Shift+C, but in bash or wsl.exe, it works fine."
technical,"Any idea when this will be fixed? Copying is a fairly standard function... Also, shouldn't this be open since CTRL+C still doesn't copy?"
technical,We'll track all copy/selection issues as part of #524 Any idea when this will be fixed? Copying is a fairly standard function...
technical,"No this should be closed, since the decided upon feature is customisable binding not hard-coded use of ctrl+c. Customisable is great! - but what's the default setting out of the box?"
technical, from what I remember this function has not yet been added to the terminal2 it still implements the copy -paste protocol from cmd and PowerShell. highlight text and press <enter to copy and <rightclick to paste the clipboard #1180
technical,"I am so tempted to chime in with something more confrontational but I don't think it'll add to the debate so I'll just say this:  - The devs have been very responsive, and have been  accommodating of the many (often mutually conflicting) requests coming in from users. By this token the ""old Microsoft"" assertion seems unfounded and potentially disheartening for the people who have plowed a lot of work in to this. - This is a beta product, if you expect it to meet all your needs straight away you probably need to realign your expectations. - Some decisions will be made that won't work in your favour. Many won't work in my favour. However these are educated decisions made in the interest of supporting the average user (not just you), by talented devs who are acknowledging some compromise but are making pragmatic decisions nonetheless. By assuming your needs are paramount, you are potentially dismissing the needs of other users (e.g. WSL users like me who support the sensible decision to align with Gnome bindings and avoid escape sequence characters for default bindings). - This is open source. If you feel you have the expertise to override their decisions, then you are free to implement changes in your own fork. If you don't have that expertise, you can't reasonably claim that the decision taken is actually wrong. ""It's not what I'm familiar with"" is probably not a good enough justification (in my view).  I won't reply to responses as I know it won't add anything and that I shouldn't have even posted the above, but I couldn't resist throwing my two cents in - sorry! I installed the terminal from the windows app store, created a powershell tab, ran a command.  I then wanted to copy that command text, so I highlighted it, then pressed ctrl + c but it doesn't appear to have copied the command text. I also tried ctrl + shift + c. Getting desperate I right clicked on the tab item itself, looking for a menu where I could select ""copy"" but no such menu seems to exist. I also tried right clicking with the text selected hoping for a menu where I could select ""copy selection"" but again no such menu exists. In desperation I gave up - copying text does not appear possible. What am I doing wrong?"
technical,"I don't think this is fixed?  The changes in #1093:  - set the default binding to ctrl+shift+c - and even if we could rebind the default to ctrl+c, doesn't implement the copy-if-text-selected-or-send-ctrl+c semantics discussed in I apologise if I've misread any of the code.  I've been fighting years of muscle memory this last week because ctrl+c doesn't work, I realise it may seem like a triviality, but I think it's important for user experience. I'll reopen it for our next Triage cycle."
technical,"Also, shouldn't this be open since CTRL+C still doesn't copy? No this should be closed, since the decided upon feature is customisable binding not hard-coded use of ctrl+c."
technical,"Also, shouldn't this be open since CTRL+C still doesn't copy? Nope, it wasn't.  Making Ctrl+Shift+C the keybinding for copy is pointless. You may as well make it anything else Ctrl+P. People expect Ctrl+C and using anything except that will just make the copy feature undiscoverable."
technical,Shoud we open a new issue requesting a feature to enable CTRL+C / CTRL+V  to copy/paste? Please don't! We'll just have to close it as a duplicate of a feature that made it into v0.3
technical,from what I remember this function has not yet been added to the terminal2 it still implements the copy -paste protocol from cmd and PowerShell. highlight text and press <enter to copy and <rightclick to paste the clipboard #1180 Right clicking the selected text will copy it to the clipboard.  I'd like a way to do this that uses the keyboard instead.
technical,Right clicking the selected text will copy it to the clipboard.  I'd like a way to do this that uses the keyboard instead. Shoud we open a new issue requesting a feature to enable CTRL+C / CTRL+V  to copy/paste?
technical,"Customisable is great! - but what's the default setting out of the box? The default keybindings for copy and paste are Ctrl+Shift+C/V. If your profiles.json file was created before #1093, then they won't contain that keybinding, and you'll need to add it manually."
technical,It would be nice to have shift+insert or Ctrl+shift+insert to have a linux like feeling or ctrl+shift+c and ctrl+shift+v like powershell does This was fixed with #1093
technical,I'll reopen it for our next Triage cycle. We'll track all copy/selection issues as part of #524
technical,"I think that he is on the right track in a sense that maybe if it is to hard to come up with general solutions for this problem(automatic one) maybe you should think of giving developers an option to specify problematic area(to help Flutter), I think that most of us know what animations are the problem and in which stage of app runtime. Actually doing an --optimization run option, would make absolute sense and is also good to sell :-)"
technical,"he discusses this in his comment on the linked issue. I just reread this reply and I think it doesn't explain in any way why we couldn't realise an automatic prewarming of Metal shaders after an optimization run of the app. He only talks about caching or bundling precompiled shaders,not of automatic prewarming on the first run. I m happy to join a direct discussion with everyone involved here. Maybe coming from the outside can give some impulses. Another thing I don't understand is while we can't ship Metal Shader binaries we probably could ship them in MSL instead of Skls saving so one translation step. Which was pointed out as one time consuming step in another comment."
technical,"A few points.   * On the Flutter site, this issue is still not clearly shown. I spent a week of full time effort learning Flutter, only to find out about this issue hidden in the shader optimization article. I feel misled because everything I first saw on homepage etc, promised 60 FPS and native performance. I think its bordering on unethical to not make this problem clearly known to new developers.  * This issue should be a higher priority than anything else, instead it still seems like its a P3 or lower. No one is even assigned to this issue yet?  * There should be an ETA / timeline for a resolution.   It looks like there's a solution available - running animations in the background to warm up the shader. We just need something around that to be made official.  To be honest. Flutter being free in this case makes it only worse. I wish it was paid so we would not get the it's free, whatever you get be happy response. At least we would not get a 100 line code commit after 1 year on such a huge issue. do you have any comments on the idea to cache and distribute MSL? Also is this an issue where you will track further progress on this issue? Do you have some kind of document or could you create one in which you would describe the setup to tray to debug this so maybe somebody also could get involved and maybe we could get more ideas? And the final question is are you the only person that is working on this?"
technical,"IOW, you want to try to capture relevant frame(s) from your actual widgets/transitions, and rasterize them so that the needed shader(s) get compiled up front. Does it make sense to put that widget in a stack below the content of login page, this way it is not going to be visible to the user, but is Fluttter going to send that widget to GPU to create shader since it is not visible(it is in stack below some other content)?"
technical,"So there's nothing concrete that's being done yet? Since you said: Guys, I think we should start reaching out to the press about this issue. Ultimately the priorities of the flutter dev team are set by business stakeholders, I think. If there's enough negative press about this issue, perhaps that will tilt their priorities enough that they'll assign more resources to this issue rather than to desktop / other platforms. Flutter is an open source project. The work that gets done is the work that contributors (some of which are employed by companies who work on Flutter) want to do based on their own priorities. You are absolutely welcome to participate. Some contributors (including myself) like to work on features and bugs that are useful for the wider community. However, acting in an entitled fashion is a violation of our code of conductbecause it is highly unpleasant for those who  are  contributing, and it is unlikely to garner you any friends or encourage anyone to work on features or bugs that would help you.  To summarise:  * If you are running into specific issues, you should file bugs with code that shows how to reproduce the problem you are running into. Nothing else will result in any improvements for issues you are seeing, especially not commenting on draft PRs. * If you have concrete ideas for things you would like implemented in Flutter, file a bug or create a design doc as is described in our contributing guides. Commenting elsewhere may lead to interesting discussions and might possible catch someone's attention but it is much more likely that you will get traction by filing a bug than doing anything else.  People who contribute to Flutter, whether that's Google, or any of a number of other companies, or any of many individuals who are not paid to do so, have their own set of priorities which are not necessarily aligned with yours. Our wiki describes how some of us determine what to work on next, but contributors are under no obligation to pay any attention to those suggestions and can prioritize work however they see fit.  If you have a particular desire then the best thing you can do is contribute or pay someone to contribute. This isn't unique to Flutter, it's true of all open source software. We have literally over a hundred contributors (many more than work on the Flutter team at Google), you can be one too."
technical,"I'm grateful to the Flutter developers for their work on Flutter, and for making it available for free.  A few points.  - On the Flutter site, this issue is still not clearly shown. I spent a week of full time effort learning Flutter, only to find out about this issue hidden in the shader optimization article. I feel misled because everything I first saw on homepage etc, promised 60 FPS and native performance. I think its bordering on unethical to not make this problem clearly known to new developers.  - This issue should be a higher priority than anything else, instead it still seems like its a P3 or lower. No one is even assigned to this issue yet?  - There should be an ETA / timeline for a resolution.  It looks like there's a solution available - running animations in the background to warm up the shader. We just need something around that to be made official. Flutter team itself said that they considered the option to go with OpenGL and Metal, but they didn't. They decided to go with Metal only. What I don't understand is how did they not see this issue when they were testing? I don't believe switching over to Metal was a 3 minute decision."
technical,"hm...  Will increasing a duration of showing Launch Screen give enough time to launch engine and load shaders to use it? from my understanding Flutter only creates the shaders when it needs them the first time, which makes sense because why create shaders for widgets you never use. My guess is if you create the widgets where you encounter the jank already behind your Startup screen, that could help, but I' not sure if shaders will be created if the widgets are hidden."
technical,"He and I talked about this a bit offline.  Caching the MSL has a few challenges:  - Apple doesn't seem to provide a good way to fetch the binary archives from the device/application for caching. - It still has many disadvantages, such as not necessarily knowing when you'd need to do this, whether your cached shaders are still valid or not, whether you're missing an important one, etc.  We're trying to come up with a more comprehensive solution for this - one that would clearly let developers know when their scene might be complex enough to result in lengthy shader compilation, and what options they have to mitigate that. from my understanding the shaders we are talking of don't get there source dynamically or are they? So you wouldn't even need to cache them you could directly bundle the precompiled metal shaders. In contrary to OpenGL you always know that the metal API will be there."
technical,"do you have any comments on the idea to cache and distribute MSL? Also is this an issue where you will track further progress on this issue? Do you have some kind of document or could you create one in which you would describe the setup to tray to debug this so maybe somebody also could get involved and maybe we could get more ideas? And the final question is are you the only person that is working on this? He and I talked about this a bit offline.  Caching the MSL has a few challenges:  - Apple doesn't seem to provide a good way to fetch the binary archives from the device/application for caching. - It still has many disadvantages, such as not necessarily knowing when you'd need to do this, whether your cached shaders are still valid or not, whether you're missing an important one, etc.  We're trying to come up with a more comprehensive solution for this - one that would clearly let developers know when their scene might be complex enough to result in lengthy shader compilation, and what options they have to mitigate that."
technical,"I think I also read somewhere that on iOS, shaders are precompiled for a certain set of animations, and they limit you in which animations you can use which allows them to cache them upfront. If that's the case, perhaps we should do the same and at least have a certain set of animations supported by Flutter for which pre-compiled shaders are provided. Things like page transitions, scaling, rotating, etc. Hixie Is there an issue where we can track the progress of the new widget type that you've mentioned? he discusses this in his comment on the linked issue. I just reread this reply and I think it doesn't explain in any way why we couldn't realise an automatic prewarming of Metal shaders after an optimization run of the app. He only talks about caching or bundling precompiled shaders,not of automatic prewarming on the first run. I m happy to join a direct discussion with everyone involved here. Maybe coming from the outside can give some impulses."
technical,"OK, then even more manually, add a new run option that creates a file of all used shader while you are running the app playing through all the janking areas. This file could then used for the next built to prewarm this shaders.  This is even better than doing it manually or parsing code. shader describes how to do just that. It isn't something we can enable on iOS/Metal though, if I understand correctly, due to limitations in Apple's APIs.  Our current area of study is a widget that would allow shaders for a part of the scene to compile asynchronously (in the background, while another, potentially animating, scene remains on the screen). This isn't trivial, unfortunately, for example, it wouldn't help if the next scene itself transitioned into yet another scene and  that  scene needed shaders to be compiled, since you'd still end up needing to compile those shaders midway through the animation. It is, however, more similar to what iOS does natively, if I understand correctly. he discusses this in his comment on the linked issue. My understanding is that they do something similar to what I describe above, holding the previous scene until the next one is ready. But I'm not an iOS expert, so my information is second-hand."
technical,I recommend diving a bit deeper in the world of shaders an crossplatform GPU programming before making such a statement. Shaders have to be compiled before you can use them the first time. There is no way around it. So caching the compiled shaders is the usual way you do this.  My guess is that iOS native Apps use shaders that are already loaded by the operating system. hm...  Will increasing a duration of showing Launch Screen give enough time to launch engine and load shaders to use it?
technical,"I recommend diving a bit deeper in the world of shaders an crossplatform GPU programming before making such a statement. Shaders have to be compiled before you can use them the first time. There is no way around it. So caching the compiled shaders is the usual way you do this.  My guess is that iOS native Apps use shaders that are already loaded by the operating system. I agree that the first couple of page transitions are the biggest issue as mentioned above. Would it be helpful if we added some of the most used skia draw methods for page transitions in the DefaultShaderWarmUp? I must say I am not an expert on this, if I say something stupid I apologize :)"
technical,"Just as an info what Fortnite did with Apple to speed up the their game startup time, I guess that difference is that game creators know upfront all the possible animations that can happen and Flutter doesn't. I pondered a bit more about this and maybe we should think about tackling this problem from a different angle.  Instead of trying to find a general way to optimize the caching or precompiling in a general way which would be great but seems difficult, why not try to give the engine some help. I could imagine several approaches:  * give all Widgets a static  preloadShaders method, so I can call that for the widgets that make the problems while my loading screen is displayed. This might feel cumbersome, but it would be a very direct and pragmatic way and probably fast to implement. preloadShaders could even save the created shaders, so that they can be used the next time faster. I'm convinced that every iOS Developer would be happy to have this, even if it needs some more lines of code.  * I could imagine that this could be automated if we give all Widgets/RenderObjects an attribute with the names of the shaders that this Widget uses. Then an App could be parsed for all used Widgets and generate so the the necessary prewarming code.  what do you think dnfield ?"
technical,"Anyone else has opinion that cashing is just a crutch? Looks like, it is just attempt to run away from the problem and try to hide it.  Guys from Flutter team, you spend tone of time to learn algorithms, data structure, low level programming for get work in Google and for now is it actually that type of solution you wanna provide being hired by the one of the best company in the world? Crutch?  All application on flutter just lagging, even Google applications, everyone understand that cashing would probably decrease lagging, but not remove them all. When you solve this issue(instead of provide workaround) Flutter will be best SaaS decision in the world, even for iOS users!  Believe in you guys. I recommend diving a bit deeper in the world of shaders an crossplatform GPU programming before making such a statement. Shaders have to be compiled before you can use them the first time. There is no way around it. So caching the compiled shaders is the usual way you do this.  My guess is that iOS native Apps use shaders that are already loaded by the operating system."
technical,"Anyone else has opinion that cashing is just a crutch? Looks like, it is just attempt to run away from the problem and try to hide it.  Guys from Flutter team, you spend tone of time to learn algorithms, data structure, low level programming for get work in Google and for now is it actually that type of solution you wanna provide being hired by the one of the best company in the world? Crutch?  All application on flutter just lagging, even Google applications, everyone understand that cashing would probably decrease lagging, but not remove them all. When you solve this issue(instead of provide workaround) Flutter will be best SaaS decision in the world, even for iOS users!  Believe in you guys. I think Dan meant ""here"" as in on GitHub, but in general yes please, keeping issues and PRs focused on one topic is a much better way of using GitHub because otherwise we end up with ""centithreads"" and GitHub starts hiding comments which buries information and ideas."
technical,"Anyone else has opinion that cashing is just a crutch? Looks like, it is just attempt to run away from the problem and try to hide it.  Guys from Flutter team, you spend tone of time to learn algorithms, data structure, low level programming for get work in Google and for now is it actually that type of solution you wanna provide being hired by the one of the best company in the world? Crutch?  All application on flutter just lagging, even Google applications, everyone understand that cashing would probably decrease lagging, but not remove them all. When you solve this issue(instead of provide workaround) Flutter will be best SaaS decision in the world, even for iOS users!  Believe in you guys. I think he is writing a design doc, hopefully he can share it soon."
technical,"he discusses this in his comment on the linked issue. My understanding is that they do something similar to what I describe above, holding the previous scene until the next one is ready. But I'm not an iOS expert, so my information is second-hand. I think I also read somewhere that on iOS, shaders are precompiled for a certain set of animations, and they limit you in which animations you can use which allows them to cache them upfront. If that's the case, perhaps we should do the same and at least have a certain set of animations supported by Flutter for which pre-compiled shaders are provided. Things like page transitions, scaling, rotating, etc. Hixie Is there an issue where we can track the progress of the new widget type that you've mentioned?"
technical,"he discusses this in his comment on the linked issue. My understanding is that they do something similar to what I describe above, holding the previous scene until the next one is ready. But I'm not an iOS expert, so my information is second-hand. I think that he is on the right track in a sense that maybe if it is to hard to come up with general solutions for this problem(automatic one) maybe you should think of giving developers an option to specify problematic area(to help Flutter), I think that most of us know what animations are the problem and in which stage of app runtime."
technical,"The problem with this approach is you can't do it with a screen transition. I tried this even putting a hidden navigator behind the current screen with a stack and pushing/popping 3-5 times behind a loading screen, and it had no impact on the perceived jank after that. I think we're starting to get a bit off topic here.  Very briefly, the need for shader compilation has to do with Flutter allowing you to dynamically create lots of different pixel configurations on the GPU. In theory, one could try to write a lot of that logic itself as a shader program, compile that ""uber shader"" once, and then never suffer from shader compilation jank during application run. In practice, that's extremely difficult to do correctly and may not even be possible to do performantly - shaders want to be small, well defined programs typically, and supporting everything that Flutter can do with such a program (on all the GPUs we have to support) is a very large problem space. Caching smaller programs can help, but it is not an ideal solution since different GPU/OS/driver versions can cause cache misses. This could be a problem even on iOS, where different phone/iPad models and iOS versions are in use. There are other design possibilities being considered as well.  As I said, we're working to come up with an overall better solution for this, but it's not an easy problem to deal with. This is considered an important issue by the team. We welcome contributions here as well, whether it's a patch or design proposals/feedback."
technical,"Does it make sense to put that widget in a stack below the content of login page, this way it is not going to be visible to the user, but is Fluttter going to send that widget to GPU to create shader since it is not visible(it is in stack below some other content)? I think you'd probably want to do this during a splash screen or at least during some other idle time in the app. Exactly when is hard to say. We'd want to see that such a method is workable too though  first :)"
technical,"Does it make sense to put that widget in a stack below the content of login page, this way it is not going to be visible to the user, but is Fluttter going to send that widget to GPU to create shader since it is not visible(it is in stack below some other content)? I wonder if you'd have better luck with that solution by actually rasterizing frames from that animation to a pixel buffer. Putting it somewhere invisible may get optimized away."
technical,"Does it make sense to put that widget in a stack below the content of login page, this way it is not going to be visible to the user, but is Fluttter going to send that widget to GPU to create shader since it is not visible(it is in stack below some other content)? I'll also mention: sometimes the solution is just to do something less expensive. See for example - we found that using drawRect calls instead of a LinearGradient saves significant amount of time on initial rendering time."
technical,"not really. I'd try to just experiment with rendering the widget into a repaint boundary and using this to render it into a small image as part of a warmup routine, or this if that makes more sense.  You might also try just directly using the SceneBuilder API and using Scene.toImage depending on how you approach it. If I understand you well we should try to convert our page ui(scene) into an image and than try to paint it inside Flutter default warmup routine am I correct in this assumption? Also for us the main issue is the the cupertino route page transition we have other animations but nothing is as important as the first few page transitions and they have issue with shader compilation, page transition shader is directly related with the content of this page, we should try to get the images of pages that user is going to see first and put them in warmup routine? Can you just confirm or deny my assumptions?"
technical,"Another thing I don't understand is while we can't ship Metal Shader binaries we probably could ship them in MSL instead of Skls saving so one translation step. Which was pointed out as one time consuming step in another comment. If you have specific suggestions, I recommend filing narrowly-focused bugs that specific describe the problem you're trying to solve and the solution you propose for that solution (in separate comments), ideally with test cases so that we can validate whether the suggestions actually solve the problem when we try implementing them."
technical,"You don't want to save the image - you want to get Skia to create the shader(s) needed to create the image.  I'm not necessarily recommending this approach, but I think it's one that might be worth experimenting with and see what we can get out of it. IOW, you want to try to capture relevant frame(s) from your actual widgets/transitions, and rasterize them so that the needed shader(s) get compiled up front."
technical," It looks like this pull request may not have tests. Please make sure to add tests before merging. If you need an exemption to this rule, contact Hixie on the #hackers channel in Chat.    Reviewers  : Read the Tree Hygiene page and make sure this patch meets those guidelines before LGTMing."
technical,"Why is it that swift apps don't need this kind of warm up caching? Do they have an Uber shader that covers all the different types of animations possible? Or do they just limit you in what you can do and that way they're able to compile a limited set of shaders up front? My educated guess is that they use shared shaders that are already created by the OS, because SwiftUI doesn't draw its content on their own, but use system Widgets. Just as an info what Fortnite did with Apple to speed up the their game startup time, I guess that difference is that game creators know upfront all the possible animations that can happen and Flutter doesn't."
technical,"It looks like this pull request may not have tests. Please make sure to add tests before merging. If you need an exemption to this rule, contact Hixie on the #hackers channel in Chat.    Reviewers  : Read the Tree Hygiene page and make sure this patch meets those guidelines before LGTMing. Just curious, why isn't it possible to cache or even distribute the MSL version of the shaders instead of doing the translation fom SKLS to Msl at runtime? Or even precompile Msl? Did you have a look how metalangle solves this?"
technical,"I'll also mention: sometimes the solution is just to do something less expensive. See for example - we found that using drawRect calls instead of a LinearGradient saves significant amount of time on initial rendering time. not really. I'd try to just experiment with rendering the widget into a repaint boundary and using this to render it into a small image as part of a warmup routine, or this if that makes more sense.  You might also try just directly using the SceneBuilder API and using Scene.toImage depending on how you approach it."
technical,"Widgets don't fundamentally know what shaders they use until they are combined into as scene. For example, a circle might use a different shader when it's inside a perspective transform than when it's not. (That's just an example, I don't know if it's a real case. But it's a plausible one.) OK, then even more manually, add a new run option that creates a file of all used shader while you are runnin the app playing through all the janking areas. This file could then used for the next built to prewarm this shaders. This is even better than doing it manually or parsing code."
technical,"Actually doing an --optimization run option, would make absolute sense and is also good to sell :-) OK, then even more manually, add a new run option that creates a file of all used shader while you are running the app playing through all the janking areas. This file could then used for the next built to prewarm this shaders.  This is even better than doing it manually or parsing code. shader describes how to do just that. It isn't something we can enable on iOS/Metal though, if I understand correctly, due to limitations in Apple's APIs.  Our current area of study is a widget that would allow shaders for a part of the scene to compile asynchronously (in the background, while another, potentially animating, scene remains on the screen). This isn't trivial, unfortunately, for example, it wouldn't help if the next scene itself transitioned into yet another scene and  that  scene needed shaders to be compiled, since you'd still end up needing to compile those shaders midway through the animation. It is, however, more similar to what iOS does natively, if I understand correctly."
technical,"That doesn't make sense to me. What has the Metal API to do with logging when skia passes a new shader over to metal? OK, then even more manually, add a new run option that creates a file of all used shader while you are running the app playing through all the janking areas. This file could then used for the next built to prewarm this shaders.  This is even better than doing it manually or parsing code. shader describes how to do just that. It isn't something we can enable on iOS/Metal though, if I understand correctly, due to limitations in Apple's APIs.  Our current area of study is a widget that would allow shaders for a part of the scene to compile asynchronously (in the background, while another, potentially animating, scene remains on the screen). This isn't trivial, unfortunately, for example, it wouldn't help if the next scene itself transitioned into yet another scene and  that  scene needed shaders to be compiled, since you'd still end up needing to compile those shaders midway through the animation. It is, however, more similar to what iOS does natively, if I understand correctly."
technical,"Just curious, why isn't it possible to cache or even distribute the MSL version of the shaders instead of doing the translation fom SKLS to Msl at runtime? Or even precompile Msl? Did you have a look how metalangle solves this? One more question regarding your statement, ""This seems to be because the primary cause of jank in pipeline state setup is the construction of the Metal shader library (SKSL - MSL - MTLLibrary (AIR?)) and not the pipeline state object construction."", could we do something to speed up creation of Metal shader library?"
technical,"OK, then even more manually, add a new run option that creates a file of all used shader while you are running the app playing through all the janking areas. This file could then used for the next built to prewarm this shaders.  This is even better than doing it manually or parsing code. shader describes how to do just that. It isn't something we can enable on iOS/Metal though, if I understand correctly, due to limitations in Apple's APIs.  Our current area of study is a widget that would allow shaders for a part of the scene to compile asynchronously (in the background, while another, potentially animating, scene remains on the screen). This isn't trivial, unfortunately, for example, it wouldn't help if the next scene itself transitioned into yet another scene and  that  scene needed shaders to be compiled, since you'd still end up needing to compile those shaders midway through the animation. It is, however, more similar to what iOS does natively, if I understand correctly. That doesn't make sense to me. What has the Metal API to do with logging when skia passes a new shader over to metal?"
technical,"from my understanding Flutter only creates the shaders when it needs them the first time, which makes sense because why create shaders for widgets you never use. My guess is if you create the widgets where you encounter the jank already behind your Startup screen, that could help, but I' not sure if shaders will be created if the widgets are hidden. The problem with this approach is you can't do it with a screen transition. I tried this even putting a hidden navigator behind the current screen with a stack and pushing/popping 3-5 times behind a loading screen, and it had no impact on the perceived jank after that."
technical,"I think Dan meant ""here"" as in on GitHub, but in general yes please, keeping issues and PRs focused on one topic is a much better way of using GitHub because otherwise we end up with ""centithreads"" and GitHub starts hiding comments which buries information and ideas. This patch wires up Metal Binary Archives for Mac and iOS. Metal Binary Archives are used to cache pipeline state objects generated at runtime on the device and serialize them to disk when rendering is paused. In this prototype, the binary archives are serialized to disk when the application moves into the background. On the next launch of the application, the serialized representation of the archives is used for the creation of new pipeline state objects.  Per the details in the linked issue, I do not recommend we land this patch. This PR is just meant to be a record of the prototyping work done to evaluate Metal Binary Archives for Flutter.   ## Observations  Once patched in, the following observations can be made while running a Flutter application.  * After first launch, the application caches directory should contain a .metallib file with the following format. This was done so any changes to the Flutter engine or the Skia version would not run the risk of using invalid cached contents. The file (example) seems to be a Mach-O binary. This is consistent with Apple's recommendation that the lipo tool be used to combine archives harvested from devices of different GPU families. * This file will get updated every time the application is backgrounded and will get reused when the application is launched. * To prevent any issues with threading (I could find no mention in the docs about the thread safety aspects of any of the APIs) only raster thread operations will using this archive. * One open question not answered in my linked comment was how much these archives would help for jank during subsequent application launches (as seeding the archive for first launch is a no-go). I tried to find determine this by making sure the cache was seeded and launching the application again. My hope was to see a reduction in the GrMtlPipelineStateBuilder::finalize trace in Observatory. This did not materialize (trace is from a warmed up archive). This seems to be because the primary cause of jank in pipeline state setup is the construction of the Metal shader library (SKSL - MSL - MTLLibrary (AIR?)) and not the pipeline state object construction. An annotated version of one of the traces is as follows."
technical,"One more question regarding your statement, ""This seems to be because the primary cause of jank in pipeline state setup is the construction of the Metal shader library (SKSL - MSL - MTLLibrary (AIR?)) and not the pipeline state object construction."", could we do something to speed up creation of Metal shader library? what if we could get skia to use language=occ instead? It seems like that should be faster..."
technical,"I agree that the first couple of page transitions are the biggest issue as mentioned above. Would it be helpful if we added some of the most used skia draw methods for page transitions in the DefaultShaderWarmUp? I must say I am not an expert on this, if I say something stupid I apologize :) Why is it that swift apps don't need this kind of warm up caching? Do they have an Uber shader that covers all the different types of animations possible? Or do they just limit you in what you can do and that way they're able to compile a limited set of shaders up front? My educated guess is that they use shared shaders that are already created by the OS, because SwiftUI doesn't draw its content on their own, but use system Widgets."
technical,"I pondered a bit more about this and maybe we should think about tackling this problem from a different angle.  Instead of trying to find a general way to optimize the caching or precompiling in a general way which would be great but seems difficult, why not try to give the engine some help. I could imagine several approaches:  * give all Widgets a static  preloadShaders method, so I can call that for the widgets that make the problems while my loading screen is displayed. This might feel cumbersome, but it would be a very direct and pragmatic way and probably fast to implement. preloadShaders could even save the created shaders, so that they can be used the next time faster. I'm convinced that every iOS Developer would be happy to have this, even if it needs some more lines of code.  * I could imagine that this could be automated if we give all Widgets/RenderObjects an attribute with the names of the shaders that this Widget uses. Then an App could be parsed for all used Widgets and generate so the the necessary prewarming code.  what do you think dnfield ? Widgets don't fundamentally know what shaders they use until they are combined into as scene. For example, a circle might use a different shader when it's inside a perspective transform than when it's not. (That's just an example, I don't know if it's a real case. But it's a plausible one.)"
technical,"that would require that I start digging into the Engine code what I can't do. My proposals were pretty concrete here. Still I wonder why neither my proposal of shipping MSL nor the proposal of an optimisation run got a response from anyone from the team. I don't see how creating small isuues for that would help. I'm just trying to help. You did get a response. It was ""If you have specific suggestions, I recommend filing narrowly-focused bugs that specific describe the problem you're trying to solve and the solution you propose for that solution (in separate comments)"". Commenting on this PR is not a good way to get a response beyond that, it's the wrong venue. Creating issues help because issues are how we track work on this project. See tree hygiene."
technical,"If I understand you well we should try to convert our page ui(scene) into an image and than try to paint it inside Flutter default warmup routine am I correct in this assumption? Also for us the main issue is the the cupertino route page transition we have other animations but nothing is as important as the first few page transitions and they have issue with shader compilation, page transition shader is directly related with the content of this page, we should try to get the images of pages that user is going to see first and put them in warmup routine? Can you just confirm or deny my assumptions? You don't want to save the image - you want to get Skia to create the shader(s) needed to create the image.  I'm not necessarily recommending this approach, but I think it's one that might be worth experimenting with and see what we can get out of it."
technical,"I like senario 2 but I view top level statement with skepticism.  ""Only one entry point"" rule is essential for most languages and it is also going on C#. I think it is a first content to learn for beginners, and explicit void Main() is better textbook than veiling with top level statement. Actually, after some thought, I think I don't against top level statement on its own. I am perfectly fine to have it in our language  But what I am against is, it will be ambiguous when there was more than one file contain top level statement in the same level  And so, if we have any error when there was a top level statement then I think I am fine with it  suppose"
technical,"Ok   Perhaps.  But that's life.  We have to use our best judgement and make a call.  That call was made here.  I get that you don't like the decision we made.  But nothing had changed that would cause us to reassess that.  --  And, as above: if you'd like to discuss this further you can do so at gitter or discord. Any further discussion on this topic will need to go to gitter or discord.  Thank you."
technical,"It would have been nice if MadsTorgersen would have given the credits to the one that came up with this Idea, namely the former Visual Basic PM AnthonyDGreen, see Aren't they colleagues? I'm sure they speak or spoke about it."
technical,"i agree with redradist  <details <summaryNon moderate message warning</summary  this is very disappointing, another half assed feature, on top of the other bloat being added in C# 9 and 10  the same happened with source generator, it was too hard for the team so you went with an half assed solution  the result will be clearly visible, nobody will bother with the features  just like single file publish wich is a zip archive  you build something very bad for the future  i thought only Rust would follow scala in the graveyard, but looks like C# will too </details as above: if you'd like to discuss this further you can do so at gitter or discord  This language feature has already been decided upon.  The disappointment of people was factored into the decision making process here.  We assessed the pros and cons of this and determined this was not something we felt was valuable enough to make it int the language."
technical,"This might be only my opinion. But I still remember when I myself was a beginner. So this is my direct experience. I have learn only a little bit of C and start learning Java and C# at the same time. I remember I feel much burden on the package/namespace and class that was unknown to me. I don't know what it is and what effect it has on my code. What can I change in that file. What is the meaning of it. Where could I start typing. Looking back now I still felt that burden, partly because I always want this feature so it always reminded me on that first day  It very easy when you have learn C or older language as a starting point to programming. You already know what a code of program really is. It really another story when you don't really know it but just start learning C# or Java as your first ever language in your life. And I think most of us here cannot experience that kind of experience anymore  I have watch one 3Blue1Brown chapter, he talk about ""This problem seems hard, then it doesn't, but it really is"". He make that video about question in math competition that the organizer think it is quite easy, but the participant cannot solve it The conclusive word he give is, ""It extremely hard to imagine what it feel like to not understand"". And I kind of thinking the same with our understanding of programming language. What we feel like easy is easy from our perspective with some experience. We then sometimes fail to understand what it feel like to not understand, because we can't imagine it anymore As for me I think this feature already benefit us if we could write only one void Main in the project. Made second and it will give a name collision error. It pin down that we would have only one entrypoint in the project ever. That was the main benefit for all singleton function  My point is, the benefit for beginner that some people talk about is also as real and also a great bonus for our language too"
technical,C++/Swift has both top-level functions and static methods and it is not annoying anyone: Both of those languages have had top-level functions since their inception.
technical,I think to make right decision for this feature is to create a poll and ask people will they use this feature and would not be it annoying that exist two solutions: static methods and free functions ... C++/Swift has both top-level functions and static methods and it is not annoying anyone:
technical,"Yeah, I talked exactly that ...  Also it will feels like scripting language ( but not ,) ) CSX exists for that scenario and there are some tools that allow running such files standalone yes"
technical,"Great feature! i love the direction C# is taking  When learning C#, even Java.. i remember the major pain was to understand why i need create a namespace and class to start the program  This will solve this issue for many people who want to get started with C#  That kind of issue doesn't exists for Switf/Kotlin/Rust, getting started and teaching becomes much easier, less frictions Does it really solve that problem, though?  Sure, you might get ""Hello World"" off the ground faster, but for anything even slightly less trivial you're going to have to jump that same hurdle.  And does that hurdle really exist, even for beginners?  Odds are that a beginner is starting from a tool like Visual Studio or dotnet new which writes all of that boilerplate for you.  If they're trying to write this program from the command line they've already had to vault significantly higher hurdles.  If adoption and outreach are the goals I'd suggest that there are significantly better opportunities that don't result in creating dialects of the language.  And if the Tiobe index is to be trusted it doesn't appear that C# is having too many issues attracting developers.  Neither do most of the other languages towards the top of that list most of which require some kind of syntactic boilerplate.  Oh, and bring back temporary solutions in Visual Studio.  The removal of that feature has been infinitely more annoying to my ability to toss together a quick&dirty project than having Visual Studio automatically generate some code around the code I want to write."
technical,"While that's something I would certainly like to have in the future, it's not something that's going to be able to ship with C# 9. Such a thing would only work for the simplest of programs, with no nuget dependencies whatsoever, as we have no way of expression those in a C# file today. CSX exists for that scenario and there are some tools that allow running such files standalone, but adding dotnet run file.cs without fully thinking through the scenarios will cause us pain in the future. dotnet run file.cs could mean just entry point in file.cs  In such scenario *.csproj file could be respected as well"
technical,"Aren't they colleagues? I'm sure they speak or spoke about it. Dunno about top-level statements. Those can be a special case for running a file through a REPL or something like that.  Top-level functions are orthogonal and IMHO worth it just for breaking the illusion of static methods being somehow ""truer"" to OOP."
technical,"I'm sure this has been discussed at some point, but would it be completely unfeasible to have first-class support for top level functions in C# (i.e. functions that actually exist outside a class/struct and are visible from an Assembly instance via reflection for example, without being inside a Type?  I'm seeing that all proposals here suggest moving the elements inside a generated class, but this seems like a workaround solution to a bigger problem to me.  I've seen multiple people that hate object oriented programming that would appreciate being able to create fully procedural programs in C#. I imagine that could help market the language to a wider audience as well. Even if they could create top-level functions they would still have to interact with an ecosystem which is completely OOP-oriented.  Can't even write output to the console without calling a static member on a class.  I doubt anyone who has such a problem with using a language because it happens to have classes would change their mind because they might be able to write some very limited code in a slightly more procedural style."
technical,"I see nothing in this proposal that claims that using directives would be supported.  That would land us firmly back into dialect territory since said directives aren't permitted mid-method.  But, if this proposal would need to support it, without a corresponding change to the language, then an external tool could also do so just as easily.  CSX already does it.  IMO, we'd get more mileage making it easier to get/use CSX, and adding support for converting CSX scripts to a C# project. For me, the simple programs I would want to write:  * Are single file. * Are ordered usings, types, functions, statements, e.g. (artificial example):  * Types don't need access to local variables from statements.  This means that for me, the suggested ordering of "" statements  right before the *namespace member declaration*s"" would not be natural, I'd prefer it the other way around.  And being able to have top-level local functions in other files and being able to access top-level local variables from other files is unnecessary and probably undesirable.  On the other hand, the suggestion by CyrusNajmabadi of having only ""top level statements/local-funcs"" goes too far: being able to declare types is necessary for the ""simple programs"" I want to write."
technical,"I'm feeling a little bit sceptical of this proposal.  Writing a static Main function is not particularly difficult, and the tooling generates it for you anyway. In terms of benefits of top level statements I would suggest there's almost none from an actual use case perspective.  Instead I feel this is more of a marketing issue. C# looks old and stuffy because you need so many things to create an app. Python you just type something and it runs.  Marketing is important, but I don't think it's worth introducing a whole load of complexity for it. Instead I would keep this extremely simple. You can have a single file in a project with top level statements, which act exactly as if they're inside an async Main method. They are not globally scoped, and can't be referenced anywhere else. That should be enough to give beginners their python feel. For my part I really like Scenario 2. That would allow for significant time savings via less typing. And having worked with VB.NET for a number of years, I do miss VB's modules and their implicit imports.  I would love to have globally accessible implicit and other operators I could define even for existing CLR types. So if the team can find a way to include those it would add even more value to Scenario 2."
technical,"Hm, I was trying to say the same thing both times.  Language is hard.  :/ I don't think this changes that at all.  We're not talking about making CSX a part of the language, we're talking about making a third dialect where the compiler pretends that everything you're writing happens to take place within Main, with all of the wonky limitations that would come with that.  CSX remains a separate incompatible dialect, for a multitude of reasons.  And due to the limited nature of this ""simple"" dialect you can't adopt it in any existing project, nor does it make sense (to me) for anyone to use it after they're done writing their first artifically simple ""HelloWorld"" program and need anything slightly more complicated.  That's what I mean about the broader appeal.  ""Simple programs"" seem to target a very small subset of users who are either first starting out and don't understand the ceremony around their code (but somehow understand all of the complicated nuance required to actually write that code), or are writing the simplest of programs that can manage to be crammed into a single method.  In either case my opinion is that the work necessary to even get that far is a much bigger hurdle in that you still have to setup a project or invoke the compiler manually, and in the former the tooling can autogenerate all of the ceremony for you which, IMO, makes its removal a moot issue.  My opinion is that for both newbies and experienced devs alike the ability to quickly prototype new code is served not by a compiler dialect but through an excellent REPL and sandbox environment.  IMO it's not necessary for the compiler to officially support that dialect.  Both JShell for Java and Ammonite for Scala support writing top-level statements directly into the REPL and neither language supports such a construct, and my experience with Scala/Ammonite is that this is not an issue for either newbies or experienced developers.  The most important thing is the ability to very quickly spin up an environment into which you can bang out a series of statements interactively while following the state during the flow, not being limited to a very small subset of the overall syntax.  Note that the shortest ""Hello World"" in CSX is literally ""Hello World"".  Same is true with JShell and Ammonite.  If you consider that to be cheating the second shortest is WriteLine(""Hello World""), with no using directives or even a semicolon.  It's normal for a REPL to optimize around the common interactive use cases, which I think aligns well with the goal of outreach to new developers as well as quick prototyping for experienced developers.  Anyway, I think (we can all agree that) I've ranted enough about the subject. From the scenario detail in this proposal I think we have 2 options we need to choose  Personally I prefer the second case, and I don't like the first case, I too think it is unnecessary. And also I wish it would have more constraint than csx, such that it would be better seamless with our current C#  But in both case it drastically reduced from current C# starting point  The point of this is not just for quickly prototype (still it is a bonus too) but  1 - The real newbie, actually really totally zero experience, will see this as a starting point to learn C#. Less verbose. Less token in the eye that could be distract them. And easier to just write something before or after  2 - Experience developer, which is us all, could really start any size project from this point. Unlike prototyping or experimenting in csx, we can really start project with this first file even without dotnet new. We could make this file in vscode, type it manually, save and press debug, without any need to open terminal and run command, we can have a service (such as omnisharp) detect one cs file in folder and start it compiler  This is just the benefit that this feature allow us to start the project with this file"
technical,"In retrospect, I agree. I've removed it. Great feature! i love the direction C# is taking  When learning C#, even Java.. i remember the major pain was to understand why i need create a namespace and class to start the program  This will solve this issue for many people who want to get started with C#  That kind of issue doesn't exists for Switf/Kotlin/Rust, getting started and teaching becomes much easier, less frictions"
technical,"Hi all, MadsTorgersen  I have read the proposal and I have one suggestion: Hi all, MadsTorgersen  Also I think if there are few files with top-level statements:  It would be also nice if there is no *.cproj run application with default settings (default language version, framework type and so on)  And going even further I think it is possible to have other file in folder without *.cproj and use them, underhood dotnet will just create one (with default settigns) and compile files in folders  In such way C# will feels like scripting language, but with all goodness of statically typed language"
technical,"why my program works fine, but when I add new file like some guy did in tutorial from 2019 then it doesnt? wtf bugged language  or  why i do have use those name spaces and classes when everything works without them anyway? loololo Hi all, MadsTorgersen  I have read the proposal and I have one suggestion:"
technical,"Which I disagree with but anyway, maintaining two different dialects and two different tools of the same language is that high bar imo but we'll see, maybe not. Hm, I was trying to say the same thing both times.  Language is hard.  :/ I don't think this changes that at all.  We're not talking about making CSX a part of the language, we're talking about making a third dialect where the compiler pretends that everything you're writing happens to take place within Main, with all of the wonky limitations that would come with that.  CSX remains a separate incompatible dialect, for a multitude of reasons.  And due to the limited nature of this ""simple"" dialect you can't adopt it in any existing project, nor does it make sense (to me) for anyone to use it after they're done writing their first artifically simple ""HelloWorld"" program and need anything slightly more complicated.  That's what I mean about the broader appeal.  ""Simple programs"" seem to target a very small subset of users who are either first starting out and don't understand the ceremony around their code (but somehow understand all of the complicated nuance required to actually write that code), or are writing the simplest of programs that can manage to be crammed into a single method.  In either case my opinion is that the work necessary to even get that far is a much bigger hurdle in that you still have to setup a project or invoke the compiler manually, and in the former the tooling can autogenerate all of the ceremony for you which, IMO, makes its removal a moot issue.  My opinion is that for both newbies and experienced devs alike the ability to quickly prototype new code is served not by a compiler dialect but through an excellent REPL and sandbox environment.  IMO it's not necessary for the compiler to officially support that dialect.  Both JShell for Java and Ammonite for Scala support writing top-level statements directly into the REPL and neither language supports such a construct, and my experience with Scala/Ammonite is that this is not an issue for either newbies or experienced developers.  The most important thing is the ability to very quickly spin up an environment into which you can bang out a series of statements interactively while following the state during the flow, not being limited to a very small subset of the overall syntax.  Note that the shortest ""Hello World"" in CSX is literally ""Hello World"".  Same is true with JShell and Ammonite.  If you consider that to be cheating the second shortest is WriteLine(""Hello World""), with no using directives or even a semicolon.  It's normal for a REPL to optimize around the common interactive use cases, which I think aligns well with the goal of outreach to new developers as well as quick prototyping for experienced developers.  Anyway, I think (we can all agree that) I've ranted enough about the subject."
technical,"Where do imported namespaces come in?  Or do we need support for using directive within a method in order to support this?  Or would the compiler take using directives and ""promote"" them outside of the generated class?  What if multiple files want to import multiple and potentially colliding namespaces?  It's difficult to not have an immediate negative visceral reaction to this proposal.  It feels like it creates yet another dialect of the language without solving for any problems or the use cases suggested.  You wouldn't be able to take CSX and run it this way, not without additional syntax work.  You couldn't use most of the language as you'd expect.  All variables end up in some mixed global scope.  Feels like tools like LINQPad already satisfy this need and do so in a vastly superior manner. I agree that simple programs should remain simple. I am not even convinced that splitting your simple program into multiple simple files is something desirable. I imagine that csc run hello.cs or dotnet run hello.cs would be the preferred mode of running them. As soon as you have multiple files you need either a csproj or some other way to refer to multiple files. The former is complicated enough that you might as well rote learn static void Main as well, while the latter leads learners away from the ""proper"" way of doing things.  Of course, the scripting dialect could invent its own way of including other files, but that's outside the scope of this issue."
technical,"I agree that simple programs should remain simple. I am not even convinced that splitting your simple program into multiple simple files is something desirable. I imagine that csc run hello.cs or dotnet run hello.cs would be the preferred mode of running them. As soon as you have multiple files you need either a csproj or some other way to refer to multiple files. The former is complicated enough that you might as well rote learn static void Main as well, while the latter leads learners away from the ""proper"" way of doing things.  Of course, the scripting dialect could invent its own way of including other files, but that's outside the scope of this issue. I agree with MadsTorgersen proposal, that this feature should be targeting simple programs and people learning the language (and should be a shortcut for what goes inside Main()).  It could be worth adding a little more detail (with examples) to the section on the scoping of local variables and functions just so everyone is clear."
technical,"This tangent doesn't seem to be going anywhere.  If you would like to discuss this further, we have several conducive avenues for that.  Please consider gitter or discord. i agree with redradist  <details <summaryNon moderate message warning</summary  this is very disappointing, another half assed feature, on top of the other bloat being added in C# 9 and 10  the same happened with source generator, it was too hard for the team so you went with an half assed solution  the result will be clearly visible, nobody will bother with the features  just like single file publish wich is a zip archive  you build something very bad for the future  i thought only Rust would follow scala in the graveyard, but looks like C# will too </details"
technical,"I don't understand the use case here. Is something wrong with 'csx' scripts? Why can't 'learners' and those who want 'simple programs' just use the already-existing scripting dialect?  If the reason is ""no one is using it""- that's a tooling and education problem, not a language design problem. You guys released  CSX and then did close to zero promotion of it- to my knowledge, no one official has ever blogged about it or demoed it at conferences, and development on  C# Interactive stopped almost as soon as it started. I agree. IMO, having both top level statements/local-funcs and ""regular"" C# code coexist as proposed here can work, but allowing them to mix might be against the simple program scenario that motivated this proposal in the first place.  However, this would pretty much make it identical to a csx, right? It seems to me that making improvement to existing C# scripting would address the simple program scenario more effectively."
technical,"In the eye of beginner programmer, even the word namespace and class  itself is already magic that require them to understand that they cannot put a logic code inside those block. They need to learn that they could only put code into the bracket of void Main  Starting with dotnet new will present them a sudden 3 layers that require understanding. This proposal can reduce to only one (or zero, if we could write a top level statement) I also want to voice my support for the opinion that the ceremony with classes, namespaces and  especially  void Main is a real obstacle for the beginners. I myself chose Pascal as a first language to learn for this literal reason. Good thing it was Pascal.NET, so I had to learn C# anyway to understand documentation."
technical,"In the eye of beginner programmer, even the word namespace and class  itself is already magic that require them to understand that they cannot put a logic code inside those block. They need to learn that they could only put code into the bracket of void Main  Starting with dotnet new will present them a sudden 3 layers that require understanding. This proposal can reduce to only one (or zero, if we could write a top level statement) I disagree that such syntax poses a burden to beginners, either to C# or to programming in general.  They're going to have to learn so much about the syntax of C# in order to put  anything  anywhere anyway, and many of those concepts (like variables, definite assignment, etc.) are so much more complicated to grasp than requiring a single container around a method (namespace has always been optional).  What's next?  Implicit variable declaration by assignment?  Auto-importing namespaces?  Automatically emitting the output of any expression to the console?  If C# is going to seriously consider the addition of top-level functions it should do so in consideration as to how they would impact and benefit developers of all skill levels and projects of all shapes."
technical,This is purely subjective and a slippery slope because this doesn't apply to many of the other features introduced to C#. I disagree.  It's been my experience that virtually every feature that has been adopted by the C# team and added into the language has had to meet this same subjective but high bar in that it must either benefit sufficient developers directly or must benefit runtime/library authors sufficiently that other developers feel that benefit.
technical,"I think that would be very useful to support an FP rather than class based style of programming. Each file becomes a sort of module and defines a number of private, internal, and public functions. I don't understand the use case here. Is something wrong with 'csx' scripts? Why can't 'learners' and those who want 'simple programs' just use the already-existing scripting dialect?  If the reason is ""no one is using it""- that's a tooling and education problem, not a language design problem. You guys released  CSX and then did close to zero promotion of it- to my knowledge, no one official has ever blogged about it or demoed it at conferences, and development on  C# Interactive stopped almost as soon as it started."
technical,"Some opinions- - Scenario 1 should be merged into scenario 3, the scripting dialect. Assuming current C# scripting dialect userbase is not that big, there can be breaking changes if necessary. And it can live in a separate repo outside Roslyn. - As per my understanding, scenario 1 feature will only be usable in a single file in the whole codebase. I think it does not clear the high bar C# has for new features.  It does not help much non-beginner devs and even be confusing for many. - To gain users (ie beginners in this case) C# scripting dialect has to be promoted. It should be the featured way to get started with C#. And the onboarding experience should be as simple as installing dotnet SDK, create a csx script file with code and then run with a simple command like dotnet run file.csx. I can't actually remember how to use C# scripts. Being a 3rd party tool, jit seems has better visibility than built-in roslyn/csc tool in Google/Internet. - 'dotnet try' should use the scripting dialect. - There can be a tool to translate a .csx script files into a C# project with a .csproj and a .cs file. Then one can start prototyping with scripts and then easily move to C# proper project when needed.  For C# proper, I would like to have scenario 2 ""Top-level functions"". I like senario 2 but I view top level statement with skepticism.  ""Only one entry point"" rule is essential for most languages and it is also going on C#. I think it is a first content to learn for beginners, and explicit void Main() is better textbook than veiling with top level statement."
technical,"It's a bit more complex than that, since the tool has to recognize the usings as well. I see nothing in this proposal that claims that using directives would be supported.  That would land us firmly back into dialect territory since said directives aren't permitted mid-method.  But, if this proposal would need to support it, without a corresponding change to the language, then an external tool could also do so just as easily.  CSX already does it.  IMO, we'd get more mileage making it easier to get/use CSX, and adding support for converting CSX scripts to a C# project."
technical,"Scenario 2 is also my preferred option.  But I disagree with MadsTorgersen suggestion of:  Their accessibility, like that of top-level classes would be internal or public, with internal being the default.  private should be supported and should be the default, in my view. Each file containing its own set of free functions should end up as a differently named static class, allowing each to contain its own private functions. I think that would be very useful to support an FP rather than class based style of programming. Each file becomes a sort of module and defines a number of private, internal, and public functions."
technical,"These two paragraph is not the logically related at all. The announcement about rejected is fine. We have rejected proposal as always. The complain is also normal reaction as always. And the explanation and reasoning is exist but that doesn't mean it must always be acceptable or even reasonable, or even corrected. And that's also normal that people who against it would like to have more acceptable reasoning or possibility of rethinking (Like generic that was rethink and reintroduce in C# 2.0)  The announcement and argument also exist here in repo and issue board of the repo so we come here to voice our argument in here to be on topic. But then the argument and debate must go into the gutter like chat channel to make it easier to forget, that's all reasoning I could understand about why we need to go there, the team just don't want to listen or even have clear evidence that people appeared to go against your decision (I might be overthinking things but this was one technique a dictator would used to suppress opinion of people)  At first I just try to point out that the argument used to reject is not sound enough. Sure I have support this feature but if there is valid argument, like breaking change, I would consider it understandable. But for assumption that this feature will have minimal usage is very unlikely and very vague argument  But not only there are no further explanation, you just demand people voice's to be drained into gitter and discord instead of the issue topic itself. This is what I really would called unreasonable  And in fact I felt very uncomfortable because this is not what we commonly do here. As I said there was always rejected proposal and people always voice their opinion under issue of that topic. But I have never seen that we need to move the discussion to chat channel. At most the issue itself would be turned into discussion. But now you just don't want discussion to be under the issue and we don't have reason why it changed  ps. I guess this comment would be marked as off topic while above comment would not I think, being more specific about- - advantages (for beginners, pros and certain use-cases) - disadvantages (similarly for beginner, pros and certain use-cases) - challenges (of implementation and adaptation)  of a feature will help everyone to agree about whether to go with it or not. And rejecting a proposal with a label likely never is not same as ""not going to do this now"".  More **measurable and mechanical** the decision making process is, more agreement it will generate. At the end, we have to go with the decision makers. But an enthusiastic community is a great asset for a language and ecosystem. People should want to be in it, not have to be with it."
technical,"Dunno about top-level statements. Those can be a special case for running a file through a REPL or something like that.  Top-level functions are orthogonal and IMHO worth it just for breaking the illusion of static methods being somehow ""truer"" to OOP. I wonder if people will start making Solution.Project.Utils namespaces to save on typing usings. Is it even a bad thing if this is already achievable with using static? Feels like the only thing separating us from Solution.Project.Utils is the ""one class per file"" ""rule"". It seems like top-level functions might introduce a lot of confusion in terms of how to organize them in files/namespaces properly."
technical,"For my part I really like Scenario 2. That would allow for significant time savings via less typing. And having worked with VB.NET for a number of years, I do miss VB's modules and their implicit imports.  I would love to have globally accessible implicit and other operators I could define even for existing CLR types. So if the team can find a way to include those it would add even more value to Scenario 2. I would absolutely use simple C# programs for throwaway scripts and tiny utilities. dotnet new console is fast, but it leaves behind a whole project folder with a bunch of binaries."
technical,"I agree with MadsTorgersen proposal, that this feature should be targeting simple programs and people learning the language (and should be a shortcut for what goes inside Main()).  It could be worth adding a little more detail (with examples) to the section on the scoping of local variables and functions just so everyone is clear. I'm feeling a little bit sceptical of this proposal.  Writing a static Main function is not particularly difficult, and the tooling generates it for you anyway. In terms of benefits of top level statements I would suggest there's almost none from an actual use case perspective.  Instead I feel this is more of a marketing issue. C# looks old and stuffy because you need so many things to create an app. Python you just type something and it runs.  Marketing is important, but I don't think it's worth introducing a whole load of complexity for it. Instead I would keep this extremely simple. You can have a single file in a project with top level statements, which act exactly as if they're inside an async Main method. They are not globally scoped, and can't be referenced anywhere else. That should be enough to give beginners their python feel."
technical,"Yes, that is the main reason. I'm sure this has been discussed at some point, but would it be completely unfeasible to have first-class support for top level functions in C# (i.e. functions that actually exist outside a class/struct and are visible from an Assembly instance via reflection for example, without being inside a Type?  I'm seeing that all proposals here suggest moving the elements inside a generated class, but this seems like a workaround solution to a bigger problem to me.  I've seen multiple people that hate object oriented programming that would appreciate being able to create fully procedural programs in C#. I imagine that could help market the language to a wider audience as well."
technical,"one file per utility/script. I've already written I don't think it's nesessary to support multi-file ""simple programs"". If all this proposal does is take the statements in the file(s) and put that into the middle of a generated Main method why does this need to be a part of the compiler or C# spec at all?  Couldn't a separate dotnet tool be shipped which does that for users?  A simplified tool could be geared towards making this as easy as possible.  And you could prototype this all out right now without having to touch Roslyn or the C# spec."
technical,"From the scenario detail in this proposal I think we have 2 options we need to choose  Personally I prefer the second case, and I don't like the first case, I too think it is unnecessary. And also I wish it would have more constraint than csx, such that it would be better seamless with our current C#  But in both case it drastically reduced from current C# starting point  The point of this is not just for quickly prototype (still it is a bonus too) but  1 - The real newbie, actually really totally zero experience, will see this as a starting point to learn C#. Less verbose. Less token in the eye that could be distract them. And easier to just write something before or after  2 - Experience developer, which is us all, could really start any size project from this point. Unlike prototyping or experimenting in csx, we can really start project with this first file even without dotnet new. We could make this file in vscode, type it manually, save and press debug, without any need to open terminal and run command, we can have a service (such as omnisharp) detect one cs file in folder and start it compiler  This is just the benefit that this feature allow us to start the project with this file If this ever becomes more than a proposition, for what its worth, I think it would be essential to limit access to that top-level code and disallow sharing it with non-top-level code. The idea to limit top-level code to a single file seems like a good start, but it may be too limiting from a learning perspective.  To have taught introduction to programming several times to multiple people having a different level of knowledge/affinity with programming, I can state that simple concepts, like writing code between { and } can be tough to understand to some. So I think that the idea of making entry into the C# world more accessible is a good idea.  However, I believe that transforming C# into a scripting-like language is a bad idea. In conclusion, from my experience, globals are your enemy when designing programs even if they first appear as friends, they almost always end up causing more troubles that good"
technical,"With all due respect your comment makes zero sense here. In retrospect, I agree. I've removed it."
technical,"Does it really solve that problem, though?  Sure, you might get ""Hello World"" off the ground faster, but for anything even slightly less trivial you're going to have to jump that same hurdle.  And does that hurdle really exist, even for beginners?  Odds are that a beginner is starting from a tool like Visual Studio or dotnet new which writes all of that boilerplate for you.  If they're trying to write this program from the command line they've already had to vault significantly higher hurdles.  If adoption and outreach are the goals I'd suggest that there are significantly better opportunities that don't result in creating dialects of the language.  And if the Tiobe index is to be trusted it doesn't appear that C# is having too many issues attracting developers.  Neither do most of the other languages towards the top of that list most of which require some kind of syntactic boilerplate.  Oh, and bring back temporary solutions in Visual Studio.  The removal of that feature has been infinitely more annoying to my ability to toss together a quick&dirty project than having Visual Studio automatically generate some code around the code I want to write. In the eye of beginner programmer, even the word namespace and class  itself is already magic that require them to understand that they cannot put a logic code inside those block. They need to learn that they could only put code into the bracket of void Main  Starting with dotnet new will present them a sudden 3 layers that require understanding. This proposal can reduce to only one (or zero, if we could write a top level statement)"
technical,"As for me I think this feature already benefit us if we could write only one void Main in the project. Made second and it will give a name collision error. It pin down that we would have only one entrypoint in the project ever. That was the main benefit for all singleton function  My point is, the benefit for beginner that some people talk about is also as real and also a great bonus for our language too In the eye of beginner programmer, even the word namespace and class  itself is already magic that require them to understand that they cannot put a logic code inside those block. They need to learn that they could only put code into the bracket of void Main  Starting with dotnet new will present them a sudden 3 layers that require understanding. This proposal can reduce to only one (or zero, if we could write a top level statement)"
technical,"I wonder if people will start making Solution.Project.Utils namespaces to save on typing usings. Is it even a bad thing if this is already achievable with using static? Feels like the only thing separating us from Solution.Project.Utils is the ""one class per file"" ""rule"". It seems like top-level functions might introduce a lot of confusion in terms of how to organize them in files/namespaces properly. It is not clear to me if this championed feature is supposed to represent the same thing as #2765."
technical,"Actually, after some thought, I think I don't against top level statement on its own. I am perfectly fine to have it in our language  But what I am against is, it will be ambiguous when there was more than one file contain top level statement in the same level  And so, if we have any error when there was a top level statement then I think I am fine with it  suppose It would have been nice if MadsTorgersen would have given the credits to the one that came up with this Idea, namely the former Visual Basic PM AnthonyDGreen, see"
technical,"If all this proposal does is take the statements in the file(s) and put that into the middle of a generated Main method why does this need to be a part of the compiler or C# spec at all?  Couldn't a separate dotnet tool be shipped which does that for users?  A simplified tool could be geared towards making this as easy as possible.  And you could prototype this all out right now without having to touch Roslyn or the C# spec. It's a bit more complex than that, since the tool has to recognize the usings as well."
technical,"Reading LDM and then I still don't think it was the right direction of decision. I don't think we need to care about BCL at all  The first thing we would do with top level function is naked Main. Everyone will start using that. And then every function for using with Main until we really need to use class or struct. At least it would be shared in the same assembly or namespace. It would never be minimal usage at all because everyone will start using it from the start of the project. It make prototyping faster  If there is any concern about collision I think it would be the same as introducing new function in static class then use static using. If that was not the concern this also should be none  As I was proposing. We can treat this feature as just automatic assuming default namespace and default class name of the assembly and just transpile at the compile time  Actually, this feature real usage are most likely internal function so it just add convenience to development time. But we are less likely expose it, we would always have using and using static for that. So it should not concern about BCL at all  I think this would be another wrong decision as generic from C# 1. You underestimate our real usage Ok   Perhaps.  But that's life.  We have to use our best judgement and make a call.  That call was made here.  I get that you don't like the decision we made.  But nothing had changed that would cause us to reassess that.  --  And, as above: if you'd like to discuss this further you can do so at gitter or discord."
technical,"Would you require more than one file? one file per utility/script. I've already written I don't think it's nesessary to support multi-file ""simple programs""."
technical,"Hi all, MadsTorgersen  Also I think if there are few files with top-level statements:  It would be also nice if there is no *.cproj run application with default settings (default language version, framework type and so on)  And going even further I think it is possible to have other file in folder without *.cproj and use them, underhood dotnet will just create one (with default settigns) and compile files in folders  In such way C# will feels like scripting language, but with all goodness of statically typed language redradist nothing in the current implementation will be blocking doing such a thing in the future, but we won't be adding that in the first version."
technical,"Would like to voice that I prefer the scenario 2 specifically  Is it true that I could assume that top level function will be internal static by default? If it is then, as for simple program purpose, just one void Main is already reduced enough from the verbose class and static  Another main reason I really support this feature is that, outermost void Main will become exactly one in the whole program, never be any conflict thereafter. It also true for any same name singleton function we want  Also I would like to repeat myself that, I wish that we could not write top level statement. Only function and property. Because, unlike class that easily make dependency chain, we cannot expect timing and order of the statement aside from void Main Scenario 2 is also my preferred option.  But I disagree with MadsTorgersen suggestion of:  Their accessibility, like that of top-level classes would be internal or public, with internal being the default.  private should be supported and should be the default, in my view. Each file containing its own set of free functions should end up as a differently named static class, allowing each to contain its own private functions."
technical,"You can use functions on the top level, but only there. They can only be declared in one file, they cannot have access modifiers, and they're treated as if they're local functions in the implicit Main method. Scripts I made or was confronted to may often fit in the 1st ""Simple programs"" category, but with source code split under multiple files. Languages like AutoIT may #include <File.au3 and have these blob-added by its wrapper, BATCH may call an external command. The ""more than one file per script"" case doesn't seem uncommon."
technical, See relevant discussion at #2765.
technical,"Thaina  That I believe is the idea ""scenario 2"" above, and I can get behind that idea.  It's similar to Kotlin in that the top-level functions in a file are ""promoted"" to static methods in an autogenerated partial class.  The compiler would then automatically bring those members into scope anytime you import the namespace in which they are defined, as if you did a using static on that autogenerated class.  You can use this for single file ""script""-like programs, or you can mix it into your existing projects.  You still need a Main entry point, but you don't need a class or namespace around it.  For newbies and prototyping alike, I want this in Visual Studio:  Swift in xcode Playground  combined with features to show the flow of execution from SharpLab like this Some opinions- - Scenario 1 should be merged into scenario 3, the scripting dialect. Assuming current C# scripting dialect userbase is not that big, there can be breaking changes if necessary. And it can live in a separate repo outside Roslyn. - As per my understanding, scenario 1 feature will only be usable in a single file in the whole codebase. I think it does not clear the high bar C# has for new features.  It does not help much non-beginner devs and even be confusing for many. - To gain users (ie beginners in this case) C# scripting dialect has to be promoted. It should be the featured way to get started with C#. And the onboarding experience should be as simple as installing dotnet SDK, create a csx script file with code and then run with a simple command like dotnet run file.csx. I can't actually remember how to use C# scripts. Being a 3rd party tool, jit seems has better visibility than built-in roslyn/csc tool in Google/Internet. - 'dotnet try' should use the scripting dialect. - There can be a tool to translate a .csx script files into a C# project with a .csproj and a .cs file. Then one can start prototyping with scripts and then easily move to C# proper project when needed.  For C# proper, I would like to have scenario 2 ""Top-level functions""."
technical,"Very interesting point svick .  Thanks!  There's something definitely more appealing to me about that as it feels much more 'natural' in terms of scoping.  i.e if i think of the 'top level locals' i declare as similar to 'method locals', then placing them after everyting else feels 'better' (since locals can't be referenced by code that is earlier than their declaration). Take a look at the grammarlet Mads posted:  Usings are supported, and they must precede the statements. I actually don't mind if CSX is merged with mainline C#, I can then just treat this proposal as the first step in that direction."
technical,"If this ever becomes more than a proposition, for what its worth, I think it would be essential to limit access to that top-level code and disallow sharing it with non-top-level code. The idea to limit top-level code to a single file seems like a good start, but it may be too limiting from a learning perspective.  To have taught introduction to programming several times to multiple people having a different level of knowledge/affinity with programming, I can state that simple concepts, like writing code between { and } can be tough to understand to some. So I think that the idea of making entry into the C# world more accessible is a good idea.  However, I believe that transforming C# into a scripting-like language is a bad idea. In conclusion, from my experience, globals are your enemy when designing programs even if they first appear as friends, they almost always end up causing more troubles that good Thaina  That I believe is the idea ""scenario 2"" above, and I can get behind that idea.  It's similar to Kotlin in that the top-level functions in a file are ""promoted"" to static methods in an autogenerated partial class.  The compiler would then automatically bring those members into scope anytime you import the namespace in which they are defined, as if you did a using static on that autogenerated class.  You can use this for single file ""script""-like programs, or you can mix it into your existing projects.  You still need a Main entry point, but you don't need a class or namespace around it.  For newbies and prototyping alike, I want this in Visual Studio:  Swift in xcode Playground  combined with features to show the flow of execution from SharpLab like this"
technical,"I agree. IMO, having both top level statements/local-funcs and ""regular"" C# code coexist as proposed here can work, but allowing them to mix might be against the simple program scenario that motivated this proposal in the first place.  However, this would pretty much make it identical to a csx, right? It seems to me that making improvement to existing C# scripting would address the simple program scenario more effectively. The motivation here is to come up with a single dialect where you wouldn't have to think about the environment regardless to proficiency or task, people who just want/need to write a ""simple program"" and run it shouldn't use a different tool just because now they want to define a top-level function or whatever but for beginners this might be a show stopper especially for new programmers with no prior knowledge and this barrier is a language concern because there are two different dialects and two different tools for the same language."
technical,"See relevant discussion at #2765. The primary thing that bothers me here is scoping.  i.e. both around 'args' as well as the scopes of variables introduced in teh statements that precede a namespace.  However, it may just be an initial aversion that i coudl get over.  Given that your goal is Proposal: Simple programs, i would say that there's actually no need for scoping to extend from teh statements elsewhere.  A simple program is just statements and little funcs.  The moment we get to namespaces/classes/etc, we're no longer ""simple"" and i personally would prefer stating that they shouldn't mix.  --  another issue for me is that while this is pitched as 'simple programs' it seems to still allow the statements to coexist with namespaces/classes.  First, this isn't really 'simple' to me anymore.  Second, it actually opens up large cans of worms for me.  For example, if i were to be able to have top-level statements that can be in scope for the rest of my program, then I absolutely would want to be able to make those top level variables readonly so they couldn't just be overwritten by the rest of my code.  I strongly like the idea of simple-programs.  But I actually don't think this goes far enough.  Perhaps a simple program should *only* be top level statements/local-funcs?"
technical,See relevant discussion at #2765. This is as unreasonable as the reasoning about minimal usage. Now you don't even have reason why discussion should be there
technical,"I also want to voice my support for the opinion that the ceremony with classes, namespaces and  especially  void Main is a real obstacle for the beginners. I myself chose Pascal as a first language to learn for this literal reason. Good thing it was Pascal.NET, so I had to learn C# anyway to understand documentation. This is purely subjective and a slippery slope because this doesn't apply to many of the other features introduced to C#."
technical,"I disagree that such syntax poses a burden to beginners, either to C# or to programming in general.  They're going to have to learn so much about the syntax of C# in order to put  anything  anywhere anyway, and many of those concepts (like variables, definite assignment, etc.) are so much more complicated to grasp than requiring a single container around a method (namespace has always been optional).  What's next?  Implicit variable declaration by assignment?  Auto-importing namespaces?  Automatically emitting the output of any expression to the console?  If C# is going to seriously consider the addition of top-level functions it should do so in consideration as to how they would impact and benefit developers of all skill levels and projects of all shapes. This might be only my opinion. But I still remember when I myself was a beginner. So this is my direct experience. I have learn only a little bit of C and start learning Java and C# at the same time. I remember I feel much burden on the package/namespace and class that was unknown to me. I don't know what it is and what effect it has on my code. What can I change in that file. What is the meaning of it. Where could I start typing. Looking back now I still felt that burden, partly because I always want this feature so it always reminded me on that first day  It very easy when you have learn C or older language as a starting point to programming. You already know what a code of program really is. It really another story when you don't really know it but just start learning C# or Java as your first ever language in your life. And I think most of us here cannot experience that kind of experience anymore  I have watch one 3Blue1Brown chapter, he talk about ""This problem seems hard, then it doesn't, but it really is"". He make that video about question in math competition that the organizer think it is quite easy, but the participant cannot solve it The conclusive word he give is, ""It extremely hard to imagine what it feel like to not understand"". And I kind of thinking the same with our understanding of programming language. What we feel like easy is easy from our perspective with some experience. We then sometimes fail to understand what it feel like to not understand, because we can't imagine it anymore"
technical,"It is not clear to me if this championed feature is supposed to represent the same thing as #2765. This proposal conflicts with the possibility of having top-level methods in C#.  I'm not sure that is intended, but I think that would be a problem."
technical,"It is entirely about that.  The language design group chooses which features are added to the language based on their personal beliefs and feelings  The features we decide on the for the language are not chosen based solely on their technical terms. Yes, that feeds into our decision making.  We are aware of what would be enabled by this, and we made a determination if this would be an overall positive for the language.  We decided it would not be, so we rejected the feature. This tangent doesn't seem to be going anywhere.  If you would like to discuss this further, we have several conducive avenues for that.  Please consider gitter or discord."
technical,"For me, the simple programs I would want to write:  * Are single file. * Are ordered usings, types, functions, statements, e.g. (artificial example):  * Types don't need access to local variables from statements.  This means that for me, the suggested ordering of "" statements  right before the *namespace member declaration*s"" would not be natural, I'd prefer it the other way around.  And being able to have top-level local functions in other files and being able to access top-level local variables from other files is unnecessary and probably undesirable.  On the other hand, the suggestion by CyrusNajmabadi of having only ""top level statements/local-funcs"" goes too far: being able to declare types is necessary for the ""simple programs"" I want to write. Very interesting point svick .  Thanks!  There's something definitely more appealing to me about that as it feels much more 'natural' in terms of scoping.  i.e if i think of the 'top level locals' i declare as similar to 'method locals', then placing them after everyting else feels 'better' (since locals can't be referenced by code that is earlier than their declaration)."
technical,It is very disappointed ... (((((((((((( I've started thinking that Microsoft consider community opinion ... I am very disappointed (((((((((((((((((( We did.  That was part of our decision making process.  But we rejected this proposal as we didn't think it was the right direction for the language.
technical,"CSX exists for that scenario and there are some tools that allow running such files standalone yes we likely want to separate the top-level statements and functions parts of this proposal, so we can have the former continue to live in the C# 9 timeframe and have something to track the ongoing work for C# 10."
technical,"What csproj file? What if there is none in the folder the .cs is in? What if the .cs provided is a path, and there are csproj files in both the current directory and the directory of the .cs file? Or there are just multiple in the current folder?  I really do like the idea of being able to run a single .cs file (and I even opened a proposal to support shebang directives in .cs files in support of this, so you could just to ./file.cs). But we absolutely cannot just rush into the scenario blind. We need to do our due diligence in designing all aspects of this feature, from tooling support, debugability, C# support, and more. This is not going to happen overnight, or in time for the C# 9 release. Further, this will need to be a cross team effort: csharplang really isn't even the repo to discuss improvements to the dotnet cli runner in the first place. We should not change what filepath.Eval Symlinks implementation."
technical,"dotnet run file.cs could mean just entry point in file.cs  In such scenario *.csproj file could be respected as well What csproj file? What if there is none in the folder the .cs is in? What if the .cs provided is a path, and there are csproj files in both the current directory and the directory of the .cs file? Or there are just multiple in the current folder?  I really do like the idea of being able to run a single .cs file (and I even opened a proposal to support shebang directives in .cs files in support of this, so you could just to ./file.cs). But we absolutely cannot just rush into the scenario blind. We need to do our due diligence in designing all aspects of this feature, from tooling support, debugability, C# support, and more. This is not going to happen overnight, or in time for the C# 9 release. Further, this will need to be a cross team effort: csharplang really isn't even the repo to discuss improvements to the dotnet cli runner in the first place."
technical,"Scripts I made or was confronted to may often fit in the 1st ""Simple programs"" category, but with source code split under multiple files. Languages like AutoIT may #include <File.au3 and have these blob-added by its wrapper, BATCH may call an external command. The ""more than one file per script"" case doesn't seem uncommon. What does mean REJECTED ? That is all, this feature with Top-Level functions will not be developed in near future ?"
technical,"The primary thing that bothers me here is scoping.  i.e. both around 'args' as well as the scopes of variables introduced in teh statements that precede a namespace.  However, it may just be an initial aversion that i coudl get over.  Given that your goal is Proposal: Simple programs, i would say that there's actually no need for scoping to extend from teh statements elsewhere.  A simple program is just statements and little funcs.  The moment we get to namespaces/classes/etc, we're no longer ""simple"" and i personally would prefer stating that they shouldn't mix.  --  another issue for me is that while this is pitched as 'simple programs' it seems to still allow the statements to coexist with namespaces/classes.  First, this isn't really 'simple' to me anymore.  Second, it actually opens up large cans of worms for me.  For example, if i were to be able to have top-level statements that can be in scope for the rest of my program, then I absolutely would want to be able to make those top level variables readonly so they couldn't just be overwritten by the rest of my code.  I strongly like the idea of simple-programs.  But I actually don't think this goes far enough.  Perhaps a simple program should *only* be top level statements/local-funcs? Where do imported namespaces come in?  Or do we need support for using directive within a method in order to support this?  Or would the compiler take using directives and ""promote"" them outside of the generated class?  What if multiple files want to import multiple and potentially colliding namespaces?  It's difficult to not have an immediate negative visceral reaction to this proposal.  It feels like it creates yet another dialect of the language without solving for any problems or the use cases suggested.  You wouldn't be able to take CSX and run it this way, not without additional syntax work.  You couldn't use most of the language as you'd expect.  All variables end up in some mixed global scope.  Feels like tools like LINQPad already satisfy this need and do so in a vastly superior manner."
technical,"I disagree.  It's been my experience that virtually every feature that has been adopted by the C# team and added into the language has had to meet this same subjective but high bar in that it must either benefit sufficient developers directly or must benefit runtime/library authors sufficiently that other developers feel that benefit. Which I disagree with but anyway, maintaining two different dialects and two different tools of the same language is that high bar imo but we'll see, maybe not."
technical,"I disagree.  It's been my experience that virtually every feature that has been adopted by the C# team and added into the language has had to meet this same subjective but high bar in that it must either benefit sufficient developers directly or must benefit runtime/library authors sufficiently that other developers feel that benefit. While that's something I would certainly like to have in the future, it's not something that's going to be able to ship with C# 9. Such a thing would only work for the simplest of programs, with no nuget dependencies whatsoever, as we have no way of expression those in a C# file today. CSX exists for that scenario and there are some tools that allow running such files standalone, but adding dotnet run file.cs without fully thinking through the scenarios will cause us pain in the future."
technical,"Even if they could create top-level functions they would still have to interact with an ecosystem which is completely OOP-oriented.  Can't even write output to the console without calling a static member on a class.  I doubt anyone who has such a problem with using a language because it happens to have classes would change their mind because they might be able to write some very limited code in a slightly more procedural style. why my program works fine, but when I add new file like some guy did in tutorial from 2019 then it doesnt? wtf bugged language  or  why i do have use those name spaces and classes when everything works without them anyway? loololo"
technical,"Take a look at the grammarlet Mads posted:  Usings are supported, and they must precede the statements. I actually don't mind if CSX is merged with mainline C#, I can then just treat this proposal as the first step in that direction. Would like to voice that I prefer the scenario 2 specifically  Is it true that I could assume that top level function will be internal static by default? If it is then, as for simple program purpose, just one void Main is already reduced enough from the verbose class and static  Another main reason I really support this feature is that, outermost void Main will become exactly one in the whole program, never be any conflict thereafter. It also true for any same name singleton function we want  Also I would like to repeat myself that, I wish that we could not write top level statement. Only function and property. Because, unlike class that easily make dependency chain, we cannot expect timing and order of the statement aside from void Main"
technical,"This proposal conflicts with the possibility of having top-level methods in C#.  I'm not sure that is intended, but I think that would be a problem. Would that be because any such top-level functions would have to be evaluated as local functions within this ""Main"" method?  Or is there another reason?"
technical,"I would absolutely use simple C# programs for throwaway scripts and tiny utilities. dotnet new console is fast, but it leaves behind a whole project folder with a bunch of binaries. Would you require more than one file?"
technical,"We should not change what filepath.Eval Symlinks implementation. Yeah, I talked exactly that ...  Also it will feels like scripting language ( but not ,) )"
technical,"Would that be because any such top-level functions would have to be evaluated as local functions within this ""Main"" method?  Or is there another reason? Yes, that is the main reason."
technical,"Would that be because any such top-level functions would have to be evaluated as local functions within this ""Main"" method?  Or is there another reason? You can use functions on the top level, but only there. They can only be declared in one file, they cannot have access modifiers, and they're treated as if they're local functions in the implicit Main method."
technical,"I just uploaded the downgrade instructions to Python Like You Mean It. Actually, anaconda fixed their repo data as well, so now it works out of the box again"
technical,"The code has been already updated, but 7.19.0 did not include this. :( As a temporary fix for anyone just trying to get things working again  It would be really nice if you could quickly release a 7.19.1. (It's already fixed on master).  Sorry for that. I did not realize that IPython with that fix was not released yet. I usually test IPython completions before doing a Jedi release, but not this time :/. It will probably also not happen in the future anymore, because I'm going to release Jedi 1.0 soon, so this is probably the last time for a long time that you have to deal with deprecations in Jedi.  Still wish you a Merry Christmas!"
technical,"installing 0.17.2 of jedi also worked for me.  Was chasing my tail trying to figure out why it wasn't working in a new virtual environment, glad to have found this!  Hope the fix is out soon.  Luckily this showed up as the top link in Google for me when searching ""ipython init got an unexpected keyword argument 'column'"". Because people at this point probably don't scroll up to my comment, this is what you should do temporarily:   pip install jedi==0.17.2"
technical,"It does work!  Nice find! Because people at this point probably don't scroll up to my comment, this is what you should do temporarily:   pip install jedi==0.17.2"
technical,"It does work!  Nice find! BTW, ""Effective Python"" (2nd edition) #89 suggests using the warnings module to handle things like deprecating arguments."
technical,"As a temporary fix for anyone just trying to get things working again  It would be really nice if you could quickly release a 7.19.1. (It's already fixed on master).  Sorry for that. I did not realize that IPython with that fix was not released yet. I usually test IPython completions before doing a Jedi release, but not this time :/. It will probably also not happen in the future anymore, because I'm going to release Jedi 1.0 soon, so this is probably the last time for a long time that you have to deal with deprecations in Jedi.  Still wish you a Merry Christmas! By the way, a 7.19.1 release with the dependency jedi<0.18.0 would also suffice."
technical,"As a temporary fix for anyone just trying to get things working again  It would be really nice if you could quickly release a 7.19.1. (It's already fixed on master).  Sorry for that. I did not realize that IPython with that fix was not released yet. I usually test IPython completions before doing a Jedi release, but not this time :/. It will probably also not happen in the future anymore, because I'm going to release Jedi 1.0 soon, so this is probably the last time for a long time that you have to deal with deprecations in Jedi.  Still wish you a Merry Christmas! Can you explain what you mean by ""same kernel ..."" vs ""using the jupyter notebook ...""   i.e.,  will conda install jedi==0.17.2 from the shell command line do the trick?  (it seems to)"
technical,"For reference, the autocomplete on my Jupyter notebook wasn't working and I was getting this same error on the terminal.  And now it's working as expected after downgrading jedi to 0.17.2 by just executing the command bl-ue mentioned. Confirmed that downgrading jedi to 0.17.2 fixed the issue for me as well. Thanks"
technical,"I am really disappointed that the maintainer of a great product as Ipython has such a limited knowledge about the problem.  The fact that Ipython has not been pinning jedi means that any version of it released in the past that depends on jedi is at its merci to stop working as soon as a no backwards compatible change is introduced and used in Ipython.  By changing Ipython to make it jedi 0.18 compatible and releasing 7.20 you make that 7.20 operational today. If tomorrow jedi makes another backwards incompatible change We will be screwed again.  And you can't blame jedi for backwards incompatible changes because it is not even 1.x and you are not even pinning the major.  You are calling for trouble.  I'm amazed that someone that contribute squat is telling me what i'm doing wrong without tryign to really think about the problem.  - Say I pin IPython 7.20 to jedi <0.19 , and david release jedi 0.19. - pip is no free to say ""hey let's install jedi 0.19 and downgrade IPython to 5.x"". - Bam you are wrong„ .     And second the change of API were in jedi 0.17 with deprecation warnings and fix in master close to a year ago, I thought that by now 8.0 would be out.  FYI about me know nothing about versioning: - I spend month implementing on pushing the python requires logic in pypi, and in pip to make sure when IPython dropped it did not break for Python 2 users - I'm one of the regular advocate to have version medata data and package content to be separated in conda/pypi for this exact reason. - It's not because something seem obvious that it's correct.  You could have just asked in for a question ""why are not you pinning to <0.19 starting with IPython 7.20"", to which I would have responded. So not only you are impolite and haven't done your research,  everybody see you incorrectly assume you know better than others and  you've lost all credibility. dude, you rock!"
technical,"I am really disappointed that the maintainer of a great product as Ipython has such a limited knowledge about the problem.  The fact that Ipython has not been pinning jedi means that any version of it released in the past that depends on jedi is at its merci to stop working as soon as a no backwards compatible change is introduced and used in Ipython.  By changing Ipython to make it jedi 0.18 compatible and releasing 7.20 you make that 7.20 operational today. If tomorrow jedi makes another backwards incompatible change We will be screwed again.  And you can't blame jedi for backwards incompatible changes because it is not even 1.x and you are not even pinning the major.  You are calling for trouble.  I'm amazed that someone that contribute squat is telling me what i'm doing wrong without tryign to really think about the problem.  - Say I pin IPython 7.20 to jedi <0.19 , and david release jedi 0.19. - pip is no free to say ""hey let's install jedi 0.19 and downgrade IPython to 5.x"". - Bam you are wrong„ .     And second the change of API were in jedi 0.17 with deprecation warnings and fix in master close to a year ago, I thought that by now 8.0 would be out.  FYI about me know nothing about versioning: - I spend month implementing on pushing the python requires logic in pypi, and in pip to make sure when IPython dropped it did not break for Python 2 users - I'm one of the regular advocate to have version medata data and package content to be separated in conda/pypi for this exact reason. - It's not because something seem obvious that it's correct.  You could have just asked in for a question ""why are not you pinning to <0.19 starting with IPython 7.20"", to which I would have responded. So not only you are impolite and haven't done your research,  everybody see you incorrectly assume you know better than others and  you've lost all credibility. Even Anaconda followed with this already, so that shouldn't matter."
technical,Thanks a lot. I thought my shell was broken. Every time when I tried to use tab completion in IPython it crashed. Glad I found a solution. Finally found a solution! thank you
technical,"Because people at this point probably don't scroll up to my comment, this is what you should do temporarily:   pip install jedi==0.17.2 For reference, the autocomplete on my Jupyter notebook wasn't working and I was getting this same error on the terminal.  And now it's working as expected after downgrading jedi to 0.17.2 by just executing the command bl-ue mentioned."
technical,"when doing It seems to work, I still have to update to the current Jedi. Got this issue as well. Pinning jedi==0.17.2 worked for me, thanks."
technical,"Confirmed that downgrading jedi to 0.17.2 fixed the issue for me as well. Thanks Hi everyone, I've stumbled on this issue while creating docker images for myself. If I understand correctly from this comment the problem has already been solved on master, but so far no release including the fix has been issued. In order to better organize my own work, may I know when do you plan a new release? Somebody here was suggesting to quickly release 7.19.1, including this patch, is it still an option?"
technical,"Even Anaconda followed with this already, so that shouldn't matter. Hmm.. Then why is it that creating a new conda environment (with conda 4.9.2) gives me: Am I missing something here?"
technical,"I'll try to release 7.20 soon, I've make it compatible with jedi 0.18 as pip also use a resolver and so pinning is not an option as pip would be free to downgrade IPython and keep jedi 0.18 which will brake.   Right now the limiting factor for the release is writing the what's new.  I am really disappointed that the maintainer of a great product as Ipython has such a limited knowledge about the problem. The fact that Ipython has not been pinning jedi means that any version of it released in the past that depends on jedi is at its merci to stop working as soon as a no backwards compatible change is introduced and used in Ipython. By changing Ipython to make it jedi 0.18 compatible and releasing 7.20 you make that 7.20 operational today. If tomorrow jedi makes another backwards incompatible change We will be screwed again. And you can't blame jedi for backwards incompatible changes because it is not even 1.x and you are not even pinning the major. You are calling for trouble. I am really disappointed that the maintainer of a great product as Ipython has such a limited knowledge about the problem.  The fact that Ipython has not been pinning jedi means that any version of it released in the past that depends on jedi is at its merci to stop working as soon as a no backwards compatible change is introduced and used in Ipython.  By changing Ipython to make it jedi 0.18 compatible and releasing 7.20 you make that 7.20 operational today. If tomorrow jedi makes another backwards incompatible change We will be screwed again.  And you can't blame jedi for backwards incompatible changes because it is not even 1.x and you are not even pinning the major.  You are calling for trouble.  I'm amazed that someone that contribute squat is telling me what i'm doing wrong without tryign to really think about the problem.  - Say I pin IPython 7.20 to jedi <0.19 , and david release jedi 0.19. - pip is no free to say ""hey let's install jedi 0.19 and downgrade IPython to 5.x"". - Bam you are wrong„ .     And second the change of API were in jedi 0.17 with deprecation warnings and fix in master close to a year ago, I thought that by now 8.0 would be out.  FYI about me know nothing about versioning: - I spend month implementing on pushing the python requires logic in pypi, and in pip to make sure when IPython dropped it did not break for Python 2 users - I'm one of the regular advocate to have version medata data and package content to be separated in conda/pypi for this exact reason. - It's not because something seem obvious that it's correct.  You could have just asked in for a question ""why are not you pinning to <0.19 starting with IPython 7.20"", to which I would have responded. So not only you are impolite and haven't done your research,  everybody see you incorrectly assume you know better than others and  you've lost all credibility."
technical,When is the team going to fix this? In just a month hundreds of users have encountered this issue. I guess pinning jedi to 0.17.2 should be added to the readme. Anyone think I should open a PR for that?
technical,"Thanks for your solution. it finally work. I had the same problem with ipython and thanks to the solution, it works again."
technical,"BTW, ""Effective Python"" (2nd edition) #89 suggests using the warnings module to handle things like deprecating arguments. I just uploaded the downgrade instructions to Python Like You Mean It."
technical,"Thank you, both of you, for the useful insights and recommendations. I really appreciate it.  It sounds like I will go the route of putting a call-out box in my website that adds a hand-holding jedi-downgrading walkthrough (although, then I have to hope that the students have the awareness to repeat that step when creating new environments)  As it stands, I am dangerously close to making *two* ""but think of the children"" pleas in a single github issue, which is a violation of one of my new years resolutions.  (mamba looks cool!) I raised an issue with conda itself, since it's probably too much (though not impossible) to do this on the conda-forge packaging side (in a way that's also compatible across several jedi-versions) - raised an issue to discuss that too...  Carreau said he'd make a new release soon, which would of course also solve things.  In the meantime, the ""hand-holding"" guide should be as easy as conda install ipython ""jedi<0.18"" (the quotes are important though, as otherwise you end up piping to nowhere)."
technical,"Thank you, both of you, for the useful insights and recommendations. I really appreciate it.  It sounds like I will go the route of putting a call-out box in my website that adds a hand-holding jedi-downgrading walkthrough (although, then I have to hope that the students have the awareness to repeat that step when creating new environments)  As it stands, I am dangerously close to making *two* ""but think of the children"" pleas in a single github issue, which is a violation of one of my new years resolutions.  (mamba looks cool!) I wonder, when you have a dependency (jedi) that is in version 0.x.y (not reached major 1), if it is not wiser to pin the minor as =0.17,<0.18."
technical,"I'll try to release 7.20 soon, I've make it compatible with jedi 0.18 as pip also use a resolver and so pinning is not an option as pip would be free to downgrade IPython and keep jedi 0.18 which will brake.  Right now the limiting factor for the release is writing the what's new. I'll try to release 7.20 soon, I've make it compatible with jedi 0.18 as pip also use a resolver and so pinning is not an option as pip would be free to downgrade IPython and keep jedi 0.18 which will brake.   Right now the limiting factor for the release is writing the what's new.  I am really disappointed that the maintainer of a great product as Ipython has such a limited knowledge about the problem. The fact that Ipython has not been pinning jedi means that any version of it released in the past that depends on jedi is at its merci to stop working as soon as a no backwards compatible change is introduced and used in Ipython. By changing Ipython to make it jedi 0.18 compatible and releasing 7.20 you make that 7.20 operational today. If tomorrow jedi makes another backwards incompatible change We will be screwed again. And you can't blame jedi for backwards incompatible changes because it is not even 1.x and you are not even pinning the major. You are calling for trouble."
technical,"Nice! I just confirmed this on my end. I guess I'll keep those callout boxes up on PLYMI for a bit in case folks had already installed anaconda. I'll try to release 7.20 soon, I've make it compatible with jedi 0.18 as pip also use a resolver and so pinning is not an option as pip would be free to downgrade IPython and keep jedi 0.18 which will brake.  Right now the limiting factor for the release is writing the what's new."
technical,"Nice! I just confirmed this on my end. I guess I'll keep those callout boxes up on PLYMI for a bit in case folks had already installed anaconda. installing 0.17.2 of jedi also worked for me.  Was chasing my tail trying to figure out why it wasn't working in a new virtual environment, glad to have found this!  Hope the fix is out soon.  Luckily this showed up as the top link in Google for me when searching ""ipython init got an unexpected keyword argument 'column'""."
technical,The following works in ipython/jupyter.   I assume it gets jedi out of the way of the built-in completer.   Good enough for me until fix is released. It does work!  Nice find!
technical,"I guess pinning jedi to 0.17.2 should be added to the readme. Anyone think I should open a PR for that? just adding a +1 to this issue, it totally disables tab completion in Jupyter, so worth pinning 0.17.2 until resolved"
technical,"Not you, but conda's dependency resolver. This is a problem and has been for years.  (Sorry for the off-topic, but I could not resist) My totally uneducated guess (because I never understood the internals of the conda resolver): conda sees the conflict for jedi==0.18.0 and ipython-7.19.0-... 1 and instead of resolving it by jedi==0.17.2 and ipython-7.19.0-... 1 it chooses jedi==0.18.0 and ipython-7.19.0-... 0"
technical,"Actually, anaconda fixed their repo data as well, so now it works out of the box again Nice! I just confirmed this on my end. I guess I'll keep those callout boxes up on PLYMI for a bit in case folks had already installed anaconda."
technical,"So the packages are definitely there (notice the  1 build number), but interestingly, are not seeing much (or even any) downloads. I don't know if your anaconda channel is behind a proxy somewhere (or indeed, if anaconda themselves do some more integration testing before a package goes ""live""), but in any case, you can work around this by adding conda config --add channels conda-forge.  Side note: For any user that's not under the strictest of regulatory requirements (where people want to be able to blame a provider like Anaconda if stuff goes bad), conda-forge is more than ready/stable/secure to be your default channel. But, fair warning, that's just my subjective opinion. Not you, but conda's dependency resolver. This is a problem and has been for years.  (Sorry for the off-topic, but I could not resist)"
technical,"pip is no[w] free to say ""hey let's install jedi 0.19 and downgrade IPython to 5.x"".  That would indeed be unfortunate, the resulting setup would be not only broken, but out-of-date to boot. Can that happen when running pip install ipython in a fresh environment? I would naively assume that this would fetch the latest version of IPython and figure out the dependencies from there, i.e. honoring jedi < 0.19. But that assumption is probably wrong?  As maintainer of my own small Python package with moderately complicated dependencies, I feel like I should read up on this :) Any tips on good material on this topic? Obviously you have a different perspective on that matter. That's fine. However, it seems rather unproductive to dissipate the time and energy of productive people like Carreau with pointless discussions. If you have deeper insights I see the following options: You can fork the project, file a PR or just sate your improvement suggestion in a way that is arguable, maybe in a dedicated issue.  For the rest of us I would suggest to basically ignore distracting comments which contain neither helpful information nor constructive criticism, but instead spread insults and negative emotion. Reactinv with :-1: should be enough attention for such comments."
technical,"So essentially you say as there are problems with pip resolver and in some cases it will not be able to upgrade correctly I will completely ignore the problem and not try to solve other uses cases that I can. Yes, fixing Ipython to work with the latest jedi is correct and pinning is needed as well. Most use cases of Ipython will be install Ipython on a new environment. Should you pin, your 7.20 will work always. Ignore it and you depend on your dependencies. The reason I didn't ask why you are planning to do what you are planning to do is because I know that solution is suboptimal. It doesn't matter what work or knowledge has been accumulated in the past, that says nothing. pip is no[w] free to say ""hey let's install jedi 0.19 and downgrade IPython to 5.x"".  That would indeed be unfortunate, the resulting setup would be not only broken, but out-of-date to boot. Can that happen when running pip install ipython in a fresh environment? I would naively assume that this would fetch the latest version of IPython and figure out the dependencies from there, i.e. honoring jedi < 0.19. But that assumption is probably wrong?  As maintainer of my own small Python package with moderately complicated dependencies, I feel like I should read up on this :) Any tips on good material on this topic?"
technical,Thank you for opening this issue.  Happy holiday! Posted a pr to pin the dependency as suggested: #12746
technical,"What the timeline looks like for this getting fixed for mainline conda users?  I ask because I have an online outreach course spinning up in about four days, meaning that 600+ highschool students are going to be encountering the ""now hit <TAB to autocomplete ˜"" portion of their virtual course, and it is going unceremoniously bomb them out of ipython.  Downgrading jedi is easy enough for me, but much harder for someone who has never written a line of code before. Releasing a new Jedi version might be easier than waiting for the next IPython release.  how about making a one time exception in this case and reintroducing the deprecated features, the lack of which breaks IPython (and a few more packages) in a Jedi 0.18.1?"
technical,"dude, you rock! So essentially you say as there are problems with pip resolver and in some cases it will not be able to upgrade correctly I will completely ignore the problem and not try to solve other uses cases that I can. Yes, fixing Ipython to work with the latest jedi is correct and pinning is needed as well. Most use cases of Ipython will be install Ipython on a new environment. Should you pin, your 7.20 will work always. Ignore it and you depend on your dependencies. The reason I didn't ask why you are planning to do what you are planning to do is because I know that solution is suboptimal. It doesn't matter what work or knowledge has been accumulated in the past, that says nothing."
technical,"Hmm.. Then why is it that creating a new conda environment (with conda 4.9.2) gives me: Am I missing something here? So the packages are definitely there (notice the  1 build number), but interestingly, are not seeing much (or even any) downloads. I don't know if your anaconda channel is behind a proxy somewhere (or indeed, if anaconda themselves do some more integration testing before a package goes ""live""), but in any case, you can work around this by adding conda config --add channels conda-forge.  Side note: For any user that's not under the strictest of regulatory requirements (where people want to be able to blame a provider like Anaconda if stuff goes bad), conda-forge is more than ready/stable/secure to be your default channel. But, fair warning, that's just my subjective opinion."
technical,"Thanks a lot! I guess the next release is going to be 1.0 anyway, so it shouldn't happen again. We might then think about pinning jedi < 2.0.0 (which is probably years away).  In general for all people like mostealth: If you are worried about older versions breaking: **Pin your dependencies!** The Python packaging system is still in a bit of a problematic state. The devs are doing a good job trying to turn it around, but there is a lot of technical debt from years ago. Keep in mind they want to keep backwards compatibility with a lot of old and new things. Just a enumerating a few things to show how complicated it is: eggs, wheels, explicit namespace packages, setuptools, distutils, import hooks, import meta paths, importlib, easy install, venvs, virtualenvs, zip imports, freezed packages, source packages for C/C++ libraries and a lot of other things that I don't even know or remember anymore.  Most of you guys complaining don't really understand how hard certain things are if you consider all consequences. Read about Chesterton's Fence and try to understand why you shouldn't want something changed unless you understand why it is that way in the first place. Thank you Carreau. Finally someone who does not follow the church of pinning stuff without thinking about the consequences. Here is another case why pinning to older versions is not always a good idea.  Unfortunately the Jedi and Parso pair are a frequent cause for headaches for packagers. Not only pip and conda, but also Linux distributions. I am looking forward to a more stable API once they reach 1.0."
technical,"I had the same problem with ipython and thanks to the solution, it works again. Thank you for opening this issue.  Happy holiday!"
technical,"Welp, spoke too soon, conda-forge has the same problem (basically, the higher build number seems to lose against having one dependency have a higher version, which is not how it  should  work. A possible solution would be backporting to the feedstock, but that really should happen here first). Have you heard of mamba already? It has a rewritten resolver but is fully compatible with the conda-ecosystem. Not only does it solve the biggest problem of the conda-solver (speed), but it also correctly sets up the environment here: Thank you, both of you, for the useful insights and recommendations. I really appreciate it.  It sounds like I will go the route of putting a call-out box in my website that adds a hand-holding jedi-downgrading walkthrough (although, then I have to hope that the students have the awareness to repeat that step when creating new environments)  As it stands, I am dangerously close to making *two* ""but think of the children"" pleas in a single github issue, which is a violation of one of my new years resolutions.  (mamba looks cool!)"
technical,"Obviously you have a different perspective on that matter. That's fine. However, it seems rather unproductive to dissipate the time and energy of productive people like Carreau with pointless discussions. If you have deeper insights I see the following options: You can fork the project, file a PR or just sate your improvement suggestion in a way that is arguable, maybe in a dedicated issue.  For the rest of us I would suggest to basically ignore distracting comments which contain neither helpful information nor constructive criticism, but instead spread insults and negative emotion. Reactinv with :-1: should be enough attention for such comments. Thanks a lot! I guess the next release is going to be 1.0 anyway, so it shouldn't happen again. We might then think about pinning jedi < 2.0.0 (which is probably years away).  In general for all people like mostealth: If you are worried about older versions breaking: **Pin your dependencies!** The Python packaging system is still in a bit of a problematic state. The devs are doing a good job trying to turn it around, but there is a lot of technical debt from years ago. Keep in mind they want to keep backwards compatibility with a lot of old and new things. Just a enumerating a few things to show how complicated it is: eggs, wheels, explicit namespace packages, setuptools, distutils, import hooks, import meta paths, importlib, easy install, venvs, virtualenvs, zip imports, freezed packages, source packages for C/C++ libraries and a lot of other things that I don't even know or remember anymore.  Most of you guys complaining don't really understand how hard certain things are if you consider all consequences. Read about Chesterton's Fence and try to understand why you shouldn't want something changed unless you understand why it is that way in the first place."
technical,Posted a pr to pin the dependency as suggested: #12746 Thanks a lot. I thought my shell was broken. Every time when I tried to use tab completion in IPython it crashed. Glad I found a solution.
technical,"Thank you Carreau. Finally someone who does not follow the church of pinning stuff without thinking about the consequences. Here is another case why pinning to older versions is not always a good idea.  Unfortunately the Jedi and Parso pair are a frequent cause for headaches for packagers. Not only pip and conda, but also Linux distributions. I am looking forward to a more stable API once they reach 1.0. Thanks everyone for the inputs, if we want to have discussion as to why/when pinning and why when not to pin, and other I suggest we don't use the bug tracker, maybe discourse.jupyter.org, but stop adding to this particular issue.  IPython is a bit in a tough spot as it is both a library and application (in general you want to have a lock file to pin deps for applications, but be as liberal as possible when building a library).  There is also a lot of issues that arise from the fact that maintainers time and desire to maintain can come and go, I definitely thought that by sept 2020 IPython 8 would be out, and that other maintainers like davidhalter from jedi would also welcome more testing and help.  The Python package ecosystem is also quite old, and PyPI relatively recent, the requirements and needs of today's infrastructure can also be much different than what was expected even 5 years ago. That plus it works from HPC system to end-user  that are starting to program so there is a lot of difficulties there.  Thanks everyone, I've now released 7.20 feel free to open bug reports if you encounter any issues. For now I'm going to lock this thread to avoid getting much more off topic."
technical,"By the way, a 7.19.1 release with the dependency jedi<0.18.0 would also suffice. Thanks for your solution. it finally work."
technical,"You can run Jupyter in one env and use it with a kernel from another env. See for example this SO question. Jedi version of the latter env matters. If you are not aware of this, you probably use one conda env for jupyter and kernel. The following works in ipython/jupyter.   I assume it gets jedi out of the way of the built-in completer.   Good enough for me until fix is released."
technical,"You might be successful if you ask the conda maintainers to yank jedi 0.18 from their repos. The last ipython builds on conda-forge already depend on jedi<0.18, so this this shouldn't even happen if you create a new environment (or do conda update ipython first)."
technical,"I raised an issue with conda itself, since it's probably too much (though not impossible) to do this on the conda-forge packaging side (in a way that's also compatible across several jedi-versions) - raised an issue to discuss that too...  Carreau said he'd make a new release soon, which would of course also solve things.  In the meantime, the ""hand-holding"" guide should be as easy as conda install ipython ""jedi<0.18"" (the quotes are important though, as otherwise you end up piping to nowhere). There was one other way I hadn't thought about - fixing the conda-forge repodata directly. This was done yesterday, and means the problem is fixed  within  conda-forge, i.e. when using strict channel priority (ignoring the main anaconda package that still has the same problem) - which unfortunately is not the out-of-the-box behaviour for conda though.  You can either do this for a single install (e.g. conda create -n test --strict-channel-priority python=3.8 ipython), but the recommended way (see e.g. here) is persisting that setting with"
technical,"Thanks everyone for the inputs, if we want to have discussion as to why/when pinning and why when not to pin, and other I suggest we don't use the bug tracker, maybe discourse.jupyter.org, but stop adding to this particular issue.  IPython is a bit in a tough spot as it is both a library and application (in general you want to have a lock file to pin deps for applications, but be as liberal as possible when building a library).  There is also a lot of issues that arise from the fact that maintainers time and desire to maintain can come and go, I definitely thought that by sept 2020 IPython 8 would be out, and that other maintainers like davidhalter from jedi would also welcome more testing and help.  The Python package ecosystem is also quite old, and PyPI relatively recent, the requirements and needs of today's infrastructure can also be much different than what was expected even 5 years ago. That plus it works from HPC system to end-user  that are starting to program so there is a lot of difficulties there.  Thanks everyone, I've now released 7.20 feel free to open bug reports if you encounter any issues. For now I'm going to lock this thread to avoid getting much more off topic. This is the repository for IPython command line, if you can try to make sure this question/bug/feature belong here and not on one of the Jupyter repositories. If it's a generic Python/Jupyter question, try other forums or discourse.jupyter.org. If you are unsure, it's ok to post here, though, there are few maintainer so you might not get a fast response. Ability of maintainers to spend time and resources on project like IPython is heavily influenced by US politics, and the current government policies have been harmful to the IPython Maintainers and Community. If you are on the fence on who to vote for or wether to vote, please cast your vote in for the democrat party in the US. Relevant traceback reads as follows: same reported in jedi repo too"
technical,"Thanks everyone for the inputs, if we want to have discussion as to why/when pinning and why when not to pin, and other I suggest we don't use the bug tracker, maybe discourse.jupyter.org, but stop adding to this particular issue.  IPython is a bit in a tough spot as it is both a library and application (in general you want to have a lock file to pin deps for applications, but be as liberal as possible when building a library).  There is also a lot of issues that arise from the fact that maintainers time and desire to maintain can come and go, I definitely thought that by sept 2020 IPython 8 would be out, and that other maintainers like davidhalter from jedi would also welcome more testing and help.  The Python package ecosystem is also quite old, and PyPI relatively recent, the requirements and needs of today's infrastructure can also be much different than what was expected even 5 years ago. That plus it works from HPC system to end-user  that are starting to program so there is a lot of difficulties there.  Thanks everyone, I've now released 7.20 feel free to open bug reports if you encounter any issues. For now I'm going to lock this thread to avoid getting much more off topic. This issue should be pinned on the GitHub repo."
technical,"When would the problem be fixed so that every Jedi version is compatible with ipython? tik9 Does installing from master work for you If so, whenever ipython releases a new version, it will be fixed for everyone."
technical,"My totally uneducated guess (because I never understood the internals of the conda resolver): conda sees the conflict for jedi==0.18.0 and ipython-7.19.0-... 1 and instead of resolving it by jedi==0.17.2 and ipython-7.19.0-... 1 it chooses jedi==0.18.0 and ipython-7.19.0-... 0 Welp, spoke too soon, conda-forge has the same problem (basically, the higher build number seems to lose against having one dependency have a higher version, which is not how it  should  work. A possible solution would be backporting to the feedstock, but that really should happen here first). Have you heard of mamba already? It has a rewritten resolver but is fully compatible with the conda-ecosystem. Not only does it solve the biggest problem of the conda-solver (speed), but it also correctly sets up the environment here:"
technical,"This issue should be pinned on the GitHub repo. What the timeline looks like for this getting fixed for mainline conda users?  I ask because I have an online outreach course spinning up in about four days, meaning that 600+ highschool students are going to be encountering the ""now hit <TAB to autocomplete ˜"" portion of their virtual course, and it is going unceremoniously bomb them out of ipython.  Downgrading jedi is easy enough for me, but much harder for someone who has never written a line of code before."
technical,"tik9 Does installing from master work for you If so, whenever ipython releases a new version, it will be fixed for everyone. when doing It seems to work, I still have to update to the current Jedi."
technical,"Because people at this point probably don't scroll up to my comment, this is what you should do temporarily:   pip install jedi==0.17.2 When is the team going to fix this? In just a month hundreds of users have encountered this issue."
technical,Finally found a solution! thank you When would the problem be fixed so that every Jedi version is compatible with ipython?
technical,"Can you explain what you mean by ""same kernel ..."" vs ""using the jupyter notebook ...""   i.e.,  will conda install jedi==0.17.2 from the shell command line do the trick?  (it seems to) You can run Jupyter in one env and use it with a kernel from another env. See for example this SO question. Jedi version of the latter env matters. If you are not aware of this, you probably use one conda env for jupyter and kernel."
technical,"Releasing a new Jedi version might be easier than waiting for the next IPython release.  how about making a one time exception in this case and reintroducing the deprecated features, the lack of which breaks IPython (and a few more packages) in a Jedi 0.18.1? You might be successful if you ask the conda maintainers to yank jedi 0.18 from their repos."
technical," Actually, the way you're doing it for Visual Studio Code is perfect - why not use the same approach here if possible? :)"
technical,"Just having automatic updates install upon next launch would be a perfectly fine behavior. Also note, there is a massive difference in these experiences: 1) windows restarts while my console apps are running and all my terminal state is lost AND my enqueued commands for long-running operations to finish while i am sleeping are in an unknown state 2) windows restarts for updates after my enqueued commands finish and all my terminal state is lost 3) windows waits to restart before my enqueued commands finish and restarts terminals with the same windows open 4) windows waits to restart before my enqueued commands finish, and upon restart, terminal relaunches to to the exact same display state 5) windows waits to restart before my enqueued commands finish, and upon restart, terminal relaunches to to the exact same display state including command history  please, on behalf of developers everywhere with long build times, consider moving us more and more to #5"
technical,"What's the point of swooping in like a hawk to close other reports of this problem if this issue is just going to sit here, stale? because having six copies of an issue is harder on us, and harder to present to our leadership, than one issue with a robust single discussion thread in it? This issue isn't stale insomuch as it has to move at the speed of Windows, rather than the speed of our open-source project."
technical,"A bit of research can help you get the info you need in order to be able to perform the necessary tests to confirm/deny your theory so you can start acting on knowledge. Don't act on theories! I support your reasoning, but a simple model proves that the more folders you have on the PATH, and the more items therein, the more time it takes to search for a specified EXE or DLL.  Of course, the actual measurable impact depends on how much optimization (e.g. caching) has been put into that search algorithm, and of the platform's performance.  And that is context and OS-dependent (the articles you refer to pertain to searches done by the bash shell on Unix, my use case is the Windows APIs that rely on EXE/DLL searches on the PATH).  My plan is to start a discussion there"
technical,"A bit of research can help you get the info you need in order to be able to perform the necessary tests to confirm/deny your theory so you can start acting on knowledge. Don't act on theories! So you did  not  install from the msixbundle available from the 'Releases' page?  That's what I just did, and how Store reports the app is 'installed'... so I expect it will still be subject to (auto)updates. Correct?  Can you please elaborate on how you got a runnable instance deployed without Store considering it's installed (and thus needs updating)?"
technical,"You can ""unzip"" the msixbundle, and then just run the exe from there. I copied the folder to Program Files and added it to my PATH env for extra convenience.  I have had no problems with auto-updates since doing it this way. I watch the github project for any releases that have fixes/features that I want. Thanks for the tip!  I chose  not  to add the folder to the PATH, because that's not absolutely required (and I have a theory where adding tons of app folders to the PATH would slow down the system...).  Something worth mentioning: uninstalling will delete where the settings.json file is located (I know, that's part of the ""good"" sandboxing idea beyond store app deployment), so you'll lose your settings if you don't make a copy first.  In the ""XCOPY-deployed"" instance, the settings.json file resides at Windows Terminal."
technical,"+1 I just lost hours worth of state because of this behavior. The other way around is also not good. Previously I had a lot of errors in my Event Viewer from Windows Update about the fact that Windows Terminal can't be updated, because the executable is running. We need a better process of updating the app. And it would be nice if we have some kind of configuration options like: automatic restart after update - on/off and so on :)"
technical,"Seeing this issue as well, its very annoying. What's the point of swooping in like a hawk to close other reports of this problem if this issue is just going to sit here, stale?"
technical,"So you did  not  install from the msixbundle available from the 'Releases' page?  That's what I just did, and how Store reports the app is 'installed'... so I expect it will still be subject to (auto)updates. Correct?  Can you please elaborate on how you got a runnable instance deployed without Store considering it's installed (and thus needs updating)? You can ""unzip"" the msixbundle, and then just run the exe from there. I copied the folder to Program Files and added it to my PATH env for extra convenience.  I have had no problems with auto-updates since doing it this way. I watch the github project for any releases that have fixes/features that I want."
technical,"I see that you've mentioned this issue from other closed issues but you have not actually commented here regarding the problem or the intention to fix it. I was about to open a new report because this issue also doesn't capture the entire story. But instead I'll post some findings here and hope that you can shed some more light on what the plans are.  First, the problem is not just that the console sessions are silently closed when the program automatically updates. I haven't been able to fully trace what set of circumstances causes this, but I've had processes just get orphaned. The only reason I even realize this is that when I go to re-open one of my command line processes, I see that the listening port is still in use and I can't open it. I've then had to go searching through TCPView and Process Explorer to find the orphaned process and kill it.  Second, I tried to circumvent all of this by installing the Terminal app via Chocolatey. However, it still gets updated via the Windows Store!? I'm not sure why that would happen if I did not install via the Store.  At the very least I would  LOVE  an option to disable automatic updates. Another solution might be to tie  all  updates to Terminal to the next Windows reboot cycle? But I think from your comments on other threads this is what you aren't able to do when distributing via the Store.  I would also make the recommendation to just not distribute this application on the Windows Store. This is not really an ideal app for that delivery mechanism. This is a power user / developer targeted app, so I would think it would be distributed more like Visual Studio or Visual Studio Code, or even Power Toys.  Thanks for your teams work on this app. Unfortunately I am going to have to abandon it until this issue is fixed because I'm losing a lot of time and work every other week when an update is pushed and my long-running processes are lost. You've got the critical points down pat.  For right now, it's possible to extract the msixbundle (and the architecture-specific msix inside it) and run WindowsTerminal.exe directly. We're not intending on breaking this."
technical,Or you could allow VB.net to **consume** types of this kind. AdamSpeight2008 i believe the decisoin was already made on that that this was not supported.  Hence the use of ObsoleteAttribute in the first place.  Note that this issue applies to C# as well as VB.
technical,"Not interested. Unsubscribing. As I said: Should we introduce a new category of obsolete (similar to how we added diagnostics to them)? i.e. one that is an error, but is always an error no matter what?"
technical,"In fact, VB can't violate ref structs rules, as this throws external runtime exceptions, so, it is not that dangerous as it will not work! So, I strongly recommend that you take no action about that. I came up with this Obsolete workaround to allow VB to use important libraries such as System.Text.Json, and it worked. All we need is to read the rsf types, not to copy or box them in any way. We can stick to that and hold responsible for any outcome. I see no benefits of denying VB access to new .NET core APIs that use ref structs. I found a cheap workaround, but seems he  wants to put money and effort to take it out of our hands. This seems a loose/loose situation for the team and VB devs, and serves no purpose at all except sending a repeated message that VB is being deliberately killed, otherwise, why it only gets changes when it is not of favor of anyone? I think VB will do much better if you left it as is. No more damage please. Thanks. I think we need to hear VB community voice here"
technical,"The issue is assignment, if all I do is pass a Span or Ref Struct to another API that understands them the concern would not hold. That way VB could access all the New high performance API's and not get into the complexity they require to implement API's using these unsafe features. This is similar as VB's IntPtr it gets from Windows and is only safe to pass back to Windows.  To be clear I don't want to write any code in VB that accesses these Obsolete types This would be allowed. Obsolete reference would not be allowed to leak out of a statement. so below would not be allowed. In fact, VB can't violate ref structs rules, as this throws external runtime exceptions, so, it is not that dangerous as it will not work! So, I strongly recommend that you take no action about that. I came up with this Obsolete workaround to allow VB to use important libraries such as System.Text.Json, and it worked. All we need is to read the rsf types, not to copy or box them in any way. We can stick to that and hold responsible for any outcome. I see no benefits of denying VB access to new .NET core APIs that use ref structs. I found a cheap workaround, but seems he  wants to put money and effort to take it out of our hands. This seems a loose/loose situation for the team and VB devs, and serves no purpose at all except sending a repeated message that VB is being deliberately killed, otherwise, why it only gets changes when it is not of favor of anyone? I think VB will do much better if you left it as is. No more damage please. Thanks."
technical,"The benefit for me is the clarity that this just isn't supported or usable at all. Rather than being in this position where somethings work, but some do not, and now random cryptic runtime errors might get thrown all over the place. That doesn't seem to be a good thing.  I'd rather this either just be supported and usable, or not supported and not usable. Locking the convo until the team can decide on the best path forward here."
technical,It seems here that Microsoft is taking an active approach to kill VB actively (as opposed to passive approach - just let it die by abandoning it). :-( Not interested. Unsubscribing.
technical,It seems here that Microsoft is taking an active approach to kill VB actively (as opposed to passive approach - just let it die by abandoning it). :-( Or you could allow VB.net to **consume** types of this kind.
technical,I think we need to hear VB community voice here Please don't change the behavior of ObsoleteAttribute. Because my projects are already using obsolete warnings to suppress obsolete errors. For example this. Our commercial products are also using this behavior to workaround limitations of 3rd-party libraries.
technical,"As I said: Should we introduce a new category of obsolete (similar to how we added diagnostics to them)? i.e. one that is an error, but is always an error no matter what? That seems like a worse outcome. Code will compile without any indications if an issue, but then at runtime you will get cryptic runtime errors that don't even indicate where the problem exists."
technical,"That seems like a worse outcome. Code will compile without any indications if an issue, but then at runtime you will get cryptic runtime errors that don't even indicate where the problem exists. The benefit for me is the clarity that this just isn't supported or usable at all. Rather than being in this position where somethings work, but some do not, and now random cryptic runtime errors might get thrown all over the place. That doesn't seem to be a good thing.  I'd rather this either just be supported and usable, or not supported and not usable."
technical,"AdamSpeight2008 i believe the decisoin was already made on that that this was not supported.  Hence the use of ObsoleteAttribute in the first place.  Note that this issue applies to C# as well as VB. The issue is assignment, if all I do is pass a Span or Ref Struct to another API that understands them the concern would not hold. That way VB could access all the New high performance API's and not get into the complexity they require to implement API's using these unsafe features. This is similar as VB's IntPtr it gets from Windows and is only safe to pass back to Windows.  To be clear I don't want to write any code in VB that accesses these Obsolete types This would be allowed. Obsolete reference would not be allowed to leak out of a statement. so below would not be allowed."
technical,"Locking the convo until the team can decide on the best path forward here. This turns out to be pretty dangerous.  It's easy to create code now that captures spans and reads/writes from them long after their backing store is invalid. This was discovered as a way to workaround the restriction against using ref-structs in VB where lifetimes are not tracked.  However, even though there is no usage of any unsafe apis, it's now trivial to get into very unsafe scenarios that could trivially lead to memory corruption.  When we discussed 'obsolete' as the mechanism to disable access to this type, i'm not sure if it was ever recognized that there was this loophole.  I personally never realized an 'obsolete error' could be suppressed by an 'obsolete warning'. Should we introduce a new category of obsolete (similar to how we added diagnostics to them)?  i.e. one that is an error, but is always an error no matter what?"
technical,"Hey guys, Atom is taking 7GB on my SSD for 1 year of using it. Maybe its a problem after all? Adding to this, Atom was taking 5.5GB on my SSD as well, most of it in local appdata. Windows 10."
technical," Atom has been reduced in size significantly in the past (#6313, #6407, #6395, #6447, atom/notifications#54, among others). As a matter of fact, the app.asar that you mention compressing  is  a compressed file. (See the atom/asar repository for more information on the file format.)  If you have examples of duplicate code or specific suggestions of methods for shrinking the size of Atom, please open Issues or Pull Requests with that information. Unfortunately, I fear that simply ""make it smaller"" is not specific enough to be actionable."
technical,"What parts of WCF do you need? Soap (with http basic auth) stuff is pretty easy to implement with giraffe. Atom was taking up 1.5GiB (*one and a half gibibytes*) in my %APPDATA% folder on Windows 10. Because this goes on my OS SSD where space is very limited, I ended up uninstalling it. This is just ridicilous for a bloody text editor, but then again it's JavaScript."
technical,"I was searching about Atom's size and found this thread. I downloaded it from atom.io for my Mac and the app is a whopping 865MB!!! So is this normal? I was searching because I was afraid the download could have been compromised on the website or something like that...  How can a text editor take more than 800 MB of disk space? compare to sublime 3 is 7.2MB I'm trying to understand why it's so large: it seems a nodejs project / embedding a nodejs v0.10.40 binaries, and suffering the same nodejs project's recursive node modules problem, The largest ones are app.asar (72M), apm (33M), and node binaries (16M+11M), and there are 90 node modules directories totally, from 277 to unique 223 package names means a lot of potential duplicate code,  The app.asar (72M) resource file seems to be a large plain text file with 16 bytes binary header, is that a webpack or something I don't understand, look like a plain text javascript file, why it needs to be 72MB large? why not try some xz or gzip compressed resource?   Could you atom developers try to reduce the size ? is there any one making effort on any node dedupe solution? if can't compare with sublime, anyway keep install size below 100M or even smaller would be much appreciated, thanks"
technical,"Atom was taking up 1.5GiB (*one and a half gibibytes*) in my %APPDATA% folder on Windows 10. Because this goes on my OS SSD where space is very limited, I ended up uninstalling it. This is just ridicilous for a bloody text editor, but then again it's JavaScript. Hey guys, Atom is taking 7GB on my SSD for 1 year of using it. Maybe its a problem after all?"
technical,"In any case, as long as Atom uses electron, it's going to be an order of magnitude larger on disk than the example you provided (sublime 3 at 7 megs) because of libchromiumcontent, etc. We're generally working on reducing the file size of the editor where we can, but I am going to close this issue in favor of more specific issues - you could, for example, have a conversation about the pros and cons of asar actually doing compression rather than concatenation on the electron repository, or file an issue about problems with dependency deduping in atom.  Still, Atom is likely going to be in the 100 meg order of magnitude, not 10 , for the forseeable future. I really agree with crquan , The package is too big, However the Atom is open source, May be This is a only advantage vs sublime, If so, How can atom accepted by common people. We all know sublime can use free for a long time which you can think. lee-dohm as a common user, We already find the ""bug"" lies, not us to fix it. Not all common people go through all the code which Atom has. This is just suggestion or mind which everyone can see it."
technical,"There are a number of things that Atom could do to reduce install size, but tbh, does it  really  matter? Engineering work costs time, and optimizing disk space isn't really worth that time given that you can buy a 16GB USB Thumb drive on Amazon for less than a cup of coffee (in San Francisco :P). That's 78 copies of Atom, for 5. I think the original user's concern is not about disk space but download bandwidth. Disk space is indeed cheap. Bandwidth isn't so, especially if you live somewhere where you can't switch ISPs."
technical,"I used to use atom a lot, and now it pushed me to vscode side, because obviously this HUGE beast become a problem~ I understand that Node.js style code must be transmitted as text, and 1.0 GB sounds reasonable for a piece of software with the versatility of Atom. Admittedly, I am also not a Node.js developer, I  haven't used it in about a year and a half. However, I'm installing software on Mint 19 right now, and it's had to download an entire gigabyte.  Part of why I picked up Atom was because it was so much quicker and more lightweight than something like Visual Studio. Is there anyway you could perhaps take a machine-compiled portion of that code, and put it together as a binary on the host, then distribute it selectively? Something like binary, dev, and src packages? Because it is losing its original grace, and if this keeps up, I may have to switch."
technical,"There is a known issue on Windows where the auto-updater may fail to remove old versions of Atom.  It is safe to delete all the older versions. I used to use atom a lot, and now it pushed me to vscode side, because obviously this HUGE beast become a problem~"
technical,"I understand that Node.js style code must be transmitted as text, and 1.0 GB sounds reasonable for a piece of software with the versatility of Atom. Admittedly, I am also not a Node.js developer, I  haven't used it in about a year and a half. However, I'm installing software on Mint 19 right now, and it's had to download an entire gigabyte.  Part of why I picked up Atom was because it was so much quicker and more lightweight than something like Visual Studio. Is there anyway you could perhaps take a machine-compiled portion of that code, and put it together as a binary on the host, then distribute it selectively? Something like binary, dev, and src packages? Because it is losing its original grace, and if this keeps up, I may have to switch. I was searching about Atom's size and found this thread. I downloaded it from atom.io for my Mac and the app is a whopping 865MB!!! So is this normal? I was searching because I was afraid the download could have been compromised on the website or something like that...  How can a text editor take more than 800 MB of disk space?"
technical,"I really agree with crquan , The package is too big, However the Atom is open source, May be This is a only advantage vs sublime, If so, How can atom accepted by common people. We all know sublime can use free for a long time which you can think. lee-dohm as a common user, We already find the ""bug"" lies, not us to fix it. Not all common people go through all the code which Atom has. This is just suggestion or mind which everyone can see it. I'll add my comment, albeit late. I am using Atom on Xubuntu running alongside Chrome OS. I only have a 16 GB SSD to play with and once you factor in the Chrome OS, Xubuntu and the necessary software that comes with them, there isn't much room left. I currently only have 2.5 GB left so Atom is taking up 10% of that."
technical,"Atom has been reduced in size significantly in the past (#6313, #6407, #6395, #6447, atom/notifications#54, among others). As a matter of fact, the app.asar that you mention compressing  is  a compressed file. (See the atom/asar repository for more information on the file format.)  If you have examples of duplicate code or specific suggestions of methods for shrinking the size of Atom, please open Issues or Pull Requests with that information. Unfortunately, I fear that simply ""make it smaller"" is not specific enough to be actionable. I'm not a node developer don't even know how to hack this project. Only providing my general feedback here, 208MB is beyond the size of an editor should be,  the app.asar is not compressed anyway, if you open it with a hex editor, you will see 90%+ of this 72MB file is plain text, which means further compressable, here I apply a simple xz compress get a 13MB file, but I don't know how to change the main binary to load resource from the compressed file"
technical,"This is challenging the person, not the idea, and is generally considered poor form in online discussions.  You still haven't addressed my concern with this Issue, crquan, namely that there isn't a method of determining if Atom is ""beyond the size of an editor should be"". You only state that 208MB is too much. A bug is nearly worthless if it cannot ever be fixed. And without a clearly defined goal, this bug cannot be fixed.  Is 207MB too big? Is 200MB? Is 10MB? Please give an example of what size is ""small enough"" and what size ""too big"" and, most importantly, why. In any case, as long as Atom uses electron, it's going to be an order of magnitude larger on disk than the example you provided (sublime 3 at 7 megs) because of libchromiumcontent, etc. We're generally working on reducing the file size of the editor where we can, but I am going to close this issue in favor of more specific issues - you could, for example, have a conversation about the pros and cons of asar actually doing compression rather than concatenation on the electron repository, or file an issue about problems with dependency deduping in atom.  Still, Atom is likely going to be in the 100 meg order of magnitude, not 10 , for the forseeable future."
technical,"**I do not think** it is a good idea to close this, even have a plan to reduce the size of atom, why not keep this open as a place where can remind us we have a really important thing to do. not just a idea. paulcbetts, I want to speak directly, the words ""very cheap to buy a USB"" seems really ridiculous. if that is right, why not make atom more bigger, to 2G. or what else...... what is the meaning of one software which exist? especially loved by people. that is ..... everyone knows it. maybe you will tell me, Atom is a open source, you just work for it as volunteer. That make no sense, why you make efforts to a open source, because you love it. when you love it, why not keep it the best. lee-dohm, Then about the purpose, 200M, 100M? crquan already gives us a good example ###sublime###. your words looks so ridiculous too, I have to say. no offence. Thanks.  anyway, Thanks all the efforts to ATOM. Thanks all you here. Thanks. Just installed it on my Mac. Gosh, it's 530 Mb..."
technical,"Adding to this, Atom was taking 5.5GB on my SSD as well, most of it in local appdata. Windows 10. Just saying: VSCode is < 100 mb download size, ~ 200 mb memory print and also build up on Electron."
technical,"I'm not a node developer don't even know how to hack this project. Only providing my general feedback here, 208MB is beyond the size of an editor should be,  the app.asar is not compressed anyway, if you open it with a hex editor, you will see 90%+ of this 72MB file is plain text, which means further compressable, here I apply a simple xz compress get a 13MB file, but I don't know how to change the main binary to load resource from the compressed file There are a number of things that Atom could do to reduce install size, but tbh, does it  really  matter? Engineering work costs time, and optimizing disk space isn't really worth that time given that you can buy a 16GB USB Thumb drive on Amazon for less than a cup of coffee (in San Francisco :P). That's 78 copies of Atom, for 5."
technical,"Just saying: VSCode is < 100 mb download size, ~ 200 mb memory print and also build up on Electron. There is a known issue on Windows where the auto-updater may fail to remove old versions of Atom.  It is safe to delete all the older versions."
technical,"Just installed it on my Mac. Gosh, it's 530 Mb... What parts of WCF do you need? Soap (with http basic auth) stuff is pretty easy to implement with giraffe."
technical, Additional stats export fall into the scope of FrontLine.
technical,"We are really sorry to hear this. We sincerely hope you will continue to use Gatling. As we explained to you earlier, our team spends time on both projects: our open-source solution and our Enterprise solution, Gatling FrontLine. The way we decided to split our R&D is as follows: Everything related to Gatling itself, like new support protocols, will be open-sourced (eg. HTTP/2, closed workload model and new feeders features in our latest release), On the other hand, everything related to reporting and automation features is in the scope of Gatling FrontLine. Please note that our Enterprise version helps develop both projects and have a team of 7 full-time employees. We hope you understand. Anyway, we would gladly continue this talk with you, feel free to contact us directly. We will now close this thread. Best regards I need to be able to push generated results to a DB, by exposing the report generator as public interface, I can provide a custom report generator which can push results to DB."
technical,"I am experiencing the exact same thing as diablodale.  I was able to eliminate the ""openssh-ssh-agent"" error messages by starting the ""ssh-agent"" service in Windows 10: I found this helpful: I am perplexed by this as I am able to run docker -H ssh:userip ps from Powershell and WSL just fine. Additionally, I see theses messages if I open the vscode developer tools from Help: mainThreadExtensionService.ts:66 Error: All configured authentication methods failed"
technical,"Ok, so I figured it out for my purposes -- finally.  My situation was practically identical to what diablodale describes.  **SOLUTION**  Even though you have your keys setup and you can do example commands like docker -H ssh:userip ps, you can still run into auth issues because this extension is specifically making using of the ""ssh-agent"" under the hood.   **IMPORTANT**: Don't get these steps confused with similar things that can be done in WSL. Things done in WSL do not affect the outcome. This extension does not use WSL under the hood even if you have your terminal configured to use WSL. You must have it setup through the means the Windows OS uses.  1. Follow directions here to enable the Windows SSH-Agent.  This addresses the errors with the pipe in the message. 2. Ensure that you have your SSH keys inside of <windows user folder/.ssh/. 3. Run ssh-add from powershell. This was the last part that I was missing and why I would get All configured authentication methods failed  In the end, would be nice if these errors were caught and the users were notified of the GUI of what actions might be needed. I agree with zifik. There's nothing we can realistically do about the fact that our dependencies (dockerode, ssh2 Node packages) require an auth agent, at least not in the short term, but we can warn the user if they have an SSH DOCKER HOST but no agent set up. A Learn More link to the Wiki page on SSH setup would probably also help."
technical,"This was the latest VS Code Insider and latest vscode-docker from 6 days ago. I don't have access to the machine right now. I tested it on a fresh VM: install VS Code, Docker extension, Docker for Windows, Git for Windows, create SSH key, make sure you can ssh from Command Prompt solely using the key. Once all of that is done, simply configure the remote docker setting to the ssh: format. I am experiencing the exact same thing as diablodale.  I was able to eliminate the ""openssh-ssh-agent"" error messages by starting the ""ssh-agent"" service in Windows 10: I found this helpful: I am perplexed by this as I am able to run docker -H ssh:userip ps from Powershell and WSL just fine."
technical,This is the Docker extension's viewlet. That should support ssh: with the current release. Moving there. joaomoreno Can you provide repro steps and what version of VS Code and vscode-docker you're using?
technical,"We have improved the warnings for this in Docker extension version 1.0.0. Related to microsoft/vscode-remote-release#1935  - I confirmed that I can successfully connect to the remote machine using SSH via command line - There is no error in the entire UI, just loading progress bars when opening the Docker viewlet - When running Attach to Running Container, nothing happens - When I open devtools, this is what I see:"
technical, This is the Docker extension's viewlet. That should support ssh: with the current release. Moving there.
technical,"joaomoreno Can you provide repro steps and what version of VS Code and vscode-docker you're using? This was the latest VS Code Insider and latest vscode-docker from 6 days ago. I don't have access to the machine right now. I tested it on a fresh VM: install VS Code, Docker extension, Docker for Windows, Git for Windows, create SSH key, make sure you can ssh from Command Prompt solely using the key. Once all of that is done, simply configure the remote docker setting to the ssh: format."
technical,"I agree with zifik. There's nothing we can realistically do about the fact that our dependencies (dockerode, ssh2 Node packages) require an auth agent, at least not in the short term, but we can warn the user if they have an SSH DOCKER HOST but no agent set up. A Learn More link to the Wiki page on SSH setup would probably also help. We have improved the warnings for this in Docker extension version 1.0.0."
technical," Admittedly, I could have read ""built and deployed for x64 architecture"" and figured that out. Can you share the build log?"
technical,"Saw the store version was updated recently. Let the affected PC update and am still experiencing the issue. As it happens, I also have the problem of recent versions of Windows Terminal crashing on startup with an empty window - and I too have an interesting crash dump! I also got a Time Travel Debugging trace in case that proves useful.  Link - sorry, it's Microsoft-internal-only because of my concerns re my private information:"
technical,"I can confirm this works. My terminal shows up. However, the settings option is unresponsive after I select it. As miniksa said, that's probably a separate issue. My machines don't have that ""Use legacy console"" feature turned on, and it doesn't work even with them on.  It seems there could be multiple factors to cause this specific response.  That said, I updated Terminal to the latest version in the store and it no longer even fails to this point, opens, shows the titlebar/border (like the previous behavior) and then crashes before the window finishes rendering. (Unlike before, the UWP window titlebar and border are destroyed)"
technical,"From anyone in this thread, I need a crash dump. I tried searching every Report ID you all posted, and none of them are coming through.  I suspect that the issues related to not being able to use hardware rendering might be solved with #1263.  There's probably also some robustness to be added to the DX renderer so it sets itself up correctly. If any of you are building from source, it would be nice to have you check if conhost.exe built from the same sources crashes when the key at HKCU\Console UseDx REG DWORD 0x1 is set (create it if it isn't there.) That will confirm if a crash is isolated to the DX renderer or Terminal startup specific.  I also have the theory that you are all experiencing like 4 different issues here, so we might have to split them up if I try to fix one and it isn't fixed for all of you.  Lastly, we don't readily have access to this type of hardware around here. I'll see what we can do, but if I can get some close assistance, that's the best. Certainly! How can I go about getting a crash dump of it from the affected machine?  I also went to my development station (x64) and attempted the UseDx change you requested. I can confirm that conhost fails to start with UseDx set to 0x1 on a machine where the same source build does work properly. Toggling UseDx back off results in a working console again.  Interestingly, I did this test on the affected tablet (once again with same sources), and conhost works in both scenarios on it, I get a command prompt as normal.. however Cascadia still fails."
technical,"i5-9600K, RTX-2070, 32Gb RAM, Host OS - blank window and crash. Into debug, crash here: crashing here , I also have a displaylink device 4k, hooked up to a mini displayport  port. and getting this error:"
technical,"Hey, Thanks for sharing this dump! I don't know what build of WT this maps to, since we're not keeping CI build symbols around, and this looks like one of those. :smile:  I'm going to try with the most recent CI build. DHowett-MSFT it might have gotten lost in the notes file I wrote, but this is: building the current master  If I build the same commit on my own box, same problem occurs.  And as far as I could tell, the Azure DevOps CI builds  do  have symbols - there are .msix and .appxsym files in the build artifacts, and the latter unzips to a bunch of .pdb files. Or is that not the symbols you're looking for?"
technical,"I have an NVIDIA GeForce GTX 1080 Ti card here with onboard Intel UHD 630, though somehow, suddenly, today the app is working again. I have no idea why or how. From anyone in this thread, I need a crash dump. I tried searching every Report ID you all posted, and none of them are coming through.  I suspect that the issues related to not being able to use hardware rendering might be solved with #1263.  There's probably also some robustness to be added to the DX renderer so it sets itself up correctly. If any of you are building from source, it would be nice to have you check if conhost.exe built from the same sources crashes when the key at HKCU\Console UseDx REG DWORD 0x1 is set (create it if it isn't there.) That will confirm if a crash is isolated to the DX renderer or Terminal startup specific.  I also have the theory that you are all experiencing like 4 different issues here, so we might have to split them up if I try to fix one and it isn't fixed for all of you.  Lastly, we don't readily have access to this type of hardware around here. I'll see what we can do, but if I can get some close assistance, that's the best."
technical,"I think the Terminal was so impressed with your desktop image it didn't want to render on top of it. FWIW, I'm receiving the same behavior on my x86 tablet after installing the *MS Store* version. Previous copies compiled by me worked, so I'm left scratching my head as to why this is occurring now. I hadn't yet tested on my x64 laptop however, so I am unsure if it's the same there..  Update: After looking at event viewer, I'm receiving an appcrash. I have pulled the events into an evtx if needed."
technical,"I tried out eight of the Azure Pipelines master builds from here: to try and determine where things started breaking.  In reverse chronological sequence, i.e. newest build last:  - #1374 update link to public preview: WORKS - #1512 fix punct readme: WORKS - #1452 about dialog contents selectable: WORKS - #1314 set default startup proj: WINDOW STAYS BLANK - #1263 fallback swren: WINDOW STAYS BLANK - #1093 connect clipboard func to keybindings: WINDOW STAYS BLANK - #929 Apply a GDI region to the top level Island window to allow dragging with a single Island: BLANK THEN CRASH - #1436 altgr: BLANK THEN CRASH  In other words, between the subsequent merges of #1452 and #1314 (both on 2019-06-24) terminal went from working to blank window at startup (but no crash), and then between the subsequent merges of #1093 and #929 (both on 2019-06-25), terminal went from blank window no crash to blank window and then crash.  (How #1314, which seems to be only a re-ordering in the OpenConsole solution file, can break terminal like this is surprising. However, uninstalling-reinstalling the builds here only confirms the working / non-working status.) here is a crash dump of the #1436 PR Azure Pipelines build crashing."
technical,"That's fine, the tablet it's on gets very light usage. The only real identifiable information on the device may be my real name (which is fine) or the user profile directory, which only contains part of my Github username.  Here is a OneDrive shared folder containing the evtx file (with both Windows Error Reporting event and Application Crash event) and for extra measure, a process dump for you! )  Let me know if you can't get that, and I'll upload it somewhere else, that process dump was larger than I expected for something taking only 5MB of RAM at that time lol.  (TIL I can easily create process dumps, that's a cool feature!) Here's a ZIP with some files extracted directly from the Event Viewer... I hope this is what you're looking for..."
technical,"As it happens, I also have the problem of recent versions of Windows Terminal crashing on startup with an empty window - and I too have an interesting crash dump! I also got a Time Travel Debugging trace in case that proves useful.  Link - sorry, it's Microsoft-internal-only because of my concerns re my private information: Hey, Thanks for sharing this dump! I don't know what build of WT this maps to, since we're not keeping CI build symbols around, and this looks like one of those. :smile:  I'm going to try with the most recent CI build."
technical," Hi,  Again, not saying this is the reason/fix for everyone, but I think I found the root cause on my PC.  I integrated Intel HD graphics plus an AMD Radeon 530 graphics card on my laptop. I think that a recent driver update has caused some sort of issue.  This may not be limited to AMD graphics, but I've found that when my Dock is connected and I'm usign Displaylink, the laptop prefers to run most programs using the 530 discreet GPU. When this is the case, Windows Terminal and mstsc.exe both crash on start. When I check AppCrashView both crashes involve a dll with crt in the name, ucrtbase.dll for Terminal and msvcrt.dll for mstsc.exe.  I have now found a way to tell Windows not to use the Radeon 530 for both mstsc.exe and wt.exe. Both programs run fine now.  So I think that a recent graphics driver and/or Windows update has cause some sort of problem with the graphics drivers."
technical," Hi, I have the same issue and my graphic card is a NVIDIA GeForce GTX 1060"
technical,"OK. All of the issues above as isolated from various discussions in this thread appear to have a follow-on ID number (except MinMaxControl which was only mentioned once while it was still fresh). Please direct conversation about ongoing ""empty window and crashes"" issues to those specific threads if they're relevant to you.  If they're not relevant, please open a new bug and describe how yours is different from one of those above. Thanks! Hi! I'm trying to run this new Windows Terminal.  After some difficulties and a few attempts I was able to build and deploy the project locally. *Great! Finally!* I said to myself, just before clicking on the Windows Terminal (Dev Build) in the Start menu...  This was the result: an empty window. After a few seconds, it simply disappeared and then... Well... Nothing more!  ---  Here are some useful (hopefully) information about my current system:   Windows 10 1903 Build 18362.175   x64 architecture   Developer mode enabled   Repo version built: v0.2.1715.0 (66cb7c4b58b0e41ffaeb952ef27f1a8c67e90db8)   Build with Visual Studio 2019   Built and deployed for x64 architecture  ---  Some time ago, I commented already here explaining the same issue... But, nobody could help me.  Maybe opening a Issue I will be luckier..."
technical," I can confirm this works. My terminal shows up. However, the settings option is unresponsive after I select it."
technical,"I've submitted an issue using the Feedback Hub  Even though I have Windows Terminal (Preview) listed in Settings | Apps, it didn't come up in the list of Apps in the Feedback Hub, so I just had to choose ""All other apps"".  I also created a crash dump (applying the same settings as documented here and attached that to the report. I did not have PowerShell Core installed on the machine that was showing this crash (but on another device, Terminal was starting without error and it did have PowerShell Core).  Out of curiosity, I installed Core on the failing machine, and now Terminal starts correctly!"
technical,"OK, thanks for the data point. On the back end, these crashes are ending up bucketed in the same giant issue, so it's been difficult for me to tease apart the different scenarios here. Every report like this helps me identify another case more easily than rummaging through stack dumps. I did not have PowerShell Core installed on the machine that was showing this crash (but on another device, Terminal was starting without error and it did have PowerShell Core).  Out of curiosity, I installed Core on the failing machine, and now Terminal starts correctly!"
technical,"MSBuild - well, precisely, I'm doing this: cd/d [repo root] & .\tools\razzle.cmd dbg & bcz dbg and the bcz.cmd batch file runs MSBuild as you know.  The app then crashes when I deploy the loose-file folder through either PowerShell or Visual Studio.  I can verify that after scorching the repo with git clean -dxf, opening the solution in Visual Studio, and building, the resulting app runs fine. The loose file folder with AppXManifest.xml also has both App.xbf and MinMaxCloseControl.xbf, both are missing from my MSBuild build and the AzDO CI build.  Perhaps we really do need to fork that separate issue now - this is a pretty bad problem in my opinion, but probably not the one this issue was originally about. I got this issue and after uncheck Use legacy console option, my terminal show up"
technical,"I have this same issue on 2 computers out of 4.  The two computers have have the issue have powershell-core (6) on them, and one of those also has the powershell 7 preview.   The default profiles.json doesn't include entries for those, and it seems to make wt.exe unhappy.   The issue also seems  to re-appear periodically, perhaps when the updates are automatically applied.  I've found that removing the profiles.json altogether fixes the issue, and Windows Terminal will create a new profiles.json file that includes entries for the additional versions of powershell.  I added a little function to my powershell profile.ps1 that will allow me to reset it, and that always fixes the launch problem on both of the problematic computers. I had this same issue right now after installing the Terminal via Store for the first time in my Surface Book with dGPU. I had just updated Windows to 1903 and installed PowerShell Core using Scoop. After removing the profiles.json file as tksunw said, it's working. I did not have these problems when doing the same thing with my desktop a few weeks ago."
technical,"I opened the NVIDIA Control Panel, under Manage 3D Settings select the Program Settings tab. At step one, the Windows Terminal is listed as ""microsoft.windowsterminal [id]"". At step two I selected the integrated graphics (sorry for the poor drawing skills :)): I have an NVIDIA GeForce GTX 1080 Ti card here with onboard Intel UHD 630, though somehow, suddenly, today the app is working again. I have no idea why or how."
technical,"Ok... I think I found out why this is happening... ”  A couple of minutes ago I tried to run again the Terminal on my PC to log any errors to attach here... Well... It runs (and it's **great**)!    So? Why now runs? What's the difference? Today I was using my notebook on my legs (without any devices connected). Usually, I use my notebook connected to this Universal Dock.  These kind of devices (DisplayLink docks) are seen by the OS as external video cards without extended support for hardware acceleration... In facts, you can't run video games or 3D graphics in general even if your GPU is the best one you can buy!  So, I think, if the video card you're trying to render the Terminal on isn't compatible with hardware acceleration (or something like that) it, simply, crashes badly.  It could be also your case ShadowEO, magiblot and tanayagar? Maybe something like:  - Cheap hardware? - Integrated video cards? - Virtual machines? - *... and so on?* I have similar issue.  After installing the Terminal, selecting the top pane dropdown, then <Settings (while the Widows Powershell is auto-selected),  the terminal loads Visual Studio blank page & crashes. I also have my (Lenovo W541) notebook on a docking station and am using Win 10 Pro x64b, Nightly, vers 1903, OS Build 18922.1000"
technical,"Was hoping it would work in my case, but sadly not. Still receiving the problem on my x86 device after installing pwsh-core. I have this same issue on 2 computers out of 4.  The two computers have have the issue have powershell-core (6) on them, and one of those also has the powershell 7 preview.   The default profiles.json doesn't include entries for those, and it seems to make wt.exe unhappy.   The issue also seems  to re-appear periodically, perhaps when the updates are automatically applied.  I've found that removing the profiles.json altogether fixes the issue, and Windows Terminal will create a new profiles.json file that includes entries for the additional versions of powershell.  I added a little function to my powershell profile.ps1 that will allow me to reset it, and that always fixes the launch problem on both of the problematic computers."
technical,"FWIW, I'm receiving the same behavior on my x86 tablet after installing the *MS Store* version. Previous copies compiled by me worked, so I'm left scratching my head as to why this is occurring now. I hadn't yet tested on my x64 laptop however, so I am unsure if it's the same there..  Update: After looking at event viewer, I'm receiving an appcrash. I have pulled the events into an evtx if needed. I just installed from MS Store and experience the same issue. 64-bit OS here. I wonder if it can be ran from the command line and see if it prints any error messages. EDIT: No, it doesn't. (wt.exe)"
technical,"Mmmh... Strange! I tried it too but, actually, it continues to run on the integrated video card (even if I choose the dedicated video card).  But, yeah... Probably, I did something wrong! I also tried on another computer equipped with an NVidia GTX 970 and it worked well.  I did some other tests and, for some reason, when I run the Terminal on the external monitor connected through the docking station in crashes (as I said before)... But, if I run it on the internal monitor (works fine, of course) and then I drag the window onto the external monitor, it continues to run without any problems.   ---  Here is my event log (I hope it can be useful): I opened the NVIDIA Control Panel, under Manage 3D Settings select the Program Settings tab. At step one, the Windows Terminal is listed as ""microsoft.windowsterminal [id]"". At step two I selected the integrated graphics (sorry for the poor drawing skills :)):"
technical,"Admittedly, I could have read ""built and deployed for x64 architecture"" and figured that out. Can you share the build log? I think the Terminal was so impressed with your desktop image it didn't want to render on top of it."
technical,"Certainly! How can I go about getting a crash dump of it from the affected machine?  I also went to my development station (x64) and attempted the UseDx change you requested. I can confirm that conhost fails to start with UseDx set to 0x1 on a machine where the same source build does work properly. Toggling UseDx back off results in a working console again.  Interestingly, I did this test on the affected tablet (once again with same sources), and conhost works in both scenarios on it, I get a command prompt as normal.. however Cascadia still fails. I tried out eight of the Azure Pipelines master builds from here: to try and determine where things started breaking.  In reverse chronological sequence, i.e. newest build last:  - #1374 update link to public preview: WORKS - #1512 fix punct readme: WORKS - #1452 about dialog contents selectable: WORKS - #1314 set default startup proj: WINDOW STAYS BLANK - #1263 fallback swren: WINDOW STAYS BLANK - #1093 connect clipboard func to keybindings: WINDOW STAYS BLANK - #929 Apply a GDI region to the top level Island window to allow dragging with a single Island: BLANK THEN CRASH - #1436 altgr: BLANK THEN CRASH  In other words, between the subsequent merges of #1452 and #1314 (both on 2019-06-24) terminal went from working to blank window at startup (but no crash), and then between the subsequent merges of #1093 and #929 (both on 2019-06-25), terminal went from blank window no crash to blank window and then crash.  (How #1314, which seems to be only a re-ordering in the OpenConsole solution file, can break terminal like this is surprising. However, uninstalling-reinstalling the builds here only confirms the working / non-working status.)"
technical,"So I attempted to build it on my own, and I am receiving the same behavior on my latest build. I did happen to try the x64 version on my laptop and it worked fine, but x86 is still not working. Sadly my x86 tablet doesn't have enough storage to be able to install VS onto it and VS won't let me set up a remote debugging profile for x86 (says Debug|x86 is missing from the project manifest when I even attempt to open the properties) so at the moment, my attempts to get to the bottom of why have stalled...  I may need to set up a x86 VM just to debug with :/ assuming the behavior persists there as well.  Now I'm just trying to grab myself an older copy of my build, just so I can at least use the new Terminal, even if it isn't as up to date.. I'm facing the same issue, don't think that there is any compatibility issues here"
technical,"As miniksa said, that's probably a separate issue. My machines don't have that ""Use legacy console"" feature turned on, and it doesn't work even with them on.  It seems there could be multiple factors to cause this specific response.  That said, I updated Terminal to the latest version in the store and it no longer even fails to this point, opens, shows the titlebar/border (like the previous behavior) and then crashes before the window finishes rendering. (Unlike before, the UWP window titlebar and border are destroyed) I've submitted an issue using the Feedback Hub  Even though I have Windows Terminal (Preview) listed in Settings | Apps, it didn't come up in the list of Apps in the Feedback Hub, so I just had to choose ""All other apps"".  I also created a crash dump (applying the same settings as documented here and attached that to the report."
technical,"That's a novel idea, but previous builds worked fine, for instance, I can reinstall one of my older dev builds and it opens and runs fine.  Yes, I am on cheap hardware for this device, it's a TMAX TM101W635L. Using an Intel HD Graphics (I am unsure of the exact model atm, as I am away from the machine.)  Virtual Machines work fine on the device, but they aren't in active use (on 2 GBs of non-expandable RAM, you can see why).  My tests are all done on device itself, so no docking stations, no external hardware at all.  I'm planning on setting up a 32-bit VS2019 VM on my main laptop tonight to see if the issue occurs there, and if so, to debug it as well. i5-9600K, RTX-2070, 32Gb RAM, Host OS - blank window and crash. Into debug, crash here:"
technical,"Hi, I have the same issue and my graphic card is a NVIDIA GeForce GTX 1060 In my case, I was able to fix it by forcing the app to use the Intel HD Graphics 630 graphics card instead of the GTX 1050 card in the NVIDIA control panel."
technical,"Thanks for the tip. May I ask you how you did that ? Mmmh... Strange! I tried it too but, actually, it continues to run on the integrated video card (even if I choose the dedicated video card).  But, yeah... Probably, I did something wrong! I also tried on another computer equipped with an NVidia GTX 970 and it worked well.  I did some other tests and, for some reason, when I run the Terminal on the external monitor connected through the docking station in crashes (as I said before)... But, if I run it on the internal monitor (works fine, of course) and then I drag the window onto the external monitor, it continues to run without any problems.   ---  Here is my event log (I hope it can be useful):"
technical,"Now, are you building with MSBuild or with VS? I can reproduce the broken package layout with MSBuild but not with Visual Studio.  On my machine, when VS builds the project, it ends up laying a copy of MinMaxCloseControl.xbf down in the appx package root, where the resource locator suggests it will be. A copy also ends up in TerminalApp.pri. The appx you're pulling from CI and the appx  layout  you're producing on your machine probably don't have the first copy (.xbf) and only have the second copy (verified in the .msix from the CI artifacts.)  There's probably a couple issues here. One, I have no idea why it's being copied in by VS but not MSBuild, Two, It should be getting rolled up into resources.pri with the other Xaml resources, Three, I have no idea why this works properly for the very similar App.xbf. MSBuild - well, precisely, I'm doing this: cd/d [repo root] & .\tools\razzle.cmd dbg & bcz dbg and the bcz.cmd batch file runs MSBuild as you know.  The app then crashes when I deploy the loose-file folder through either PowerShell or Visual Studio.  I can verify that after scorching the repo with git clean -dxf, opening the solution in Visual Studio, and building, the resulting app runs fine. The loose file folder with AppXManifest.xml also has both App.xbf and MinMaxCloseControl.xbf, both are missing from my MSBuild build and the AzDO CI build.  Perhaps we really do need to fork that separate issue now - this is a pretty bad problem in my opinion, but probably not the one this issue was originally about."
technical,"Ok... I think I found out why this is happening... ”  A couple of minutes ago I tried to run again the Terminal on my PC to log any errors to attach here... Well... It runs (and it's **great**)!    So? Why now runs? What's the difference? Today I was using my notebook on my legs (without any devices connected). Usually, I use my notebook connected to this Universal Dock.  These kind of devices (DisplayLink docks) are seen by the OS as external video cards without extended support for hardware acceleration... In facts, you can't run video games or 3D graphics in general even if your GPU is the best one you can buy!  So, I think, if the video card you're trying to render the Terminal on isn't compatible with hardware acceleration (or something like that) it, simply, crashes badly.  It could be also your case ShadowEO, magiblot and tanayagar? Maybe something like:  - Cheap hardware? - Integrated video cards? - Virtual machines? - *... and so on?* No dock, as stated in my earlier response to someone else who suggested the same thing, previous builds ran perfectly, store build and current tree build does not.  Intel HD Graphics, all internal. No VM, running Windows 10 Pro Latest skip ahead. TMAX TM101W638L tablet. (Once again, earlier builds ran perfectly, so it doesn't seem to be an issue with the graphics, and the DxRenderer conhost works fine as well)"
technical,"This conversation is now locked because people are using it as a dumping ground for literally any crash on start.  I've identified a ton of issues in this thread: - Launching with Legacy Console selected - This was fixed in #1935 - Launching with an Invalid font - This was fixed in #2153 - Launching without Powershell Core 6 - This is #1348 and #1458 and #982 - Launching with Powershell Core 7 - The scenario as described in #1399 is the same thing that #982 would address - Launching on a portable device that uses switchable GPUs or an external dock GPU - DHowett-MSFT has a Surface Book with the switchable GPU thing and hasn't seen this. I totally believe it's a thing and it might even be a mishandling in the DirectX renderer, but we should extract this problem to another issue AND we need to get someone comfortable with debugging this who owns this hardware. - This is now moved to #2183 - The MinMaxControl wasn't stable yet - It's stable-er now. This should be fine. - I use the wrong architecture for my machine - #1648 blocks you from doing this now  I will edit this with the resolutions. But no more piling on here. If you have any of these 7 things, follow up on the individual threads or wait for the update if its already fixed. Note, I'm not saying don't report things. I just need to break this out into individual threads because it's too much to handle all in one place. Let's break the conversations into correlated threads."
technical,"That sure, are you? Well better get that separate issue open, because clicking this link will BLOW YOUR MIND!!!!!  As I said in passing above, building on my own computer and running the resulting debug build Terminal app produces the same crash as I got from the Azure DevOps CI release build. Like in the CI release build, the problem's proximate cause is that was not found during a call to Windows.UI.Xaml.Application.LoadComponent(). That call in turn comes from C++/WinRT generated code starting with this call from winrt::TerminalApp::implementation::App:: Create() to the generated constructor for winrt::TerminalApp::MinMaxCloseControl Now I'm assuming something peculiar is going on in the Azure DevOps CI build and the build on my own box. Either that, or I could just be deploying the app improperly - I unzip the CI build .appx/.msix or find the folder of loose files I personally built that already has an AppXManifest.xml in it, and then run powershell.exe -command Add-AppXPackage -Register X:\wherever\it\is\AppXManifest.xml. Now, are you building with MSBuild or with VS? I can reproduce the broken package layout with MSBuild but not with Visual Studio.  On my machine, when VS builds the project, it ends up laying a copy of MinMaxCloseControl.xbf down in the appx package root, where the resource locator suggests it will be. A copy also ends up in TerminalApp.pri. The appx you're pulling from CI and the appx  layout  you're producing on your machine probably don't have the first copy (.xbf) and only have the second copy (verified in the .msix from the CI artifacts.)  There's probably a couple issues here. One, I have no idea why it's being copied in by VS but not MSBuild, Two, It should be getting rolled up into resources.pri with the other Xaml resources, Three, I have no idea why this works properly for the very similar App.xbf."
technical,"I did not have PowerShell Core installed on the machine that was showing this crash (but on another device, Terminal was starting without error and it did have PowerShell Core).  Out of curiosity, I installed Core on the failing machine, and now Terminal starts correctly! OK, thanks for the data point. On the back end, these crashes are ending up bucketed in the same giant issue, so it's been difficult for me to tease apart the different scenarios here. Every report like this helps me identify another case more easily than rummaging through stack dumps."
technical,"Note, I'm not saying don't report things. I just need to break this out into individual threads because it's too much to handle all in one place. Let's break the conversations into correlated threads. OK. All of the issues above as isolated from various discussions in this thread appear to have a follow-on ID number (except MinMaxControl which was only mentioned once while it was still fresh). Please direct conversation about ongoing ""empty window and crashes"" issues to those specific threads if they're relevant to you.  If they're not relevant, please open a new bug and describe how yours is different from one of those above. Thanks!"
technical,"I'm facing the same issue, don't think that there is any compatibility issues here Ok... I think I found out why this is happening... ”  A couple of minutes ago I tried to run again the Terminal on my PC to log any errors to attach here... Well... It runs (and it's **great**)!    So? Why now runs? What's the difference? Today I was using my notebook on my legs (without any devices connected). Usually, I use my notebook connected to this Universal Dock.  These kind of devices (DisplayLink docks) are seen by the OS as external video cards without extended support for hardware acceleration... In facts, you can't run video games or 3D graphics in general even if your GPU is the best one you can buy!  So, I think, if the video card you're trying to render the Terminal on isn't compatible with hardware acceleration (or something like that) it, simply, crashes badly.  It could be also your case ShadowEO, magiblot and tanayagar? Maybe something like:  - Cheap hardware? - Integrated video cards? - Virtual machines? - *... and so on?*"
technical,"I had this same issue right now after installing the Terminal via Store for the first time in my Surface Book with dGPU. I had just updated Windows to 1903 and installed PowerShell Core using Scoop. After removing the profiles.json file as tksunw said, it's working. I did not have these problems when doing the same thing with my desktop a few weeks ago. Ok... I think I found out why this is happening... ”  A couple of minutes ago I tried to run again the Terminal on my PC to log any errors to attach here... Well... It runs (and it's **great**)!    So? Why now runs? What's the difference? Today I was using my notebook on my legs (without any devices connected). Usually, I use my notebook connected to this Universal Dock.  These kind of devices (DisplayLink docks) are seen by the OS as external video cards without extended support for hardware acceleration... In facts, you can't run video games or 3D graphics in general even if your GPU is the best one you can buy!  So, I think, if the video card you're trying to render the Terminal on isn't compatible with hardware acceleration (or something like that) it, simply, crashes badly.  It could be also your case ShadowEO, magiblot and tanayagar? Maybe something like:  - Cheap hardware? - Integrated video cards? - Virtual machines? - *... and so on?*"
technical,Here's a ZIP with some files extracted directly from the Event Viewer... I hope this is what you're looking for... Saw the store version was updated recently. Let the affected PC update and am still experiencing the issue.
technical,"I just installed from MS Store and experience the same issue. 64-bit OS here. I wonder if it can be ran from the command line and see if it prints any error messages. EDIT: No, it doesn't. (wt.exe) So I attempted to build it on my own, and I am receiving the same behavior on my latest build. I did happen to try the x64 version on my laptop and it worked fine, but x86 is still not working. Sadly my x86 tablet doesn't have enough storage to be able to install VS onto it and VS won't let me set up a remote debugging profile for x86 (says Debug|x86 is missing from the project manifest when I even attempt to open the properties) so at the moment, my attempts to get to the bottom of why have stalled...  I may need to set up a x86 VM just to debug with :/ assuming the behavior persists there as well.  Now I'm just trying to grab myself an older copy of my build, just so I can at least use the new Terminal, even if it isn't as up to date.."
technical,here is a crash dump of the #1436 PR Azure Pipelines build crashing. Thanks for doing a rough bisect for us!  This doesn't look safe  at all .  I'm really glad our build pipelines hold on to symbols. :smile:
technical,"In my case, I was able to fix it by forcing the app to use the Intel HD Graphics 630 graphics card instead of the GTX 1050 card in the NVIDIA control panel. Thanks for the tip. May I ask you how you did that ?"
technical,"Yeah, your issue is that MinMaxCloseControl isn't ending up in the right place (possibly -- maybe refuted by evidence)  only for CI builds . Thanks Xaml pipeline! :smile: That sure, are you? Well better get that separate issue open, because clicking this link will BLOW YOUR MIND!!!!!  As I said in passing above, building on my own computer and running the resulting debug build Terminal app produces the same crash as I got from the Azure DevOps CI release build. Like in the CI release build, the problem's proximate cause is that was not found during a call to Windows.UI.Xaml.Application.LoadComponent(). That call in turn comes from C++/WinRT generated code starting with this call from winrt::TerminalApp::implementation::App:: Create() to the generated constructor for winrt::TerminalApp::MinMaxCloseControl Now I'm assuming something peculiar is going on in the Azure DevOps CI build and the build on my own box. Either that, or I could just be deploying the app improperly - I unzip the CI build .appx/.msix or find the folder of loose files I personally built that already has an AppXManifest.xml in it, and then run powershell.exe -command Add-AppXPackage -Register X:\wherever\it\is\AppXManifest.xml."
technical,"Yes, I do have an Intel GPU (HD Graphics 520) but drivers and harware acceleration are in order. I don't use a docking station or VM, altough I do have this laptop connected to an external display through VGA/DP. But unplugging it changed nothing. That's a novel idea, but previous builds worked fine, for instance, I can reinstall one of my older dev builds and it opens and runs fine.  Yes, I am on cheap hardware for this device, it's a TMAX TM101W635L. Using an Intel HD Graphics (I am unsure of the exact model atm, as I am away from the machine.)  Virtual Machines work fine on the device, but they aren't in active use (on 2 GBs of non-expandable RAM, you can see why).  My tests are all done on device itself, so no docking stations, no external hardware at all.  I'm planning on setting up a 32-bit VS2019 VM on my main laptop tonight to see if the issue occurs there, and if so, to debug it as well."
technical," That's fine, the tablet it's on gets very light usage. The only real identifiable information on the device may be my real name (which is fine) or the user profile directory, which only contains part of my Github username.  Here is a OneDrive shared folder containing the evtx file (with both Windows Error Reporting event and Application Crash event) and for extra measure, a process dump for you! )  Let me know if you can't get that, and I'll upload it somewhere else, that process dump was larger than I expected for something taking only 5MB of RAM at that time lol.  (TIL I can easily create process dumps, that's a cool feature!)"
technical,"DHowett-MSFT it might have gotten lost in the notes file I wrote, but this is: building the current master  If I build the same commit on my own box, same problem occurs.  And as far as I could tell, the Azure DevOps CI builds  do  have symbols - there are .msix and .appxsym files in the build artifacts, and the latter unzips to a bunch of .pdb files. Or is that not the symbols you're looking for? They do, but we're not archiving them anywhere  durable  (like the symbol server) because these builds are not intended to be used. I did end up digging up the right symbols for your build. Here's where we're failing out. I don't believe this is the same issue as #1364, so I'm going to fork this into another issue. Thanks!"
technical,"Hi,  Again, not saying this is the reason/fix for everyone, but I think I found the root cause on my PC.  I integrated Intel HD graphics plus an AMD Radeon 530 graphics card on my laptop. I think that a recent driver update has caused some sort of issue.  This may not be limited to AMD graphics, but I've found that when my Dock is connected and I'm usign Displaylink, the laptop prefers to run most programs using the 530 discreet GPU. When this is the case, Windows Terminal and mstsc.exe both crash on start. When I check AppCrashView both crashes involve a dll with crt in the name, ucrtbase.dll for Terminal and msvcrt.dll for mstsc.exe.  I have now found a way to tell Windows not to use the Radeon 530 for both mstsc.exe and wt.exe. Both programs run fine now.  So I think that a recent graphics driver and/or Windows update has cause some sort of problem with the graphics drivers. This conversation is now locked because people are using it as a dumping ground for literally any crash on start.  I've identified a ton of issues in this thread: - Launching with Legacy Console selected - This was fixed in #1935 - Launching with an Invalid font - This was fixed in #2153 - Launching without Powershell Core 6 - This is #1348 and #1458 and #982 - Launching with Powershell Core 7 - The scenario as described in #1399 is the same thing that #982 would address - Launching on a portable device that uses switchable GPUs or an external dock GPU - DHowett-MSFT has a Surface Book with the switchable GPU thing and hasn't seen this. I totally believe it's a thing and it might even be a mishandling in the DirectX renderer, but we should extract this problem to another issue AND we need to get someone comfortable with debugging this who owns this hardware. - This is now moved to #2183 - The MinMaxControl wasn't stable yet - It's stable-er now. This should be fine. - I use the wrong architecture for my machine - #1648 blocks you from doing this now  I will edit this with the resolutions. But no more piling on here. If you have any of these 7 things, follow up on the individual threads or wait for the update if its already fixed."
technical,"I did not have PowerShell Core installed on the machine that was showing this crash (but on another device, Terminal was starting without error and it did have PowerShell Core).  Out of curiosity, I installed Core on the failing machine, and now Terminal starts correctly! Was hoping it would work in my case, but sadly not. Still receiving the problem on my x86 device after installing pwsh-core."
technical,"They do, but we're not archiving them anywhere  durable  (like the symbol server) because these builds are not intended to be used. I did end up digging up the right symbols for your build. Here's where we're failing out. I don't believe this is the same issue as #1364, so I'm going to fork this into another issue. Thanks! Yeah, your issue is that MinMaxCloseControl isn't ending up in the right place (possibly -- maybe refuted by evidence)  only for CI builds . Thanks Xaml pipeline! :smile:"
technical,"I have similar issue.  After installing the Terminal, selecting the top pane dropdown, then <Settings (while the Widows Powershell is auto-selected),  the terminal loads Visual Studio blank page & crashes. I also have my (Lenovo W541) notebook on a docking station and am using Win 10 Pro x64b, Nightly, vers 1903, OS Build 18922.1000 Yes, I do have an Intel GPU (HD Graphics 520) but drivers and harware acceleration are in order. I don't use a docking station or VM, altough I do have this laptop connected to an external display through VGA/DP. But unplugging it changed nothing."
technical,"mdo: pinging for the final review :D, We still need to make the build pass, but the preview looks pretty good. Oh and like MartijnCuppens mentioned we might want to have this off by default for 4.2, but we should mention it clearly in the release notes/blog post so that people can experiment with it. After a conversation I had with XhmikosR on slack, I applied some changes:  **Moved the mixin to a vendor folder.** There are some variables in  rfs.scss because the file is copied from the original project. We don't want to allow this in other Bootstrap scss files, so I moved this a vendor folder so it's clear it's a file included from another repo. I used the same approach for including vendor content as is used for mixins and utilities (one scss/ vendor.scss file which includes the vendor scss).  Other changes: - use abbr tag - Update to v7.1.5 (caching of some function calls)"
technical,"Maybe change the custom mixins to not using font-size? Also, what is the official way to do responsive fonts then? Any new Dos?"
technical,thanks for another great PR. I think that responsive font sizing is a killer feature to have.  My worried is that what this feature asks contributors might outweigh its value. Once we merge it we'll be asking contributors to include responsive-font-size every time they define a base font size. This adds a new level of complexity that makes Bootstrap harder to maintain and develop.  It'll be up to him to decide if he wants this feature in Bootstrap or not. If we decide to add it I think we should either have a very strong linter or some kind of postCSS that automatically adds the artifact around the font size declarations. at this moment there's also a border-radius and a transition mixin for other properties.  I've also added the font-size property to the declaration-property-value-blacklist which will trigger an error if the font-size property is instead of the mixin.
technical,"Just updated to 4.3.0. All of the font sizes are messed up.  I use this custom mixins with the Bootstrap 4. I am confused because I add this mixins to as own which should not effect any updates from Bootstrap. All of updates below 4.3.0 are fine.  When I inspect it, it looks like outputting the SASS. Bootstrap now uses the font-size mixin from RFS. It looks like you're passing a sass map to the mixin while you should just pass a font size to it. It doesn't work the same as the article you're referring to."
technical,at this moment there's also a border-radius and a transition mixin for other properties.  I've also added the font-size property to the declaration-property-value-blacklist which will trigger an error if the font-size property is instead of the mixin. can you rebase and squash the patches/rebase the branch?
technical,"when you have some time please review this. It seems it works fine, it's disabled by default.  The fusv workaround, we'll try to fix it soon and remove it from here. Current status: ~WIP~ Requested changes applied"
technical,"Update: It's now possible to disable responsive font-sizes for page by adding the .disable-responsive-font-size class to the body or html. This class can also be added to other elements. Minimum font-size is also increased to 1rem, which prevents the default text from scaling down. enable-hover-media-query was not deprecated"
technical,"we want to provide a solution that works out of the box without the need of further configuration. If you want you own implementation of the font sizes you can overwrite the mixin in your own custom configuration.  I'm going to lock the conversation on this PR because it we're deviating from the subject of it. Fixes #23053  - RFS is disabled by default and can be switched on with enable-responsive-font-sizes - font-size-properties are switched to the include font-size()-mixin. Stylelint prevents the usage of font-size property. - Basic documentation added + link to github repo for further documentation. - RFS is enabled to rescale font-sizes of titles on the docs page.  Demo with RFS enabled available here (this is not the default behaviour, but would be the behaviour if enable-responsive-font-sizes was true)  TODO:  -  Remove fusv false warning workaround -  Decide whether or not we're going to keep input-height-sm and input-height-lg  no -  Test if we should increase the minimum font size to 1.25rem  yes"
technical,"that might be a good idea. This wouldn't cause a lot of changes by default while developers can still easily enable responsive font sizes if they want to. I discovered changing the padding of buttons and inputs from em to rem can cause issues when upgrading Bootstrap, working on a solution for that. I'll try to implement RFS in a way that it wouldn't have any influence on the current Bootstrap at all when it's disabled."
technical,can you rebase and squash the patches/rebase the branch? I'll have a look at it tomorrow
technical,can you rebase and squash the patches/rebase the branch? I'm going to make some changes upstream and sync the versions this week. I'm going to increase rfs-minimum-font-size (which will be called rfs-base-font-size in RFS v8) to 1.25rem because this will not rescale the input fields by default.
technical,"I discovered changing the padding of buttons and inputs from em to rem can cause issues when upgrading Bootstrap, working on a solution for that. I'll try to implement RFS in a way that it wouldn't have any influence on the current Bootstrap at all when it's disabled. I've now disabled the responsive font sizing by default so that the css files are exactly the same (apart from property order). Responsive font sizing can be enabled with enable-responsive-font-sizes."
technical,Bootstrap now uses the font-size mixin from RFS. It looks like you're passing a sass map to the mixin while you should just pass a font size to it. It doesn't work the same as the article you're referring to. Is there a compiled css on the cdn with rfs enabled?
technical,"This is all voodoo to me, but one request: make sure that even if using vh/vw etc, that there's a component for font sizing tied to something else like px/rem/em. Otherwise (when a font size is purely related to viewport dimensions and nothing else) you kill the user's ability to (full page) zoom on desktop and change the size the text is rendered at (as yes, the viewport dimensions change, but the font size would then change proportionally in the opposite direction, meaning the apparent font size is never actually altered). It's possible to add the disable-responsive-font-size class to achieve this. Also, the font size is never purely related to viewport."
technical,"No, because there's no official build with the option enabled. Maybe change the custom mixins to not using font-size?"
technical,"Working on it mdo: pinging for the final review :D, We still need to make the build pass, but the preview looks pretty good. Oh and like MartijnCuppens mentioned we might want to have this off by default for 4.2, but we should mention it clearly in the release notes/blog post so that people can experiment with it."
technical,"Is there a compiled css on the cdn with rfs enabled? No, because there's no official build with the option enabled."
technical,I've now disabled the responsive font sizing by default so that the css files are exactly the same (apart from property order). Responsive font sizing can be enabled with enable-responsive-font-sizes. thanks for another great PR. I think that responsive font sizing is a killer feature to have.  My worried is that what this feature asks contributors might outweigh its value. Once we merge it we'll be asking contributors to include responsive-font-size every time they define a base font size. This adds a new level of complexity that makes Bootstrap harder to maintain and develop.  It'll be up to him to decide if he wants this feature in Bootstrap or not. If we decide to add it I think we should either have a very strong linter or some kind of postCSS that automatically adds the artifact around the font size declarations.
technical,"what do you think about this? Thanks for keeping this up to date. I'm slating this for sure for 4.1, but I'll see if we should put it into 4.0 depending on a few other things."
technical,"why not a enable-rfs: false, (disabled by default) param like other in  variables.scss ? that might be a good idea. This wouldn't cause a lot of changes by default while developers can still easily enable responsive font sizes if they want to."
technical,I'll have a look at it tomorrow that's not a rebase/squash. I see this branch has too many conflicts and it'll be hard to do a proper rebase.
technical,"It's possible to add the disable-responsive-font-size class to achieve this. Also, the font size is never purely related to viewport. There are several ways to implement this depending on what should be the default value for rfs-class.  In the current implementation I've set rfs-class: ""disable"",. This way, it's easy to disable responsive font sizes within the html by adding the .disable-responsive-font-size class. An example of this can be viewed here. The class can also be added to the html or the body to disable responsive font sizes on the whole page. Downside of this implementation is that more css is generated (that's why I needed to increase the maxSize in package.json) and it increases specificity.  Another approach is to use rfs-class: false,. This is the most performant solution but the .disable-responsive-font-size class cannot be added to elements. It can of course still be changed in scss, but I can imagine some people load the Bootstrap css via a CDN may prefer their font-size doesn't change. A possible solution for this is to generate 2 css versions: one with and one without responsive font-sizes."
technical,Current status: ~WIP~ Requested changes applied this needs a rebase and squash into one patch.
technical,"I'm going to make some changes upstream and sync the versions this week. I'm going to increase rfs-minimum-font-size (which will be called rfs-base-font-size in RFS v8) to 1.25rem because this will not rescale the input fields by default. Time for some updates. - As suggested by ysds, I've increased the rfs-base-font-size to 1.25rem. The result: - Large inputs don't rescale by default - It generates less css - The displays rescale enough on smaller viewports, because I've also increased the rfs-factor - Because of the changes to the form controls, the input-height-inner-sm and input-height-inner-sm became unneeded. I eventually dropped them because these are variables that are generated by other variables and not used anywhere else in the code, they were only used to calculate input-height-sm & input-height-lg.  I did some changes upstream and made a branch for the next major version of RFS which is in sync with this PR. This branch has some automated testing, so that we'll not break anything if we're going to update things in the future."
technical,"Thanks for keeping this up to date. I'm slating this for sure for 4.1, but I'll see if we should put it into 4.0 depending on a few other things. Update: It's now possible to disable responsive font-sizes for page by adding the .disable-responsive-font-size class to the body or html. This class can also be added to other elements. Minimum font-size is also increased to 1rem, which prevents the default text from scaling down."
technical,"What if the sizes are not what I want that from the automatic rescaling on smaller devices? This is a really good feature, but also what about line-height? In typographic design, line-height should alway works with font size.  I think the option to disable this new feature should be not overwriting the font-size property. Currently, enable-responsive-font-sizes: false is still overwriting the font-size property. Overwriting the entire font-size property is limiting custom mixins. we want to provide a solution that works out of the box without the need of further configuration. If you want you own implementation of the font sizes you can overwrite the mixin in your own custom configuration.  I'm going to lock the conversation on this PR because it we're deviating from the subject of it."
technical,"Also, what is the official way to do responsive fonts then? Any new Dos? We're not going to change the name of the mixin since font-size is the most appropriate name. You can find the documentation about the responsive font sizes in the Github repo."
technical, what do you think about this?
technical,"We're not going to change the name of the mixin since font-size is the most appropriate name. You can find the documentation about the responsive font sizes in the Github repo. What if the sizes are not what I want that from the automatic rescaling on smaller devices? This is a really good feature, but also what about line-height? In typographic design, line-height should alway works with font size.  I think the option to disable this new feature should be not overwriting the font-size property. Currently, enable-responsive-font-sizes: false is still overwriting the font-size property. Overwriting the entire font-size property is limiting custom mixins."
technical,"After a conversation I had with XhmikosR on slack, I applied some changes:  **Moved the mixin to a vendor folder.** There are some variables in  rfs.scss because the file is copied from the original project. We don't want to allow this in other Bootstrap scss files, so I moved this a vendor folder so it's clear it's a file included from another repo. I used the same approach for including vendor content as is used for mixins and utilities (one scss/ vendor.scss file which includes the vendor scss).  Other changes: - use abbr tag - Update to v7.1.5 (caching of some function calls) when you have some time please review this. It seems it works fine, it's disabled by default.  The fusv workaround, we'll try to fix it soon and remove it from here."
technical,"There are several ways to implement this depending on what should be the default value for rfs-class.  In the current implementation I've set rfs-class: ""disable"",. This way, it's easy to disable responsive font sizes within the html by adding the .disable-responsive-font-size class. An example of this can be viewed here. The class can also be added to the html or the body to disable responsive font sizes on the whole page. Downside of this implementation is that more css is generated (that's why I needed to increase the maxSize in package.json) and it increases specificity.  Another approach is to use rfs-class: false,. This is the most performant solution but the .disable-responsive-font-size class cannot be added to elements. It can of course still be changed in scss, but I can imagine some people load the Bootstrap css via a CDN may prefer their font-size doesn't change. A possible solution for this is to generate 2 css versions: one with and one without responsive font-sizes. why not a enable-rfs: false, (disabled by default) param like other in  variables.scss ?"
technical,that's not a rebase/squash. I see this branch has too many conflicts and it'll be hard to do a proper rebase. Working on it
technical,"Any news on this one? After a long long time, did flutter has fixed this issue? I want to use appbundle again"
technical,"we started to have crash issue for kitkat users  Xperia T3 (D5103) after lates flutter sdk After the release on PlayMareket, we have the same problem with the model Huawei Y7 Prime 2019 (HWDUB-Q), Android 8.1. Collected through App Bundle."
technical,"I have the same concerns with build number. In my build pipeline (using Fastlane), I create flavors to build per architectures. On every build run, I retrieve the latest live apk number from playstore, then increment it by using these scheme. this would slow down the build number increment and way more controllable. Hope this helps! Any news on this one?"
technical,I don't talk about this log I mean the logcat error Any update on this issue? Not an ideal situation not recommending the recommended way to publish to the Play Store in the documentation. Several months have passed...
technical,thanks a lot. Any updates on this? Is App Bundle fixed?
technical,"Half a year passed and no proper fix, neither a single update about this major issue. Disappointing! Unfortunately, this is how Google treats developers lately... with an immense ammount of disrespect. Doesn't even matter if I build an appbundle, or split the apks, my app's been sitting in Pending Publication for over 12 days, no answers provided by support team, or chat. I don't really know why we even have to pay the developer fee nor the 30% cut for these kind of prehistorical type of services to be honest. Now I'm starting to understand why so much people leave Google for Apple. anyone can tell me about stability of flutter if I deploy app to google play right now?"
technical,"Yeah since this issue was opened, i have built apk --split, not appbundle anymore. I hope the issue can fix in appbundle, to optimize the app like playstore recommendation as appbundle is the Play Store recommendation, I think this issue should be kept open until a proper fix is found and if it not found, it should be underlined extremely well in the docs (or even by the tool during a build) that appbundle builds are currently unstable and not recommended by Flutter"
technical,"Same here. This issue started for us yesterday trying to upload appbundle. It was working fine one month ago This is also reported in unity's forum. Crashes on Android 6 (Xiomi), when the solution will happen"
technical,"same problem here, tested release build on Xiaomi Redmi Note4, android 6.0, my logs: Building app using 'flutter build appbundle', installing using bundletool. did you solve the issue by splitting apk or  by splitting bundle"
technical," few days ago i am also facing same problem , know it resolved. because i changed in version no. (last digit)  in pubspec.yaml. last digit  +1 it create fat apk. it create flavors to build per architectures. Hope it will solve problem"
technical,"psovit Second approach doesn't work. I'm not sure about the first approach. We are building APKs separately like this Guys, I am following this issue too. We have major incident after release appbundle. A lot of android 6 device crash.  Well, after move to split-apk approach (described here), I finally can install the app on Android 6.0. Still I need more tests to ensure not happening on other device."
technical," Half a year passed and no proper fix, neither a single update about this major issue. Disappointing! Unfortunately, this is how Google treats developers lately... with an immense ammount of disrespect. Doesn't even matter if I build an appbundle, or split the apks, my app's been sitting in Pending Publication for over 12 days, no answers provided by support team, or chat. I don't really know why we even have to pay the developer fee nor the 30% cut for these kind of prehistorical type of services to be honest. Now I'm starting to understand why so much people leave Google for Apple."
technical,"After a long long time, did flutter has fixed this issue? I want to use appbundle again HI I am also facing same problem, app is running in debug mode but in release mode it getting crash on startup after analyze release apk,  I found that the debug apk size is greater in size with compare to release apk. also structure is different. Debug mode APK Release Mode APk Please Help me , i am unable to release APK on Play Store"
technical,"yes this regression happened since i used appbundle in playstore. when i tried to remove any plugin and all code, then i uploaded it to playstore it's still happened. So, i rechecked anything but there is no something wrong. i just confused, why it's running in release mode with apk, but it crashed after internal teesting with appbundle? Hmm, that's interesting. And you're using Flutter 1.7.4+hotfix.4? (Check with flutter doctor -v, if you wouldn't mind -- I want to make sure you're running the very latest hotfix revision.)  So to confirm:  - Uploaded as an APK with Flutter 1.7.4+hotfix.4, the app does not crash on these devices - Uploaded as an APPBUNDLE with Flutter 1.7.4+hotfix.4, the app crashes on the same devices?"
technical,"The recommended solution is to use flutter build apk --split-per-abi instead of app bundles. We should probably add this information to flutter.dev. I did uploaded the apk instead of app bundle and still the issue persists. I even raised a new issue, regarding this."
technical, I don't talk about this log I mean the logcat error
technical,"see this -- once we receive devices to try to reproduce the crashes on, we will have a look.  In the meantime, if you can send us symbolicated stack traces, that would be helpful. i got the same problem... any solutions??? ?"
technical,"I'm going to close this issue (and edit the OP for fast reference) since the issue is deeper in the OS and the Flutter framework can't resolve it.  The root of the issue is that leaving native .so uncompressed is a feature supported in Android M for both AABs and APKs. But the Xiaomi devices' OS in question have a bug when reading native libraries in uncompressed form. The issue is reported to the manufacturer. You can follow along in this but there's nothing we can do to resolve it without the manufacturer shipping an OS patch.  It also seems like Xiaomi supports the feature correctly in newer versions of Android. For you to disable this AAR / APK feature altogether for all your users to avoid this crash on Xiaomi:  1. If you build an App Bundle Edit android/gradle.properties and add the flag 2. If you build an APK. Make sure this doesn't set android in the application tag. This will have the side effect of the app consuming more disk space in your users' devices. I have launched my app in playstore, but i have found so many crash after the app is opened. And according my data on playstore, all crash happened in android 6.0. I have tried to testing in debug and released mode and everything is work. So this is strange for me. So this crash decreases my rating on playstore. Flutter team please help me.  This is my data. This is error messages. And this is the log while app crash. We're closing this bug since it's an issue in the Xiaomi OS and we can't fix it. See more info in this."
technical,"Thank you for the clarification. If I understood correctly splitting per ABI with the current flutter build script is done by adding 1000/2000/3000/4000/5000 to the base build number. This means it would start causing potential issues with a base build number above 1000. Even if it seems a relatively high number, it is currently not that high (we are already at build number 200 after 18 months of release cycles). It's also important to pay attention when switching back to appbundle, as the build number needs to go above the highest build number already released. Switching back and forth multiple times between these two modes is not trivial. For this reason we have currently decided to release fat apks with binary code for both architectures, accepting a 8MB overhead in the size, hoping to see this bug solved soon. I have the same concerns with build number. In my build pipeline (using Fastlane), I create flavors to build per architectures. On every build run, I retrieve the latest live apk number from playstore, then increment it by using these scheme. this would slow down the build number increment and way more controllable. Hope this helps!"
technical,"Wrote you an email regarding this issue, hope you can help me out, it's been more than 16 days and I'm literally dissapointed. I looked at the email. Have you tried reaching out the Play Store help center? That might be the right channel in your case."
technical,"is there a new update for this issue? because i used to build apk command rather than build appbundle since this issue makes problem to my users. Thanks I received the phone, but it came with Android 7.0 instead of 6.0. The crash doesn't occur on 7.0. To install 6.0, the phone needs to be rooted first... I will try that when I get a chance."
technical,"Hmm, that's interesting. And you're using Flutter 1.7.4+hotfix.4? (Check with flutter doctor -v, if you wouldn't mind -- I want to make sure you're running the very latest hotfix revision.)  So to confirm:  - Uploaded as an APK with Flutter 1.7.4+hotfix.4, the app does not crash on these devices - Uploaded as an APPBUNDLE with Flutter 1.7.4+hotfix.4, the app crashes on the same devices? i used the latest beta. All Android licenses accepted. Doctor found issues in 1 category. And when i uploaded only apk in playstore it's running well in android 6.0 but when i uploaded appbundle the crash appears"
technical,I looked at the email. Have you tried reaching out the Play Store help center? That might be the right channel in your case. I'm going to close this issue (and edit the OP for fast reference) since the issue is deeper in the OS and the Flutter framework can't resolve it.  The root of the issue is that leaving native .so uncompressed is a feature supported in Android M for both AABs and APKs. But the Xiaomi devices' OS in question have a bug when reading native libraries in uncompressed form. The issue is reported to the manufacturer. You can follow along in this but there's nothing we can do to resolve it without the manufacturer shipping an OS patch.  It also seems like Xiaomi supports the feature correctly in newer versions of Android. For you to disable this AAR / APK feature altogether for all your users to avoid this crash on Xiaomi:  1. If you build an App Bundle Edit android/gradle.properties and add the flag 2. If you build an APK. Make sure this doesn't set android in the application tag. This will have the side effect of the app consuming more disk space in your users' devices.
technical,"Guys, I am following this issue too. We have major incident after release appbundle. A lot of android 6 device crash.  Well, after move to split-apk approach (described here), I finally can install the app on Android 6.0. Still I need more tests to ensure not happening on other device. I'm now using same approach (split-per-abi)  Noticing, in case of building using appbundle, the device i'm using (Redmi Note 4  android-arm64  Android 6.0 (API 23)) gets strange apk of bundletool. Installed app is **much smaller** (installed app normal size is about 20Mb, while this app - about 3Mb), definitely missing native libs!"
technical, I'm sorry to hear that. It looks like your app has been waiting for a review for a very long time.  Would you mind sharing the package name of your app?
technical,"Please sign the CLA before sending us any source code, or we can't look at it '‚ i'm sorry, i didn't get permission to share my code,"
technical,"HI I am also facing same problem, app is running in debug mode but in release mode it getting crash on startup after analyze release apk,  I found that the debug apk size is greater in size with compare to release apk. also structure is different. Debug mode APK Release Mode APk Please Help me , i am unable to release APK on Play Store in Android studio you will see in the log the error details can you check Android Studio log?"
technical,"i'm sorry, i didn't get permission to share my code, Is it possible to expedite fix for this issue? Around 30% of our users are on these devices and we already reached 2,500 crashes."
technical,"I'm now using same approach (split-per-abi)  Noticing, in case of building using appbundle, the device i'm using (Redmi Note 4  android-arm64  Android 6.0 (API 23)) gets strange apk of bundletool. Installed app is **much smaller** (installed app normal size is about 20Mb, while this app - about 3Mb), definitely missing native libs! is there a new update for this issue? because i used to build apk command rather than build appbundle since this issue makes problem to my users. Thanks"
technical,"as appbundle is the Play Store recommendation, I think this issue should be kept open until a proper fix is found and if it not found, it should be underlined extremely well in the docs (or even by the tool during a build) that appbundle builds are currently unstable and not recommended by Flutter Just to clarify. This isn't a Flutter bug. I forwarded this issue to the Android team, and they are currently looking into ways to mitigate the problem at the Play Store level. A4267 can keep the issue open and close once the upstream issue is fixed."
technical,"We have requested a device to test this issue. It's expected to arrive between Aug 19 and Sep 30. In the meanwhile, would it be possible to access the app's source code? If yes, feel free to send the link via Gitter. Please sign the CLA before sending us any source code, or we can't look at it '‚"
technical, psovit Second approach doesn't work. I'm not sure about the first approach. We are building APKs separately like this
technical,"Is it possible to expedite fix for this issue? Around 30% of our users are on these devices and we already reached 2,500 crashes. Same for me. Have crash on redmi 4 since 1.2, 1.0 is working so I have to keep using it."
technical,"After the release on PlayMareket, we have the same problem with the model Huawei Y7 Prime 2019 (HWDUB-Q), Android 8.1. Collected through App Bundle. Same here. This issue started for us yesterday trying to upload appbundle. It was working fine one month ago This is also reported in unity's forum."
technical,"We had to upload APKs with split ABIs to solve this issue. I tried to reproduce this issue in a Redmi Note 4 (mido). But it is only causing in Android 6.0 and I'm unable to find the stock images for it because it is too old. same problem here, tested release build on Xiaomi Redmi Note4, android 6.0, my logs: Building app using 'flutter build appbundle', installing using bundletool."
technical,"Same for me. Have crash on redmi 4 since 1.2, 1.0 is working so I have to keep using it. see this -- once we receive devices to try to reproduce the crashes on, we will have a look.  In the meantime, if you can send us symbolicated stack traces, that would be helpful."
technical,"yeah, it happens in every xiaomi devices who used android 6.0, and the most of my user use xiaomi devices Sorry to hear about the crashes. Couple of questions -- is this a regression from previous versions, or can you reproduce it on older builds? And are you able to narrow this down to a reproducible crash with a small sample? I'm not aware that we're seeing general issues with these devices, so it would be nice to know the source of this (could be Flutter itself, a plug-in, or something else, of course)."
technical,"Just to clarify. This isn't a Flutter bug. I forwarded this issue to the Android team, and they are currently looking into ways to mitigate the problem at the Play Store level. A4267 can keep the issue open and close once the upstream issue is fixed. Thank you for the clarification. If I understood correctly splitting per ABI with the current flutter build script is done by adding 1000/2000/3000/4000/5000 to the base build number. This means it would start causing potential issues with a base build number above 1000. Even if it seems a relatively high number, it is currently not that high (we are already at build number 200 after 18 months of release cycles). It's also important to pay attention when switching back to appbundle, as the build number needs to go above the highest build number already released. Switching back and forth multiple times between these two modes is not trivial. For this reason we have currently decided to release fat apks with binary code for both architectures, accepting a 8MB overhead in the size, hoping to see this bug solved soon."
technical,"few days ago i am also facing same problem , know it resolved. because i changed in version no. (last digit)  in pubspec.yaml. last digit  +1 it create fat apk. it create flavors to build per architectures. Hope it will solve problem thanks a lot."
technical, The device codenames in the third graph are Xiaomi devices
technical,"Crashes on Android 6 (Xiomi), when the solution will happen The recommended solution is to use flutter build apk --split-per-abi instead of app bundles. We should probably add this information to flutter.dev."
technical,"I received the phone, but it came with Android 7.0 instead of 6.0. The crash doesn't occur on 7.0. To install 6.0, the phone needs to be rooted first... I will try that when I get a chance. The stacktrace you posted is very similar to this one. The Android App Bundle team is looking into this issue and I will update as I find out more. In the meanwhile, have you tried to upload two APKs to the Play Store instead of an app bundle? You can use flutter build apk --split-per-abi to generate two APKs for 32 and 64bit."
technical,"The device codenames in the third graph are Xiaomi devices The top device (santoni), for instance, is the Xiaomi Redmi 4X"
technical,"The stacktrace you posted is very similar to this one. The Android App Bundle team is looking into this issue and I will update as I find out more. In the meanwhile, have you tried to upload two APKs to the Play Store instead of an app bundle? You can use flutter build apk --split-per-abi to generate two APKs for 32 and 64bit. The workaround is to use flutter build apk --split-per-abi to generate two APKs for 32 and 64bit. I'm closing the issue, but feel free to reopen if the problem persists after uploading the two APKs."
technical,"I'm sorry to hear that. It looks like your app has been waiting for a review for a very long time.  Would you mind sharing the package name of your app? There's a canonical issue on this with suggested workarounds for App Bundles and APKs. If you are experiencing a crash of this nature, don't hesitate to comment on the bug - it's helpful if you provide the application package name, so the team can determinate next steps."
technical,i got the same problem... any solutions??? ? We had to upload APKs with split ABIs to solve this issue. I tried to reproduce this issue in a Redmi Note 4 (mido). But it is only causing in Android 6.0 and I'm unable to find the stock images for it because it is too old.
technical,"i used the latest beta. All Android licenses accepted. Doctor found issues in 1 category. And when i uploaded only apk in playstore it's running well in android 6.0 but when i uploaded appbundle the crash appears We have requested a device to test this issue. It's expected to arrive between Aug 19 and Sep 30. In the meanwhile, would it be possible to access the app's source code? If yes, feel free to send the link via Gitter."
technical,Any update on this issue? Not an ideal situation not recommending the recommended way to publish to the Play Store in the documentation. Several months have passed... we started to have crash issue for kitkat users  Xperia T3 (D5103) after lates flutter sdk
technical,"There's a canonical issue on this with suggested workarounds for App Bundles and APKs. If you are experiencing a crash of this nature, don't hesitate to comment on the bug - it's helpful if you provide the application package name, so the team can determinate next steps. Wrote you an email regarding this issue, hope you can help me out, it's been more than 16 days and I'm literally dissapointed."
technical,"The workaround is to use flutter build apk --split-per-abi to generate two APKs for 32 and 64bit. I'm closing the issue, but feel free to reopen if the problem persists after uploading the two APKs. Yeah since this issue was opened, i have built apk --split, not appbundle anymore. I hope the issue can fix in appbundle, to optimize the app like playstore recommendation"
technical,"The top device (santoni), for instance, is the Xiaomi Redmi 4X yeah, it happens in every xiaomi devices who used android 6.0, and the most of my user use xiaomi devices"
technical,"Sorry to hear about the crashes. Couple of questions -- is this a regression from previous versions, or can you reproduce it on older builds? And are you able to narrow this down to a reproducible crash with a small sample? I'm not aware that we're seeing general issues with these devices, so it would be nice to know the source of this (could be Flutter itself, a plug-in, or something else, of course). yes this regression happened since i used appbundle in playstore. when i tried to remove any plugin and all code, then i uploaded it to playstore it's still happened. So, i rechecked anything but there is no something wrong. i just confused, why it's running in release mode with apk, but it crashed after internal teesting with appbundle?"
technical,"how is a clearly wrong result due to variability? anyone can see that cnn time to interactive is not 11s-22s Ah, from your bug report it appeared as though you were asking why it's 11s in one environment and 22s in another, which is an extremely common question answered by the linked documentation :)   anyone can see that cnn time to interactive is not 11s-22s  I think there's a misunderstanding with what the ""Time to Interactive"" metric measures then. Have you read through the TTI docs? CNN definitely has a ton of main-thread work and is one of the canonical examples of a high TTI."
technical,"So the docs say:   Measuring TTI is important because some sites optimize content visibility at the expense of interactivity. This can create a frustrating user experience: the site appears to be ready, but when the user tries to interact with it, nothing happens.  But this is simply not true, it does not take 11s for CNN to be interactive. from web.dev   TTI measures how long it takes a page to become fully interactive. A page is considered fully interactive when:  The page displays useful content, which is measured by the First Contentful Paint, Event handlers are registered for most visible page elements, and The page responds to user interactions within 50 milliseconds. Just open a browser to cnn.com and you can start clicking interacting with it after 1 second latest. No frustration from the user.  Just making the case if TTI or LCP even try to model the REAL user experience or have we given up on that already."
technical,"Thanks! Appreciate you filing this bug.  This is a known issue, most well described in #10657. So, **we'll automatically close this as a duplicate**. how is a clearly wrong result due to variability? anyone can see that cnn time to interactive is not 11s-22s"
technical,"The page responds to user interactions within 50 milliseconds.  That is the root factor at play here. Lighthouse computes this value faithfully in the case of CNN because holy crap look at that main-thread (16"" Macbook i9).  It sounds like you disagree that this 50ms task limit is a reasonable condition to have in an interactivity metric, but that's not really something that's up for debate. That's the metric definition after years of work by a Chrome team dedicated to metrics and dozens of evaluated alternatives you're free to read about if you wish.  If it helps, the weight of TTI in the overall score has been steadily declining over the past 2 years and is unlikely to increase in the near future. It does, however, capture an important moment when the page is actually done loading all of its stuff, and it won't be disappearing anytime soon. screenshot from pagespeed, which says 11s, my own i9 macbook16 says 22s with lighthouse"
technical,"Ah, from your bug report it appeared as though you were asking why it's 11s in one environment and 22s in another, which is an extremely common question answered by the linked documentation :)   anyone can see that cnn time to interactive is not 11s-22s  I think there's a misunderstanding with what the ""Time to Interactive"" metric measures then. Have you read through the TTI docs? CNN definitely has a ton of main-thread work and is one of the canonical examples of a high TTI. So the docs say:   Measuring TTI is important because some sites optimize content visibility at the expense of interactivity. This can create a frustrating user experience: the site appears to be ready, but when the user tries to interact with it, nothing happens.  But this is simply not true, it does not take 11s for CNN to be interactive."
technical," Thanks! Appreciate you filing this bug.  This is a known issue, most well described in #10657. So, **we'll automatically close this as a duplicate**."
technical,"from web.dev   TTI measures how long it takes a page to become fully interactive. A page is considered fully interactive when:  The page displays useful content, which is measured by the First Contentful Paint, Event handlers are registered for most visible page elements, and The page responds to user interactions within 50 milliseconds. Just open a browser to cnn.com and you can start clicking interacting with it after 1 second latest. No frustration from the user.  Just making the case if TTI or LCP even try to model the REAL user experience or have we given up on that already. The page responds to user interactions within 50 milliseconds.  That is the root factor at play here. Lighthouse computes this value faithfully in the case of CNN because holy crap look at that main-thread (16"" Macbook i9).  It sounds like you disagree that this 50ms task limit is a reasonable condition to have in an interactivity metric, but that's not really something that's up for debate. That's the metric definition after years of work by a Chrome team dedicated to metrics and dozens of evaluated alternatives you're free to read about if you wish.  If it helps, the weight of TTI in the overall score has been steadily declining over the past 2 years and is unlikely to increase in the near future. It does, however, capture an important moment when the page is actually done loading all of its stuff, and it won't be disappearing anytime soon."
technical,"Linux and MacOSX have separated files. It also applies partly to Linux and Android. It was suggested to put all BSDs (we have OpenBSD in review for few months, FreeBSD and NetBSD are waiting for the OpenBSD merge) into one file. Ah, makes sense! Thanks for the clarification!"
technical,"Linux and MacOSX have separated files. It also applies partly to Linux and Android. It was suggested to put all BSDs (we have OpenBSD in review for few months, FreeBSD and NetBSD are waiting for the OpenBSD merge) into one file. An interesting initiative at  also if here we rely on Eigen tensor extension."
technical,OpenBSD dropped Linux compatibility a couple releases ago. Any chance of a FreeBSD port?
technical,"Finally I tried with git cl upload origin/master but it complained that I should probably rebase my feature branch. Last time I did that it removed my master so I stuck to my upstream-OpenBSD-support branch ,) Any news about the merge of this CL ? I'm waiting for it to submit the same for FreeBSD."
technical,"Finally I tried with git cl upload origin/master but it complained that I should probably rebase my feature branch. Last time I did that it removed my master so I stuck to my upstream-OpenBSD-support branch ,) Any update on this? As a potential Dart user on OpenBSD, I would really like to see this materialize.  I hope this porting effort is not going to be wasted, as a lot of work seem to have been done here."
technical,"Real life example we had bsd.. it was difficult to support it, for example the **bsdi** system was obsoleted and it was making incremental debt on us. In the result freebsd and openbsd upstreamed new separated platform files detaching from the ifdef mess. Apparently there was some confusion what supported means in my comment. Le me clarify: In this context a supported platform means, the Dart team ships install ready binaries from the official dartlang.org website. We do want this to be a two-way street which is why we are even having this discussion in the first place.  ""are Linux users asked to test OSX.. or any of the other supported platforms?"" We do expect our developers to test and fix the supported platforms (Linux, MacOSX and Windows) and architectures (ia32, x64, arm, arm64 and mips). This sometimes means to work with another team member who is more familiar with the other platform, but the burden is on the developer making the change. It is obviously ""easier"" to verify as we do have automated buildbots which will flag issues quickly.  This second point is exactly what my original statement was about: Since we do not have any currently and do not plan to add automated buildbots for any of the BSD variants in the near future. Therefore the responsibility to make sure that the different ""unsupported"" platforms are working correctly lies with the various maintainers.  Thirdly, we have learned that there is more commonality between related OS platforms than initially meets the eye at least when working in the Dart VM code base. The split between Linux and Android was mentioned here and we have come to the conclusion that it was a mistake. The vast majority of the files only differ in arbitrary places and we have discussed unifying them which will lead to a simpler handling of OS dependent code. You might have noticed that we do not have made the same mistake when adding iOS. Given that the OS interaction is significantly simpler than what is for example being done in libgtop, I still request that we start with a single * bsd.[cc|h] set of files. If down the line the complexity within the files warrants a split into sub pieces, we can address it at that point in time.  Thanks."
technical,I understand your frustration at the delay in getting this in. I am trying to move this issue forward and get a resolution. It is possible that moving the files to third party is not ideal but it does provide us a clean separation of the hosts that the Dart team ships periodically as install ready binaries in a SDK vs other hosts. If you have other ideas on how we can maintain this separation I am willing to listen.  I understand the comment earlier from iposva-google about the possibility of refactoring the OS specific files to share code but as indicated in that comment we request that you start with a bsd version and look at refactoring later. by third party I meant the hosts that are not actively shipped by the Dart team as install ready binaries in a SDK.
technical,"could you please move this discussion to the reviewboard. If that will push it forward it's OK to rename it to bsd then. what's the problem to setup BSD buildbots? I run several NetBSD ones, like for LLVM, Clang, LLDB or GDB. Consider then a  posix.cc/h or  unix.cc/h since Linux, Android and Mac/BSD's can share a lot of the existing code. The OpenBSD port used a lot of Linux & Android and some Mac OS X files, the FreeBSD port used a lot of Linux & Mac OS X files. Pushing just for  bsd.cc/h doesn't make sense if you look at it that way. Regarding buildbots. Do you allow third party contributed & managed machines?"
technical,"Apparently there was some confusion what supported means in my comment. Le me clarify: In this context a supported platform means, the Dart team ships install ready binaries from the official dartlang.org website. We do want this to be a two-way street which is why we are even having this discussion in the first place.  ""are Linux users asked to test OSX.. or any of the other supported platforms?"" We do expect our developers to test and fix the supported platforms (Linux, MacOSX and Windows) and architectures (ia32, x64, arm, arm64 and mips). This sometimes means to work with another team member who is more familiar with the other platform, but the burden is on the developer making the change. It is obviously ""easier"" to verify as we do have automated buildbots which will flag issues quickly.  This second point is exactly what my original statement was about: Since we do not have any currently and do not plan to add automated buildbots for any of the BSD variants in the near future. Therefore the responsibility to make sure that the different ""unsupported"" platforms are working correctly lies with the various maintainers.  Thirdly, we have learned that there is more commonality between related OS platforms than initially meets the eye at least when working in the Dart VM code base. The split between Linux and Android was mentioned here and we have come to the conclusion that it was a mistake. The vast majority of the files only differ in arbitrary places and we have discussed unifying them which will lead to a simpler handling of OS dependent code. You might have noticed that we do not have made the same mistake when adding iOS. Given that the OS interaction is significantly simpler than what is for example being done in libgtop, I still request that we start with a single * bsd.[cc|h] set of files. If down the line the complexity within the files warrants a split into sub pieces, we can address it at that point in time.  Thanks. could you please move this discussion to the reviewboard. If that will push it forward it's OK to rename it to bsd then. what's the problem to setup BSD buildbots? I run several NetBSD ones, like for LLVM, Clang, LLDB or GDB."
technical,"Well maybe this is anti-competitive behavior in the open source space (in terms of operating systems) by Google who happens to be one of the largest (if not largest) Linux vendor (via Android and Chrome OS) and Linux user (via their web services and Google Cloud Platform). It might be possible that the Dart developers may have been disallowed (as part of a larger strategy by upper management) from bringing support for any open source non-Linux/non-Google-developed operating system into the mainline of the Dart SDK.  Maybe this suggestion is silly, but I really have a hard time believing the argument that allowing the Dart open source community to support other operating systems puts a burden on the developers of Dart that outweighs the benefits (for Dart programmers) of being able to use the Dart SDK on other operating systems. Dart won't take me switch from NetBSD to Linux."
technical,"We are stuck in few projects with these panbsd platform files. In the practice they don't work well. Patching for NetBSD might break FreeBSD and vice versa and we will end up with spaghetti of ifdefs. Don't think that BSD developers use or test more than one BSD system, it's like asking Linux users to test AIX, because it's also UNIX.  BSD systems diverged more than 20 years ago! They have different kernels, different userland different 3rd party software managers. Don't think that BSD developers use or test more than one BSD system, it's like asking Linux users to test AIX, because it's also UNIX. are Linux users asked to test OSX.. or any of the other supported platforms?"
technical,"So basically, the tool need to be able to figure out  what  the diff is, you can always give it a commit to diff against, e.g., if you branched in #RANDOM, you can do: in your case, you could probably have done:  there is a git cl help if you are interested :-) Finally I tried with git cl upload origin/master but it complained that I should probably rebase my feature branch. Last time I did that it removed my master so I stuck to my upstream-OpenBSD-support branch ,)"
technical,"So basically, the tool need to be able to figure out  what  the diff is, you can always give it a commit to diff against, e.g., if you branched in #RANDOM, you can do: in your case, you could probably have done:  there is a git cl help if you are interested :-) Fresh checkout of this repository + my 2 branches as remote.  Got exception while uploading -- saving description to /home/mulander/.git cl description backup  that is not a nice experience for the amount of time all of this took."
technical,I gave up when building NSPR. Original Mozilla version has files for building with FreeBSD but in patched Dart branch are files somewhat missing. Even with proper files i am unable to compile it :( Has there been any update on this?
technical,"I gave up when building NSPR. Original Mozilla version has files for building with FreeBSD but in patched Dart branch are files somewhat missing. Even with proper files i am unable to compile it :( Have you considered trying to make the ninja build work, rather than the make build?  It may be that this new build system is more rational, and the generation of the build files by gyp is clearer, than the ""make"" gyp generator.  Documentation for gyp is here."
technical,I now have a fully built runtime & sdk on OpenBSD added as a proper 'target platform'.  What's left: - build all - review and implement runtime specifics (like procfs path)  Can someone from the dart team update on how the upstream wants to move forward?  All of my changes are in a feature branch of my repo merged periodically with latest upstream so the code is not stale.  Should I open a pull request now? Do you want me to first go through other steps? I think the code could be merged early as none of the existing files are modified in a way that would impact other build targets (I'm just adding openbsd entries to gypi files + * openbsd.cc/h files). Here is a full test run from a debug runtime + sdk build. 5 tests failed 3780 passed.
technical,"thanks for the response.  I will give stubbing out a shot over the next weekend, good to know that you are moving to Boring SSL.  Two little requests, please make it possible to pick up system wide Boring SSL during the build. Additionally I would be interested in using LibreSSL for dart on OpenBSD. Considering that both projects are an OpenSSL fork it should not be that hard if the build is not hard wired to Boring SSL. Hi guys, I'm from the migrated code.google.com comments (the guy trying to get Dart running on #OpenBSD).  I didn't give up and I'm really happy that BoringSSL replaced nss/nspr.  Though since the changes are only on head and adding OpenBSD support will be a much larger endeavour than just patching a file or two here & there I made a fork off the official repository and plan to do a proper port hopefully working with the upstream to make adding the BSD platform in a smooth way that's acceptable on both sides.  According to the Contributing guide I should give an up front notice with larger changes possibly coming down the line to coordinate.  Goals for my branch: - build the base runtime on OpenBSD - build the whole SDK - allow using LibreSSL instead of BoringSSL - eventually build the Dartium browser  There's no action that upstream needs to take now of course. The initial porting effort with an updated status can be found here."
technical,"Thanks! I decided that trying to accomplish this will be one of my new year resolutions. On January I will setup my machine to be able to build and then will start. I will then contact you on freenode (nick pbgc) Hi! A quick status update! I was able to build the runtime on FreeBSD by patching 25 files (using linux plataform as base with some files from macos), including several gyp files. I will now try to build all the sdk and then make a proper Platform for FreeBSD and will try to coordinate my effort with mulander so we can have FreeBSD and OpenBSD (NetBSD will be also easy). Currently I have 10/3785 failed tests: I don't know if the tests are failing because of the port ... or because of the PRE-1.14 status of the rep."
technical,"Have you considered trying to make the ninja build work, rather than the make build?  It may be that this new build system is more rational, and the generation of the build files by gyp is clearer, than the ""make"" gyp generator.  Documentation for gyp is here. I didn't try the ninja build. Essentially I got this one working flawlessly by putting symlinks to gcc, g++ & cc in a local directory that is in front of PATH.  I would personally say the build feels to be going quite well so far and I think I'm not that far away from having a working runtime. Though the porting effort is paused until the next weekend."
technical,"Have you considered trying to make the ninja build work, rather than the make build?  It may be that this new build system is more rational, and the generation of the build files by gyp is clearer, than the ""make"" gyp generator.  Documentation for gyp is here. I gave up when building NSPR. Original Mozilla version has files for building with FreeBSD but in patched Dart branch are files somewhat missing. Even with proper files i am unable to compile it :("
technical,"by third party I meant the hosts that are not actively shipped by the Dart team as install ready binaries in a SDK. I have some suggestions on maintaining clear separation between the hosts that the Dart team ships periodically and ones that are supported by the community:  - Add the separation via the build system so that the build step that you use to build the install ready binaries does not by default build binaries for non-supported hosts. - Keep the files ending with  openbsd.cc and  openbsd.h in the source directory (if you don't want to see the OpenBSD files in the bin/runtime directory for whatever reason, consider moving the host specific files into a single separate directory for host-specific code or to multiple separate directories based on what is being specialized for each host). - Add disclaimers on each of the files that are specific to non-supported hosts explaining that these hosts are not being supported directly by Google and that the Dart SDK is only available on these hosts because of generous contributions from people from the community who spent their limited time porting the Dart SDK to a new host so that they could enable the usage of the Dart SDK on that host for the benevolence of the Dart community.  I also think it's unreasonable ask mulander to add support for all of the BSD operating systems in one set of files ending in  bsd.cc and bsd.hh as mulander would have to then add support for at least two different operating systems which is quite extreme (show some empathy and think about the human cost (in terms of time and effort) that would be required to learn about all the different BSD operating systems and then validating and testing the code on a few of the major BSD operating systems just to get the work that was already done for OpenBSD merged upstream)."
technical,Dart won't take me switch from NetBSD to Linux. I know its very distant goal from having a native BSD port but has anyone tried running Dart Linux binaries in FreeBSD using Linux Binary Compatibility[1]? Maybe formal support for FreeBsd LBC would be a good start?
technical,"Please upstream BSD bits as soon as possible, I will mirror them for NetBSD. I now have a fully built runtime & sdk on OpenBSD added as a proper 'target platform'.  What's left: - build all - review and implement runtime specifics (like procfs path)  Can someone from the dart team update on how the upstream wants to move forward?  All of my changes are in a feature branch of my repo merged periodically with latest upstream so the code is not stale.  Should I open a pull request now? Do you want me to first go through other steps? I think the code could be merged early as none of the existing files are modified in a way that would impact other build targets (I'm just adding openbsd entries to gypi files + * openbsd.cc/h files)."
technical,"It is awful. I realize this is a very passionate issue for FreeBSD users, but the issue here is we don't have the resources to do this ourselves, and have little need for it (the VM is currently focusing on deploying to mobile devices for Flutter, for example).  It's fine to be disappointed, but please be respectful."
technical,It is awful. I started work on a port for OpenBSD.  Hacky at this point but I want to get at least the runtime built to see how the whole thing will feel. I'll start adding proper platform support for OpenBSD as soon as I get a single binary going (work limited to weekend so don't expect fast progress).  People wanting to help out are of course welcome.
technical,So should this issue just be closed? I see no point in lying to anyone else that something will be done here. I'm still a believer.
technical,"Here is a full test run from a debug runtime + sdk build. 5 tests failed 3780 passed. If this do not need Google, why this need you?"
technical,"Ah, makes sense! Thanks for the clarification! if you will be relying on BSD port maintainers to take care of keeping those platforms working, and have people who are currently willing to do that work (though I sense they are starting to get rather disillusioned at this point...), wouldn't it be better to defer to them regarding whether it's better to have a single file with a whole stack of #ifdefs or separate files for these (rather different) OS?"
technical,"I've likewise abandoned Dart because I see nothing in the Dart project to give me confidence that it will be ported. I'm willing to give it another look if things change, but I can't use something if it doesn't support the systems I deploy on.  As such, I've moved my focus to other platforms that *do* have support for \*BSD. I understand that the team may have different priorities, and I also understand that the majority of their customer base likely wouldn't benefit directly from \*BSD support.  This may have changed in the past, but getting set up to even work on this was a daunting task since it was (again, not sure if this has changed) part of the monolithic Chromium source tree. As such, I put *some* effort into porting (about a day of work), but it was such a huge barrier to entry that I lost interest and moved on to other projects. It does not seem like we have any immediate plans to support FreeBSD"
technical,It does not seem like we have any immediate plans to support FreeBSD It is awful.
technical,"Any news about the merge of this CL ? I'm waiting for it to submit the same for FreeBSD. It's been 1 week, 5 days since I got any feedback on the pending code review  This is blocking any further work on the port not to mention that upstream is moving off and will require additional work to catch up.  I don't see a way to get any platform support merged back with 1-2 week feedback cycles.  Is the dart project actually interested in BSD ports or are we all wasting time here?"
technical,"Consider then a  posix.cc/h or  unix.cc/h since Linux, Android and Mac/BSD's can share a lot of the existing code. The OpenBSD port used a lot of Linux & Android and some Mac OS X files, the FreeBSD port used a lot of Linux & Mac OS X files. Pushing just for  bsd.cc/h doesn't make sense if you look at it that way. Regarding buildbots. Do you allow third party contributed & managed machines? It's been 3 months since the pull request and over 2 months since last update.  Are there any plans regarding supporting BSD as a platform that can be shared with the general public? I'm still interested in helping & supporting an OpenBSD port of the Dart language."
technical,"Don't think that BSD developers use or test more than one BSD system, it's like asking Linux users to test AIX, because it's also UNIX. are Linux users asked to test OSX.. or any of the other supported platforms? Linux and MacOSX have separated files. It also applies partly to Linux and Android. It was suggested to put all BSDs (we have OpenBSD in review for few months, FreeBSD and NetBSD are waiting for the OpenBSD merge) into one file."
technical,Fresh checkout of this repository + my 2 branches as remote.  Got exception while uploading -- saving description to /home/mulander/.git cl description backup  that is not a nice experience for the amount of time all of this took. My branches. git cl upload was called from OpenBSD-support branch.
technical,I know its very distant goal from having a native BSD port but has anyone tried running Dart Linux binaries in FreeBSD using Linux Binary Compatibility[1]? Maybe formal support for FreeBsd LBC would be a good start? OpenBSD dropped Linux compatibility a couple releases ago.
technical,"It's been 3 months since the pull request and over 2 months since last update.  Are there any plans regarding supporting BSD as a platform that can be shared with the general public? I'm still interested in helping & supporting an OpenBSD port of the Dart language. Ping, we are still alive."
technical,"Running the tests with this, I get 8 tests failed - 2 could not be categorized or are in multiple categories  So ... I'm confused .....  Can someone give me some guidance on what to look for (thread implementation, etc..) if any of the failed tests are unexpected failures?  I forgot to mention ... The build is done with CLANG: Pedro Costa Please upstream BSD bits as soon as possible, I will mirror them for NetBSD."
technical,"if you will be relying on BSD port maintainers to take care of keeping those platforms working, and have people who are currently willing to do that work (though I sense they are starting to get rather disillusioned at this point...), wouldn't it be better to defer to them regarding whether it's better to have a single file with a whole stack of #ifdefs or separate files for these (rather different) OS? Real life example we had bsd.. it was difficult to support it, for example the **bsdi** system was obsoleted and it was making incremental debt on us. In the result freebsd and openbsd upstreamed new separated platform files detaching from the ifdef mess."
technical,"if you will be relying on BSD port maintainers to take care of keeping those platforms working, and have people who are currently willing to do that work (though I sense they are starting to get rather disillusioned at this point...), wouldn't it be better to defer to them regarding whether it's better to have a single file with a whole stack of #ifdefs or separate files for these (rather different) OS? Regarding the patch in comment 4:  You might have a much easier time if you did say as this will pickup all of the relevant build configurations for a make/gcc based build."
technical,"I'm still a believer. sincere apologies for taking so long and thanks for still being a believer... We talked about this and came up with a proposal that might help us get support for OpenBSD landed while addressing some of the raised concerns.  If we could change the CL to move files specific to these additional ports into runtime/third-party/ that would help make the distinction clearer and maintenance easier, e.g. rather than having the files we would have them in bin. Similarly, files would be moved from runtime/bin to runtime/third party/bin."
technical,"yeah I managed to get it working by doing a clean dart-lang/sdk for. Adding my two remote branches like I did before and forking a new upstream-OpenBSD-support branch from your origin master. I now merge my remote branch changes into yours and the cl tools picks them up.  Your suggested workflow works nice when someone works on a checkout of your repository for a short lived feature. My branch had to survive months hence I did it in my own repo and had yours checked out for occasional merges against my feature branches. So basically, the tool need to be able to figure out  what  the diff is, you can always give it a commit to diff against, e.g., if you branched in #RANDOM, you can do: in your case, you could probably have done:  there is a git cl help if you are interested :-)"
technical,"Ping, we are still alive. So should this issue just be closed? I see no point in lying to anyone else that something will be done here."
technical,"If this do not need Google, why this need you? Thanks for the promising progress updates.  Please make sure to run the full test suite and report its failure rate (e.g. omitting the ""language"" parameter). Also note that the default timeout for debug builds is 120 seconds and you would likely want to run with ""-t120"" or let the harness chose the right default value for you.  Once you have run the full test harness, please follow the steps at to create a CL and send it to me for comments. Thanks!"
technical,"We are working on moving the TLS support from Mozilla NSS to Boring SSL. If the NSS library and its dependencies are providing issues, then maybe just stubbing it out for now, and not support TLS until the change to use Boring SSL has landed. thanks for the response.  I will give stubbing out a shot over the next weekend, good to know that you are moving to Boring SSL.  Two little requests, please make it possible to pick up system wide Boring SSL during the build. Additionally I would be interested in using LibreSSL for dart on OpenBSD. Considering that both projects are an OpenSSL fork it should not be that hard if the build is not hard wired to Boring SSL."
technical,I started work on a port for OpenBSD.  Hacky at this point but I want to get at least the runtime built to see how the whole thing will feel. I'll start adding proper platform support for OpenBSD as soon as I get a single binary going (work limited to weekend so don't expect fast progress).  People wanting to help out are of course welcome. Thanks for working on this! I'll try to jump in and help a little bit at some point but the schedule is full!
technical,"I was able to last work on this port on September 19th. Unfortunately since then work has been busy which left no time for further dart work. I did not abandon the porting effort - it's just on hold for now. Initial porting effort. Normal branch to work on upstreaming proper support. he started a similar effort to port Dart to NetBSD based on my current status, his repository is here: (also on hold from what I know).  though it should be noted I am porting this for OpenBSD not FreeBSD though as you might be aware the work should be really similar (same with NetBSD as we found out). You can start by forking the dart repository and adding * freebsd.cc/h files based on the ones I added in my repository. That should get you pretty far building the code on FreeBSD. It just needs time. I'm also not the best person to do this but look at the age of this ticket. If people like me and you don't attempt it then we can wait another 3 years without anyone else picking the issue up. I will gladly help you get to the same spot on FreeBSD that I am with OpenBSD. Feel free to ask me question or just grab query with me on IRC on freenode. Thanks! I decided that trying to accomplish this will be one of my new year resolutions. On January I will setup my machine to be able to build and then will start. I will then contact you on freenode (nick pbgc)"
technical,"The issue seems to be my branch setup.   With the above setup I can move forward and now the linter complains (I'm fixing linter errors). The git cl tool is just more stupid than standard pull requests - that's all ,)  I now pull from OpenBSD-support to upstream-OpenBSD-support and cl picks that up. The fact that you have your 2 branches as remote here seems odd, that way there is no diff to our main branch. You need to have it create the cl against our origin/master  EDIT: Saw your comment, glad you figured it out"
technical,"The issue seems to be my branch setup.   With the above setup I can move forward and now the linter complains (I'm fixing linter errors). The git cl tool is just more stupid than standard pull requests - that's all ,)  I now pull from OpenBSD-support to upstream-OpenBSD-support and cl picks that up. The issue seems to be my branch setup.   With the above setup I can move forward and now the linter complains (I'm fixing linter errors). The git cl tool is just more stupid than standard pull requests - that's all ,)  I now pull from OpenBSD-support to upstream-OpenBSD-support and cl picks that up."
technical,"I realize this is a very passionate issue for FreeBSD users, but the issue here is we don't have the resources to do this ourselves, and have little need for it (the VM is currently focusing on deploying to mobile devices for Flutter, for example).  It's fine to be disappointed, but please be respectful. This issue was originally filed by  Is there posibility that Dart VM will support BSD (like Go)?"
technical,"Any update on this? As a potential Dart user on OpenBSD, I would really like to see this materialize.  I hope this porting effort is not going to be wasted, as a lot of work seem to have been done here. We are looking at how to integrate a non-supported and non-tested platform into the bleeding-edge tree without adding an undue burden on the mainline development. The current plan is that only officially supported platforms are going to be maintained by the Dart team in general. This means that for example any of the BSD ports can and will likely break often as we rely on new OS features or make changes to the OS dependent APIs.  To reduce the maintenance effort for the supported platforms, we will likely need that all of the BSD variants (Open, Free, Net) coexist in a single set of xyz bsd.cc and h files. As you may have noticed the OP is about supporting the Dart VM on FreeBSD, but the current code review is for OpenBSD. So we know that we have interest in at least two of the variants.  Thank you for your continued patience."
technical,"As someone who has worked in the open source ecosystem for the better part of 15 years, I am frankly fed up with this posture in open source projects. As you have chosen to open up your project for collaboration, you need to provide a street that goes both directions. As people contribute to you, they are establishing themselves as your user base. Whether or not you set out to support a certain platform or not, as your project grows that idea of ""support"" may have to change. Looking at how you advertise dart, I do not see a way you can accomplish your goals without supporting the BSD projects. While they may not carry the same numbers as linux, it is rather self deprecating to reject work that someone has done for you on the claim that it creates more work for you. Frankly, it is confusing and frustrating that some projects at google choose to embrace the open source ecosystem in its entirety, (golang comes to mind, supporting quite a few hardware archs on several of the BSD's), while dart seems to want to limit itself. You have drawn your obvious line in the sand, and I wish you the complete return on your decision. We are stuck in few projects with these panbsd platform files. In the practice they don't work well. Patching for NetBSD might break FreeBSD and vice versa and we will end up with spaghetti of ifdefs. Don't think that BSD developers use or test more than one BSD system, it's like asking Linux users to test AIX, because it's also UNIX.  BSD systems diverged more than 20 years ago! They have different kernels, different userland different 3rd party software managers."
technical,"As someone who has worked in the open source ecosystem for the better part of 15 years, I am frankly fed up with this posture in open source projects. As you have chosen to open up your project for collaboration, you need to provide a street that goes both directions. As people contribute to you, they are establishing themselves as your user base. Whether or not you set out to support a certain platform or not, as your project grows that idea of ""support"" may have to change. Looking at how you advertise dart, I do not see a way you can accomplish your goals without supporting the BSD projects. While they may not carry the same numbers as linux, it is rather self deprecating to reject work that someone has done for you on the claim that it creates more work for you. Frankly, it is confusing and frustrating that some projects at google choose to embrace the open source ecosystem in its entirety, (golang comes to mind, supporting quite a few hardware archs on several of the BSD's), while dart seems to want to limit itself. You have drawn your obvious line in the sand, and I wish you the complete return on your decision. We are working on moving the TLS support from Mozilla NSS to Boring SSL. If the NSS library and its dependencies are providing issues, then maybe just stubbing it out for now, and not support TLS until the change to use Boring SSL has landed."
technical,"I started the FreeBSD port 1 year ago ... and was able (like I commented here above) to build and pass most of the tests. I had plans to make a proper port (with separate files and not patches on the linux ones) but after seeing how the Dart team treated the BSD ports efforts and specially Mulander's work ... I really got disappointed and lost all interest in Dart! I've abandoned (a long time ago) the port effort and don't even use Dart anymore and will stop watching this rep. Well maybe this is anti-competitive behavior in the open source space (in terms of operating systems) by Google who happens to be one of the largest (if not largest) Linux vendor (via Android and Chrome OS) and Linux user (via their web services and Google Cloud Platform). It might be possible that the Dart developers may have been disallowed (as part of a larger strategy by upper management) from bringing support for any open source non-Linux/non-Google-developed operating system into the mainline of the Dart SDK.  Maybe this suggestion is silly, but I really have a hard time believing the argument that allowing the Dart open source community to support other operating systems puts a burden on the developers of Dart that outweighs the benefits (for Dart programmers) of being able to use the Dart SDK on other operating systems."
technical,"sincere apologies for taking so long and thanks for still being a believer... We talked about this and came up with a proposal that might help us get support for OpenBSD landed while addressing some of the raised concerns.  If we could change the CL to move files specific to these additional ports into runtime/third-party/ that would help make the distinction clearer and maintenance easier, e.g. rather than having the files we would have them in bin. Similarly, files would be moved from runtime/bin to runtime/third party/bin. what does 3rd party mean in your definition?"
technical,"The fact that you have your 2 branches as remote here seems odd, that way there is no diff to our main branch. You need to have it create the cl against our origin/master  EDIT: Saw your comment, glad you figured it out yeah I managed to get it working by doing a clean dart-lang/sdk for. Adding my two remote branches like I did before and forking a new upstream-OpenBSD-support branch from your origin master. I now merge my remote branch changes into yours and the cl tools picks them up.  Your suggested workflow works nice when someone works on a checkout of your repository for a short lived feature. My branch had to survive months hence I did it in my own repo and had yours checked out for occasional merges against my feature branches."
technical,"It seems like the bug was in the closed source vscodedistro component. Ah, so there's some proprietary software that VSCode uses that apparently does something which can lead to a security hole. We don't know what it does or how risky it is to run."
technical,"Just received update 1.47.1 linking to this issue, but it doesn't have details ˜ I updated the link. The MITRE copy is not yet updated."
technical,"The milestone page is ""empty,"" but if you click ""closed"" you'll see it. Probably should tweak the link to show closed issues. Is there a commit/PR we can see fixing this bug? It's not super helpful to know a CVE existed if we can't verify that it was fixed properly."
technical,Is there a commit/PR we can see fixing this bug? It's not super helpful to know a CVE existed if we can't verify that it was fixed properly. It seems like the bug was in the closed source vscodedistro component.
technical," Just received update 1.47.1 linking to this issue, but it doesn't have details ˜"
technical,"I updated the link. The MITRE copy is not yet updated. The link currently just goes to this milestone, is that expected?"
technical,"I updated the link. The MITRE copy is not yet updated. The milestone page is ""empty,"" but if you click ""closed"" you'll see it. Probably should tweak the link to show closed issues."
technical,"Ansible community heavily relies on Code of Conduct being followed, which lets us address issues more effectively and be respectful to each other. Posting issues with offensive content is definitely not going to encourage productive discussion, but normally results in confrontation. For best outcome please consider writing your messages mindfully and calmly explaining both your problem and what is the suggested resolution or expectation for it. TIA. Akasurde I don't see how offensive title makes it less of an issue. The code in the issue seems self-descriptive to several people in this thread. Do you really need any additional feedback from issue author?"
technical," Greetings! Thanks for taking the time to open this issue. In order for the community to handle your issue effectively, we need a bit more information.  Here are the items we could not find in your description: - issue type - ansible version - component name  Please set the description of this issue with this template. click here for bot help"
technical,"Possibly, using a more generic would be a better way of finding the default route. Using a well-known address is not necessarily a good practice. ipv6 default route lookup is also broken"
technical,I agree that this an issue. But there is a certain way to describe an issue in Open Source Community. Using sexual abusive language is not the way to express your opinion. Please refer Code of Conduct for more details.
technical,"Don't be so agitated. It's just a lookup for route in local routing table, no network queries are actually performed. Possibly, using a more generic would be a better way of finding the default route. Using a well-known address is not necessarily a good practice."
technical,"Wow, was it fixed that fast? title seems offensive so I closed it."
technical,"ipv6 default route lookup is also broken Wow, was it fixed that fast?"
technical," All comments in PR #9665 have been addressed. Just waiting on you, pal."
technical,"All comments in PR #9665 have been addressed. Just waiting on you, pal. Use md-stretch-height to fill available space in parent element.  Closes won't fix issue #2254 abandoned by core team for material 2 surge. Previously approved for milestones 1.1.6 and 1.1.7 by ThomasBurleson in pull request #9665 that was locked by mistake by Splaktar as explained in PR #11131. This PR is being opened at Splaktar's request as noted in #9665"
technical,"Maybe there could be a keyword (maybe the first one?), that is ranked as if it was part of the title. So that way GitLens could have  git , Vetur could have  vue , Go could have  golang , etc. Already did that, just need marketplace to index the keywords in package.json"
technical,we would have the fix for the issue you reported rolled out by next week or so. any details on what changes are being made exactly?
technical,"I am from the Marketplace team. Thanks for bringing this issue to our notice. We are looking into it and will get it fixed. Any updates after half a month? If it hasn't been looked into, an estimate on when it will be looked into would be nice. If it has been looked into, an update on what was found/not found would also be nice. If the Marketplace team has higher priority tasks that's fine, I completely understand, I think we would just like to know that."
technical," Disregarding the fact that the Marketplace doesn't return any results when searching for an extension's id, we have fixed the Marketplace instructions since a while. ext install was our way of guiding users to install extensions inside VS Code. Since we now have URL handlers for Mac and Windows, those instructions became almost irrelevant. We now only show them for Linux users. Running this in VS Code will correctly trigger the search for id.name. The name format should've never been there. Only theid.name is unequivocal in order to find an extension in the Marketplace.  One idea I have for alleviating this pain is to improve the ext install name (without id) experience: what if we showed in quick open a list of extensions which match that name? This would simply mitigate the fact that there are instructions for ext install name out there. But in any case, I recommend switching your instructions to ext install id.name.  We've notified the Marketplace of the search issues."
technical,"Already did that, just need marketplace to index the keywords in package.json exactly!"
technical,"Hey guys, I want to thank you for all the feedback. Please keep them coming. We are discussing this issue internally and I'll update this thread as soon as we have something to share. Here there's another weird one, searching for ""monokai"" and ordering by downloads returns those language packs at the top. The only place where those language packs mention ""monokai"" is inside their package.json as the value of some contributes.localizations[0].translations.id keys, why are you guys even indexing those fields?"
technical,"This probably happens because of this. The update count should not effect the search ranking. Yeah that's another major problem, one could automatically push a new update every day and downloads will go through the roof even though the same number of people are using it. I guess technically those users are  downloading  the update, but once this things are counted the downloads counter becomes less meaningful. The download counter for extensions on Chrome's store decreases when somebody uninstalls your extension and I think it doesn't increase just because the extension gets updated. Hey guys, I want to thank you for all the feedback. Please keep them coming. We are discussing this issue internally and I'll update this thread as soon as we have something to share."
technical,"thanks for the update.  Any timeframe for when the move will happen? Holy crap changing my extension name to ""AREPL for python"" moved it to the *third* position in the search rankings when searching for python.  Thanks for the tip!"
technical,"Well, I would still consider whatever algorithm you guys are using broken because if I search for ""open in node modules"" I get these search results (some samples randomly picked): If I had to guess what's wrong with your approach I'd say:  - Lack of stopwords, kind of meaningless keywords like ""vscode"" or ""in"" are given too much weight. - Poor query parsing, if keywords are joined in some sort or another the whole thing falls apart (vscode-foo-bar, foo bar, GitLens etc.) - Maybe you're giving too much weight to downloads and ratings, the first thing to sort for is relevancy.   One way to improve your extension rank is to break word node modules into separate words node modules. This will increase the string matching score and push your extension up in the result.  I'm not going to rename the extension to something wrong (node modules is a folder, I'm not talking about node modules, as in ""NPM packages"", here) just to work around this. I agree, the search ranking seems wierd. Some examples (when searching for ""python""): Py Files generator is above autoDocString, despite having about 50k less installs and 0 reviews Same thing with ladieratheme - it is above autoDocString despite having about 49k less installs and 3 less reviews. Python paste and indent has 3 stars but apparently it's 2k extra downloads trumps Python (pydev)'s 5 star rating.  Seems like download count is weighted more heavily than rating in the ranking (or maybe python paste and indent has better keywords)  Python Coding Conventions has 773 downloads and is unrated, yet somehow is above magicpython (with 742  thousand  downloads and 3.5 stars) and several other extensions with far more downloads / good ratings.  So you could have a very popular or well-rated extension but if you don't have the right keywords you will still be ranked down.  Wait... but looking at the Python Coding Conventions package.json, it doesn't even have any keywords! While searching an extension, we also take into account the community inputs like number of downloads,  You mean the download count that is  also  the update count?  This has been a outstanding issue ever since 2016 - when you release an update the marketplace shows your downloads as having increased. The update count should not effect the search ranking.That's good, but what algorithim are you using to calculate the weighted rating?  Not sure if that is open source but hopefully it isn't something like RatingA - RatingB or RatingA/RatingB"
technical,"My only theory to how these are being ranked, is that the marketplace has grabbed 200 results and accidentally sorted them by trending-ness instead of search-query relevance. Since this is an upstream problem, is there a more direct repo we can reach out to? If not then I agree with octref, its been 2 months, it's probably time for the community to start building their own solution. I don't understand how something this critical can fail without tests catching it and a rollback being issued. I am from the Marketplace team. Thanks for bringing this issue to our notice. We are looking into it and will get it fixed."
technical,"Just wanted to open this as an issue. I often read search for [xxx] and install the first result. I search the exact name of an extension and the one I'm looking for never is the first result :( I can confirm this is still a bad issue. ### This isn't a 'bad' search algorithm, something is broken here.  When searching for ""line endings"" (picture below)<br The following 4 circles are all similar extensions. The red circles have MORE downloads, installs, and better ratings, and I checked the package.json all of them have ""line endings"" in the title and as keywords.<br  How can 'Line Note' (4th result) which - doesn't have 'endings' in the title - has 203 downloads - no reviews - doesn't even have 'endings' in the package.json or readme  Beat out both 'line-endings' and 'code-eol (2019) Line Endings' - both contain the full search term in the title - they have 4K and 6K downloads respectively - they have 4 stars and 5 stars respectively -  and have 'line endings', 'line', 'endings' in the keywords of the package.json and readme  And it's not like 'Line Note' is some anomaly, there are 25 just as bad results that come before the relevant one."
technical,"Oh and here's a really weird one - python (pydev) is ranked above python test explorer, despite having the same number of reviews, python mentioned in description of both, worse reviews on average, and ~45k less downloads. I'm not sure what algorithm is being used but there's some existing ones that may be helpful:"
technical,"Here there's another weird one, searching for ""monokai"" and ordering by downloads returns those language packs at the top. The only place where those language packs mention ""monokai"" is inside their package.json as the value of some contributes.localizations[0].translations.id keys, why are you guys even indexing those fields? I've just realized that the readmes aren't indexed at all. I searched for ""cyclomatic"" in the marketplace, and I've got this result: Then I tried searching for ""marketplace cyclomatic"" on Google, and this is the result. This is a bit ridiculous. I'm not saying that the search functionalities in the marketplace should be as good as Google's under **all** circumstances, but that's a full-word match in the readme, this query just can't return 0 results. do you have any updates for us?"
technical,"Disregarding the fact that the Marketplace doesn't return any results when searching for an extension's id, we have fixed the Marketplace instructions since a while. ext install was our way of guiding users to install extensions inside VS Code. Since we now have URL handlers for Mac and Windows, those instructions became almost irrelevant. We now only show them for Linux users. Running this in VS Code will correctly trigger the search for id.name. The name format should've never been there. Only theid.name is unequivocal in order to find an extension in the Marketplace.  One idea I have for alleviating this pain is to improve the ext install name (without id) experience: what if we showed in quick open a list of extensions which match that name? This would simply mitigate the fact that there are instructions for ext install name out there. But in any case, I recommend switching your instructions to ext install id.name.  We've notified the Marketplace of the search issues. I've only recently noticed the change (from ext-name to owner.ext-name) because somebody opened an issue about that in one of my repositories, has this change been mentioned in any changelog in the past? I would have replaced those ext install (...) instructions with URL handlers, but I've tried them once and after that, for a few days/weeks, a ""Do you want to install (...)"" message kept popping up every time I opened a new window. By the time that was fixed I guess I had forgotten about them. what if we showed in quick open a list of extensions which match that name? Why providing 2 different interfaces for discovering extensions? Shouldn't searching ""just work""?"
technical,"we have rolled out the change. You should see the better search results. Thanks for bringing it to our notice and keep the feedback coming in! It seems that some issues have been addressed, but some things got even worse:  - Now searching for both vscode-todo-plus and fabiospampinato.vscode-todo-plus returns 0 results, IMHO that's unacceptable (and this was the original problem reported in this issue).  - Sometimes if I type open in browsers and then hit enter the query becomes open in browser, deleting the last character '‚.  - The marketplace doesn't search automatically until I hit enter,  unless  I delete the query, IMHO that's pretty confusing (and that's also not what's happening in the homepage).  - If I clear the input field then the cursor disappears for a while until the results are returned. - There's probably no need to wait for those results to be returned in the first place, the top 55 extensions by downloads won't change very often, I'd just cache them (refreshing the cache once a day or so) and just ship them along with the rest of the page to the browser."
technical, Just to reiterate on how bad the search engine works:  - I've just published an extension named Open in node modules - Searching for open in node modules (without the underscore) won't even show said extension within the first 60 results '‚
technical,While I don't disagree -- I was just trying to offer an alternative that would hopefully be easy to implement but improves things somewhat and therefore likely to happen ˜ Just wanted to open this as an issue. I often read search for [xxx] and install the first result. I search the exact name of an extension and the one I'm looking for never is the first result :(
technical,"Thanks, I appreciate it. The ""Line endings"" search seems to be fixed so I'm pretty happy about that.  After a few searches I do agree with Almenon the search is sadly still pretty broken.  **For me, the biggest issue is its unclear why it is broken (both three months ago, and now)** Is there some kind of AI returning the results? A handcrafted weighting system? Some transparency about how the search works would go a long way. I know the code probably can't be made open source, but if the marketplace team made an API that would allow for exact-name matches and regex matches I'm pretty confident the VS Code community could fix this permanently within 48 hours. Here's an example of the search still being pretty messed up. It's an improvement from last time, at least the relevant results are somewhere in the top 10, and the other results are popular. But on the other hand it is pretty sad that most students in my undergrad Info Storage and Retrieval class wrote better ranking systems in less than a month. Marketplace team has recently updated their search strategy to use Azure based search and which will expect to fix quiet a number of search related issues. There could be still some gaps which might exist and can be fixed. Since this issue is overwhelmed with lot of feedback and it is getting hard for us to understand what is fixed and what is not, I am locking this issue and ask users to create separate issues if they see any new bugs. At the same time, I would go through this complete issue and extract those which are not yet fixed into new issues which will help us and Marketplace team to better track and fix them. Thanks for the feedback provided here."
technical,"Which I would refuse. What do I call it? Vetur Vue? Another example: Searching for ""Golang"".  What should Go extension do? Renaming it to Golang? After which the query Go wouldn't return it as top result? Maybe it's better to start a community curation of good extensions for each is a good example. At least it wouldn't be impossible for new users to find which extension to use, because the search results are not ranked helpfully. Maybe there could be a keyword (maybe the first one?), that is ranked as if it was part of the title. So that way GitLens could have  git , Vetur could have  vue , Go could have  golang , etc."
technical,"I can confirm this is still a bad issue. ### This isn't a 'bad' search algorithm, something is broken here.  When searching for ""line endings"" (picture below)<br The following 4 circles are all similar extensions. The red circles have MORE downloads, installs, and better ratings, and I checked the package.json all of them have ""line endings"" in the title and as keywords.<br  How can 'Line Note' (4th result) which - doesn't have 'endings' in the title - has 203 downloads - no reviews - doesn't even have 'endings' in the package.json or readme  Beat out both 'line-endings' and 'code-eol (2019) Line Endings' - both contain the full search term in the title - they have 4K and 6K downloads respectively - they have 4 stars and 5 stars respectively -  and have 'line endings', 'line', 'endings' in the keywords of the package.json and readme  And it's not like 'Line Note' is some anomaly, there are 25 just as bad results that come before the relevant one. My only theory to how these are being ranked, is that the marketplace has grabbed 200 results and accidentally sorted them by trending-ness instead of search-query relevance. Since this is an upstream problem, is there a more direct repo we can reach out to? If not then I agree with octref, its been 2 months, it's probably time for the community to start building their own solution. I don't understand how something this critical can fail without tests catching it and a rollback being issued."
technical,"So those keywords I've been adding are actually useless you say? Nice...  I'm not sure if it's worse that the search engine works as badly as it does or that it hasn't been fixed already. Not useless, but just ranked quite a lot below what is in the name -- at least that used to be the case"
technical,"Yep things still aren't working right. Earlier they said that ""While searching an extension, we also take into account the community inputs like number of downloads, number of ratings and average rating along with the string matching"". However, when I search for ""python"" ""python extended"" is above my extension despite having literally 0 reviews. In terms of download count we are very close (130k vs. 150k) so if the reviews were taken into account I would expect AREPL to be ranked higher.  I would even go so far as to say that reviews should be  more  important than download count in influencing search ranking (assuming you have enough reviews to be statistically valid). As a user when I search for things I want to find the best extensions, not the most downloaded extensions, but my extension has good reviews so I am biased on that front :P  can you explain the change in more detail please? I don't know what ""tweaking the index analyzers"" means. Oh and here's a really weird one - python (pydev) is ranked above python test explorer, despite having the same number of reviews, python mentioned in description of both, worse reviews on average, and ~45k less downloads."
technical,"Previously. Now if you search for Vue, Vetur doesn't even show up in the first page despite being the most popular Vue extension. The install count of the 24 Vue extensions in first page combined is not even half of Vetur's install count. A new Vue user coming to VS Code have a hard time finding Vetur by organic search. He might be misled to install a lot of the random extensions in Marketplace and believe VS Code has poor support for Vue files. Oops, misclick."
technical,"Sorry we are still in early exploration stages, cannot comment on a timeline yet. Previously. Now if you search for Vue, Vetur doesn't even show up in the first page despite being the most popular Vue extension. The install count of the 24 Vue extensions in first page combined is not even half of Vetur's install count. A new Vue user coming to VS Code have a hard time finding Vetur by organic search. He might be misled to install a lot of the random extensions in Marketplace and believe VS Code has poor support for Vue files."
technical,"Just to reiterate on how bad the search engine works:  - I've just published an extension named Open in node modules - Searching for open in node modules (without the underscore) won't even show said extension within the first 60 results '‚ regarding the issue of searching your new extension: While searching an extension, we also take into account the community inputs like number of downloads, number of ratings and average rating along with the string matching. Among the string matching, exact string matches carry a higher weight than prefix match.  Since node modules is one word in your extension name, word node in search text gets prefix match here. However, there are lot of extensions on Marketplace which have exact word 'node' in their extension name. So they carry a higher weight for matching the word 'node'. Additionally, since your extension is new, download count and ratings of other extensions are way higher, pushing their total score to the top. One way to improve your extension rank is to break word node modules into separate words node modules. This will increase the string matching score and push your extension up in the result."
technical,"This has been a long standing issue -- it is the reason I had to rename my extension to *Git Lens* rather than its actual name *GitLens* (no space), because when searching for git it would never show up. And even with that rename -- I had to pack in other words into the title (SEO hacking style) to even get it to show up on some other keywords -- even if those keywords were in the package.json keywords list. So those keywords I've been adding are actually useless you say? Nice...  I'm not sure if it's worse that the search engine works as badly as it does or that it hasn't been fixed already."
technical,"Holy crap changing my extension name to ""AREPL for python"" moved it to the *third* position in the search rankings when searching for python.  Thanks for the tip! Sorry we are still in early exploration stages, cannot comment on a timeline yet."
technical,"Took me quite a while to test, but nothing reported from this issue was fixed and search by author ID, extension ID, or combination of both are broken. Thanks for consolidating already (I was planning to do it this week). These issues were already raised with Marketplace team and here is their explanation  ### Searching by id: GitHub.vscode-pull-request-github-insiders  We don't support search on id of the extension(Not supported in SqlFTS too). When we search for GitHub.vscode-pull-request-github-insiders, then we search for two words GitHub and vscode-pull-request-github-inside.  That's why all the extension's containing Github are coming at the TOP and results are looking better than SqlFTS.  **Tip**: You can search by extension id using the phrase id. Let's track it there.  ### Searching by publisher id: dias  Azure search provides us custom analyzer to control search relevance. We had done stemming during indexing. So when user searches for dias, it returns all the extensions containing dia or dias both. Here DialogScript is coming at the top, because it contains dia , while dias is matched in publisher display name of Open Folder Context Menus for VS Code extension.  Extension display name prefix match has more weight than publisher display name. That's why DialogScript is  coming at the top. We have fixed this issue. We are not doing stemming now. So if you will search for dias, it will return the extension's containing dias only.  **Tip**: You can search by publisher using the phrase publisher:""Pine Wu"". You can also click on Publisher name in the extension editor. I think this is a good solution for searching by publisher.  ### Vue We do have weights on tags too, but it's very less (1/100)  in compare to extension's display name. That's why Vetur extension's is not coming at the top, when user searches for ˜Vue'. ˜Vue' search term is a  special case, where we expect Vetur extension as top result, because it's a popular extension of ˜Vue' language. We were unbale to solve this problem using SqlFTS, but we can solve this using AzureSearch. We can add Synonym Maps property to the index field definition in AzureSearch.  Please add any details if missing. After consolidating, one of the improvements we would like to see from you is with respect to vue phrase in which case giving weightage to tags might help."
technical,"Yes, unfortunately we do not index readme.md.. sorry about that. We use SQL FTS in our backend today and that's kind of the limiting factor. There's only so much custom logic we can run over the results returned by FTS to make it 'more' relevant - and more importantly it doesn't scale in the long run. We are exploring moving our search platform to Azure Search or Bing which are techologies that are being actively developed and should provide us with more features and capabilities. I'm afraid we are not going to invest more in trying to optimize search with FTS. thanks for the update.  Any timeframe for when the move will happen?"
technical,"I'm not sure what algorithm is being used but there's some existing ones that may be helpful: Thanks, I appreciate it. The ""Line endings"" search seems to be fixed so I'm pretty happy about that.  After a few searches I do agree with Almenon the search is sadly still pretty broken.  **For me, the biggest issue is its unclear why it is broken (both three months ago, and now)** Is there some kind of AI returning the results? A handcrafted weighting system? Some transparency about how the search works would go a long way. I know the code probably can't be made open source, but if the marketplace team made an API that would allow for exact-name matches and regex matches I'm pretty confident the VS Code community could fix this permanently within 48 hours. Here's an example of the search still being pretty messed up. It's an improvement from last time, at least the relevant results are somewhere in the top 10, and the other results are popular. But on the other hand it is pretty sad that most students in my undergrad Info Storage and Retrieval class wrote better ranking systems in less than a month."
technical,"Thanks for consolidating already (I was planning to do it this week). These issues were already raised with Marketplace team and here is their explanation  ### Searching by id: GitHub.vscode-pull-request-github-insiders  We don't support search on id of the extension(Not supported in SqlFTS too). When we search for GitHub.vscode-pull-request-github-insiders, then we search for two words GitHub and vscode-pull-request-github-inside.  That's why all the extension's containing Github are coming at the TOP and results are looking better than SqlFTS.  **Tip**: You can search by extension id using the phrase id. Let's track it there.  ### Searching by publisher id: dias  Azure search provides us custom analyzer to control search relevance. We had done stemming during indexing. So when user searches for dias, it returns all the extensions containing dia or dias both. Here DialogScript is coming at the top, because it contains dia , while dias is matched in publisher display name of Open Folder Context Menus for VS Code extension.  Extension display name prefix match has more weight than publisher display name. That's why DialogScript is  coming at the top. We have fixed this issue. We are not doing stemming now. So if you will search for dias, it will return the extension's containing dias only.  **Tip**: You can search by publisher using the phrase publisher:""Pine Wu"". You can also click on Publisher name in the extension editor. I think this is a good solution for searching by publisher.  ### Vue We do have weights on tags too, but it's very less (1/100)  in compare to extension's display name. That's why Vetur extension's is not coming at the top, when user searches for ˜Vue'. ˜Vue' search term is a  special case, where we expect Vetur extension as top result, because it's a popular extension of ˜Vue' language. We were unbale to solve this problem using SqlFTS, but we can solve this using AzureSearch. We can add Synonym Maps property to the index field definition in AzureSearch.  Please add any details if missing. After consolidating, one of the improvements we would like to see from you is with respect to vue phrase in which case giving weightage to tags might help. There are a few major problems when searching by an extension's name. First of all, if I remember correctly, the marketplace had this thing on every extension's page: So I've added an analogous message to all of my extensions' readme files, and I think other extension authors have done this too.  I have an extension called Todo+ (full id). This is what happens when the command ext install vscode-todo-plus is executed: - vscode-todo-plus in an exact match of the id minus my username, and the extension is not even in the top 10 results. - Searching for fabiospampinato.vscode-todo-plus, which is an exact match of the full id, leads to the same results. - Searching for id:fabiospampinato.vscode-todo-plus works, **but only inside VSC**, it doesn't work on this. - Where are these ext install and id: commands documented anyway?  Let's try to search for another extension of mine: - Not only vscode-open-in-browsers is an exact match of the extention id minus my username, but ""Open in Browsers"" is the extension's title, and ""vscode"" is practically a meaningless keyword, as is ""in"". What's the first result for this search?   I hope we do agree there's a problem here.  If I may suggest a few improvements: 1. Always check if an extension's id (minus the username) is an exact match of the current query. 2. Implement stop words.  Please don't close this issue as ""non actionable""."
technical," This has been a long standing issue -- it is the reason I had to rename my extension to *Git Lens* rather than its actual name *GitLens* (no space), because when searching for git it would never show up. And even with that rename -- I had to pack in other words into the title (SEO hacking style) to even get it to show up on some other keywords -- even if those keywords were in the package.json keywords list."
technical,"I agree, the search ranking seems wierd. Some examples (when searching for ""python""): Py Files generator is above autoDocString, despite having about 50k less installs and 0 reviews Same thing with ladieratheme - it is above autoDocString despite having about 49k less installs and 3 less reviews. Python paste and indent has 3 stars but apparently it's 2k extra downloads trumps Python (pydev)'s 5 star rating.  Seems like download count is weighted more heavily than rating in the ranking (or maybe python paste and indent has better keywords)  Python Coding Conventions has 773 downloads and is unrated, yet somehow is above magicpython (with 742  thousand  downloads and 3.5 stars) and several other extensions with far more downloads / good ratings.  So you could have a very popular or well-rated extension but if you don't have the right keywords you will still be ranked down.  Wait... but looking at the Python Coding Conventions package.json, it doesn't even have any keywords! While searching an extension, we also take into account the community inputs like number of downloads,  You mean the download count that is  also  the update count?  This has been a outstanding issue ever since 2016 - when you release an update the marketplace shows your downloads as having increased. The update count should not effect the search ranking.That's good, but what algorithim are you using to calculate the weighted rating?  Not sure if that is open source but hopefully it isn't something like RatingA - RatingB or RatingA/RatingB This probably happens because of this. The update count should not effect the search ranking. Yeah that's another major problem, one could automatically push a new update every day and downloads will go through the roof even though the same number of people are using it. I guess technically those users are  downloading  the update, but once this things are counted the downloads counter becomes less meaningful. The download counter for extensions on Chrome's store decreases when somebody uninstalls your extension and I think it doesn't increase just because the extension gets updated."
technical,"exactly! This sounds too specific to me, IMHO the solution is transitioning from full-text search to a real search engine. Readmes, titles, descriptions and keywords should be properly indexed and the search query should be properly parsed too."
technical,"Marketplace team has recently updated their search strategy to use Azure based search and which will expect to fix quiet a number of search related issues. There could be still some gaps which might exist and can be fixed. Since this issue is overwhelmed with lot of feedback and it is getting hard for us to understand what is fixed and what is not, I am locking this issue and ask users to create separate issues if they see any new bugs. At the same time, I would go through this complete issue and extract those which are not yet fixed into new issues which will help us and Marketplace team to better track and fix them. Thanks for the feedback provided here. Took me quite a while to test, but nothing reported from this issue was fixed and search by author ID, extension ID, or combination of both are broken."
technical,any details on what changes are being made exactly? we are tweaking our index analyzers.
technical,we are tweaking our index analyzers. we have rolled out the change. You should see the better search results. Thanks for bringing it to our notice and keep the feedback coming in!
technical,"Any updates after half a month? If it hasn't been looked into, an estimate on when it will be looked into would be nice. If it has been looked into, an update on what was found/not found would also be nice. If the Marketplace team has higher priority tasks that's fine, I completely understand, I think we would just like to know that. we would have the fix for the issue you reported rolled out by next week or so."
technical,"regarding the issue of searching your new extension: While searching an extension, we also take into account the community inputs like number of downloads, number of ratings and average rating along with the string matching. Among the string matching, exact string matches carry a higher weight than prefix match.  Since node modules is one word in your extension name, word node in search text gets prefix match here. However, there are lot of extensions on Marketplace which have exact word 'node' in their extension name. So they carry a higher weight for matching the word 'node'. Additionally, since your extension is new, download count and ratings of other extensions are way higher, pushing their total score to the top. One way to improve your extension rank is to break word node modules into separate words node modules. This will increase the string matching score and push your extension up in the result. Well, I would still consider whatever algorithm you guys are using broken because if I search for ""open in node modules"" I get these search results (some samples randomly picked): If I had to guess what's wrong with your approach I'd say:  - Lack of stopwords, kind of meaningless keywords like ""vscode"" or ""in"" are given too much weight. - Poor query parsing, if keywords are joined in some sort or another the whole thing falls apart (vscode-foo-bar, foo bar, GitLens etc.) - Maybe you're giving too much weight to downloads and ratings, the first thing to sort for is relevancy.   One way to improve your extension rank is to break word node modules into separate words node modules. This will increase the string matching score and push your extension up in the result.  I'm not going to rename the extension to something wrong (node modules is a folder, I'm not talking about node modules, as in ""NPM packages"", here) just to work around this."
technical,"Oops, misclick. Which I would refuse. What do I call it? Vetur Vue? Another example: Searching for ""Golang"".  What should Go extension do? Renaming it to Golang? After which the query Go wouldn't return it as top result? Maybe it's better to start a community curation of good extensions for each is a good example. At least it wouldn't be impossible for new users to find which extension to use, because the search results are not ranked helpfully."
technical,"This sounds too specific to me, IMHO the solution is transitioning from full-text search to a real search engine. Readmes, titles, descriptions and keywords should be properly indexed and the search query should be properly parsed too. While I don't disagree -- I was just trying to offer an alternative that would hopefully be easy to implement but improves things somewhat and therefore likely to happen ˜"
technical,"It seems that some issues have been addressed, but some things got even worse:  - Now searching for both vscode-todo-plus and fabiospampinato.vscode-todo-plus returns 0 results, IMHO that's unacceptable (and this was the original problem reported in this issue).  - Sometimes if I type open in browsers and then hit enter the query becomes open in browser, deleting the last character '‚.  - The marketplace doesn't search automatically until I hit enter,  unless  I delete the query, IMHO that's pretty confusing (and that's also not what's happening in the homepage).  - If I clear the input field then the cursor disappears for a while until the results are returned. - There's probably no need to wait for those results to be returned in the first place, the top 55 extensions by downloads won't change very often, I'd just cache them (refreshing the cache once a day or so) and just ship them along with the rest of the page to the browser. Yep things still aren't working right. Earlier they said that ""While searching an extension, we also take into account the community inputs like number of downloads, number of ratings and average rating along with the string matching"". However, when I search for ""python"" ""python extended"" is above my extension despite having literally 0 reviews. In terms of download count we are very close (130k vs. 150k) so if the reviews were taken into account I would expect AREPL to be ranked higher.  I would even go so far as to say that reviews should be  more  important than download count in influencing search ranking (assuming you have enough reviews to be statistically valid). As a user when I search for things I want to find the best extensions, not the most downloaded extensions, but my extension has good reviews so I am biased on that front :P  can you explain the change in more detail please? I don't know what ""tweaking the index analyzers"" means."
technical,"I've just realized that the readmes aren't indexed at all. I searched for ""cyclomatic"" in the marketplace, and I've got this result: Then I tried searching for ""marketplace cyclomatic"" on Google, and this is the result. This is a bit ridiculous. I'm not saying that the search functionalities in the marketplace should be as good as Google's under **all** circumstances, but that's a full-word match in the readme, this query just can't return 0 results. do you have any updates for us? Yes, unfortunately we do not index readme.md.. sorry about that. We use SQL FTS in our backend today and that's kind of the limiting factor. There's only so much custom logic we can run over the results returned by FTS to make it 'more' relevant - and more importantly it doesn't scale in the long run. We are exploring moving our search platform to Azure Search or Bing which are techologies that are being actively developed and should provide us with more features and capabilities. I'm afraid we are not going to invest more in trying to optimize search with FTS."
technical,"I happen to know which chips were used in computers of that era. Most they could do was simple stuff, scanline tricks, scrolling and hardware sprites. The first real blitter co-processor with programmable minterms which could do arbitrary composition of raster images using DMA that I am aware of in personal computers was in Commodore Amiga. Commodore 64 to which I was referring had 320x200 pixels screen resolution while the area being redrawn in my example is 162x18 pixels (and I am being generous here by selecting wider area than the actual 8 hex-digit counter): Commodore's MC 6502 also had interrupts to service (vblank, keyboard, joysticks, floppy, sound, etc) and it somehow managed to offer decent single-task experience.  On the other hand, you are telling me that (mostly idle) i7-6700K running at 4 GHz with 32 GB of (mostly unused) ultra-low latency DDR4 3200 MHz RAM, 3.2 GB/sec NVMe SSD and watercooled NVIDIA 1080 Ti clocked at almost 2 GHz is supposed to struggle with (a single-task of) redrawing of 162x18 pixels because of?  Security? Interrupts? High-level software abstractions? You might have as well mentioned Santa Claus stuck in the chimney because that would sound equally ridiculous.  TL,DR -- There are plenty of choices for hardware acceleration of drawing on a modern PC under supposedly modern OS like Windows 10. There are plenty of performance analyzers (Intel's VTune just to name one) and to me it seems none of those methods and tools were actually used to produce the actual code. Please next time put the blame where it belongs. Alright gentlemen, enough with the pissing contest :) I think the answer is clear: bitcrazed explained above that the glyph rendering is being done by GDI/GDI+, an ancient CPU-bound renderer with its roots in the original NT. Yes, there was hardware acceleration added during the win7/2008r2 era, but clearly this isn't enough anymore. The technological successors to GDI and GDI+ are Direct2D and DirectWrite (mentioned above), which also arrived with win7/2008r2. Whenever this renderer upgrade will happen, who knows..."
technical,Thanks all. Have reached out to the DWM team. Let's see what they say. Any news?
technical,I'm always glad to hear when my blog posts are useful. Any update?
technical,Any update? Are you still seeing this issue on recent Insider builds?
technical,"These versions of Windows also have numerous bugs and missing features with their console implementation.  Can't go full-screen, can't display the full Unicode character set, can't resize while retaining line-wraps, can't use the full range of VT-100 codes, can't remote the console to another machine..... the list goes on and on.  Rich covered a bunch of this in a blog post earlier this year. If you haven't read it yet, you should.  I understand that it's fun to be snotty on the Internet, Igor... but I think you'll find it's a lot easier to understand an observed performance regression in Windows 10's console when you contextualize it within the vast amount of dev work the team is putting in to modernize this part of Windows. Believe these problems and complaints are caused by the development strategy chosen for this project.  (Almost) everyone loves the new work you've done, but no one wants to deal with new bugs or performance regressions in a component that is entrenched and hasn't been touched in twenty-ish years (until recently).  With some hindsight it's probably better to revert console back to legacy and start fresh on a modern DirectX/Unicode/Fonts first, Posix PTY model, replacement.  This is likely possible without throwing out much work.  For example the bridges built to run console apps over ssh, ConPTY, and ANSI parsing should continue to be used, perhaps in a different context.  This strategy should make more folks happy.  End users that won't have to deal with new bugs in the old stuff, as well as developers who won't have to step around eggshells in the new stuff.  Trying to do both in a single code-base makes things harder for everyone."
technical,"Interestingly enough, I updated my Win10 Pro 1709 Friday night, and now (build **10.0.16299.309**) conhost.exe is no longer querying the registry keys mentioned above, but DWM queries this value: 10 times per scroll. Still digging fontdrvhost.exe shows up in the traces on machines where this is slow. On machines where this isn't slow, fontdrvhost.exe uses no CPU at all.  And it seems related to scrolling."
technical,"Hi, understood that there is a lot of graphics to push around on 4k.  However, what about the issue mentioned above with querying the registry 10x per scroll?  Might be an easy fix to cache that value: Hello, I would like to point out that poor console performance in Windows 10 is still present in 1809.  Resolution is 1920x1080, scaling is 100%, console font size is 16pt Consolas.  In my case, running IDA Pro interactive disassembler (idaw.exe) in the console results in a totally absurd situation where conhost.exe is using more CPU than the program itself: Looking at the threads of conhost.exe the culprits are obvious: The only part of the console window being updated is highlighted:Why is drawing 8 characters at a fixed position using 10% of the 3.2 GHz quad-core CPU, and what is more important, why is this slowing down the console program so much that something which took minutes in Windows XP takes hours in Windows 10?  People were drawing full screen graphics on 1 MHz Motorola 6502 CPU 3 decades ago without dedicated graphics hardware, this is how far modern programmers have fallen."
technical,"Thanks. Will take a look. Hey Powercode - any chance you could share repro steps - we'd like to see, trace, and measure the perf issue you're seeing."
technical,"Start powershell. Set font size to 36. Run ls.  Still on a computer with high resolution (3840x2160) and scaling on 200%. Hey. I've been doing some repro'ing on my SP4 with an external 4K 28"" screen: With the font set at 36pt, there is indeed a marked slow-down, but remember: 1. Console currently uses GDI to draw text which uses your CPU to render text glyphs 1. GDI generates and caches glyphs, and then BLITs them onto the screen 1. The larger the glyph, the fewer can be cached 1. At 18pt, the client area is 1053x675. At 36pt, the client area is 2025x1350 and the resulting glyphs are 4x bigger ... that's a lot of LARGE bitmaps to BLIT 1. And since scrolling often results in significant text changes between iterations of the render loop, the entire client area can end up being ""dirtied"", resulting in the full Console client area having to be fully re-rendered each time! 2025x1350 x 4bytes each pixel == 10MB per frame x 60 fps == 656MB/s we have to force from your CPU to the GPU's frame buffer every second!.  That's a lot of data by anyone's measure, and it makes one's machine do a lot of work: Now, can we go faster? HELLS YEAH! The GDI team recently improved their glyph caching mechanism, with more improvements planned for future releases, but increased the Glyph Cache buffer to mitigate the above factors until those improvements can be made. Also, Console plans on replacing our GDI renderer to DirectWrite at some point in the future which should eliminate this issue anyhow.  Also  we have some other tricks up our sleeves that we hope will noticeably improve text rendering perf, esp. while scrolling. Bear with us ,) I'll leave this issue open for now, and we'll update it when we have any solid perf improvements to share."
technical,"Hey. I've been doing some repro'ing on my SP4 with an external 4K 28"" screen: With the font set at 36pt, there is indeed a marked slow-down, but remember: 1. Console currently uses GDI to draw text which uses your CPU to render text glyphs 1. GDI generates and caches glyphs, and then BLITs them onto the screen 1. The larger the glyph, the fewer can be cached 1. At 18pt, the client area is 1053x675. At 36pt, the client area is 2025x1350 and the resulting glyphs are 4x bigger ... that's a lot of LARGE bitmaps to BLIT 1. And since scrolling often results in significant text changes between iterations of the render loop, the entire client area can end up being ""dirtied"", resulting in the full Console client area having to be fully re-rendered each time! 2025x1350 x 4bytes each pixel == 10MB per frame x 60 fps == 656MB/s we have to force from your CPU to the GPU's frame buffer every second!.  That's a lot of data by anyone's measure, and it makes one's machine do a lot of work: Now, can we go faster? HELLS YEAH! The GDI team recently improved their glyph caching mechanism, with more improvements planned for future releases, but increased the Glyph Cache buffer to mitigate the above factors until those improvements can be made. Also, Console plans on replacing our GDI renderer to DirectWrite at some point in the future which should eliminate this issue anyhow.  Also  we have some other tricks up our sleeves that we hope will noticeably improve text rendering perf, esp. while scrolling. Bear with us ,) I'll leave this issue open for now, and we'll update it when we have any solid perf improvements to share. Hi, understood that there is a lot of graphics to push around on 4k.  However, what about the issue mentioned above with querying the registry 10x per scroll?  Might be an easy fix to cache that value:"
technical,"If you're talking about the Commodore VIC 20 or Commodore 64, they did have dedicated graphics hardware chips, such as the MOS 6567.  They were also drawing at something like 160x200 resolution, with 4-bit colour, using an 8x8 font with no anti-aliasing, no back-buffering, no virtualization of memory, no consideration for security, no concept of ""processes"" or ""threads"", no constant stream of interrupts coming from connected devices, no API layer or ""drivers"" to abstract away the hardware, with no ability to protect against or recover from crashes....  I mean, it's really not comparable at all.  Also keep in mind that the number of cores your computer has isn't pertinent to how quickly a single thread of operation will run, especially in the case you're presenting here.  Drawing the console isn't multi-threaded. ˜ I happen to know which chips were used in computers of that era. Most they could do was simple stuff, scanline tricks, scrolling and hardware sprites. The first real blitter co-processor with programmable minterms which could do arbitrary composition of raster images using DMA that I am aware of in personal computers was in Commodore Amiga. Commodore 64 to which I was referring had 320x200 pixels screen resolution while the area being redrawn in my example is 162x18 pixels (and I am being generous here by selecting wider area than the actual 8 hex-digit counter): Commodore's MC 6502 also had interrupts to service (vblank, keyboard, joysticks, floppy, sound, etc) and it somehow managed to offer decent single-task experience.  On the other hand, you are telling me that (mostly idle) i7-6700K running at 4 GHz with 32 GB of (mostly unused) ultra-low latency DDR4 3200 MHz RAM, 3.2 GB/sec NVMe SSD and watercooled NVIDIA 1080 Ti clocked at almost 2 GHz is supposed to struggle with (a single-task of) redrawing of 162x18 pixels because of?  Security? Interrupts? High-level software abstractions? You might have as well mentioned Santa Claus stuck in the chimney because that would sound equally ridiculous.  TL,DR -- There are plenty of choices for hardware acceleration of drawing on a modern PC under supposedly modern OS like Windows 10. There are plenty of performance analyzers (Intel's VTune just to name one) and to me it seems none of those methods and tools were actually used to produce the actual code. Please next time put the blame where it belongs."
technical,"This is absolutely unacceptable behavior. This rule is simply assuming it owns the entire namespace of all functions beginning with ""use"", and starts screaming about unrelated functions in the rest of the project that have nothing to do with React.  This is much more atrocious than carelessly declaring variables in the global namespace! Whoever came up with this idea clearly doesn't understand why ESLint was created in the first place. There are plenty of domains where you would like to give a function a name beginning with ""use"". Not all of us are React developers and we shouldn't have to change code in unrelated parts of the application.  We're having a real problem with this because ESLint ignores the .eslintrc.json file (due to some other bug in ESLint), and it breaks the build. I know it always seems like a fine and dandy idea to say ""hey we're going to break back-compat for thousands of existing apps so we can add some new features for people"" but we like to hold ourselves to a higher standard than that. We've found a path where we can do good for all of the existing users of the console codebase, and slowly introduce new features and modernize, without leaving customers behind.  Sure, we could have deprecated the old console and parked the console subsystem and implemented a new PTY subsystem. That's a path we could have taken, but that would have left everyone with an existing console application asking ""why did Microsoft deprecate their entire console API with no plan to modernize those existing apps? Are they *fools*?""  We're just now in a place where the console subsystem *can* support conpty, where it can support other rendering heads, and where the backing buffer could possibly support emoji and other unicode codepoints. It might not be the path that makes *you* the most happy, but it's the path that makes the most developers the most happy.  We're working on a new rendering head. No commitments as to when it'll be finished, but we're working on it. When it's done, then that will likely resolve the performance issues described in this thread. I haven't personally debugged into this issue, but my gut agrees with Rich's analysis that the issue is coming from GDI. I certainly wouldn't call this a major perf regression, and I doubt it's due to any of our code in particular. It's likely just due to a really old CPU-bound graphics API that's not optimized for running on new hardware.  As warrenrumak mentioned, if you're unhappy with how the console is performing, go ahead and switch back to legacy mode."
technical,"Hello, I would like to point out that poor console performance in Windows 10 is still present in 1809.  Resolution is 1920x1080, scaling is 100%, console font size is 16pt Consolas.  In my case, running IDA Pro interactive disassembler (idaw.exe) in the console results in a totally absurd situation where conhost.exe is using more CPU than the program itself: Looking at the threads of conhost.exe the culprits are obvious: The only part of the console window being updated is highlighted:Why is drawing 8 characters at a fixed position using 10% of the 3.2 GHz quad-core CPU, and what is more important, why is this slowing down the console program so much that something which took minutes in Windows XP takes hours in Windows 10?  People were drawing full screen graphics on 1 MHz Motorola 6502 CPU 3 decades ago without dedicated graphics hardware, this is how far modern programmers have fallen. If you're talking about the Commodore VIC 20 or Commodore 64, they did have dedicated graphics hardware chips, such as the MOS 6567.  They were also drawing at something like 160x200 resolution, with 4-bit colour, using an 8x8 font with no anti-aliasing, no back-buffering, no virtualization of memory, no consideration for security, no concept of ""processes"" or ""threads"", no constant stream of interrupts coming from connected devices, no API layer or ""drivers"" to abstract away the hardware, with no ability to protect against or recover from crashes....  I mean, it's really not comparable at all.  Also keep in mind that the number of cores your computer has isn't pertinent to how quickly a single thread of operation will run, especially in the case you're presenting here.  Drawing the console isn't multi-threaded. ˜"
technical,"Just checked with the engineer who worked on this: The specific issue you're seeing has been mitigated which should result in you no longer seeing it's effect. A more comprehensive fix is on the backlog and will be triaged into a future release.  To test / confirm, you'll need to install a recent RS5 Insider build, or wait until RS5 ships later this year. Once you do get onto RS5, please update this thread with your findings and close this issue if it's resolved.  Many thanks again for filing and your help in diagnosing this issue. Installed RS5 - Issue still present.  11 seconds to dir a folder with fondsize 36. 1.8 on size 14. Both horribly slow."
technical,"It is correlated to font size. Almost grinds to a halt with font size set to 36. Interestingly enough, I updated my Win10 Pro 1709 Friday night, and now (build **10.0.16299.309**) conhost.exe is no longer querying the registry keys mentioned above, but DWM queries this value: 10 times per scroll. Still digging"
technical,It is not obviously DWM. It is correlated to font size. Almost grinds to a halt with font size set to 36.
technical,"So I've definitely noticed this from time to time on my own laptop, which is an HP something or other - though it's definitely not something I've found to be consistently reproducible I also know that conhost isn't the one doing this - at least not directly. From the sounds of it, DWM is getting involved during our paint and slowing us down, but I wouldn't have the faintest clue how to start debugging that. bitcrazed anyone on the DWM, or composition, or something team that we can forward this issue to to have them take a look? It is not obviously DWM."
technical," It's caused by conhost.exe excessively hammering the registry to query the following two values: Having previously reported this issue through Feedback Hub (to no avail), let me offer my observations from debugging this issue:  - It's not specific to Dell's XPS series - I've been able to reproduce on any Windows 10 installation from version 1703 and up - It only occurs when the console application writes output that causes the console to *scroll* - It only occurs when the console application in question is in foreground/focus - Each reg value mentioned above is queried 6 times, per scroll!!!  An easy way of show the resulting difference in speed is to compare to anything piped to Out-String: Even though we add overhead from Out-String, and the number of lines that the console host eventually need to write to the screen buffer are exactly the same, you'll find that the MultiString measurement is significantly larger that SingleString, presumably because the former caused the console host to scroll WindowsHeight where as the latter only had to cause a single scroll."
technical,"That machine is not on insider builds :( Just checked with the engineer who worked on this: The specific issue you're seeing has been mitigated which should result in you no longer seeing it's effect. A more comprehensive fix is on the backlog and will be triaged into a future release.  To test / confirm, you'll need to install a recent RS5 Insider build, or wait until RS5 ships later this year. Once you do get onto RS5, please update this thread with your findings and close this issue if it's resolved.  Many thanks again for filing and your help in diagnosing this issue."
technical,"They have.  Don't struggle with this, Mike.  Again, if your mythical ""twenty year old stuff"" doesn't work with the new Windows 10 console, by all means, use the old console, link in ANSICON, and keep relying on monkeypatching GetProcAddress (euughhhh). obviously they haven't or this bug wouldn't exist."
technical,"powercode - Okay, we have some suspicions as to what's happening here (thanks CD for your help with this ˜)  Could we ask: 1. What is the size & resolution of your screen?  15.6""  3200 x 1800? 1. What DPI scaling factor are you using? 1. What is the font face & size of your affected Console(s)? 1. Does this problem disappear if you decrease your Console font size to ~12pt when running at 200% DPI?  BTW - MANY thanks for capturing traces - they've been ENORMOUSLY helpful! Resolution 3840x2160. Scaling 250. Both the recommended setup.  I have tried different fonts but have not stumbled upon any that worked, but have not on the other hand made exhaustive tests. Consolas is affected, as is Deja Vu Sans Mono for PowerLine, and several other PowerLine fonts.  Wow! Setting the resolution scaling to 200% improves perf  a lot."
technical,"It's caused by conhost.exe excessively hammering the registry to query the following two values: Having previously reported this issue through Feedback Hub (to no avail), let me offer my observations from debugging this issue:  - It's not specific to Dell's XPS series - I've been able to reproduce on any Windows 10 installation from version 1703 and up - It only occurs when the console application writes output that causes the console to *scroll* - It only occurs when the console application in question is in foreground/focus - Each reg value mentioned above is queried 6 times, per scroll!!!  An easy way of show the resulting difference in speed is to compare to anything piped to Out-String: Even though we add overhead from Out-String, and the number of lines that the console host eventually need to write to the screen buffer are exactly the same, you'll find that the MultiString measurement is significantly larger that SingleString, presumably because the former caused the console host to scroll WindowsHeight where as the latter only had to cause a single scroll. So I've definitely noticed this from time to time on my own laptop, which is an HP something or other - though it's definitely not something I've found to be consistently reproducible I also know that conhost isn't the one doing this - at least not directly. From the sounds of it, DWM is getting involved during our paint and slowing us down, but I wouldn't have the faintest clue how to start debugging that. bitcrazed anyone on the DWM, or composition, or something team that we can forward this issue to to have them take a look?"
technical,"Hey Powercode - any chance you could share repro steps - we'd like to see, trace, and measure the perf issue you're seeing. Start powershell. Set font size to 36. Run ls.  Still on a computer with high resolution (3840x2160) and scaling on 200%."
technical,"The machines where you have seen it being slow, did they all have touch screens?  See the IncDevice!vector scalar destructor in the call stack. That doesn't show up on my machines where this is a lot faster. Thanks all. Have reached out to the DWM team. Let's see what they say."
technical,Uploaded trace. It's a 7z archive with an added zip extension to allow the upload. Thanks. Will take a look.
technical,Are you still seeing this issue on recent Insider builds? That machine is not on insider builds :(
technical,"obviously they haven't or this bug wouldn't exist. The console is a very polarizing Windows component--everybody has an opinion on how we should run the team, where we should make our investments, and what we should put on our roadmap. We can't do  everything , but we also can't do  nothing  and remain technologically relevant in 2018. The v1/legacy console is representative of this polarization. Please bear with us as we try to improve the state of the art for everybody.  As always, thanks for reporting this. We'll definitely be triaging this with the rest of our bug fixes. In the meantime, however, please keep the discourse civil."
technical,"obviously they haven't or this bug wouldn't exist. The machines where you have seen it being slow, did they all have touch screens?  See the IncDevice!vector scalar destructor in the call stack. That doesn't show up on my machines where this is a lot faster."
technical,"fontdrvhost.exe shows up in the traces on machines where this is slow. On machines where this isn't slow, fontdrvhost.exe uses no CPU at all.  And it seems related to scrolling. There is some ping-pong between conhost.exe and fontdrvhost.exe."
technical,"Are you suggesting that some other, faster and more modern renderer was used in Windows XP, Windows 7, and Windows 8.1? Because in those OSes everything is still working as intended (i.e. the console application is not being slowed down more than 25 times). Is Microsoft's official answer to reporting a serious performance regression in a subsystem used every day by developers and admins who are managing and maintaining their user base really ""yeah we are aware of it but whether we will fix it and when, who knows""? That's a rhetorical question, don't bother. These versions of Windows also have numerous bugs and missing features with their console implementation.  Can't go full-screen, can't display the full Unicode character set, can't resize while retaining line-wraps, can't use the full range of VT-100 codes, can't remote the console to another machine..... the list goes on and on.  Rich covered a bunch of this in a blog post earlier this year. If you haven't read it yet, you should.  I understand that it's fun to be snotty on the Internet, Igor... but I think you'll find it's a lot easier to understand an observed performance regression in Windows 10's console when you contextualize it within the vast amount of dev work the team is putting in to modernize this part of Windows."
technical,Installed RS5 - Issue still present.  11 seconds to dir a folder with fondsize 36. 1.8 on size 14. Both horribly slow. Uploaded trace. It's a 7z archive with an added zip extension to allow the upload.
technical,"Any news? We are looking into this. Sorry for the delay - Build prep is eating up a lot of people x hours right now ,)"
technical,"The ""you're either with us entirely or you're against us"" is disgusting. There's also this. I got a similar message from the same Ryan White guy. Stop this behavior. Grow up. Wanting a better solution doesn't mean we're in favor of what ICE is doing. This change was slacktivism at its finest. Wanna really stick it to MS, get off of GitHub. Delete your accounts now, Microsoft is going to be the owner of GitHub. It was symbolic at best, and even that was kind of laughable. If you don't delete your GitHub and are still trying to argue that this change being reverted is somehow endorsing what ICE is doing, guess what -- so does having a GitHub account if you're trying to use that argument, your credibility was just shattered. Also -- if you use Windows -- might I suggest not using it anymore? :arrow down: :microphone: :clap: :clap:"
technical,"This whole thing has turned into a tire fire. ""If you don't support us entirely, you're against our cause"" is the most toxic thing ever. In my view, this hurts lerna's reputation. James Kyle's behavior is absolutely disgusting and I support his removal from this project. I'd also support levying a complaint with GitHub since he definitely violating their TOS, but I am not going to do that myself, but I definitely encourage those who were targeted to do so.  I do support what was initially done, what needs to happen is that the license is crafted in a way that spells out everything. Make sure that it can't be challenged as being ambiguous. Ambiguity is your enemy here. I am also rather distressed that I was personally emailed and harassed and accused of supporting the disgusting policies of ICE. ICE needs to go and it can't be gone soon enough.  Rather than being toxic -- which doesn't advance your cause -- it hurts your credibility because nobody wants to listen to you, consider trying to lobby the companies. I wish people would learn that the kind of stuff James Kyle pulled is absolutely disgusting and hurts the cause, which I support...it was just botched epically. Also evocateur and the other maintainers -- don't let this take an emotional toll on you -- there are people who do see why it happened the way it did. I'm rational and capable of seeing that what was there initially was horribly botched."
technical,"I'll go ahead and suggest this is discussion for the sake of discussion as well. We certainly aren't talking about anything even remotely relevant to Lerna. Also, lhorie hallister is there anything else you'd like to decide we shouldn't discuss? I might as well ask now, since you two are already providing answers."
technical,"Also evocateur and the other maintainers -- don't let this take an emotional toll on you -- there are people who do see why it happened the way it did. I'm rational and capable of seeing that what was there initially was horribly botched. Also, one other thing: Microsoft owns GitHub -- maybe make the change after moving to something like GitLab :thinking: which isn't owned by a company you explicitly excluded in your license..."
technical,"Sure, I'm sure you could argue Adolf was a pretty cool guy if you ignore all the other stuff he did and just focus on his gun control policy.   If you just excuse it as ""unfortunate"" then you're just trying to justify your own opinions.  The difference between having a strict license vs claiming it's open. As I said numerous times, if the goal is to have lerna be proprietary that's a different discussion. At the current time, and as far as I know, lerna plans to stay as an *open* source project. Then none of them are open source anymore. You have collectively made a proprietary license. We're not arguing that so it's a moot point to even mention. He made multiple issues in the same project that all state the same thing, linking to the same resources which are there to incite hatred from a group of people towards a company. See this definition: My god, it's exactly what he did. And we're done. I'm going to spend the rest of the long weekend restoring my mental health. I'll resume my pariah duties on Tuesday."
technical,"The only thing distressing is that this entire event took place. You do realize that the big tech companies even let us think for ourselves?  I work for Amazon and not a single project in our org uses lerna. I do, personally. This wasn't about bias, this was about virtue signaling, particularly on such a terrible platform for it. Frankly I'm not sure where else this discussion has to go, since it's just boiling down to ""if you work for any of the companies listed you shouldn't have input"". Apologies if this is not an appropriate channel to voice opinions about the contents of the hypothetical RFC mentioned in the title of this issue, but assuming it is, I want to voice that I strongly disagree with the idea of applying Jamie's modified MIT license to Lerna. Not because it has anything to do with ICE, but due to the reasons cited here - TL,DR: it would make Lerna fundamentally non-open-source.  I think that people that want a fundamentally non-open-source licensed monorepo management tool should be prepared to fork Lerna, find maintainers who agree with said non-open-source license, and potentially even pay for its ongoing maintenance, rather than be questioning the decision of the mainline Lerna maintainers (namely, evocateur who created the PR, as well as TheLarkInn and hzoo who approved it). The Lerna maintainers are entirely within their rights to do as they please with Lerna.  I'd also like to remind everyone involved that an RFC doesn't necessarily mean that dissenters will somehow be able to articulate an overwhelmingly convincing argument and magically overturn the maintainers' decision to retain the unmodified MIT license (or the decision to remove Jamie from the Lerna org). For better or worse, the maintainers are well aware now of the implications of what has transpired and trying to nitpick perceived flaws of logic is, at best, splitting hairs at this point.  Lastly, evocateur even if there is a deluge of people accusing of you supporting political positions that you do not subscribe to, and that is taking an emotional toll on you, please remember that there are also those of us that understand your position on ICE, on OSS and on being publicly vulnerable. I personally believe your standing by the MIT license was the right move, and I'm not alone in thinking this. Stay strong."
technical,"I am in agreement that Sean does great work for the open source community and I'm really happy about what he's done with webpack. I know he cares a lot and means the best. In fact I'm confident every maintainer involved here cares a lot and had the best intentions.  That said, I stand by Hannah's comment above. As an outsider (I've only ever just used Lerna once, ended up coming by because I smelled fire yesterday)  This thread, from the outside, is a train wreck. It was opened -- with good intentions -- by someone external, when it should have been opened by someone in the team. It has turned into a means for TheLarkInn to be beaten with, at best, this is poor form, at worst, it's harassing the man.  I agree with evocateur w/r/t baseless canard. I'd honestly hope for better out of the free software community, but I guess the 90s never ended, and it is easier to accuse someone of horrible things than to give them benefit of the doubt. If the Lerna team members think he should recuse himself *then that is their choice*, but a lynch mob here serves no purpose other than to harass him.  I'm going to suggest that this thread be restricted to members of the Lerna team, shy of that, closing this thread and opening one which is restricted to Lerna team members. I agree transparency is important -- *especially* in TheLarkInn 's case. Right now, however, this is an issue for the lerna team to hash out and find a point where they feel it's settled to where the general internet can come and see what has happened, voice their thoughts, and a final choice can be made."
technical,"you and SRGOM agree, they were being sarcastic...... as I said perhaps he made some unfortunate statements - what I'm trying to say is that if you look at the license change PR where he elaborated, it does not look like he thinks he personally owns the project at the exclusion of anybody else. He did not push the change without review. When people say ""my"" thing, it can mean a lot of different things - its usually not helpful to draw strong conclusions from the use of that word.  Looking at the commit history, it does look like he was heavily involved in the early days of Lerna. I assume he means ""my"" in a similar sense - ""Lerna is a project which I was heavily involved in and I share significant responsibility for creating""."
technical,"The damage has already been done. If the Lerna project was serious about its future after this trashing of it's credibility, it would strictly prohibit and immediately purge all social justice warriors from it's ranks. Because a guy among 10s of thousands was going to put up the big fight to stay off the list, and now can't because it doesn't exist anymore. I suppose, instead, he can now argue to not have contracts with ICE, but then this silly move wouldn't get any credit.  Jamie already had a conflict of interest in targeting specific companies he didn't like. Your argument here would imply only people that are independent should play a role in the decision-making. Except nobody is independent."
technical,"catcher-in-the-try I disagree. It is relevant for two reasons:  1. There is significant dissent about the decision to revert the changes in the Licenses and remove Jamie, and an RFC was suggested as a mechanism for discussion, but does not exist. This is a placeholder to simply register there is unaddressed dissent and a promise of a future conversation.  2. There seems to be a fear of having a discussion while things are ""heated"" -- I would argue that this is a good time to have discussion, as it's the time when there is likely to be the widest set of perspectives and the most voices are likely to be heard. Calls for ""civility"" and ""cooling down"" are invariably intended to cease discussion, not enhance it. Hannah's right - the time to discuss is now."
technical,"in case it wasn't clear, while I disagree with your recommendation I'm confident it was made with the best of intentions and the goal of facilitating the best solution for all. :) Can you social justice warriors stop using (and hence promoting) this platform owned by a major ICE collaborator? Remember just like bad news is good news in show biz, any users and discussions on githib are (or will be in 4 momths) good news for MS. Only you enlightened people can kill Microsoft and consequently ICE in one fell swoop by altogether stopping using github. also somebody pointes out that except launch pad all other major git hosta are equally tainted- so id suggest using that as the signature list or whatever drama you will do next."
technical,"Thank you for speaking up and registering your dissent and representing the dissent of like-minded Lerna users.  As an open source project we have every right to provide no reason for his removal.  However, as stewards of transparency, trust, and care for our users and the lerna ecosystem, we will provide clarity.  According to the current Lerna Code of Conduct:  Additionally our rights as Org Members of the project for handling unacceptable behavior which do not align with the current Code of Conduct.  These are the segments of the current lerna Code of Conduct that justify our decisions made to revoke all of James involvement and ownership privileges over this GitHub Organization.  In multiple GitHub issues (if you need me to cite them, I will do so but in additional comments or edits from this response), on Twitter [important to note he claims ownership of lerna in this context making him profesionally represented and in the capacity of a maintainer] acts in a very unprofessional, rude, and harassing manner.  These also have been occurring since July which evocateur official states in our license change PR #1633 that we made a mistake in this regard to not address earlier violations in a swift and timely manner.  As a core team, going forward, we want to not only protect the interest of the project itself, but also the transparency of the project. And that means we need to adopt a much less vague, less interpretive, and more structured Code of Conduct. For that we have open up #1636 per request (I on twitter officially asked Coraline to create this issue).  On behalf of the core team who is all actively monitoring this issue, I hope it provides the clarity that you are seeking. Thank you for sticking up for a transparent and responsible process.   Note: hannahhoward once you have your clarifications received (even if you don't explicitly request them), would you confirm your question has been answered? That way I can prevent thread abuse by locking and maintainers can add cited incidents in the issue. Cannot speak for hannahhoward or others but TheLarkInn I think where I need additional clarity is: * Why did you decide to revert the license change despite maintainers having originally been involved in approving it? * Why did you choose to link Jamie's dismissal with the license change? * Why did you open a pull request to remove your own employer (Microsoft) from the restricted list?"
technical,"My simple question, given the lack of due discussion for the previously MIT license modification, would the maintainers be open to a new PR with legally more robust language along with input from the community?  I definitely think the previous process was rushed, but the core idea is worth exploring. catcher-in-the-try I disagree. It is relevant for two reasons:  1. There is significant dissent about the decision to revert the changes in the Licenses and remove Jamie, and an RFC was suggested as a mechanism for discussion, but does not exist. This is a placeholder to simply register there is unaddressed dissent and a promise of a future conversation.  2. There seems to be a fear of having a discussion while things are ""heated"" -- I would argue that this is a good time to have discussion, as it's the time when there is likely to be the widest set of perspectives and the most voices are likely to be heard."
technical,have you heard the phrase- will somebody please think of the children? That's it. Benevolent krainbiltgreene is just thinking of the children [traffickers]. did we just say the exact same thing?  the image you posted is making me wonder if you have yet acted on evocateur's mirror suggestion yet.
technical,"I work at Amazon, should I also recuse myself from the discussion? This idea that employees of the company are too tainted to *discuss* the future of a project is absurd. If we want to debate the merits of this decision, which is how it should have gone in the first place, let's do so. But asking anyone to recuse themselves from *discussion* is the literal opposite of everything opensource is supposed to mean.  This entire situation wouldn't exist if it weren't for some poor choices, and at least one of the maintainers realized that. Had this rash choice not been made in the first place, nobody would be here demanding that discussion take place and an RFC on taking Lerna to a non-opensource license would exist. I'm fine if the maintainers want to go that direction, but if I were in their position, my vote would be to close this issue and everybody go back to building software which is *supposed to be the point* of projects like this.  But now that pandoras box has been opened, everyone with an agenda is here to fight for it, so much so, they want to silence anyone with an opinion that differs because ""ethics"" or whatever silly nonsense they can use to make this issue an echo chamber Ethics aren't silly, let's not trivialize them. Software engineers at VW who were asked by their management to implement defeat devices were held criminally liable for poor ethical decisions."
technical,"I made the decision after speaking with Sean. It is possible to talk to somebody and then make an independent decision. If that's hard to believe, it's not my problem. Feel free to show appreciable change as a result of this. At Amazon someone sent an email that said ""Don't use lerna above version v3.2.1"". I could just feel ICE crumbling under the pressure.  Yes, only people with no vested interest in this can be involved in the decision making.  That makes a ton of sense."
technical,"Oh boy...  The mentions of reverting to the original MIT license has been met with this statement enough times in enough issues, twitter, and reddit. It's not word for word what is said but it is heavily implied by ""you support what ICE does"". No, it's a terrible idea. Any restriction on the license means it's no longer open source. If that's the goal then I think a lot of people will just drop Lerna because of licensing risk. If that's what you want just to make *a statement* then that's a very poor software decision.  Multiple links   one of which is the license of who gets to use it, in order to affect change in the world. You may say that this is political, but I would argue that by taking no action, you are in fact taking an action and affecting change. We must battle with which one is more positive or has a better ethical standing.  The ethical standing is to honor an open license. If lerna doesn't want to be an open source project anymore that's a different story. You cannot restrict *anyone*. The discussion that everyone seems to be missing is that this change converts Lerna to a proprietary piece of software. Also you're nearing the ethical dilemma of the trolley problem, not exactly, but by saying ""inaction is an action in itself"" is a  poor argument.  ICE isn't going anywhere and it probably won't in the next 10 years. Whether that is a positive or negative to whoever reads this is an unfortunate thing we have to deal with. I'm 99.9% sure lerna is nothing special at a government scale, and if ICE (or company in a binding contract with ICE) needed something like lerna they'd build it themselves or simply fork the old version. This license change effectively does nothing to the target. So the discussion is resulting in debating who can virtue signal the strongest.  This is a fundamental misunderstanding of what ""open"" means if one cannot see the issue with a non-open license. You will have to fight the literally brick wall of Stallman to change what ""open"" source means if you want to challenge this. Non open licenses greatly increase risk of using the software (depending on the license, the bastardization of the MIT-ICE is a huge risk). If you want to build a company you simply don't pick licenses which can destroy your business in a day. In what realm of reality does a **strictly** always open door, close? Open is open, closed is closed. Open source is non discriminatory, if you want to go down the path of ""restrictive licensing is open source"" then I don't see the issue with Facebook being able to restrict the people using React under their Patent license. Absolutely zero issues with a single entity controlling who uses the software while claiming to be open source. Folks who use terms like ""virtue signaling"" might not realize that is incredibly politically charged.  Also it's simply not true that restrictive clauses have no precedent in open source work. Everyone screaming that ""this is no longer open source"" needs to realize that A) we are not beholden to the OSI's definition of open source, and B) there is plenty of precedent with Commons Clause, Fair Source, License Zero, and even the original license for JSON (thanks to Mikeal Rogers [formerly of the Node.js Foundation] for pointing that out on Twitter). (source)"
technical,"Apologies if this is not an appropriate channel to voice opinions about the contents of the hypothetical RFC mentioned in the title of this issue, but assuming it is, I want to voice that I strongly disagree with the idea of applying Jamie's modified MIT license to Lerna. Not because it has anything to do with ICE, but due to the reasons cited here - TL,DR: it would make Lerna fundamentally non-open-source.  I think that people that want a fundamentally non-open-source licensed monorepo management tool should be prepared to fork Lerna, find maintainers who agree with said non-open-source license, and potentially even pay for its ongoing maintenance, rather than be questioning the decision of the mainline Lerna maintainers (namely, evocateur who created the PR, as well as TheLarkInn and hzoo who approved it). The Lerna maintainers are entirely within their rights to do as they please with Lerna.  I'd also like to remind everyone involved that an RFC doesn't necessarily mean that dissenters will somehow be able to articulate an overwhelmingly convincing argument and magically overturn the maintainers' decision to retain the unmodified MIT license (or the decision to remove Jamie from the Lerna org). For better or worse, the maintainers are well aware now of the implications of what has transpired and trying to nitpick perceived flaws of logic is, at best, splitting hairs at this point.  Lastly, evocateur even if there is a deluge of people accusing of you supporting political positions that you do not subscribe to, and that is taking an emotional toll on you, please remember that there are also those of us that understand your position on ICE, on OSS and on being publicly vulnerable. I personally believe your standing by the MIT license was the right move, and I'm not alone in thinking this. Stay strong. For left-leaning software devs who are interested in constructively affecting political change, please check out Tech For Campaigns, which pairs volunteer expert technologists with Democratic campaigns (even at the state and local level). I'm not affiliated.  Using Palantir's open-source tools does serve as promotion / brand-awareness for a company that you might think has abhorrent practices and I do think that there should be room for opening tickets to request a different vendor on that basis alone. Or even a ticket on the open-source provider's repo to request transparency. But there is obviously a reasonable and an unreasonable way to go about that -- opening those issues was a clear waste of good reputation by somebody who could have made real waves with a more level-headed approach. Perhaps community input before big decisions is not a burden but actually serves to prevent bad decisions"
technical,"Please help to mulander , next will be NetBSD (or FreeBSD if pbgc will be quicker). Thanks FWIW jwietelmann is right that Lerna is getting much attention, eg see this Vice motherboard news article. I did not know about this project myself till I saw the debate."
technical,The comments he made on other projects is a violation of the CoC and that's why he was removed. I think that may have been the nth time someone has mentioned that to you.  So either you're illiterate or have an agenda. have you heard the phrase- will somebody please think of the children? That's it. Benevolent krainbiltgreene is just thinking of the children [traffickers].
technical,"I'll just point out a particular sentence from the original licence change Emphasis on ""helped""  I think there's quite a bit of needless character assassination going on. Perhaps James has made some unfortunate statements at different times, but in the context of this issue, it hardly looks like he has claimed personal ownership of Lerna or any other open source project. He did.  Here is one where he directly states Lerna is his project. Here is one where he claims babel is his project  Another one. Why does it keep happening D:? There is no character assassination going on, he acted shitty and is being called out on it."
technical,"You completely missed the point here. Silencing someone in the name of ethics isn't ethics. hannahhoward explicitly stated that TheLarkInn shouldn't even be a part of the *discussion* for ethical reasons. That's abusing something good (ethics) for a clear agenda. That's after arguing that this discussion has some degree of immediacy so that a broad variety of opinions can be heard. Except, obviously, the opinions of the people that they are well aware disagree with them.  It's either dishonest or naive. I choose to assume the latter as that requires the least assumption. But you're correct in that I'm going to call out anyone that tries to abuse something as meaningful as ethics, whether or not it was out of malice or poor judgement. Hey so I don't work at any of the companies involved or know any of the maintainers personally so I don't have any potential ethical dilemmas. Here's my thoughts:  The key part of the whole conflict of interest idea is that you have to be making a decision on behalf of a 3rd party, and the conflict would give rise to the appearance that you're being unduly influenced by this secondary consideration. I believe evocateur made the decision. TheLarkInn had no conflict of interest, he's providing his input as a major stakeholder. As long as we all understand his role (and I think we do) then I think we'd be far richer for his input than not.  This also makes the assumption that the rash & unannounced decision to change the license was in the project's best interest, where one could easily argue that it was not for all the obvious reasons (community split, legal viability, no community feedback solicited, etc).  Generally the notion that someone's involvement with an affected organization means that they can't speak from their expertise would cause all sorts of problems if it were applied to government. As much as lobbyists have (deservedly) earned a bad reputation, they also provide vital guidance to governments so they can make decisions that aren't destructive.  So let's get everyone's best opinions in here, and then let the maintainers make their decision."
technical,"So for all this grandstanding you're doing -- why not delete your GitHub account...seriously -- MS will own GitHub soon. Also -- change your OS from Windows -- if you use that... I agree that this is the intent, but in practice, that's not the impression I got from reading this thread so far. The majority of comments are either trolling or doing passive-aggressive witch hunts (and sadly I'm seeing this behaviour from both sides of the discussion...)  I'm not sure if there needs to be more clarity in terms of an RFC is typically supposed to look like (given that it is, admittedly, a new process in this specific org), which is why I offered my opinions on what is appropriate. If people disagree, they could potentially look at other orgs' RFC processes to get an idea of how one is structured (React) and Ember both have RFC processes that could serve as reference, for example). In any case, I think we would all agree that all commenters should be following the basic rule of thumb of ""if a comment would be inappropriate in a technical discussion, it has no place in an RFC discussion either"".  I understand this intent, but I have not seen anyone address the concerns about what happens with  non-blacklisted companies being unable to use such licensed software. This concern was dismissed as if it were a problem that did not exist, rather than a significant one (especially for something that is so geared towards big company workflows such as Lerna). For example, evocateur (or whoever is the maintainer at any time)'s employer could disallow him from using a hypothetically non-OSI-compliant Lerna at work. If that was the only reason he was working on Lerna, there's a good chance the project would lose its steward and effectively die. Likewise, I might spend company time to contribute a PR to a MIT-licensed Lerna if I need the fix for work, but I will certainly not know to do so if I'm not allowed to use it due to licensing issues in the first place.  No, the point is that we  can't possibly estimate the impact , both positive and negative. We cannot make assumptions that the positives will outweigh the negatives or vice versa. The argument for a non-OSS license boils down to ""something must be done, this is something, therefore it must be done"". No, it must not. Whatever was the latest heated Javascript-related topic of the week on Hacker News is statistically unlikely to be even remotely close to being an appropriate response to an issue that likely no one here has any real life understanding of, other than what they read on some news site while sipping their coffees.  It's especially disingenuous to say this must be done, if the person saying so did not even bother to try to research what other things could have been done instead, or what is being done, even. To illustrate the level of myopia here, there was a PR on Jamie's repo to add Uber to the blacklist, despite the fact that Uber is  donating rides and meals to families separated by ICE  For those who are not familiar with the RFC processes I linked above, there are very important sections that are absent in virtually the entirety of the discourse here: Alternatives, Drawbacks and Unresolved Questions. For all the talk about how now is the best time to discuss, there's surprisingly little content that can be used to fill these sections that are designed to address contentiousness. As I mentioned in an earlier comment, there's already a well established way of running non-OSS software: pay for it. This very discussion is open because the maintainers feel that inclusion of all parties is of utmost importance to the success of this project. They could just tell everyone to go away, or even pull a uws and  they'd be entirely within their rights to do so . Using the privilege of having this forum to argue that the maintainers should adopt a restrictive license is biting the hand that feeds. I think the non-starter here is to suggest that maintainers ought to maintain a project that they themselves may not be able to use under some circumstances."
technical,"I wish I could say your response made me feel discussion should be locked on this issue and consider it resolved. Unfortunately, it has the opposite effect for me.  What I can see is: 1. A change was made to the license that spoke ill of Microsoft was made that evocateur approved of and merged 2. You, a Microsoft employee, attempted to remove Microsoft, and only Microsoft, from the list of companies in said change (I didn't know that part) 3. evocateur then spoke to you, a Microsoft employee, at length, after which the change was reverted. Speaking to you at length was specifically called in the decision to revert.  I am going to assume best intention that you genuinely feel calling out Microsoft was unfair, and you genuinely sought changes because of that, and not in an effort to protect your employer. However, as an employee of one of the companies that were called out in the change, clearly the ethical decision would be for you to recuse yourself from discussions of this issue completely. There's really no way in which you're a vaguely objective party -- or even if you are it's pretty reasonable for other people not to perceive you that way.  My original issue (an RFC to reconsider the License) is not addressed, and moreover, I honestly suggest the right course for you all is for TheLarkInn to recuse himself from discussion going forward.  * When I say ""ethically"" I purely speaking from what I think is right and in terms of staying impartial whenever one is given a governance role. My previous experience to being in tech is on non-profit boards, where for example, you would recuse yourself from any decision that might affect you personally as well as the organization. Obviously there is no legal duty in this case for an informal organization like Lerna * I am in agreement that Sean does great work for the open source community and I'm really happy about what he's done with webpack. I know he cares a lot and means the best. In fact I'm confident every maintainer involved here cares a lot and had the best intentions.  That said, I stand by Hannah's comment above."
technical,"Those questions aren't relevant to the thread. You'd have to ask Jamie himself. I did not decide to revert the license change, I approved it. This decision was made by evocateur and his resoning is very clear in #1633. I did not link Jamie's dismissal. evocateur explained his reasoning in #1633  I thought Microsoft was being treated unfairly and wanted to set the story straight."
technical,"If you want to restrict who can use lerna, then lerna can no longer be called free, it will be proprietary. Please let everyone know with advance notice so they can make preparations to move away from it if it's going to become proprietary software. I don't think anyone's particularly heated, just excited to talk about this."
technical,"I'm inclined to view this like a roach: one person visible is indicative of a lot more you don't see. The point is that this caused at least one person to publicly say ""this resulted in me fighting to oppose ICE contracts."" I believe that's resulted in a lot more people than just him doing similar things. I know I've had a few conversations with people who hadn't previously considered the issue before this.  That's not a conflict of interest. Judges don't recuse because of what they believe, they recuse themselves because they have a material interest in the result of the ruling, e.g. they've invested stocks in the companies involved. You're correct that nobody is independent, because no one is objective, but conflicts of interest and lack of independence are not synonymous. I know you're dealing with a lot more than you expected right now but I'm afraid that as the maintainer of a project with a decently-sized community it really is your problem. I don't envy you it. I frankly don't know if it's even possible to clear it up completely at this point.  At a charitable reading, TheLarkInn seems to have jumped in feet-first without thinking about what it looks like for a Microsoft employee to get involved in a situation which affects Microsoft's access (particularly given that Microsoft is in fact well known to have an active engagement with ICE, & that the original license change didn't allow for equivocation about legacy systems). To be clear, this is how  I  think it went down too. I've met Sean and he cares a lot about open-source communities and open-source projects. But there are less charitable readings which are equally plausible from what's publicly known, or even just public on GitHub vs public on Twitter. You know as well as I do that ""big MS puts the screws to open-source project"" is a headline going back years and years, and Sean's involvement in  any  capacity with this chain of events means you've inadvertently leaned into that old stereotype."
technical,"It's still funny the discussion is primarily ""If you don't revert it you are complacent with separating families!"". Which is a stupid untrue argument. You're providing equal values of  absolutely nothing  to this discussion. I laid out the factual arguments both pro and against the question of whether Lerna should be MIT or not. I also clearly prefaced my personal opinions as being my opinions. If you're only interested in making ad-hominen arguments, you're not adding to the discussion.  To be clear, my intention was not to dictate, but to spell out reasons why some forms of commentary are not fruitful in the context of what an RFC is typically designed for. You are more than welcome to make new arguments, if you have them, as long as they are on topic.  To that end, I'm going to repeat what I believe is the most important argument to this discussion: if you agree with the tenet that a non-MIT license will have any impact whatsoever on ICE, it follows that doing so may have a  negative  impact due to its indiscriminate and non-surgical nature. In other words, disrupting ICE operations can delay families from being reunited.  I don't believe even the most hardcore proponent of the non-MIT license can say in good conscience and good faith that unintended damage to the very cause it purports to serve is not a real possibility, and I think that reason alone should be enough for everyone to support the current decision."
technical,"Let's test if that's the actual goal (Emphasis added by me) I could probably stop here. That's enough for removal. Full stop.  * Blocking people on the lerna repository that politely called him out. * Blocking all non-contributors on the repo from posting issues/comments/PR's  * Suggesting he may not do a major version bump  This is in addition to his horrible conduct in every issue posted regarding adding his company restrictions to the MIT license. That's pretty transparent. That's pretty objectively trolling, personal attacks and insults. Now the question is, do you really want transparency, or are you looking for a fight? I made no such implication. I said, quite explicitly, that Lerna is currently a target for those disinterested in the project and overly interested in the politics. By waiting for the dust to settle, we can ensure that any RFC is targeting the actual community. I'm not suggesting that everyone that supported Jamie is a troll, I'm saying that right now Lerna is going to attract them. Wait a few days and they'll be busy trolling someone else so we can get active interest in the discussion, including those that disagree with reverting the change."
technical,"You explicitly stated on Twitter that changes were being made after speaking with Sean, a Microsoft employee. If this isn't what happened, then what did happen is certainly hard to follow.  ETA: in case it's not clear I have never suggested you were secretly paid, that's not the point. I made the decision after speaking with Sean. It is possible to talk to somebody and then make an independent decision. If that's hard to believe, it's not my problem."
technical,"Also, lhorie hallister is there anything else you'd like to decide we shouldn't discuss? I might as well ask now, since you two are already providing answers. I never said it shouldn't or couldn't be discussed, I said it was discussion for the sake of discussion. Relax buddy, and read what people write before you respond."
technical,"I don't think anyone's particularly heated, just excited to talk about this. I see these as two separate issues. If the community and the maintainers want to use a modified MIT license, that is fully within their right. However, Jamie's behavior was plainly abusive and in violation of the CoC."
technical,"Hey so I don't work at any of the companies involved or know any of the maintainers personally so I don't have any potential ethical dilemmas. Here's my thoughts:  The key part of the whole conflict of interest idea is that you have to be making a decision on behalf of a 3rd party, and the conflict would give rise to the appearance that you're being unduly influenced by this secondary consideration. I believe evocateur made the decision. TheLarkInn had no conflict of interest, he's providing his input as a major stakeholder. As long as we all understand his role (and I think we do) then I think we'd be far richer for his input than not.  This also makes the assumption that the rash & unannounced decision to change the license was in the project's best interest, where one could easily argue that it was not for all the obvious reasons (community split, legal viability, no community feedback solicited, etc).  Generally the notion that someone's involvement with an affected organization means that they can't speak from their expertise would cause all sorts of problems if it were applied to government. As much as lobbyists have (deservedly) earned a bad reputation, they also provide vital guidance to governments so they can make decisions that aren't destructive.  So let's get everyone's best opinions in here, and then let the maintainers make their decision. I think this is mostly not accurate. As a maintainer Jamie had the ability to make changes and to persuade the other maintainers. When he made this decision to pursue goals that are unrelated to the project's unstated goal (make multi-package repos easy) he was arguably engaged in his own conflict of interest.  That said if he was the only maintainer and decided that he was changing the project's goals, then he could not be in a conflict of interest because his goals and the project's were always aligned."
technical,"I'm confident benwiley4000 and krainboltgreene are just trolling at this point. Don't give them the effort of a response. I want to start by saying that I agree that the license changes, as were initially implemented, were objectively poor and probably unenforceable from a legal perspective, and that it is probably justified to revert at least until a suitable replacement could be drafted and reviewed. What concerns me are a few things, however. First, the sentence from evocateur in #1633  The very fact that there was a huge amount of drama and publicity around this change signifies that it is important and that it did affect appreciable progress. There was never any delusion, as far as I could tell, by those who supported this change, that this single license change would have ICE quake in their boots, as has been suggested to be the measure of what ""affecting change"" would be by many. However, it could have served as a stepping stone and a beacon which other projects could use, and eventually create change not only in relation to ICE directly but to the notion of ethics in open source coding and licenses. I haven't seen anyone use this as an argument in this entire thread actually, so I'm not sure where you're getting that this discussion is ""primarily"" about that.  I think this is fairly silly--as has been alluded in other posts in this thread, Open Source licenses are exactly what you are arguing against -- political protests against contemporary political issues. These were seen as ridiculous not long ago in history, and are only recently gaining widespread approval. Just because they are now the status quo for many large pieces of software doesn't mean they are particularly old, and the fact is that they were very political pieces of activism, especially in the time when they were first being penned.  I believe this is exactly why we want an RFC -- to discuss whether this change (or one like it) should be implemented or not. And I believe this is also what hannahhoward meant by ""discussion is needed,"" not just ""discussion for the sake of discussion"". I don't believe that this intention is at all trollish, though the way you present it makes it sound slightly so. The idea is that we can use software and the control over we have over the software we write, one of which is the license of who gets to use it, in order to affect change in the world. You may say that this is political, but I would argue that by taking no action, you are in fact taking an action and affecting change. We must battle with which one is more positive or has a better ethical standing. I believe that this is a perfectly legitimate thing to do. However, as I said in the beginning, in the case of a license it does need to be more premediated and thought out because of the nature of legality and enforcement. This assumes that any movement against ICE is inherently bad because it assumes that the short term negative impact would out weigh any long term benefit of having ICE gone. It also ignores the potential ways in which there would be benefits throughout the entire software development and sharing community, as I mentioned near the top of my post.  I believe that this argument is somewhat of a non-starter, as it asserts that ""open source"" is fundamentally better than ""not open source."" It also asserts that those definitions of ""open source"" are the correct ones and are not open for discussion. I think this is somewhat silly however, as what is and is not open source has been malleable and changed before, and I don't think it is out of the question for it to change again."
technical,"I know you're dealing with a lot more than you expected right now but I'm afraid that as the maintainer of a project with a decently-sized community it really is your problem. I don't envy you it. I frankly don't know if it's even possible to clear it up completely at this point.  At a charitable reading, TheLarkInn seems to have jumped in feet-first without thinking about what it looks like for a Microsoft employee to get involved in a situation which affects Microsoft's access (particularly given that Microsoft is in fact well known to have an active engagement with ICE, & that the original license change didn't allow for equivocation about legacy systems). To be clear, this is how  I  think it went down too. I've met Sean and he cares a lot about open-source communities and open-source projects. But there are less charitable readings which are equally plausible from what's publicly known, or even just public on GitHub vs public on Twitter. You know as well as I do that ""big MS puts the screws to open-source project"" is a headline going back years and years, and Sean's involvement in  any  capacity with this chain of events means you've inadvertently leaned into that old stereotype. I wish I could say your response made me feel discussion should be locked on this issue and consider it resolved. Unfortunately, it has the opposite effect for me.  What I can see is: 1. A change was made to the license that spoke ill of Microsoft was made that evocateur approved of and merged 2. You, a Microsoft employee, attempted to remove Microsoft, and only Microsoft, from the list of companies in said change (I didn't know that part) 3. evocateur then spoke to you, a Microsoft employee, at length, after which the change was reverted. Speaking to you at length was specifically called in the decision to revert.  I am going to assume best intention that you genuinely feel calling out Microsoft was unfair, and you genuinely sought changes because of that, and not in an effort to protect your employer. However, as an employee of one of the companies that were called out in the change, clearly the ethical decision would be for you to recuse yourself from discussions of this issue completely. There's really no way in which you're a vaguely objective party -- or even if you are it's pretty reasonable for other people not to perceive you that way.  My original issue (an RFC to reconsider the License) is not addressed, and moreover, I honestly suggest the right course for you all is for TheLarkInn to recuse himself from discussion going forward.  * When I say ""ethically"" I purely speaking from what I think is right and in terms of staying impartial whenever one is given a governance role. My previous experience to being in tech is on non-profit boards, where for example, you would recuse yourself from any decision that might affect you personally as well as the organization. Obviously there is no legal duty in this case for an informal organization like Lerna *"
technical,"It's funny how much of the argument is going on trying to express that you all have the same political views but you're too ignorant to really listen to each other. It's always jumping to conclusions that there must be conflicts of interest and conspiracy theories.  At what point are people going to realize the arguing you're doing is for the most part wasting time trying to explain to someone, *who agrees with you*, why they should agree with you. Law is not based on feelings, licenses are law, licenses are not meant to be interpreted by feelings. Whatever they may be the license has to be rigid and precise, any lack thereof leaves the license weak and unenforceable.  This isn't so much directed to the Lerna team as much as it is to every single person who is trying to ensure they're *on the right side of history* and bring the crusade Jamie was trying to create. I work at Amazon, should I also recuse myself from the discussion? This idea that employees of the company are too tainted to *discuss* the future of a project is absurd. If we want to debate the merits of this decision, which is how it should have gone in the first place, let's do so. But asking anyone to recuse themselves from *discussion* is the literal opposite of everything opensource is supposed to mean.  This entire situation wouldn't exist if it weren't for some poor choices, and at least one of the maintainers realized that. Had this rash choice not been made in the first place, nobody would be here demanding that discussion take place and an RFC on taking Lerna to a non-opensource license would exist. I'm fine if the maintainers want to go that direction, but if I were in their position, my vote would be to close this issue and everybody go back to building software which is *supposed to be the point* of projects like this.  But now that pandoras box has been opened, everyone with an agenda is here to fight for it, so much so, they want to silence anyone with an opinion that differs because ""ethics"" or whatever silly nonsense they can use to make this issue an echo chamber"
technical,"Oh Jesus nevermind -- I'm an idiot I guess :joy: -- apologies SRGOM :) I would agree if this were the actual RFC thread, but this is actually just a thread requesting an RFC should exist in the first place. Therefore I don't think it's valid to judge peoples' comments here as what they would have said in an actual RFC.  And yet the reason we have OSS licenses as a staple of modern software development today was the continued preaching of outcast mailing list users in the last few decades, and you could extend this to basically any successful political protest as well. There's also lots of not OSS software that is free as in beer. And I appreciate that this is true, but being ""non-OSS"" would not preclude this from happening by default anyway.  And yet Stallman's own license is one of the most restrictive and closed-door OSS licenses there is. He just closes the door on a different group of people: the group of people that want to use or modify his code without sharing back their changes. There's not a fundamental difference between that and precluding groups for another reason. This is also why I think the OSI page on ""evil"", originated from the JSON license, is fairly crap. I think the ""do no evil line"" is also terrible because it's even less enforceable than what the MIT-ICE license was, but the discussion in OSS around being able to preclude wrongdoers from using code has been very backwards IMO.  As I said, Lerna's own license change is not meant to harm ICE by itself in any significant way, it's meant to  1. Deal with the ethical concerns of allowing a harmful entity to use your code 2. Show other OSS projects that this is a real dilemma and hopefully spark them to discuss and maybe take the same action  If many OSS projects do the same thing, then there could be real, large-scale change over time.  None of those show a trollish intent behind the action at all, and in the actual PR that was made, the changes were discussed in a very professional manner, all (at the time available) protocols were followed, and a calm and deliberate reasoning was given. Being terse and (arguably) rude to companies over their using tools he contributed to does not constitute the action being trollish."
technical,"Feel free to show appreciable change as a result of this. At Amazon someone sent an email that said ""Don't use lerna above version v3.2.1"". I could just feel ICE crumbling under the pressure.  Yes, only people with no vested interest in this can be involved in the decision making.  That makes a ton of sense. I would refer you to this comment. Up to you whether you consider this ""appreciable,"" but I suspect because you don't see direct, large results (e.g. MSFT didn't turn around and cancel its ICE contracts), you're not likely to consider this ""appreciable."" I personally think this action has definitely moved the needle on this issue. Court judges recuse themselves from decisions involving conflicts of interest all the time. The idea that one might have such a conflict and thus should not be involved in the decision is not particularly far-fetched.  That said, I don't really have a problem with his involvement here. Sean is well-versed in open-source and would be worthwhile to consult regarding the impact of this decision."
technical,It's also really weird to see so many people say:  - It's harmful to [foss|companies|people] - I't not [going to work|effective|doing anything] I'll go ahead and suggest this is discussion for the sake of discussion as well. We certainly aren't talking about anything even remotely relevant to Lerna.
technical,"did we just say the exact same thing?  the image you posted is making me wonder if you have yet acted on evocateur's mirror suggestion yet. I'll just point out a particular sentence from the original licence change Emphasis on ""helped""  I think there's quite a bit of needless character assassination going on. Perhaps James has made some unfortunate statements at different times, but in the context of this issue, it hardly looks like he has claimed personal ownership of Lerna or any other open source project."
technical,You really wanna stick it to Microsoft? Delete your GitHub account. Better yet -- move the repository off of GitHub! They're soon going to be its owner. I'm confident benwiley4000 and krainboltgreene are just trolling at this point. Don't give them the effort of a response.
technical,"Because a guy among 10s of thousands was going to put up the big fight to stay off the list, and now can't because it doesn't exist anymore. I suppose, instead, he can now argue to not have contracts with ICE, but then this silly move wouldn't get any credit.  Jamie already had a conflict of interest in targeting specific companies he didn't like. Your argument here would imply only people that are independent should play a role in the decision-making. Except nobody is independent. I'm inclined to view this like a roach: one person visible is indicative of a lot more you don't see. The point is that this caused at least one person to publicly say ""this resulted in me fighting to oppose ICE contracts."" I believe that's resulted in a lot more people than just him doing similar things. I know I've had a few conversations with people who hadn't previously considered the issue before this.  That's not a conflict of interest. Judges don't recuse because of what they believe, they recuse themselves because they have a material interest in the result of the ruling, e.g. they've invested stocks in the companies involved. You're correct that nobody is independent, because no one is objective, but conflicts of interest and lack of independence are not synonymous."
technical,"It's frustrating to engage in good faith and assume best intent of others, and get accused of, in the last several comments: silencing, virtue signaling, abusing ethics to support an agenda, and engaging only to insure I'm on the right side of history (whatever that means), attempting to make an echo chamber, etc.  I stated my own personal view of the ethical way to proceed most likely to maintain the trust of the community. I specifically posited it as a suggestion. Perhaps ethics is the wrong word -- maybe I mean ""best practice"". If it sounded judgemental I'm sorry.  The echo chamber accusation sounds seems uniquely weird given this thread is 55 comments worth of multiple perspectives and disagreement.  I personally don't think it's a trash fire, but YMMV. It's specifically the kind of discussion I feel is needed, with people having to think about these questions discuss and disagree. I'm regretting ever even entering this discussion as it's just caused me grief and emotional distress. I can't even imagine how evocateur is feeling right now -- he has my support 100% -- some of us are rational. Most aren't necessarily against what happened, but  HOW  it happened. Even still, GitHub would be in their right to terminate James' account since he definitely violated the GitHub TOS. Bullying and harassment is not how you accomplish things. This is wrong and just horrible. I know there are others who got a similar email to the one I received and if you are feeling any distress, you have  my sympathies and such. This is a dumpster fire right now."
technical,"making a fork suggests you are engaging in this discussion in bad faith, and would simply like people who have a different viewpoint to go away. If the broader open-source community wants to discuss the politics of this, they should do so in a forum that makes sense. The Lerna repo isn't that forum. This repo is intended for end-users and maintainers of the project. As such, decisions about the goals and motivations of the project should be made by the Lerna community. I realize some people feel like open source maintainers need to write books for every decision they make, but I disagree. It was decided by multiple maintainers that he violated the CoC. That's subjective, absolutely, but debating the issue to death isn't suddenly going to give us objectivity."
technical,"Can you social justice warriors stop using (and hence promoting) this platform owned by a major ICE collaborator? Remember just like bad news is good news in show biz, any users and discussions on githib are (or will be in 4 momths) good news for MS. Only you enlightened people can kill Microsoft and consequently ICE in one fell swoop by altogether stopping using github. also somebody pointes out that except launch pad all other major git hosta are equally tainted- so id suggest using that as the signature list or whatever drama you will do next. If the Lerna community does decide to adopt a new licence in the future, I would like to encourage you to pick a licence that has been written and reviewed by lawyers.  Regardless of political opinions, ideologies or agendas, copyright licences written by non-lawyers who are unlikely to fully understand the legal ramifications, or otherwise, of the clauses they include are likely not going to be accepted by the wider community of developers, nor the companies they work for that often have internal policies regarding acceptable open source licences for software they can use.  The company I work for, and I can only assume many other companies, are taking a very close look at the licence Jamie wrote to understand the impact upon our own codebases. Lerna just happens to be the most high profile case of it being added, and so far the only case I'm aware of where it got reverted. It also got added to his other repositories and NPM packages, which remain a serious concern.  I would also strongly advise against attempting to use a copyright licence for political protests against contemporary political issues. Software copyright licences are long-lived documents that can outlive any government-of-the-day that happens to be disrupting or upsetting certain groups of people. It's also likely to be an extremely ineffective, if not completely counter-productive, form of protest that can have potentially far reaching consequences beyond the intended goal, including setting a terrible precedent for the industry.  Today, we get a list of companies rejected for working with a government agency carrying out extreme right wing policies. Tomorrow, we get another one from another group of people rejecting companies that support left wing policies. Where does it end?  I urge those in the community advocating for either reinstating the licence or creating a new one in a similar vein to just pause and seriously think about what you're trying to achieve, what you can realistically achieve, and the costs of the way in which you attempt to achieve those goals."
technical,"Ethics aren't silly, let's not trivialize them. Software engineers at VW who were asked by their management to implement defeat devices were held criminally liable for poor ethical decisions. If you can't see the difference between that and this situation then your opinion isn't that valid."
technical,"With the exception of a few trolls -- we all agree mostly...with a few people that have some conflicts of interest but I don't feel that they should be excluded from the discussion because of those. in case it wasn't clear, while I disagree with your recommendation I'm confident it was made with the best of intentions and the goal of facilitating the best solution for all. :)"
technical,"I disagree that this is very clear:  ""the impact of this change was almost 100% negative, with no appreciable progress toward the ostensible goal aside from rancorous sniping and harmful drama.""  The claims made here are not supported by evidence or arguments.  This was the plural ""you"" here, as in ""you, the maintainers."" In any case, I still don't understand why Jamie's dismissal was linked as part of the same announcement, clearly implying that his dismissal had to do with his introduction of the modified MIT license.  Respectfully, given the conflict of interest between your role as a maintainer and your role as a Microsoft employee, I think you should have sat out of that decision. It certainly appears to many as though your loyalty to your employer influenced the revert back to the old license. It did not. Please stop repeating this baseless canard."
technical,"Only you have suggested that these discussions are ""for the sake of discussion"". It's also really weird to see so many people say:  - It's harmful to [foss|companies|people] - I't not [going to work|effective|doing anything]"
technical,"Also, one other thing: Microsoft owns GitHub -- maybe make the change after moving to something like GitLab :thinking: which isn't owned by a company you explicitly excluded in your license... It's frustrating to engage in good faith and assume best intent of others, and get accused of, in the last several comments: silencing, virtue signaling, abusing ethics to support an agenda, and engaging only to insure I'm on the right side of history (whatever that means), attempting to make an echo chamber, etc.  I stated my own personal view of the ethical way to proceed most likely to maintain the trust of the community. I specifically posited it as a suggestion. Perhaps ethics is the wrong word -- maybe I mean ""best practice"". If it sounded judgemental I'm sorry.  The echo chamber accusation sounds seems uniquely weird given this thread is 55 comments worth of multiple perspectives and disagreement.  I personally don't think it's a trash fire, but YMMV. It's specifically the kind of discussion I feel is needed, with people having to think about these questions discuss and disagree."
technical,"As an outsider (I've only ever just used Lerna once, ended up coming by because I smelled fire yesterday)  This thread, from the outside, is a train wreck. It was opened -- with good intentions -- by someone external, when it should have been opened by someone in the team. It has turned into a means for TheLarkInn to be beaten with, at best, this is poor form, at worst, it's harassing the man.  I agree with evocateur w/r/t baseless canard. I'd honestly hope for better out of the free software community, but I guess the 90s never ended, and it is easier to accuse someone of horrible things than to give them benefit of the doubt. If the Lerna team members think he should recuse himself *then that is their choice*, but a lynch mob here serves no purpose other than to harass him.  I'm going to suggest that this thread be restricted to members of the Lerna team, shy of that, closing this thread and opening one which is restricted to Lerna team members. I agree transparency is important -- *especially* in TheLarkInn 's case. Right now, however, this is an issue for the lerna team to hash out and find a point where they feel it's settled to where the general internet can come and see what has happened, voice their thoughts, and a final choice can be made. It's funny how much of the argument is going on trying to express that you all have the same political views but you're too ignorant to really listen to each other. It's always jumping to conclusions that there must be conflicts of interest and conspiracy theories.  At what point are people going to realize the arguing you're doing is for the most part wasting time trying to explain to someone, *who agrees with you*, why they should agree with you. Law is not based on feelings, licenses are law, licenses are not meant to be interpreted by feelings. Whatever they may be the license has to be rigid and precise, any lack thereof leaves the license weak and unenforceable.  This isn't so much directed to the Lerna team as much as it is to every single person who is trying to ensure they're *on the right side of history* and bring the crusade Jamie was trying to create."
technical,"I never said it shouldn't or couldn't be discussed, I said it was discussion for the sake of discussion. Relax buddy, and read what people write before you respond. It's still funny the discussion is primarily ""If you don't revert it you are complacent with separating families!"". Which is a stupid untrue argument. You're providing equal values of  absolutely nothing  to this discussion."
technical,"I think this is mostly not accurate. As a maintainer Jamie had the ability to make changes and to persuade the other maintainers. When he made this decision to pursue goals that are unrelated to the project's unstated goal (make multi-package repos easy) he was arguably engaged in his own conflict of interest.  That said if he was the only maintainer and decided that he was changing the project's goals, then he could not be in a conflict of interest because his goals and the project's were always aligned. Just for clarification, TheLarkInn is not the only Microsoft Employee who has been a part of this ongoing discussion which makes this even more distressing."
technical,"Right now Lerna is a target for people only interested in politics -- I am a professional coder. I am interested in code. I am also interested in politics, FWIW. Given the original issue was a political one, it seems particularly important that a wide variety of political perspectives are included. To wait until there is a safe space for those ""only interested in code"" excludes a variety of political views, as the idea that politics and code can and should be separate is itself a political position that many disagree with.  I am not personally prepared to say Jamie's removal is a settled issue because the announcement he is being removed does not include any accounting or transparency on what specific violations the maintainers feel occurred. But I agree it is a separate question, except in as much the maintainers announced it in the same PR where they reverted the license change, suggesting they consider the two issues intertwined. making a fork suggests you are engaging in this discussion in bad faith, and would simply like people who have a different viewpoint to go away."
technical," My simple question, given the lack of due discussion for the previously MIT license modification, would the maintainers be open to a new PR with legally more robust language along with input from the community?  I definitely think the previous process was rushed, but the core idea is worth exploring."
technical,"He did.  Here is one where he directly states Lerna is his project. Here is one where he claims babel is his project  Another one. Why does it keep happening D:? There is no character assassination going on, he acted shitty and is being called out on it. no I said for people who are against the change being reverted to look at the fact that they are using MS products -- which makes them hypocrites. If you want to boycott Microsoft, maybe don't use their products -- or even better get off of GitHub. I'm not even sure what you said -- to be honest I stopped reading it after the first sentence."
technical,"If you can't see the difference between that and this situation then your opinion isn't that valid. Of course they're different - I'm pointing out that pooh-poohing ""ethical concerns"" as silly nonsense is is trollish."
technical,"Also -- if you use Windows -- might I suggest not using it anymore? :arrow down: :microphone: :clap: :clap: Oh boy...  The mentions of reverting to the original MIT license has been met with this statement enough times in enough issues, twitter, and reddit. It's not word for word what is said but it is heavily implied by ""you support what ICE does"". No, it's a terrible idea. Any restriction on the license means it's no longer open source. If that's the goal then I think a lot of people will just drop Lerna because of licensing risk. If that's what you want just to make *a statement* then that's a very poor software decision.  Multiple links   one of which is the license of who gets to use it, in order to affect change in the world. You may say that this is political, but I would argue that by taking no action, you are in fact taking an action and affecting change. We must battle with which one is more positive or has a better ethical standing.  The ethical standing is to honor an open license. If lerna doesn't want to be an open source project anymore that's a different story. You cannot restrict *anyone*. The discussion that everyone seems to be missing is that this change converts Lerna to a proprietary piece of software. Also you're nearing the ethical dilemma of the trolley problem, not exactly, but by saying ""inaction is an action in itself"" is a  poor argument.  ICE isn't going anywhere and it probably won't in the next 10 years. Whether that is a positive or negative to whoever reads this is an unfortunate thing we have to deal with. I'm 99.9% sure lerna is nothing special at a government scale, and if ICE (or company in a binding contract with ICE) needed something like lerna they'd build it themselves or simply fork the old version. This license change effectively does nothing to the target. So the discussion is resulting in debating who can virtue signal the strongest.  This is a fundamental misunderstanding of what ""open"" means if one cannot see the issue with a non-open license. You will have to fight the literally brick wall of Stallman to change what ""open"" source means if you want to challenge this. Non open licenses greatly increase risk of using the software (depending on the license, the bastardization of the MIT-ICE is a huge risk). If you want to build a company you simply don't pick licenses which can destroy your business in a day. In what realm of reality does a **strictly** always open door, close? Open is open, closed is closed. Open source is non discriminatory, if you want to go down the path of ""restrictive licensing is open source"" then I don't see the issue with Facebook being able to restrict the people using React under their Patent license. Absolutely zero issues with a single entity controlling who uses the software while claiming to be open source."
technical,"Also -- if you use Windows -- might I suggest not using it anymore? :arrow down: :microphone: :clap: :clap: Only you have suggested that these discussions are ""for the sake of discussion""."
technical,"really amazing point.  the fear seems to be if we open ourselves to political discussions how will we get our important professional work done.... but the very idea that open source/FSF is the domain where important money making professional work gets done, not where academic and politically radical weirdos hang out in their spare time, is pretty darn new. And that we're at this point it's the work of fairly intense political advocacy and organizing. Please help to mulander , next will be NetBSD (or FreeBSD if pbgc will be quicker). Thanks"
technical,"I made no such implication. I said, quite explicitly, that Lerna is currently a target for those disinterested in the project and overly interested in the politics. By waiting for the dust to settle, we can ensure that any RFC is targeting the actual community. I'm not suggesting that everyone that supported Jamie is a troll, I'm saying that right now Lerna is going to attract them. Wait a few days and they'll be busy trolling someone else so we can get active interest in the discussion, including those that disagree with reverting the change. Re: ""cooling off"" and ""people interested only in politics""...  I'm seeing an extremely ill-informed opinion being bandied about that implies the current state of open source is apolitical. FSF was founded on some radically collectivist ideals. OSI on more radically libertarian ones. ""Open source principles"" are what they are because people planted the flag and did the work to make them that way. The reason this project is MIT instead of GPL in the first place is because of decades of ideological debate and changes in community expectations.  Planting a different flag and doing a different thing is not more or less political than the status quo. To suggest otherwise is ahistorical nonsense. It's not more political, it's merely more participatory.  I'm simultaneously intrigued by the effort to change the license and skeptical of what its effects might be. Maybe reverting was a good idea! It would have for sure been  more interesting  to see the experiment run its course, but I understand the fear to the health of the project that makes someone walk it back.  Of course this attracted wider attention. Of course people who were otherwise mostly foreign to lerna were intrigued and came running. You were doing something mad and exciting and extremely political. A big project was finally taking an ethical question and putting it front and center and saying ""Yeah, it's time to talk about this.""  Post-blowback, you don't wanna be political pioneers anymore. And who does? A) It's hard, and B) that's not how most of us want to spend our OSS time.  If you're overwhelmed, frazzled, can't handle being the center of this ethical discussion you started, and just want to be left alone to code now... You could just say so? I mean that completely earnestly. It's okay to get in over your head and ask to be let back out. At least  I  think so."
technical,"Transparency for a decision comes from the people who made the decision. Right now what we have is:   Despite his numerous (and appreciated) contributions in the past, it has been very clear for quite some time now that he has decided to cease making constructive contributions to the Lerna codebase as well as actively and willfully disregarding the code of conduct that he himself added to the project.   You and I can quibble indefinitely over whether the things you posted are actually CoC violations. The point is neither of us are maintainers. really amazing point.  the fear seems to be if we open ourselves to political discussions how will we get our important professional work done.... but the very idea that open source/FSF is the domain where important money making professional work gets done, not where academic and politically radical weirdos hang out in their spare time, is pretty darn new. And that we're at this point it's the work of fairly intense political advocacy and organizing."
technical,"Jamie's removal was a necessity and shouldn't be open for debate. Blocking people from contributing because they disagreed, which eventually lead to him blocking everyone but contributors, attacking companies/individuals, claiming community tools (like Babel) are ""his"" is overwhelming evidence that he violated the CoC.  Calls for civility are perfectly apt after an issue like this. Suggesting that asking people to ""be nice"" is an attempt at ending discussion is one of the strangest stretches I've ever heard. As someone that called maintainers cowards for reverting the changes and removing Jamie, I can get a strong sense of why you'd argue that point, however.  As for this issue, TheLarkInn made it pretty clear that they'd like to wait for the dust to settle prior to a formal RFC. That makes sense. Right now Lerna is a target for people only interested in politics, and once the dust settles they will likely have something new to chase, and the people actually interested in the project can contribute to the suggested RFC. Right now Lerna is a target for people only interested in politics -- I am a professional coder. I am interested in code. I am also interested in politics, FWIW. Given the original issue was a political one, it seems particularly important that a wide variety of political perspectives are included. To wait until there is a safe space for those ""only interested in code"" excludes a variety of political views, as the idea that politics and code can and should be separate is itself a political position that many disagree with.  I am not personally prepared to say Jamie's removal is a settled issue because the announcement he is being removed does not include any accounting or transparency on what specific violations the maintainers feel occurred. But I agree it is a separate question, except in as much the maintainers announced it in the same PR where they reverted the license change, suggesting they consider the two issues intertwined."
technical,"Folks who use terms like ""virtue signaling"" might not realize that is incredibly politically charged.  Also it's simply not true that restrictive clauses have no precedent in open source work. Everyone screaming that ""this is no longer open source"" needs to realize that A) we are not beholden to the OSI's definition of open source, and B) there is plenty of precedent with Commons Clause, Fair Source, License Zero, and even the original license for JSON (thanks to Mikeal Rogers [formerly of the Node.js Foundation] for pointing that out on Twitter). (source) So we neither have a compelling reason for Jamie K. to have been kicked nor a compelling reason why the license was reverted. I am *shocked*.  So to recap:  - All maintainers accepted the original pull request. - There's absolutely prior art on doing this sort of license change. - Jamie K.'s behavior was apparently considered bad, but not until big companies were involved. - No one in charge of this has listed any behavior that violated in the current implemented code of conduct. - We're still waiting on an official response to this issue, *which was created because of (what is now) a clear attempt at silencing*. - Multiple employees from a company on the list swooped down to convince a maintainer to revert the change (but not before specifically moving to revert their company's inclusion in the list, *awkward*).  And the result?  - An immense amount of character assassination - The gamergate, anti-coc, and hard right crowd have been riled up - A fork that will surely not see life longer than a week but will have eaten plenty of valuable engineering time - A new surge of engineers who think ethics aren't their problem - ESR is talking again"
technical,"FWIW jwietelmann is right that Lerna is getting much attention, eg see this Vice motherboard news article. I did not know about this project myself till I saw the debate. Thank you for speaking up and registering your dissent and representing the dissent of like-minded Lerna users.  As an open source project we have every right to provide no reason for his removal.  However, as stewards of transparency, trust, and care for our users and the lerna ecosystem, we will provide clarity.  According to the current Lerna Code of Conduct:  Additionally our rights as Org Members of the project for handling unacceptable behavior which do not align with the current Code of Conduct.  These are the segments of the current lerna Code of Conduct that justify our decisions made to revoke all of James involvement and ownership privileges over this GitHub Organization.  In multiple GitHub issues (if you need me to cite them, I will do so but in additional comments or edits from this response), on Twitter [important to note he claims ownership of lerna in this context making him profesionally represented and in the capacity of a maintainer] acts in a very unprofessional, rude, and harassing manner.  These also have been occurring since July which evocateur official states in our license change PR #1633 that we made a mistake in this regard to not address earlier violations in a swift and timely manner.  As a core team, going forward, we want to not only protect the interest of the project itself, but also the transparency of the project. And that means we need to adopt a much less vague, less interpretive, and more structured Code of Conduct. For that we have open up #1636 per request (I on twitter officially asked Coraline to create this issue).  On behalf of the core team who is all actively monitoring this issue, I hope it provides the clarity that you are seeking. Thank you for sticking up for a transparent and responsible process.   Note: hannahhoward once you have your clarifications received (even if you don't explicitly request them), would you confirm your question has been answered? That way I can prevent thread abuse by locking and maintainers can add cited incidents in the issue."
technical,"I want to start by saying that I agree that the license changes, as were initially implemented, were objectively poor and probably unenforceable from a legal perspective, and that it is probably justified to revert at least until a suitable replacement could be drafted and reviewed. What concerns me are a few things, however. First, the sentence from evocateur in #1633  The very fact that there was a huge amount of drama and publicity around this change signifies that it is important and that it did affect appreciable progress. There was never any delusion, as far as I could tell, by those who supported this change, that this single license change would have ICE quake in their boots, as has been suggested to be the measure of what ""affecting change"" would be by many. However, it could have served as a stepping stone and a beacon which other projects could use, and eventually create change not only in relation to ICE directly but to the notion of ethics in open source coding and licenses. I haven't seen anyone use this as an argument in this entire thread actually, so I'm not sure where you're getting that this discussion is ""primarily"" about that.  I think this is fairly silly--as has been alluded in other posts in this thread, Open Source licenses are exactly what you are arguing against -- political protests against contemporary political issues. These were seen as ridiculous not long ago in history, and are only recently gaining widespread approval. Just because they are now the status quo for many large pieces of software doesn't mean they are particularly old, and the fact is that they were very political pieces of activism, especially in the time when they were first being penned.  I believe this is exactly why we want an RFC -- to discuss whether this change (or one like it) should be implemented or not. And I believe this is also what hannahhoward meant by ""discussion is needed,"" not just ""discussion for the sake of discussion"". I don't believe that this intention is at all trollish, though the way you present it makes it sound slightly so. The idea is that we can use software and the control over we have over the software we write, one of which is the license of who gets to use it, in order to affect change in the world. You may say that this is political, but I would argue that by taking no action, you are in fact taking an action and affecting change. We must battle with which one is more positive or has a better ethical standing. I believe that this is a perfectly legitimate thing to do. However, as I said in the beginning, in the case of a license it does need to be more premediated and thought out because of the nature of legality and enforcement. This assumes that any movement against ICE is inherently bad because it assumes that the short term negative impact would out weigh any long term benefit of having ICE gone. It also ignores the potential ways in which there would be benefits throughout the entire software development and sharing community, as I mentioned near the top of my post.  I believe that this argument is somewhat of a non-starter, as it asserts that ""open source"" is fundamentally better than ""not open source."" It also asserts that those definitions of ""open source"" are the correct ones and are not open for discussion. I think this is somewhat silly however, as what is and is not open source has been malleable and changed before, and I don't think it is out of the question for it to change again. The ""you're either with us entirely or you're against us"" is disgusting. There's also this. I got a similar message from the same Ryan White guy. Stop this behavior. Grow up. Wanting a better solution doesn't mean we're in favor of what ICE is doing. This change was slacktivism at its finest. Wanna really stick it to MS, get off of GitHub. Delete your accounts now, Microsoft is going to be the owner of GitHub. It was symbolic at best, and even that was kind of laughable. If you don't delete your GitHub and are still trying to argue that this change being reverted is somehow endorsing what ICE is doing, guess what -- so does having a GitHub account if you're trying to use that argument, your credibility was just shattered."
technical,"So we neither have a compelling reason for Jamie K. to have been kicked nor a compelling reason why the license was reverted. I am *shocked*.  So to recap:  - All maintainers accepted the original pull request. - There's absolutely prior art on doing this sort of license change. - Jamie K.'s behavior was apparently considered bad, but not until big companies were involved. - No one in charge of this has listed any behavior that violated in the current implemented code of conduct. - We're still waiting on an official response to this issue, *which was created because of (what is now) a clear attempt at silencing*. - Multiple employees from a company on the list swooped down to convince a maintainer to revert the change (but not before specifically moving to revert their company's inclusion in the list, *awkward*).  And the result?  - An immense amount of character assassination - The gamergate, anti-coc, and hard right crowd have been riled up - A fork that will surely not see life longer than a week but will have eaten plenty of valuable engineering time - A new surge of engineers who think ethics aren't their problem - ESR is talking again The comments he made on other projects is a violation of the CoC and that's why he was removed. I think that may have been the nth time someone has mentioned that to you.  So either you're illiterate or have an agenda."
technical,"I would refer you to this comment. Up to you whether you consider this ""appreciable,"" but I suspect because you don't see direct, large results (e.g. MSFT didn't turn around and cancel its ICE contracts), you're not likely to consider this ""appreciable."" I personally think this action has definitely moved the needle on this issue. Court judges recuse themselves from decisions involving conflicts of interest all the time. The idea that one might have such a conflict and thus should not be involved in the decision is not particularly far-fetched.  That said, I don't really have a problem with his involvement here. Sean is well-versed in open-source and would be worthwhile to consult regarding the impact of this decision. The damage has already been done. If the Lerna project was serious about its future after this trashing of it's credibility, it would strictly prohibit and immediately purge all social justice warriors from it's ranks."
technical,"Just for clarification, TheLarkInn is not the only Microsoft Employee who has been a part of this ongoing discussion which makes this even more distressing. The only thing distressing is that this entire event took place. You do realize that the big tech companies even let us think for ourselves?  I work for Amazon and not a single project in our org uses lerna. I do, personally. This wasn't about bias, this was about virtue signaling, particularly on such a terrible platform for it. Frankly I'm not sure where else this discussion has to go, since it's just boiling down to ""if you work for any of the companies listed you shouldn't have input""."
technical,"If the broader open-source community wants to discuss the politics of this, they should do so in a forum that makes sense. The Lerna repo isn't that forum. This repo is intended for end-users and maintainers of the project. As such, decisions about the goals and motivations of the project should be made by the Lerna community. I realize some people feel like open source maintainers need to write books for every decision they make, but I disagree. It was decided by multiple maintainers that he violated the CoC. That's subjective, absolutely, but debating the issue to death isn't suddenly going to give us objectivity. The purpose isn't objectivity, it's transparency."
technical,"It did not. Please stop repeating this baseless canard. this is transparently absurd. You can't just say ""I'm not influenced by the company who pays me"" and thus make it so. I sometimes laugh at the Twitter refrain of ""tech workers need to be trained in ethics,"" but perhaps it's true."
technical,"For left-leaning software devs who are interested in constructively affecting political change, please check out Tech For Campaigns, which pairs volunteer expert technologists with Democratic campaigns (even at the state and local level). I'm not affiliated.  Using Palantir's open-source tools does serve as promotion / brand-awareness for a company that you might think has abhorrent practices and I do think that there should be room for opening tickets to request a different vendor on that basis alone. Or even a ticket on the open-source provider's repo to request transparency. But there is obviously a reasonable and an unreasonable way to go about that -- opening those issues was a clear waste of good reputation by somebody who could have made real waves with a more level-headed approach. Perhaps community input before big decisions is not a burden but actually serves to prevent bad decisions This whole thing has turned into a tire fire. ""If you don't support us entirely, you're against our cause"" is the most toxic thing ever. In my view, this hurts lerna's reputation. James Kyle's behavior is absolutely disgusting and I support his removal from this project. I'd also support levying a complaint with GitHub since he definitely violating their TOS, but I am not going to do that myself, but I definitely encourage those who were targeted to do so.  I do support what was initially done, what needs to happen is that the license is crafted in a way that spells out everything. Make sure that it can't be challenged as being ambiguous. Ambiguity is your enemy here. I am also rather distressed that I was personally emailed and harassed and accused of supporting the disgusting policies of ICE. ICE needs to go and it can't be gone soon enough.  Rather than being toxic -- which doesn't advance your cause -- it hurts your credibility because nobody wants to listen to you, consider trying to lobby the companies. I wish people would learn that the kind of stuff James Kyle pulled is absolutely disgusting and hurts the cause, which I support...it was just botched epically."
technical,"I did not decide to revert the license change, I approved it. This decision was made by evocateur and his resoning is very clear in #1633. I did not link Jamie's dismissal. evocateur explained his reasoning in #1633  I thought Microsoft was being treated unfairly and wanted to set the story straight. Those questions are relevant to the thread. Read the title of the thread.  Edit: Since some people like mAAdhaTTah don't seem to understand the relevance of the questions, here is the relevant part of the title: ""Open an RFC to discuss decision to ... remove Jamie""  If you can't answer the questions, that's answer enough for this thread."
technical,"Why did he keep opening so many issues, harassing other people and why did he repeatedly refer to these free software projects that many people have contributed to as ""my tools""? Is all the code his property? Are all the contributions by everyone else worthless?  ""I kinda hope they do try to keep using **my** tools though"" - From the motherboard article, emphasis added ""Also, stop using my tools (such as Babel)""  Babel is his property as well? Can someone help my understand why these are his property? Thanks. Those questions aren't relevant to the thread. You'd have to ask Jamie himself."
technical,"Re: ""cooling off"" and ""people interested only in politics""...  I'm seeing an extremely ill-informed opinion being bandied about that implies the current state of open source is apolitical. FSF was founded on some radically collectivist ideals. OSI on more radically libertarian ones. ""Open source principles"" are what they are because people planted the flag and did the work to make them that way. The reason this project is MIT instead of GPL in the first place is because of decades of ideological debate and changes in community expectations.  Planting a different flag and doing a different thing is not more or less political than the status quo. To suggest otherwise is ahistorical nonsense. It's not more political, it's merely more participatory.  I'm simultaneously intrigued by the effort to change the license and skeptical of what its effects might be. Maybe reverting was a good idea! It would have for sure been  more interesting  to see the experiment run its course, but I understand the fear to the health of the project that makes someone walk it back.  Of course this attracted wider attention. Of course people who were otherwise mostly foreign to lerna were intrigued and came running. You were doing something mad and exciting and extremely political. A big project was finally taking an ethical question and putting it front and center and saying ""Yeah, it's time to talk about this.""  Post-blowback, you don't wanna be political pioneers anymore. And who does? A) It's hard, and B) that's not how most of us want to spend our OSS time.  If you're overwhelmed, frazzled, can't handle being the center of this ethical discussion you started, and just want to be left alone to code now... You could just say so? I mean that completely earnestly. It's okay to get in over your head and ask to be let back out. At least  I  think so. Transparency for a decision comes from the people who made the decision. Right now what we have is:   Despite his numerous (and appreciated) contributions in the past, it has been very clear for quite some time now that he has decided to cease making constructive contributions to the Lerna codebase as well as actively and willfully disregarding the code of conduct that he himself added to the project.   You and I can quibble indefinitely over whether the things you posted are actually CoC violations. The point is neither of us are maintainers."
technical,"And we're done. I'm going to spend the rest of the long weekend restoring my mental health. I'll resume my pariah duties on Tuesday. While choosing to close TheLarkInn writes:    I'm going to close this issue, in favor or a more structured, RFC style of a proposal for what decisions you are questioning. Thanks! We don't have a template RFC format, or spec template, but hopefully looking to have one once we catchup and dust settles.   It's not clear what specifically such an RFC would look like, and what it's format would take. Since then no issue has been opened.  This issue is simply to register there is dissent among a portion of Lerna users about the decision of the core team to revert Jamie's license change and remove him from the project until an RFC can be opened.  It is precisely the time when people are paying attention when discussion is necessary, when a wider group of perspectives can be heard."
technical,"Cannot speak for hannahhoward or others but TheLarkInn I think where I need additional clarity is: * Why did you decide to revert the license change despite maintainers having originally been involved in approving it? * Why did you choose to link Jamie's dismissal with the license change? * Why did you open a pull request to remove your own employer (Microsoft) from the restricted list? Why did he keep opening so many issues, harassing other people and why did he repeatedly refer to these free software projects that many people have contributed to as ""my tools""? Is all the code his property? Are all the contributions by everyone else worthless?  ""I kinda hope they do try to keep using **my** tools though"" - From the motherboard article, emphasis added ""Also, stop using my tools (such as Babel)""  Babel is his property as well? Can someone help my understand why these are his property? Thanks."
technical,"I'm regretting ever even entering this discussion as it's just caused me grief and emotional distress. I can't even imagine how evocateur is feeling right now -- he has my support 100% -- some of us are rational. Most aren't necessarily against what happened, but  HOW  it happened. Even still, GitHub would be in their right to terminate James' account since he definitely violated the GitHub TOS. Bullying and harassment is not how you accomplish things. This is wrong and just horrible. I know there are others who got a similar email to the one I received and if you are feeling any distress, you have  my sympathies and such. This is a dumpster fire right now. With the exception of a few trolls -- we all agree mostly...with a few people that have some conflicts of interest but I don't feel that they should be excluded from the discussion because of those."
technical,"I agree that this is the intent, but in practice, that's not the impression I got from reading this thread so far. The majority of comments are either trolling or doing passive-aggressive witch hunts (and sadly I'm seeing this behaviour from both sides of the discussion...)  I'm not sure if there needs to be more clarity in terms of an RFC is typically supposed to look like (given that it is, admittedly, a new process in this specific org), which is why I offered my opinions on what is appropriate. If people disagree, they could potentially look at other orgs' RFC processes to get an idea of how one is structured (React) and Ember both have RFC processes that could serve as reference, for example). In any case, I think we would all agree that all commenters should be following the basic rule of thumb of ""if a comment would be inappropriate in a technical discussion, it has no place in an RFC discussion either"".  I understand this intent, but I have not seen anyone address the concerns about what happens with  non-blacklisted companies being unable to use such licensed software. This concern was dismissed as if it were a problem that did not exist, rather than a significant one (especially for something that is so geared towards big company workflows such as Lerna). For example, evocateur (or whoever is the maintainer at any time)'s employer could disallow him from using a hypothetically non-OSI-compliant Lerna at work. If that was the only reason he was working on Lerna, there's a good chance the project would lose its steward and effectively die. Likewise, I might spend company time to contribute a PR to a MIT-licensed Lerna if I need the fix for work, but I will certainly not know to do so if I'm not allowed to use it due to licensing issues in the first place.  No, the point is that we  can't possibly estimate the impact , both positive and negative. We cannot make assumptions that the positives will outweigh the negatives or vice versa. The argument for a non-OSS license boils down to ""something must be done, this is something, therefore it must be done"". No, it must not. Whatever was the latest heated Javascript-related topic of the week on Hacker News is statistically unlikely to be even remotely close to being an appropriate response to an issue that likely no one here has any real life understanding of, other than what they read on some news site while sipping their coffees.  It's especially disingenuous to say this must be done, if the person saying so did not even bother to try to research what other things could have been done instead, or what is being done, even. To illustrate the level of myopia here, there was a PR on Jamie's repo to add Uber to the blacklist, despite the fact that Uber is  donating rides and meals to families separated by ICE  For those who are not familiar with the RFC processes I linked above, there are very important sections that are absent in virtually the entirety of the discourse here: Alternatives, Drawbacks and Unresolved Questions. For all the talk about how now is the best time to discuss, there's surprisingly little content that can be used to fill these sections that are designed to address contentiousness. As I mentioned in an earlier comment, there's already a well established way of running non-OSS software: pay for it. This very discussion is open because the maintainers feel that inclusion of all parties is of utmost importance to the success of this project. They could just tell everyone to go away, or even pull a uws and  they'd be entirely within their rights to do so . Using the privilege of having this forum to argue that the maintainers should adopt a restrictive license is biting the hand that feeds. I think the non-starter here is to suggest that maintainers ought to maintain a project that they themselves may not be able to use under some circumstances. you and SRGOM agree, they were being sarcastic......"
technical,"Of course they're different - I'm pointing out that pooh-poohing ""ethical concerns"" as silly nonsense is is trollish. You completely missed the point here. Silencing someone in the name of ethics isn't ethics. hannahhoward explicitly stated that TheLarkInn shouldn't even be a part of the *discussion* for ethical reasons. That's abusing something good (ethics) for a clear agenda. That's after arguing that this discussion has some degree of immediacy so that a broad variety of opinions can be heard. Except, obviously, the opinions of the people that they are well aware disagree with them.  It's either dishonest or naive. I choose to assume the latter as that requires the least assumption. But you're correct in that I'm going to call out anyone that tries to abuse something as meaningful as ethics, whether or not it was out of malice or poor judgement."
technical,"Explain how me, the person who made the decision, not employed by Microsoft, was secretly paid by Microsoft to ruin open source. Should be an amusing story. You explicitly stated on Twitter that changes were being made after speaking with Sean, a Microsoft employee. If this isn't what happened, then what did happen is certainly hard to follow.  ETA: in case it's not clear I have never suggested you were secretly paid, that's not the point."
technical,"Explain how me, the person who made the decision, not employed by Microsoft, was secretly paid by Microsoft to ruin open source. Should be an amusing story. You laid out opinions. You can't just declare a comment to be a ""opinion"" and have it be so. Come on, now.  ""An RFC is authored by engineers and computer scientists in the form of a memorandum describing methods, behaviors, research, or innovations applicable to the working of the Internet and Internet-connected systems. It is submitted either for peer review or to convey new concepts, information""  ""Peer review methods are employed to maintain standards of quality, improve performance, and provide credibility""  I'm more than happy to remove this from the argument list if you think it's not one, though I understand it to be the primary motivation behind the original change by Jamie.  It's a fact that engineers in my company would not be able to use software with such a license, for example. There are precedents documented elsewhere for React's old license as well.  Yes, as I said, I prefaced those with ""Things  I believe  should not be part of the discussion"". Those are my opinions. Even so, as I said, they are motivated by a desire to keep conversation productive, unlike those who are only here to make reddit-esque quips.  If I may be blunt, it feels like you're acting in bad faith, and I've no interest in petty bickering."
technical,"Explain how me, the person who made the decision, not employed by Microsoft, was secretly paid by Microsoft to ruin open source. Should be an amusing story. You really wanna stick it to Microsoft? Delete your GitHub account. Better yet -- move the repository off of GitHub! They're soon going to be its owner."
technical,"The purpose isn't objectivity, it's transparency. your implication that commenters have no relation to Lerna is unsubstantiated. I for one am actively involved in the ecosystem surrounding Lerna and have been preparing to add Lerna to a project I maintain, although I am now considering alternative options."
technical,Thanks for creating this issue. We think this issue is unactionable or unrelated to the goals of this project. Please follow our issue reporting guidelines. Happy Coding! Also add SantaHat icon
technical,Please add also an ultra-offensive debugger that shitstorms you on every error Thanks for creating this issue. We think this issue is unactionable or unrelated to the goals of this project. Please follow our issue reporting guidelines. Happy Coding!
technical,"should we instead add as a section in the tutorial below/above ""Loading an example dataset"" Also could you tag this ""Question"", ""Documentation"" and reopen it?"
technical,I wrote a short tutorial on how to get the dataset from a text format to a pandas DataFrame for use by sklearn here fixed in #7516
technical,"we understand your eagerness to solve your problem, and your frustration when it is not solved.  However, you seem quite misinformed about what is scikit-learn, how it works, and how the project is developed. Therefore, I would like to make some points clear for you. As you can see from, scikit-learn is a community effort that is developed by a team of volunteers, mostly on their free time. Gal is one of the creators of the project and its current leader: scikit-learn would certainly not be the same without his contribution (the same for other volunteers), and he certainly did not deserve your dismissive words.  What I would like to emphasize is that there is no such thing as a scikit-learn ""product"", or scikit-learn ""staff"" (only a handful of people have worked full time on the project). You mention ""we as users are customers"", but how much are you paying for using scikit-learn? Despite the important development cost, users get scikit-learn for free (and of course that's how it's intended to be). In fact, the development of the project relies on a fragile alchemy: users' needs being a top priority for developers, and users reporting bugs and concerns in the most positive way. The kind of ""ranting"" that you wrote can be very discouraging for developers, who contribute their free time and their expertise just because they believe that scikit-learn is a useful tool for the community. Some prominent developers stopped contributing to open-source software precisely because of such ""customer-like attitude"" of a few people underlining only shortcomings, and dismissing the huge development efforts. Please try to see the bright side as well: you received advice and comments from a lot of people, I'm sure that there was something for you to learn out of it, even if it did not solve your problem.  Also, although users' needs are indeed a top priority of scikit-learn (it has an amazing documentation, of which most scientific Python packages can be jealous!), each software addresses a well-targeted niche of users, and it is just normal that scikit-learn cannot fit all users. For example, it is preferable to use scikit-learn with already a good knowledge of Scientific Python. So, I'm really glad that you found a package that suited your needs better, but please also acknowledge the time and good will that people gave away when answering you.  So, folks, let's all show some good will and keep a constructive dialog. That's how the project we love will keep on rocking! For this reason, and in order to save your time, I would like to recommend some tools to assist you in data mining procedures. For instance, Waikato Environment for Knowledge Analysis (WEKA), last version is WEKA 3-7-13, is a collection of machine learning algorithms for data mining tasks. WEKA allows you to use its schemes either from GUI or writing Java code, so its very easy for non-programmers. Additional to WEKA, R is also an excellent tool for data mining stuff, you can also perform tasks of R from WEKA or vice versa. However, if you have a patience to design a prediction process manually (drag/drop), RapidMiner is a great tool for this propose where you can design a very nice flow to achieve your target.  Maybe we should make clear that scikit-learn is a Python **library**. It does not have the same scope as WEKA or RapidMiner. It fits perfectly into the scientific Python ecosystem but you should be willing to write code if you want to use it."
technical,"Thanks for your contribution.  I tried ""sudo apt-get install python-dateutil"", but it is not clear to me at what stage should indicate  this code? Do you think that there is an easy way to load my (excel or csv) file suing any simple ways such as open folder (regular way). There is another matter also which how to determine the class label that I want to predict form my dataset using scikit-learn. But anyway this step supposed to be after loading the file itself. Not easy process at all.  Is there any youtube tutorial about loading dataset (not iris which is everywhere or other famous. stuff). Video is easy than links HI all, I wrote the following code:  could you plz guide me."
technical,"I agree that loading data is a difficult and important thing. However, it is a domain-specific problem. You have a particular type of data. I have another. My data is medical images of brain activity. I can tell you how I organize my data and load them. I can even tell you that we have written a whole package about this, with its own documentation. But that will probably not help you.  What you want is something that tells you how to organize and load  your  data. Now, it may be that your data is something fairly classic, that many people have, for instance tabular data most often stored in CSV files. In which case there is a need for a package doing this data loading. I don't believe that it should be in scikit-learn. It needs to be in a package that is specialized for this data. For instance, we are not going to put image processing in scikit-learn. For tabular data, the dedicated package is pandas, as I mentioned in my reply we need to point to it. We, the scikit-learn team, want to make plugin pandas into scikit-learn easier. But it is not as easy as it may seem and it takes time (one of our core devs is prototyping something).  I realize rereading your post that your data is most likely text documents. So my two examples of data (medical images and tabular data) were both wrong :. Maybe the documentation on processing text with scikit-learn could indeed be improved and touch a bit on data organization. I don't know, I very seldom process text. But if you want to do add a few words on this, you are most welcomed to do a pull request. Anyhow, this illustrate my point about the diversity of the data: this whole thread is mostly about loading CSV files, as can be seen from earlier comments (before the thread exploded into a rant). The important thing is not the ""CSV"", which is the container, but the data model that underlies a CSV file. This data model is that of columns of data with different nature. It's a very different data model than processing text documents.  And finally, you are unhappy that teaching people ""how to import your data"" is time consuming. I don't think that there is an easy fix for this, even in a specific domain. The reason being that data meaning (ie data semantics) is still very much an open area. It's intrinsically hard to describe what the data means and how it's organized. You can try a simple experience: grab a dataset from someone you don't know, about an experiment you don't know, and try understanding it. Not even loading it, just understanding it. I am sure that it will take time. What takes a human time tends to be very difficult for a computer. Hm I don't think we added pointers to the FAQ yet. It's certainly a FAQ."
technical,Kindly refer - - How do I load my data to work with scikit-learn? - How to load data from CSV file? How do I load my data to work with scikit-learn?     How to load data from CSV file?  We should add these in the FAQ.
technical,"Perhaps I should elaborate on my original frustration, to give you some context.  I've been programming in Python almost exclusively for a year now (I am a late convert), and am fairly familiar with the ecosystem---I've done lot's of webservice related things, but also manipulation of resources related to automatic speech recognition.  I do my scientific work in Julia since a couple of years, and before that, in R, octave, c++/c (some 30 years in total).  The Julia ecosystem is quite dynamic, and it is all very exciting, but Python just has this very large ecosystem and very clean coding, which makes it very attractive to use for little side experiments.  This time I had to do some topic classification of (single sentence) text documents.  Now there is an abundant choice of language technology tools in Python, and I believe that via lda I got to scikit-learn.  Great tutorials, lovely datasets and all, but I found it very difficult to find out how to organize my own data so that I could load this in.  Just now, I browsed through the user guide again to find the docs for ""load files"", but I could't find an entry.  So a google search for ""sklearn.datasets.load files"" got me there just now, and I happened to remember the particular module path from more painstaking searches yesterday (it is mentioned somewhere in a tutorial).  For me, the essential information would have been: ""Organize your data one document per file, one directory per class""---more or less what's under the documentation for  load files.   This all makes perfect sense, but I come from a community where usual formats are ""one datapoint per line"", often with the class label on that line.  But having said all this, I am pretty impressed how the Python (text) community has standardized data representation, from what I've seen so far.  But perhaps because of the widely used standard data representation, this aspect has naturally less attention in documentation.  As a final note, whenever I try to teach students how to use some scientific tool set or another, I have to spend quite some time on ""how to import your data"".  Nobody likes to do it, it can be a lot of effort for what you potentially use only once, and is therefore always a difficult threshold. I agree that loading data is a difficult and important thing. However, it is a domain-specific problem. You have a particular type of data. I have another. My data is medical images of brain activity. I can tell you how I organize my data and load them. I can even tell you that we have written a whole package about this, with its own documentation. But that will probably not help you.  What you want is something that tells you how to organize and load  your  data. Now, it may be that your data is something fairly classic, that many people have, for instance tabular data most often stored in CSV files. In which case there is a need for a package doing this data loading. I don't believe that it should be in scikit-learn. It needs to be in a package that is specialized for this data. For instance, we are not going to put image processing in scikit-learn. For tabular data, the dedicated package is pandas, as I mentioned in my reply we need to point to it. We, the scikit-learn team, want to make plugin pandas into scikit-learn easier. But it is not as easy as it may seem and it takes time (one of our core devs is prototyping something).  I realize rereading your post that your data is most likely text documents. So my two examples of data (medical images and tabular data) were both wrong :. Maybe the documentation on processing text with scikit-learn could indeed be improved and touch a bit on data organization. I don't know, I very seldom process text. But if you want to do add a few words on this, you are most welcomed to do a pull request. Anyhow, this illustrate my point about the diversity of the data: this whole thread is mostly about loading CSV files, as can be seen from earlier comments (before the thread exploded into a rant). The important thing is not the ""CSV"", which is the container, but the data model that underlies a CSV file. This data model is that of columns of data with different nature. It's a very different data model than processing text documents.  And finally, you are unhappy that teaching people ""how to import your data"" is time consuming. I don't think that there is an easy fix for this, even in a specific domain. The reason being that data meaning (ie data semantics) is still very much an open area. It's intrinsically hard to describe what the data means and how it's organized. You can try a simple experience: grab a dataset from someone you don't know, about an experiment you don't know, and try understanding it. Not even loading it, just understanding it. I am sure that it will take time. What takes a human time tends to be very difficult for a computer."
technical,Hm I don't think we added pointers to the FAQ yet. It's certainly a FAQ. I wrote a short tutorial on how to get the dataset from a text format to a pandas DataFrame for use by sklearn here
technical,"Hi, Thank you so much for your help, I really appreciate your  cooperation.  I tried applying your code. Thus, once I interned (import pandas as pd). Directly I had the following message in red color:  What should I do? Thanks a lot It just means you do not have the dateutil module installed. You can install it by doing   sudo apt-get install python-dateutil"
technical,"Just want to support --- I am a scikit-learn newbie and have just have spent a frustrating time going thought the docs, and I can't find anywhere how to read my own data (and not a prepared toy dataset), and what the python format of data is. Kindly refer - - How do I load my data to work with scikit-learn? - How to load data from CSV file?"
technical,"For this reason, and in order to save your time, I would like to recommend some tools to assist you in data mining procedures. For instance, Waikato Environment for Knowledge Analysis (WEKA), last version is WEKA 3-7-13, is a collection of machine learning algorithms for data mining tasks. WEKA allows you to use its schemes either from GUI or writing Java code, so its very easy for non-programmers. Additional to WEKA, R is also an excellent tool for data mining stuff, you can also perform tasks of R from WEKA or vice versa. However, if you have a patience to design a prediction process manually (drag/drop), RapidMiner is a great tool for this propose where you can design a very nice flow to achieve your target.  Maybe we should make clear that scikit-learn is a Python **library**. It does not have the same scope as WEKA or RapidMiner. It fits perfectly into the scientific Python ecosystem but you should be willing to write code if you want to use it. Perhaps I should elaborate on my original frustration, to give you some context.  I've been programming in Python almost exclusively for a year now (I am a late convert), and am fairly familiar with the ecosystem---I've done lot's of webservice related things, but also manipulation of resources related to automatic speech recognition.  I do my scientific work in Julia since a couple of years, and before that, in R, octave, c++/c (some 30 years in total).  The Julia ecosystem is quite dynamic, and it is all very exciting, but Python just has this very large ecosystem and very clean coding, which makes it very attractive to use for little side experiments.  This time I had to do some topic classification of (single sentence) text documents.  Now there is an abundant choice of language technology tools in Python, and I believe that via lda I got to scikit-learn.  Great tutorials, lovely datasets and all, but I found it very difficult to find out how to organize my own data so that I could load this in.  Just now, I browsed through the user guide again to find the docs for ""load files"", but I could't find an entry.  So a google search for ""sklearn.datasets.load files"" got me there just now, and I happened to remember the particular module path from more painstaking searches yesterday (it is mentioned somewhere in a tutorial).  For me, the essential information would have been: ""Organize your data one document per file, one directory per class""---more or less what's under the documentation for  load files.   This all makes perfect sense, but I come from a community where usual formats are ""one datapoint per line"", often with the class label on that line.  But having said all this, I am pretty impressed how the Python (text) community has standardized data representation, from what I've seen so far.  But perhaps because of the widely used standard data representation, this aspect has naturally less attention in documentation.  As a final note, whenever I try to teach students how to use some scientific tool set or another, I have to spend quite some time on ""how to import your data"".  Nobody likes to do it, it can be a lot of effort for what you potentially use only once, and is therefore always a difficult threshold."
technical,"Well you could add some information about common errors that happen when loading own data. For example I have bumped on Unknown label type: 'continuous-multioutput' and not a hint, anywhere, what that could mean. I've tried passing: - a numpy array created from csv reader - a numpy array created from pandas - regular array parsed line by line - pandas dataframe - a numpy array created from csv reader converted to regular array - etc...  I'd recommend adding section summarizing assumptions about data, for example it was sheer luck that I found information that data sets may not contain NaN's Nulls etc. Also, what should be the parameters of a numpy array and how should pandas dataframe look like. Perhaps that error message could be clearer, but I think passing regression targets to a classifier (as in your #7801) is a usage error nothing to do with loading your own dataset."
technical,"How do I load my data to work with scikit-learn?     How to load data from CSV file?  We should add these in the FAQ. should we instead add as a section in the tutorial below/above ""Loading an example dataset"""
technical,"This is something that could have a bit more documentation than is in there currently. You might find Pandas useful. Should we reopen this issue and add a new section in the documentation? For example in this section: (""Loading your own data"")."
technical,"Thank you to brought up this issue. I am facing your problem now. I want to upload my IRIS like dataset. Which is 500 rows and 16 columns. and Thank you , I have seen your tutorial titled ""Can I go to the bathroom"" , that is a real work , real help to users like us.  And Thank you all developers, You worked free (or almost free) to deliver this library to people. I appreciate that.  MartinLion has spirit of learning, I praise that. Actually stubbornness is a virtue in academic. Thanks for the kind words.  Please let me know if you still have any issues with your data or learning process."
technical,"Thank you to brought up this issue. I am facing your problem now. I want to upload my IRIS like dataset. Which is 500 rows and 16 columns. and Thank you , I have seen your tutorial titled ""Can I go to the bathroom"" , that is a real work , real help to users like us.  And Thank you all developers, You worked free (or almost free) to deliver this library to people. I appreciate that.  MartinLion has spirit of learning, I praise that. Actually stubbornness is a virtue in academic. Thanks for the link. I checked it out, but the process looks complicated. Perhaps if there is a short youtube video explains the process much easier, otherwise I do not know what to do to solve this matter."
technical,"You can have a look at this for more details, Thanks for your contribution.  I tried ""sudo apt-get install python-dateutil"", but it is not clear to me at what stage should indicate  this code? Do you think that there is an easy way to load my (excel or csv) file suing any simple ways such as open folder (regular way). There is another matter also which how to determine the class label that I want to predict form my dataset using scikit-learn. But anyway this step supposed to be after loading the file itself. Not easy process at all.  Is there any youtube tutorial about loading dataset (not iris which is everywhere or other famous. stuff). Video is easy than links"
technical, The documentation of sklearn is really very useful and should answer your question: (basically you have to put your data in numpy arrays)
technical,The documentation of sklearn is really very useful and should answer your question: (basically you have to put your data in numpy arrays) This is something that could have a bit more documentation than is in there currently. You might find Pandas useful.
technical,"HI all, I wrote the following code:  could you plz guide me. We could tell you what the problem is but I think in this case you will learn more from it if you find it on your own. You should read the error message carefully. It is a Python syntax error."
technical,"fixed in #7516 Well you could add some information about common errors that happen when loading own data. For example I have bumped on Unknown label type: 'continuous-multioutput' and not a hint, anywhere, what that could mean. I've tried passing: - a numpy array created from csv reader - a numpy array created from pandas - regular array parsed line by line - pandas dataframe - a numpy array created from csv reader converted to regular array - etc...  I'd recommend adding section summarizing assumptions about data, for example it was sheer luck that I found information that data sets may not contain NaN's Nulls etc. Also, what should be the parameters of a numpy array and how should pandas dataframe look like."
technical,"It just means you do not have the dateutil module installed. You can install it by doing   sudo apt-get install python-dateutil You can have a look at this for more details,"
technical,It just means you do not have the dateutil module installed. You can install it by doing   sudo apt-get install python-dateutil You could also have a look at np.genfromtxt . Might be useful.
technical,"Having the same problem here. Would really appreciate a fix Also, got a few asking questions about this issue today."
technical,"Also, got a few asking questions about this issue today. Any fix on this? Still having problems and I know a lot of others are too. Thanks so much"
technical,"This is not on our roadmap. I do not think that will change for the next year. I will lock this thread, because added repeated requests will not change the roadmap. Several users are still using Python32 bits and they cannot install TensorFlow. For them, pip install tensorflow fails as no wheel matches the tags expected by their environment (to debug, pip debug --verbose shows only tags that don't math the filenames of our wheels). There is some requests to support 32 bits, see for example #31431. This is not going to be easy as we need to also compile the C++ codebase in 32 bits mode and that would cause issues with code written assuming types have a certain bit width.There is no change in the user visible API, just a new set of wheels to support more users. Opening this to reference in all similar issues."
technical,"As I stated, there is not enough time to duplicate all CI builds and to fix all the bugs that would get uncovered from there. And clearly this is not an issue that can be solved overnight. This is a high priority for me someone just asked me again a few min ago on how to install TensorFlow 2.0 for windows 10 32bit version. This thread will be going wild. Maybe I should tag Windows contributes into this thread."
technical,"Any fix on this? Still having problems and I know a lot of others are too. Thanks so much This is best effort, we are not going to work into this this year. Compiling for both 32 bits and 64 bits is not that easy so we don't have this included in any milestones plan at the moment. However, if the community wants to contribute patches so 32 bit support can be provided, they would be very welcomed."
technical,"more people have asked me private for a fix today This is not on our roadmap. I do not think that will change for the next year. I will lock this thread, because added repeated requests will not change the roadmap."
technical,a few people have been asking me today about pip install 32 bit tensorflow support. Will raise the issue upward.
technical,"It Is not trolling Is that when MS applies too extreme ""politically correct"" reactions it Is healty if the community react with a little bit of satire. Also, seems, censoring everything just keep self ""clean"""
technical,"I'm from New Zealand, and in the Southern Hemisphere, it's not cold, but hot during the holiday season. I suggest you geo-locate the user's position and prompt them for the icon of their choice, e.g. snow, ice, fire, sun shine. BTW. To original author of this storm - Christian-Schiffer. I wonder if his name not offending him? Maybe he should change it..."
technical,"We could make the snowflake toggable. Smile If it was possible to just turn the slowflake off, this whole debacle wouldn't happen :thinking:"
technical,"His name is **Christian**, that must be a troll account, too good to be true :D If you need further reference why snowflakes remind me of cold, pain and hurt see:"
technical,"Perhaps this is not the correct platform for trolling? It Is not trolling Is that when MS applies too extreme ""politically correct"" reactions it Is healty if the community react with a little bit of satire."
technical,Please stop offending other users. Christian-Schiffer created enough havoc already. ok boomer
technical,This is really offending for people that are suffering for global warming and desertfication areas. Also is that a natural snow icon or a fake snow icon? Please think about how much water Is wasted to create artificial snow just for games. Perhaps this is not the correct platform for trolling?
technical,"Please stop offended me! You're not my dad, I can use everything I like / dislike! Please stop offending other users. Christian-Schiffer created enough havoc already."
technical,"Winter is one good buff against invaders in Russia. Praise the winter! Let it snow! Also I think climate calculations are wrong and new ice age is nearby. There will be more snow. Thank you global warming fighters, for helping to make even more snow! Thanks for creating this issue. We think this issue is unactionable or unrelated to the goals of this project. Please follow our issue reporting guidelines. Happy Coding!"
technical,"Also, seems, censoring everything just keep self ""clean"" We could make the snowflake toggable. Smile"
technical,"I guess we will be patching every update from here out.  One way to handle your examples is to add an ""invalid"" attribute: 0 if valid, 1 if invalid. Then discard fragments for which the interpolated attribute value is non-zero. Also, your patch accommodates your use case, but it does not accommodate an arbitrary use case with invalid data. So, your patch is not a ""fix"".  what case would it not fix?  It fixes unknowns in position array from flickering. Could I add a switch to turn off probe lighting?   One way to handle your examples is to add an ""invalid"" attribute: 0 if valid, 1 if invalid. Then discard fragments for which the interpolated attribute value is non-zero.  Where woudl you implement this.  Seems alot like an alpha map but culling vertices else where?  Edit: thanks for the help.   three.js does not validate data -- much less accommodate invalid data. That is the app's responsibility.  its semantics(and not my project)  but in my mind"
technical,"three.js does not validate data -- much less accommodate invalid data. That is the app's responsibility.  Also, your patch accommodates your use case, but it does not accommodate an arbitrary use case with invalid data. So, your patch is not a ""fix"". I guess we will be patching every update from here out.  One way to handle your examples is to add an ""invalid"" attribute: 0 if valid, 1 if invalid. Then discard fragments for which the interpolated attribute value is non-zero."
technical,"looks like a Problem with the nvideo driver?  i tested this on two Systems, with amd driver (linux) without any problem I haven't gone through the diff of 103-104, but ideas on where to look that would make this happen?"
technical,RESOLVED  fixed it.  Is this worth committing?  Someone who knows get and three should commit this I use PlaneBufferGeometry with null z values to represent unknown elevations(holes).  You have NaN's in your position data.  You also have NaN's in your vertex normals. Hence their length is undefined.  three.js does not support NaNs in data pushed to the GPU.
technical,"Its something that occurred between 103 and 104.  103 works, 104 flickers.... Looking at change log I'm afraid a screenshot is not sufficient to debug this issue. Please always provide live examples when reporting such problems."
technical,"why not just map NaNs to some big but finite values to move the triangles out of the view. that would mean removing the index attribute as well, however (rough idea here) In any event, I don't see a three.js bug here. I suppose this happens also with pure WebGL and with any other 3D engine so I think it's more correct to close the issue and move the dicussion to stackoverflow or the forum."
technical," Its something that occurred between 103 and 104.  103 works, 104 flickers.... Looking at change log"
technical,"video: Multiple instances of the window seem to be the issue, even with duplicate fiddles it appears. I have also tested on msi lapiop with NVIDIA GeForce GTX 980M and the problem is probably worse.  All other devs have windows machines, but we could check on a mac if that helps looks like a Problem with the nvideo driver?  i tested this on two Systems, with amd driver (linux) without any problem"
technical,"single change then adding that change in so this is messing it up. sorry my github skills suck , we are svn: looks like probe light is causing this issue WestLangley  sciecode :  Added support for light probes.   Edit as i walk through for future refrence: 1.  Light probe is part of ambient light so removing ambient light fixes the issue. 2. light probe is a measure of ambient light not a child.  So no ambient light no light probe? 3. Hemisphere lighting does not show flicker 4. Hemisphere white light is super flickery 5.  Looks like the more white the light the worse the flicker, equally bad on hemisphere and ambient."
technical,"Also, your patch accommodates your use case, but it does not accommodate an arbitrary use case with invalid data. So, your patch is not a ""fix"".  what case would it not fix?  It fixes unknowns in position array from flickering. Could I add a switch to turn off probe lighting?   One way to handle your examples is to add an ""invalid"" attribute: 0 if valid, 1 if invalid. Then discard fragments for which the interpolated attribute value is non-zero.  Where woudl you implement this.  Seems alot like an alpha map but culling vertices else where?  Edit: thanks for the help.   three.js does not validate data -- much less accommodate invalid data. That is the app's responsibility.  its semantics(and not my project)  but in my mind My suggestion is similar to alpha-testing and would require injecting code into the fragment shader. Without knowing the details of your use case, I can only offer it as a suggestion.  Please use the three.js forum if you need additional help."
technical,"We have a valid use case for putting NaN's in our data set that as far as I can tell can not be done in any other way, efficiently.  In any real world application dealing with NaN's is just a fact of life .  Bounding Volumes was actually my last post.  It all stems from not handling NaN's properly.  Just a simple test for isNaN would resolve most these.  Why is there such push back and not a desire to resolve this issue?  Are there any suggestions for dealing with real world data that has NaN's?  We have bandied around alpha mapping, but then we increase overhead with the map and need to access the alpha map and the vertex positions.  Also what if the numbers are very large and overflows?  Will that cause a NaN that might need to be dealt with. NaN values is an app level problem from our point of view since the user is in some sense the data producer. Geometric data like position, normal or color data are meant to be numerical. NaN data are no numerical data.  Handling NaN values is use case specific since proper default values depend on what the application actually does.  This is not only an issue in graphics. When you process huge data in context of KDD and machine learning, data cleanup is one important preparation step before you execute the actual algorithm (which is not responsible for handling missing, undefined or corrupt values)."
technical,"You may have more luck by doing a PR with that ""simple test"" rather than asking us to do it for you. of course you could just create un-indexed buffer geometry to begin with, and not use ExplodeModifier or convert geometries otherwise. well, you know, a bunch of IF-s in the shader that almost noone would need any way... so, probably not. no flicker here at all"
technical,"I use PlaneBufferGeometry with null z values to represent unknown elevations(holes).  You have NaN's in your position data.  You also have NaN's in your vertex normals. Hence their length is undefined.  three.js does not support NaNs in data pushed to the GPU. one should always account for NaN whether it is intended or not.  And its a simple fix.  We need this to work for our production and it doesnt harm anything having a check in.   Could we please add, is there any harm?"
technical,"one should always account for NaN whether it is intended or not.  And its a simple fix.  We need this to work for our production and it doesnt harm anything having a check in.   Could we please add, is there any harm? one should always account for NaN whether it is intended or not.  No, YOU need to account for it in your app and pass valid data."
technical,updated fiddle with super flicker:  turn off ambient light for no flicker RESOLVED  fixed it.  Is this worth committing?  Someone who knows get and three should commit this
technical,"Was able to replicate on Windows with NVidia GPU, but not on Mac with intel integrated GPU.  Couldn't find anything on the change logs from r103 - r104 that would explain that.  But, then again, you are making use of unsupported behavior, as you said it yourself. Seems very odd that separate instances of three are causing it.  Even though unsupported seems like it could be a canary for a scoping issue?"
technical,"We are constantly updating the surface with new known position.  Might be worth testing but would take a considerable amount of work.  If anything this would double the overhead(two geometries needed) of multi-million point grid geometries.  Do you have any docs for ExplodeModifier?  Also we have to have bufferGeometry.  These are very large surface models.  I do not think that is true because the issue is not seen in r103.  There was a change in the code base that introduced this bug.  If it never worked, then i would agree it is a  webGL/GPU issue, but it was working up to r103 and we have it in production at r92 for a pretty long time(since r92 release) single change then adding that change in so this is messing it up. sorry my github skills suck , we are svn:"
technical,"NaN values is an app level problem from our point of view since the user is in some sense the data producer. Geometric data like position, normal or color data are meant to be numerical. NaN data are no numerical data.  Handling NaN values is use case specific since proper default values depend on what the application actually does.  This is not only an issue in graphics. When you process huge data in context of KDD and machine learning, data cleanup is one important preparation step before you execute the actual algorithm (which is not responsible for handling missing, undefined or corrupt values). so this is a ""no"" on resolving?  Is there a reason not to other than ""we don't support this""?  Its a one line PR.  I guess we will be patching every update from here out."
technical,"The issue gets worse when multiple 3js windows are open and active(open the fiddle in 2 windows).  Also noticeably when ""lightShot"" screen capture was opened.  Also noticed when whats app message received.  I have a dev team of 4 guys finding inconsistencies of producing on different graphics cards with different results, but all see a flicker at different times Sorry, but I can't see any flickering on my system. Tested with Chrome 78.0.3904.97, FF 70.0.1 and macOS 0.14.6. Are you on Windows?"
technical,"I'm afraid a screenshot is not sufficient to debug this issue. Please always provide live examples when reporting such problems. thank you for your help,  Flicker is seen when not moving."
technical,"thank you for your help,  Flicker is seen when not moving. The issue gets worse when multiple 3js windows are open and active(open the fiddle in 2 windows).  Also noticeably when ""lightShot"" screen capture was opened.  Also noticed when whats app message received.  I have a dev team of 4 guys finding inconsistencies of producing on different graphics cards with different results, but all see a flicker at different times"
technical,"so this is a ""no"" on resolving?  Is there a reason not to other than ""we don't support this""?  Its a one line PR.  I guess we will be patching every update from here out. three.js does not validate data -- much less accommodate invalid data. That is the app's responsibility.  Also, your patch accommodates your use case, but it does not accommodate an arbitrary use case with invalid data. So, your patch is not a ""fix""."
technical,"looks like probe light is causing this issue WestLangley  sciecode :  Added support for light probes.   Edit as i walk through for future refrence: 1.  Light probe is part of ambient light so removing ambient light fixes the issue. 2. light probe is a measure of ambient light not a child.  So no ambient light no light probe? 3. Hemisphere lighting does not show flicker 4. Hemisphere white light is super flickery 5.  Looks like the more white the light the worse the flicker, equally bad on hemisphere and ambient. updated fiddle with super flicker:  turn off ambient light for no flicker"
technical,"yes the debt is too big to resolve, so lets never fix it mrdoob .  Why not start here?  If I do a PR to fix this issue will get any traction? Or if i add a switch to light probe to turn on/off like shadows and transparency? makc Single if.  You also turn off other features like shading, and transparency.  Why not make this new undocumented featured(only example) that effect primary lighting switchable? Well great let me just ship your computer around so everyone can use it. Or even better we will buy everyone a plane ticket and then can come stand in line to use your computer where the software does work. Updated from v92 to v110 and have flicker on the edges.  I use PlaneBufferGeometry with null z values to represent unknown elevations(holes).  The sparse grid shows a flicker where the nulls' edges are.  Was not seen with flatShading: true or v92: I know you are going to say NaN are not supported in geometries, but any chance you could fix this or point me in proper direction? ##### Hardware Requirements (graphics card, VR Device, ...)"
technical,"Sorry, but I can't see any flickering on my system. Tested with Chrome 78.0.3904.97, FF 70.0.1 and macOS 0.14.6. Are you on Windows? video: Multiple instances of the window seem to be the issue, even with duplicate fiddles it appears. I have also tested on msi lapiop with NVIDIA GeForce GTX 980M and the problem is probably worse.  All other devs have windows machines, but we could check on a mac if that helps"
technical,"I haven't gone through the diff of 103-104, but ideas on where to look that would make this happen? Was able to replicate on Windows with NVidia GPU, but not on Mac with intel integrated GPU.  Couldn't find anything on the change logs from r103 - r104 that would explain that.  But, then again, you are making use of unsupported behavior, as you said it yourself."
technical,"In any event, I don't see a three.js bug here. I suppose this happens also with pure WebGL and with any other 3D engine so I think it's more correct to close the issue and move the dicussion to stackoverflow or the forum. We are constantly updating the surface with new known position.  Might be worth testing but would take a considerable amount of work.  If anything this would double the overhead(two geometries needed) of multi-million point grid geometries.  Do you have any docs for ExplodeModifier?  Also we have to have bufferGeometry.  These are very large surface models.  I do not think that is true because the issue is not seen in r103.  There was a change in the code base that introduced this bug.  If it never worked, then i would agree it is a  webGL/GPU issue, but it was working up to r103 and we have it in production at r92 for a pretty long time(since r92 release)"
technical,"Yes, this is something that needs to be fixed on app level.   three.js does not support NaNs in data pushed to the GPU.  NaN geometry data also corrupt operations on the CPU like intersection tests or the computation of bounding volumes. So again, this is no library bug. We have a valid use case for putting NaN's in our data set that as far as I can tell can not be done in any other way, efficiently.  In any real world application dealing with NaN's is just a fact of life .  Bounding Volumes was actually my last post.  It all stems from not handling NaN's properly.  Just a simple test for isNaN would resolve most these.  Why is there such push back and not a desire to resolve this issue?  Are there any suggestions for dealing with real world data that has NaN's?  We have bandied around alpha mapping, but then we increase overhead with the map and need to access the alpha map and the vertex positions.  Also what if the numbers are very large and overflows?  Will that cause a NaN that might need to be dealt with."
technical,"Seems very odd that separate instances of three are causing it.  Even though unsupported seems like it could be a canary for a scoping issue? why not just map NaNs to some big but finite values to move the triangles out of the view. that would mean removing the index attribute as well, however (rough idea here)"
technical,"one should always account for NaN whether it is intended or not.  No, YOU need to account for it in your app and pass valid data. Yes, this is something that needs to be fixed on app level.   three.js does not support NaNs in data pushed to the GPU.  NaN geometry data also corrupt operations on the CPU like intersection tests or the computation of bounding volumes. So again, this is no library bug."
technical,"My suggestion is similar to alpha-testing and would require injecting code into the fragment shader. Without knowing the details of your use case, I can only offer it as a suggestion.  Please use the three.js forum if you need additional help. You may have more luck by doing a PR with that ""simple test"" rather than asking us to do it for you."
technical,"Yes, run go get and then start godoc: Open localhost:6060/fmt in your browser and your changes should be there. Alternatively you can create a new fmt repo on Github, copy the source files, push your changes there, then go to e.g. to see the docs."
technical,"Fprintf was taken also, I'll take Fprintln Anyone know how often the official go documentation is updated? i.e. how long our commits should take to show up here"
technical,"Is there any particular reason for why it's not called strings.Buffer, next to bytes.Buffer (which exists)? It was a neutral-tone, you might have never actually attached or imagined negative connotations to it if it weren't for me saying ""excuse me for my tone"", which I have only said because I know how often people imagine hostility where there is none. :relaxed: bytes.Buffer is also an io.Reader, strings.Builder is not. I disagree about the tone, phrases like ""this chaotic madness"" and statements that the core developers do not keep documentation up to date, without giving any examples, are not helpful.  Please see, search for ""Avoid destructive behavior"".  Thanks."
technical,"Alternatively you can create a new fmt repo on Github, copy the source files, push your changes there, then go to e.g. to see the docs. Change mentions this issue: fmt: add doc example for Fprintf"
technical,"Alternatively you can create a new fmt repo on Github, copy the source files, push your changes there, then go to e.g. to see the docs. Change mentions this issue: fmt: add example for Sprint"
technical,"Yet you're continuing to double down, in the face of multiple people telling you your behavior is not being appropriate. What I don't understand is you're clearly capable of writing and behaving constructively, for example on this. Despite what you say your words and actions do have a large impact on how people feel about what you write and how people interact with you. Change mentions this issue: fmt: add example Sscanf"
technical,"Yet you're continuing to double down, in the face of multiple people telling you your behavior is not being appropriate. What I don't understand is you're clearly capable of writing and behaving constructively, for example on this. Despite what you say your words and actions do have a large impact on how people feel about what you write and how people interact with you. Fprintf was taken also, I'll take Fprintln"
technical,"Yet you're continuing to double down, in the face of multiple people telling you your behavior is not being appropriate. What I don't understand is you're clearly capable of writing and behaving constructively, for example on this. Despite what you say your words and actions do have a large impact on how people feel about what you write and how people interact with you. I can create example for Fscan."
technical,"Yet you're continuing to double down, in the face of multiple people telling you your behavior is not being appropriate. What I don't understand is you're clearly capable of writing and behaving constructively, for example on this. Despite what you say your words and actions do have a large impact on how people feel about what you write and how people interact with you. I'll do GoStringer"
technical,The problem with updating golang.org is if we add some new API for Go 1.12 (like strings.Builder) it would not be good if it showed up in the docs before most people were able to use it. Is there a way we can run our new ExampleFunc locally to ensure it works? Else I can copy it to play.golang.org
technical,"bytes.Buffer is also an io.Reader, strings.Builder is not. I disagree about the tone, phrases like ""this chaotic madness"" and statements that the core developers do not keep documentation up to date, without giving any examples, are not helpful.  Please see, search for ""Avoid destructive behavior"".  Thanks. It doesn't need to be an io.Reader, there are differences between the two regardless of the naming convention, but that doesn't necessarily warrant a completely different convention. Either way, one has to consult the documentation to get an idea of what they actually are and do, and they do largely share a similar naming convention, just compare the two on the documentation page, or the source code[1].  We can disagree all we want, my tone and intentions are irrelevant. In fact, I would argue that ""chaotic madness"" is a clear expression of dislike of inconsistency (which in itself is not an empty statement), albeit could be perceived exaggerated which I am not arguing against, anything could be perceived to be exaggerated and/or hostile. I am sure the reasons are identical to why Rob picked it up, judging by the new issue he opened, let me quote parts of it: ""A consistent set [...]"", ""The examples in the fmt package are inconsistent"", which means inconsistency isn't something he prefers. What you can argue in favor of is that I'm merely expressing my dislike of such, without doing anything, but that's the thing: expressing dislike is enough. There are often polls regarding the syntax of a language where they ask the community about such, etc., so it seems that expressing my dislike alone could be sufficient for various purposes and is definitely not an empty, or meaningless statement.  If we were to lecture people about ""useless statements"" (not quoting you), or ""destructive behavior"" (not sure how it applies to my case, feel free to explain if you would like, but I elaborated above why this is not the case) and enforce the conduct, there would not be too many people left here, so you can pick on me all you want and moderate me out of this discussion, but then at least have the decency to call yourself biased and note that you are not doing it consistently. It's useful against people whom we dislike, sure. I might pull out the transgender card in the future if necessary, in case that magically makes me more tolerable, which seems to be a common case these days.  What exactly do you mean by ""bytes.Buffer is also an io.Reader"", could you provide a reference to that? The API of strings.Builder and bytes.Buffer are remarkably similar, and they both *seem* to implement io.Reader per type Reader in the documentation, are you talking about that? If not, then what exactly are you referring to?  [1] Speaking of inconsistencies (albeit a minor in this case, I suppose) And note how per documentation and source code, they BOTH implement a type Reader, so ultimately, according to you AND the documentation, the naming convention is as follows Buffer: write and read Builder: write Reader: read (implemented for both strings and bytes) Is this correct? Edit: on second thought, by ""bytes.Buffer is also an io.Reader"", did you mean func (b *Buffer) ReadFrom(r io.Reader) (n int64, err error) in buffer.go? Would it be impossible to implement the same thing for strings.Builder so it would justify its rename to strings.Buffer, or is it enough of a reason to name it strings.Builder instead of strings.Buffer, the mere existence of one function?"
technical,"Change mentions this issue: fmt: add example Sscanf Let's add examples. I think it would be good to add examples to each function (if possible?) as well as to illustrate some points - not every example should make every point but it would be good to cover these. When you open a change, put this at the bottom of the commit message: That way gopherbot will post a comment here with a link to your CL. Add a comment if you want to fix one and I'll put your name next to the func in question."
technical,Change mentions this issue: fmt: add doc example for Fprintf Looking great!
technical,"First off, I don't see how this could be an issue if you are having docs per version (docs for 1.11, docs for 1.12, and so on), and secondly, what is strings.Builder and do we really need it? Are we going to have strings.Factory, too? For a moment I thought go is finally taking a better route, but it makes me reconsider that thought. On another note, yes, I agree with Rob here. Inconsistency has been a problem for go, it just seems like people come and write completely different and random examples to similar, related functions. That's not how it should be done. Apparently Rob had to assign it to himself, and tell others to stop posting examples for this reason. Thanks Rob, this chaotic madness needs to stop. Go linters complain when exported variables or constants are not documented (despite them being obvious in many cases), yet when core developers make major changes to go, they keep the old documentation, and don't even bother making it up-to-date. Excuse me for my tone, but it irks me a bit, anyways: thanks for putting an end to it, for the time being. Please do keep a polite tone.  Thanks. The comment about updating golang.org was in the context of somebody asking how they could see the current docs.  The answer was: use tip.golang.org, with an explanation for why golang.org is not updated. For strings.Builder see this."
technical,Change mentions this issue: fmt: add example for Sprint Replacing with issue. Please stop adding examples to the package for now.
technical,I can create example for Fscan. Sprintln is done
technical,"They should appear at tip.golang.org almost immediately, as I believe that project fetches from HEAD every 15 minutes.  The docs on golang.org will update the next time there's a new Go release, or earlier if the patches are backported from master (which targets Go 1.12) to the Go 1.11.1 point release. The problem with updating golang.org is if we add some new API for Go 1.12 (like strings.Builder) it would not be good if it showed up in the docs before most people were able to use it."
technical,"Anyone know how often the official go documentation is updated? i.e. how long our commits should take to show up here They should appear at tip.golang.org almost immediately, as I believe that project fetches from HEAD every 15 minutes.  The docs on golang.org will update the next time there's a new Go release, or earlier if the patches are backported from master (which targets Go 1.12) to the Go 1.11.1 point release."
technical,Anyone know how often the official go documentation is updated? i.e. how long our commits should take to show up here waits there is actually a CL open for that one at the moment from mooreds  - I should have added it to the sheet. how about Fprintf?
technical,"Is there a way we can run our new ExampleFunc locally to ensure it works? Else I can copy it to play.golang.org Yes, run go get and then start godoc: Open localhost:6060/fmt in your browser and your changes should be there."
technical,"It doesn't need to be an io.Reader, there are differences between the two regardless of the naming convention, but that doesn't necessarily warrant a completely different convention. Either way, one has to consult the documentation to get an idea of what they actually are and do, and they do largely share a similar naming convention, just compare the two on the documentation page, or the source code[1].  We can disagree all we want, my tone and intentions are irrelevant. In fact, I would argue that ""chaotic madness"" is a clear expression of dislike of inconsistency (which in itself is not an empty statement), albeit could be perceived exaggerated which I am not arguing against, anything could be perceived to be exaggerated and/or hostile. I am sure the reasons are identical to why Rob picked it up, judging by the new issue he opened, let me quote parts of it: ""A consistent set [...]"", ""The examples in the fmt package are inconsistent"", which means inconsistency isn't something he prefers. What you can argue in favor of is that I'm merely expressing my dislike of such, without doing anything, but that's the thing: expressing dislike is enough. There are often polls regarding the syntax of a language where they ask the community about such, etc., so it seems that expressing my dislike alone could be sufficient for various purposes and is definitely not an empty, or meaningless statement.  If we were to lecture people about ""useless statements"" (not quoting you), or ""destructive behavior"" (not sure how it applies to my case, feel free to explain if you would like, but I elaborated above why this is not the case) and enforce the conduct, there would not be too many people left here, so you can pick on me all you want and moderate me out of this discussion, but then at least have the decency to call yourself biased and note that you are not doing it consistently. It's useful against people whom we dislike, sure. I might pull out the transgender card in the future if necessary, in case that magically makes me more tolerable, which seems to be a common case these days.  What exactly do you mean by ""bytes.Buffer is also an io.Reader"", could you provide a reference to that? The API of strings.Builder and bytes.Buffer are remarkably similar, and they both *seem* to implement io.Reader per type Reader in the documentation, are you talking about that? If not, then what exactly are you referring to?  [1] Speaking of inconsistencies (albeit a minor in this case, I suppose) And note how per documentation and source code, they BOTH implement a type Reader, so ultimately, according to you AND the documentation, the naming convention is as follows Buffer: write and read Builder: write Reader: read (implemented for both strings and bytes) Is this correct? Edit: on second thought, by ""bytes.Buffer is also an io.Reader"", did you mean func (b *Buffer) ReadFrom(r io.Reader) (n int64, err error) in buffer.go? Would it be impossible to implement the same thing for strings.Builder so it would justify its rename to strings.Buffer, or is it enough of a reason to name it strings.Builder instead of strings.Buffer, the mere existence of one function? Your tone and intentions do matter because they make a difference in the way people feel about what you're saying, for one, and also because we all agreed to abide by the Code of Conduct, which says that they matter.   The entire discussion of the strings interface is off topic for this issue. I raised it only as an example of an API that had been recently added. If you want to discuss it send a message to the golang-nuts mailing list."
technical,"You are making toxic and unhelpful statements. Please refrain from doing so. Sarcasm and profanity will not help you get your issues resolved.  I have replied in your main question. If you would like to engage in constructive discussion, go there. If you want to continue to make mean and unhelpful assumptions and remarks, go ahead, but then we can just close down this whole thread as well. An experiment to see whether ANYONE on the SF team actually receives GitHub notifications."
technical,"You are most likely misinterpreting the girhub list of issues. And even if that was not the case, it's still not a problem."
technical,"I'm certainly not the biggest contributor to Godot, but maybe 2 of my 34 contributions are related to C#. I also donate almost double my Unity subscription to Godot, I know most of that money will not be used for C# development... and I'm 100% happy with that.  I've been using Unity since the 1.x days. I tried switching to Godot a couple of years ago and found it impractical to do so but am finding it pretty easy to do so now. What is finally enabling me to switch completely over to Godot? Despite it being a great engine now and also with a lot of promise in the future, the largest thing that is making the transition possible is C# :)  I agree, I think you're interpreting the issue list wrong.  It seems to me that you're assuming that:  1) That no new C++ contributors are here because of C# 2) That C# issues have increased, but the new contributors are unable to fix these issues, thus the core team members must pick up the slack 3) The newcomers to Godot who use C# don't contribute to non-C# issues.  Every one of these points can easily be proven false and I think you'll see that the momentum of Godot is only going to increase due to C# being a standard part of the engine. :) c# was the biggest mistake i've ever witnessed in godot. name me, one god damn reason why a developer would favour C# over Godot's GDScript? for the godot engine?  what you will say is, ohh but mahh c# sharp unity users! it will help bring people from unity over! then, on the other hand, everyone says ""we're not competing against unity!"". yeah, that's horse shit.  second reason: ""ohhh the performance is so much better!"". oh yeah? what are the use cases to back that up? 1% of the godot users having performance issues with gdscript? i doubt it. if a developer is having a performance issue with gdscript, they'd use fucking gdnative.  look through all the **contributors** in those issues. 80% of them have participated in adding other helpful features to Godot too, not just C# stuff. this means, by nature their time is **inherently being focused** on c#.  oh yeah, and here's another problem with this entire ""only implement low level features"" stuff. want to tell me then, why **smiley emojis were added to the core of the engine** then? surely that's not low level is it? but when i suggest **that an inventory system** and other 2d features be added, you spit in my  face and tell me ""it's not low level!"".  meter la mula, cotoco."
technical," I stand by my words, that's exactly what happened."
technical,"And even if that was not the case, it's still not a problem. I'm certainly not the biggest contributor to Godot, but maybe 2 of my 34 contributions are related to C#. I also donate almost double my Unity subscription to Godot, I know most of that money will not be used for C# development... and I'm 100% happy with that.  I've been using Unity since the 1.x days. I tried switching to Godot a couple of years ago and found it impractical to do so but am finding it pretty easy to do so now. What is finally enabling me to switch completely over to Godot? Despite it being a great engine now and also with a lot of promise in the future, the largest thing that is making the transition possible is C# :)  I agree, I think you're interpreting the issue list wrong.  It seems to me that you're assuming that:  1) That no new C++ contributors are here because of C# 2) That C# issues have increased, but the new contributors are unable to fix these issues, thus the core team members must pick up the slack 3) The newcomers to Godot who use C# don't contribute to non-C# issues.  Every one of these points can easily be proven false and I think you'll see that the momentum of Godot is only going to increase due to C# being a standard part of the engine. :)"
technical,"right back at you then. since there is no ""leadership"" in this open source project, you sir, cannot claim C# is superior to gdscript then. i just gave you two god damn good reasons why gdscript  C#, and you still continue to dodge. gg faker.  you also completely dodged my point about the smiley emoji implementations. so i recommend a good feature for the engine **(inventory and other 2d stuff, that ALL GAME DEVELOPERS WILL BENEFIT FROM)**, but **you spit in my face**, claim that it's not ""low level enough"", but then go around merging PRs for ""emoji smiley"" support for fonts? that's somehow still low level though? wtf??   websocket support still has never been implemented. are you going to pay Faless a quarter of the 20,000 grant that mozilla gave? since he actually implemented the websocket module? or you going to say ""ahhh, software conservancy handles the funds, we don't know what to do with it!"" as you always do? then keep it for akien, you, and karroffel? doesn't really seem fair to me.  look at it from my perspective please.  in any event. complete and utter crap. but wait, akien-mga will continue to block users, close discussions, etc. seems like i need to go make a new fucking account again for the 5th time... can't have discussions without bypassing  / cheating github it seems! so much for ""open source"". This went out of hands. There is not a drop of constructive criticism here, only very hostile behavior and personal attacks. Locking."
technical,"I stand by my words, that's exactly what happened. You are most likely misinterpreting the girhub list of issues."
technical,"by failing to mount the disk? because i can not mount disk in OMV5 or i would never learn about this issue. failing to mount disk does not look like work-around at all. let's think what we can do to make space-containing partitions mounted by OMV. It seems to be a kind of ""communication breakdown"" between Salt and OMV5, they expect and provide for mututally incompatible things. here is minimally patched. it makes space-containing mount point visible. If there still is something not working - i can not see what it is and how could i test it using salt-call scripts and also this. And now the most curious thing to me. I changed the partition label, then i mounted the disk from OMV Web UI ....and there is no any space-containing mountpoint path regardless of partition label. The ""workaround"" seems to needlessly shoot down the perfectly working function! Maybe it is only with MBR/NTFS disks, maybe GPT or XFS disks would use something else in fstab, dunno."
technical,"i mentioned systemd because of this. See... i know very little about Linux and nothing about Python, so i was googling everything i could think of :-) But i am glad to hear from you.  So, how can we scratch this itch, is Salt team is not with us on it... Can you make some scripts demonstrting the alleged Salt bug that i could run from bash ? Also are there some hidden option in OMV5 to re-enable mounting space-containing partitions? But i really am worried about potential race conditions in OMV when different partitions would have same label... IF you use label as ""primary key"" as persistent ID for all the other settings (user rights, sharing folders, etc), it might be quite a gotcha..."
technical,"Some of the ""deep changes"" i mentioned above. I now think those are dead end, but just in case they would be useful to someone, maybe even us later. Using ""Raw"" non-escaped string would probably be more proper design, but might really require deep refactoring of many Salt modules and then testing on many different systems. Horror.. one cannot trust mount with space-containing path at least on Linux - by failing to mount the disk? because i can not mount disk in OMV5 or i would never learn about this issue. failing to mount disk does not look like work-around at all. let's think what we can do to make space-containing partitions mounted by OMV. It seems to be a kind of ""communication breakdown"" between Salt and OMV5, they expect and provide for mututally incompatible things. here is minimally patched. it makes space-containing mount point visible. If there still is something not working - i can not see what it is and how could i test it using salt-call scripts and also this."
technical,"Please stop blaming and ranting me. This raised issue here has nothing to do with OMV. please set this issue to read-only, I had to do the same on the OMV issues to stop these rants. Currently it is not possible to mount a filesystem to a mount point that contains blanks. There are many problems in the states.mount.mounted code path, e.g. the mount command arguments are not quoted in the modules.mount.mount function, see this. Code should look like this IMO: But this will fix only a small piece of the whole problem. Another one is that states.mount.mounted does not detect correctly that the filesystem might be mounted already, i think it's because the key in the active table is not unquoted, so a comparison between My Passport Blue fails. To me it looks like the whole mount state and module is not able to handle blanks in device names and mount points properly."
technical,"I don't think this is the right place to discuss OMV related things. Escaping blanks is not my idea, it is used by every userland command that processes mount points, e.g. mount. Either systemd want to have escaped paths in mount units too, there is a special command to convert paths for you, see systemd-escape. OMV already workarounds this issue, thus it is not affected by this reported issue here. I don't think Salt will drop mount.mounted because it is a somewhat essential functionality of Linux systems. You can do that, but don't blame the software then. Using USB devices in a NAS is no good idea, but that's a different thing. IMO devices using in a NAS should be already connected to the NAS, no plug-and-play, this is not how a NAS is intended to work. If devices are always connected, then you will never run into the situation that duplicate labels might harm your system. This issue is user introduced and should be handled by them. If you want to discuss this issue please open an issue in the OMV Git repository. I think it is ok to use systemd escaped paths in this since systemd handles filesystem mounting nowadays."
technical,"Hello, I am trying to build myself home NAS with old Atom mini-ITX board... So i install OMV5, i plug in dad's old NTFS drive... and here we go... Frankly, i wish Salt guys put the comments inside this source, listing all the bugs related to this module. So any hacker which for whatever reason would change it - would be instantly notifie on old pending bugs.  Salt seems extremely fragile here, probably no one else except for OMV5 uses it for partitions. Maybe OMV6 could do it outside Salt? Like good old UDEV rules or anything. I mean, before Salt porject might decide to drop this functionality that almost no one use, instead of burden of maintaining it for OMV5 alone.... Well, ranting aside, i am rather puzzled with your  device. Where do you even get this hex substitution from??? Thing is, the whole mounting escaping is one uber-ancient legacy mess. Putting it here so maybe someone would use it. I spent like 3 hours googling around and experimenting with Python that i never used before. Tryied to google some standard about Posix/Linux/bash filename mangling/escaping.... and then Python module to undo it. To no avail.  Okay, so, to document it down. - mtab/fstab and friends is one-of-a-kind ancient mess. - it started with ancient BSD (not FreeBSD) function strunvis, which behaviour  not documented. Probably that was OS-specific function (a la virtual methods).  - when Linux was mimicking good old BSd it only made ad hoc substitutions for 4 specific chars. There is no any systematic/generic pattern at all.  So, whatever comes from Linux mounts information - should be de-mangled for those four special cases. Every single space-separated column of every single line. Ugly, and undocumented, but that is what it is. And, frankly, it is not that hard...  BUT, why do you want to compare with some arbitrary hex-escaped string? what can be a real use-case for that??? Linux kernel just does not have hex-escaping code for disk mounts. Now, to be frank, even this would NOT be enough, because i can have multiple disks with the same partition label. Like many USB thumb drives with ""DATA"" partition. I can even have several partitions with the same name on singe disk! Again, it can be fixed by detecting collisions and adding extra data, like counters or GUID or whatever, but... What gonna OMV do if OMV's user has two drives with partitions having same labels, and then he hotplugs one disk, or another, or both in any order? Is it race condition now? Is it okay for OMV to have race condition? Seems whatever use cases Salt imagined for them here is very different from what OMV users might face. Output from Linux's mount. Spaces are NOT escaped there! Dunno how it is done on BSD/Darwin. And then we have this."
technical,"it is sad how fast you were to say Salt is all wrong and how protective you fet about OMV. You still try to push Salt to adhere to OMV data format, while common sense says it should be otherwise. Salt users would not suffer from it. OMV users would. Demanding PR from OMV users like me is funny when you did not make any PR to Salt, or maybe i am wrong and you did.  So, back to this: The intention was and is to make OMV work with disks users insert.Without forcing them to go ssh sudo. So simple. You make it look that making OMV ""just work"" is bad goal. Because that woul be consistent with bash/Salt data format. But i alreeady said it was kneejerk impulse, so you eems to be crashing through door wide open. Some we are on the same page here. You blaze of ego is called for.  Since eysterday i was asking you to show me at the seams between OMV and Salt, the exact borderleines, didn't i? I am glad you seem to did so above. And when i showed those links, i commented upon them. Salt is based on Python, not PHP. The code you're ranting about never runs in the Salt context. I never said so. Both Salt and OMV are ""black boxes"" with some data exchange. And i was asking you to point me to the raw places of exchange and raw data being exchanged, didn't i? Yesterday i spent hours looking into Salt code and patching it along your suggestions. First i took your suggestions as correct and thought through. And just followed them. An then had to undo it all. Now you imply it was your time wasted not mine.  That Example SLS: - many times from yesterday i asked you how can i reproduce this activity from bash command line. You kind of answered by showing PHP code for SLS generation - after many requests and hours. But you still not answered how to trigger that action from bash. I asked you yesterday how to make OMV code trigger that action of Salt, allegedly buggy Salt. And you refused to help me doing it. What can i patch in OMV5 to make this notification gone? Why do you want to know that? What do you expect to improve? You made me look into Linux kernel i am not familiar with, at the same time you are not very willing to point me to specific OMV code and Salt commands you are familiar with. You are blocking any attempt to debug OMV and Salt interaction - and you demand perfectly polished PRs. It is not consistent. And it is would not help anyone. Not me, not you, not OMV users. Please consider this fix to Salt above. That is a clear bug in Salt that can be reproduced on Linux box (and probably on other UNIX-likes) independently on OMV"
technical,"Please consider this fix to Salt above. That is a clear bug in Salt that can be reproduced on Linux box (and probably on other UNIX-likes) independently on OMV Please stop blaming and ranting me. This raised issue here has nothing to do with OMV. please set this issue to read-only, I had to do the same on the OMV issues to stop these rants."
technical,"Output from Linux's mount. Spaces are NOT escaped there! Dunno how it is done on BSD/Darwin. And then we have this. re: escaping names for calling mount - i think that is what was intended to do so. but that was only called when from then Extended is set to True, if ever. And similar code inside. So it seems Salt prefers to keep space-containing names mangled, but mangled differently. So, no escaping when calling mount or umount is needed. I am not even sure that de-escaping mount point in Salt would be correct way to go. There can be a point: since that module serves as abstraction layer and should hide UNIX-likes peculiarities from generic Salt modules, all IDs better be unmangled. But not sure. However IF to do this de-mangling, then quoting arguments for calling mount becomes required indeed. But anyway, this line i believe  should not had ended in /etc/fstab and whoever added it was at fault..."
technical,"But i really am worried about potential race conditions in OMV when different partitions would have same label... IF you use label as ""primary key"" as persistent ID for all the other settings (user rights, sharing folders, etc), it might be quite a gotcha... Some of the ""deep changes"" i mentioned above. I now think those are dead end, but just in case they would be useful to someone, maybe even us later. Using ""Raw"" non-escaped string would probably be more proper design, but might really require deep refactoring of many Salt modules and then testing on many different systems. Horror.. one cannot trust mount with space-containing path at least on Linux -"
technical,"As you can see i am talking about your docs. So, i am not interested running around the world And there is nothing to explain on *manipulating the request*. It contains of headers and data. I want to modify it. Like thousands of tools are doing.  The same for **raw response as string/byte[]** - thats clear, too."
technical,"whats the problem to answer this question? Thats an issue for help me writing my app. Thats a topic in your issues As you can see i am talking about your docs. So, i am not interested running around the world"
technical,LOL - you simply closed my issue - where is your interest? Because you do not know the answer about the raw response in gitter?  Go home Because this is not the right place for your question. I tried to point you in the right direction as far as the information you provided allowed.  Please also check this.
technical,"Because this is not the right place for your question. I tried to point you in the right direction as far as the information you provided allowed.  Please also check this. i need to manipulate HTTP requests before they are sent, i want no manipulate the whole request (headers, data) and i think, all this could be possible starting here at the docs. Is it possible? I think.  But for getting **raw** response as string/byte[] to extract all needed details, where should i start? I am not sure. I want to work with the response like a normal response and i want to work besides with all headers of this response to check exactly whats in - not consumed already, only **raw response string/byte[]**  Can you give me the right starting points?"
technical," You can do in Flutter what dart:io can do (if not create an issue). Your issue should be filed in this, but better first check on StackOverflow if someone can answer your question. Being a bit more specific what exactly package http or dart:io don't allow you to do that ""manipulating complete request"" would allow you to do, and why you think you need that, would probably help to get a good answer."
technical,"I'm not sure what that difference might be. Nobody appreciates random/out of context quoting, straightforward answer avoidance, jumping to conclusions and whatnot. Well, some might be more tolerant to it than others but still,  Well, I appreciate they're appreciated but at the same time I don't really care. I did not contribute to get praise, I only contributed for the fun of it. And it stops being fun if I have to deal with nonsense. If I want to deal with nonsense I can probably find that pretty easily at work, there's no need for me to come here and ruin my evening. And to be clear: as far as I'm concerned this matter is closed. I have no idea why terrajobst felt the need to comment on it after more than a month."
technical,"Somehow I missed this when improving block op codegen. Or perhaps I thought it's not relevant because it's only useful when VEX is not available and that should be rare these days. Well, unless you're crossgening Seems like Microsoft.Diagnostics.Tracing.TraceEvent still hasn't gave up on its plan of taking over the world by copying itself thousands of times... Are there any penalties to using movups vs movdqu on modern CPUs? What about on older CPUs where VEX isn't available?"
technical,"What bypass delay? These are load/stores so they're handled by the load/store units and not by FP/int units. For example, on Core2:  The load/store unit is closely connected with the integer unit, so that there is no additional latency when transferring data between the integer unit and the load/store unit. There is a one clock latency when transferring data from memory (load unit) to the floating point unit, but there is no additional latency when transferring data from the floating point unit to memory (store unit).  On the Nehalem section (in regards to a store):  Replacing the last MOVDQA with MOVAPS has no influence on latencies, but it may have on future processors.  On the Sandy Bridge and Ivy Bridge pipeline:  There is only rarely a bypass delay when using the wrong type of move instruction, for example MOVAPS instead of MOVDQA.  etc..."
technical,"Mike you are a valued JIT contributor and I just thought that your last comment was a bit too harsh.  I didn't really think about it too much.  I was just trying to keep things peaceful here. I didn't read the comment (it's deleted). In general, for comments that are just a bit too harsh, we typically ""hide"" them instead of deleting them. It's a soft way of communicating that a comment included some text that could be offensive to someone, and then only people that are really curious see it, and everyone generally gets on with the topic. That's what we've done elsewhere and something to consider."
technical,"For example, on Core2:  The load/store unit is closely connected with the integer unit, so that there is no additional latency when transferring data between the integer unit and the load/store unit. There is a one clock latency when transferring data from memory (load unit) to the floating point unit, but there is no additional latency when transferring data from the floating point unit to memory (store unit).  On the Nehalem section (in regards to a store):  Replacing the last MOVDQA with MOVAPS has no influence on latencies, but it may have on future processors.  On the Sandy Bridge and Ivy Bridge pipeline:  There is only rarely a bypass delay when using the wrong type of move instruction, for example MOVAPS instead of MOVDQA.  etc... It doesn't look like something that is needed to be a consideration given that these are largely just for block copies and will generally be used with the integer pipeline in the end anyways. It was just something I was interested in as I understood their ""could"" be penalties in some scenarios."
technical,"Are there any penalties to using movups vs movdqu on modern CPUs? What about on older CPUs where VEX isn't available? Looks like Agner's indicates no penalty on newer processors, but on some older processors it can be an additional latency between 1-4 cycles: see bypass delay for various CPUs)."
technical," Somehow I missed this when improving block op codegen. Or perhaps I thought it's not relevant because it's only useful when VEX is not available and that should be rare these days. Well, unless you're crossgening Seems like Microsoft.Diagnostics.Tracing.TraceEvent still hasn't gave up on its plan of taking over the world by copying itself thousands of times..."
technical,"It doesn't look like something that is needed to be a consideration given that these are largely just for block copies and will generally be used with the integer pipeline in the end anyways. It was just something I was interested in as I understood their ""could"" be penalties in some scenarios. This seems reasonable to me, especially as it is most relevant to the crossgen scenario where I believe the size is a bigger issue than at JIT time. briansull for consideration of the PerfScore changes.  I take it you are on board with this change as well?"
technical,"It doesn't look like something that is needed to be a consideration given that these are largely just for block copies and will generally be used with the integer pipeline in the end anyways. It was just something I was interested in as I understood their ""could"" be penalties in some scenarios. What bypass delay? These are load/stores so they're handled by the load/store units and not by FP/int units."
technical,"Tanner acted in good faith by asking questions to the best of his understanding. Telling him that he doesn't know what he's talking about is unacceptable. First of all, it's not an insightful comment from your end. If you want other people to learn, you need to give them more information beyond ""you're wrong"". But more importantly it's just plain rude. You're basically sending him the signal of ""you're not welcome here"".  But instead of apologizing or even just letting that feedback sink in, you're now trying to turn the tables by saying you got victimized by Tanner because he asked you a technical question and that it's unreasonable from our end to ask you to be respectful in your responses to him.  You're more than welcome to continue to contribute to .NET. But I'll be clear: if we observe behavior like this in the future, we'll ultimately block you. We can't have other contributors stop engaging with us because of your behavior. While your contributions are valued, they don't warrant losing other contributors as collateral damage. When VEX encoding is not availbale, movups encoding is one byte shorter. With VEX the two instructions have same length encoding so we can just use movups all the time.  Also fix perf score latency for movups & co., it was incorrectly set higher than movdqu's latency."
technical,"If it is not possible to follow semver, could we come up with some sort of way to enable backwards compatibility / legacy behavior in our code? I am currently dealing with two major issues that I view as breaking changes:  # Consuming a library where the .d.ts files are compiled in TS version higher than my project.  Example: Using TS 2.0, consuming a project that exports an interface with a member that is of type object.  A few ideas on how to fix this: + Allow TypeScript to compile to a previous version's output. + Allow TypeScript to compile to multiple versions of types output, and then include a root file that routes the consuming compiler to the right definition using some sort of markup (comments, conditional compilation, etc.)  # Consuming a library where the .d.ts files are compiled in a TS version lower than my project.  Some examples include: All of these issues stem from stricter type checking introduced in a later version of TS. This could probably be mitigated by doing the following: 1. Having .d.ts files contian the compiler version via comments 2. Having the compiler check the imported code's compiler version, and only apply new/stricter rules if they existed in the compiled version. And what about a new package.json property : ""breaking"" which values can be ""MAJOR"" (default), ""MINOR""?"
technical,"Even with public apis, semver should never be applied. Version update is a way to note software changes, not software compatibility. Main versions for architectural or big updates. Minor versions for small changes and new feature additions. Patch versions for bug fixes. Either one can break compatibility. Semver is stupid focusing on compatibility, which is nothing to do with versions.  Compatibility should be maintained by developers and test cases.  npm is stupid in auto version updates which cause security issues and instability. The false assumption is originated from the idea semver bearing where versions should to be compatible within main version. Another principal should be never update until updates are fully tested."
technical,"Some respected libraries use semver, they always obey the semver rules and it is very useful for consumers.  If you are going to rant, go to twitter. Can the maintainers here please remove calidion's last comment?"
technical,"I mean, even semver's *own definition* of ""breaking change"" is arguably wrong. Minor updates can add new functionality under new properties, and new properties can break existing codepaths because JS is full of do-x-if-y-property-is-present patterns. Fixing a performance bug, which would in theory be a bugfix version update, could cause two async operations which previously always resolved in one order to instead always resolve in another order.  It is simply not the case that you can safely upgrade code, with semver *as used* today by normal package maintainers, from 34.1 of some library to 34.9 and be guaranteed that your program will still behave the same way.  What semver means *in practice* is that the major version bump is ""You will probably need to update your code in a lot of places"", and the minor version bump is ""You should always be OK for the most part"". *TypeScript never makes updates of the first kind*. We only make compat-breaking changes where we believe you should always be OK modulo a small number of fixes we think you'll be happy making (because we found new bugs).  We're not going to take a major version bump because there was a bug in the compiler where it failed to identify early errors, even though that's technically a breaking change - we think you should be ""along for the ride"" on that one if you didn't shrinkwrap. That's how semver is used *in practice* by everyone anyway. Can we at least get a section on ""TypeScript and Semver"" added to the docs? The fundamental problem is that npm install --save typescript will add ""typescript"":""2.4.0"" by default. Consumers need to be aware that this is dangerous and you need to change it to ""2.4.0"" (tilde, not carat). I'm happy to do the PR if you can advise on where you want such information.  But for what it's worth:   ""You will probably need to update your code in a lot of places"" ... TypeScript never makes updates of the first kind.  The Promise changes in 2.4 **is** resulting in lots of little changes all over the place. I'm not saying I don't agree with the changes, but they are there nonetheless. I've been caught only because I've wrongly assumed that TypeScript was following Semver."
technical,"This is one of the go to arguments against SemVer and it's a classic strawman. The very first point of the SemVer spec invalidates the argument here that ""semvers own definition is wrong. here's why:""   1. Software using Semantic Versioning MUST declare a public API. This API could be declared in the code itself or exist strictly in documentation. However it is done, it should be precise and comprehensive.  If your public API declared that two operations resolve in a given order then, yes, a performance fix is probably a breaking change. However I don't know any library that declares two public functions **and** declares that one is always fast than the other.  I agree if you don't have a public API then the SemVer is meaningless. However most of the issues with SemVer are a result of a problematic public API. Also Just because a change breaks a workflow does not mean it is a SemVer breaking change if the public API never documented that workflow. Even with public apis, semver should never be applied. Version update is a way to note software changes, not software compatibility. Main versions for architectural or big updates. Minor versions for small changes and new feature additions. Patch versions for bug fixes. Either one can break compatibility. Semver is stupid focusing on compatibility, which is nothing to do with versions.  Compatibility should be maintained by developers and test cases.  npm is stupid in auto version updates which cause security issues and instability. The false assumption is originated from the idea semver bearing where versions should to be compatible within main version."
technical,"And what about a new package.json property : ""breaking"" which values can be ""MAJOR"" (default), ""MINOR""? Fixing a performance bug, which would in theory be a bugfix version update, could cause two async operations which previously always resolved in one order to instead always resolve in another order.  If the resolution order is *documented*, then indeed it is breaking change and should bump major version, otherwise it is *implementation details* and user is responsible for depending on this."
technical,"It is semver's problem not typescript.  semver is the most misleading rule set ever.  npm's default rules make things worse.  as now npm Inc. is sold to Microsoft, maybe we can see further advance in npm. How is semver misleading?"
technical,"Personally, I think it is a good idea to always specify exact versions of critical stack components such as compilers, loaders, bundlers, and of course frameworks. There are not that many of these tools in a single project and they do not tend to release more than once a week or so. This makes explicitly upgrading a relatively straightforward process. Also, reading the changelogs for updates to such key dependencies is almost certainly something that one should be doing.  That said, I think it's fine to version more liberally. Each project is different in this regard. The trade-off for getting millions of dollars of engineering investment in the TypeScript project is that marketing gets to control version numbers to a certain extent.  That  is  a trade well worth making. Furthermore, TypeScript is by no means the only project that does this. I think any project that is high profile enough is likely subject to this, at least to some extent. Even if it is not the marketing department, it may be the maintainers' own self-consciousness that leads to such versioning. Any time we produced the wrong type or emitted the wrong code or failed to issue a correct error, that's a breaking change, and we fix dozens of bugs like that in every release.  TypeScript really releases at a blisteringly unprecedented pace for a programming language so I think this is somewhat inevitable. I also think it's common across almost all software. Minor versions of most software contain breaking changes, but they often go unnoticed. The more high-profile the project, the more users that has,  the more likely it is that this will be noticed.  The TypeScript team do an incredible job and they ship a wonderfully high quality product. I can't agree with aluanhaddad more. Personally, I think using language version 81 and browser version 127 is terrible. It looks ugly and these high numbers quickly become meaningless. In the browser case that's the intention - forcing consumers to update to the latest version. However, for a language it's out of place and makes following new features and important changes extremely hard. Every version, no matter how big or small, looks the same way as every other. Flow has fallen in that trap and it doesn't seem to reap many benefits out of it.  For TypeScript, if you still want automatic updates without worrying too much, just lock the minor version in and everything will fall into place."
technical,"I thought this was implicit, but now I have to say it explicitly: Comparisons to Hitler are neither necessary nor welcome here, and anyone unable to debate the merits of *versioning schemes* without relying on that as a rhetorical device should rethink whether their participation here is going to lead to a constructive conversation. I guess this can be considered as a bug as it is breaking my apps. If version 2.1.0 has breaking changes, why don't set it to 3.0.0?  Semantic versioning, right? Typescript lives and has important role in the nodejs ecosystem, therefore should follow some basic rules. We can not use the npm's semver features to block only major changes.  **Expected behavior:** Follow semantic versioning rules **Actual behavior:** Just follows marketing versioning rules can be related to issue created a year ago and closed without being solved"
technical,"I can't agree with aluanhaddad more. Personally, I think using language version 81 and browser version 127 is terrible. It looks ugly and these high numbers quickly become meaningless. In the browser case that's the intention - forcing consumers to update to the latest version. However, for a language it's out of place and makes following new features and important changes extremely hard. Every version, no matter how big or small, looks the same way as every other. Flow has fallen in that trap and it doesn't seem to reap many benefits out of it.  For TypeScript, if you still want automatic updates without worrying too much, just lock the minor version in and everything will fall into place. I mean, even semver's *own definition* of ""breaking change"" is arguably wrong. Minor updates can add new functionality under new properties, and new properties can break existing codepaths because JS is full of do-x-if-y-property-is-present patterns. Fixing a performance bug, which would in theory be a bugfix version update, could cause two async operations which previously always resolved in one order to instead always resolve in another order.  It is simply not the case that you can safely upgrade code, with semver *as used* today by normal package maintainers, from 34.1 of some library to 34.9 and be guaranteed that your program will still behave the same way.  What semver means *in practice* is that the major version bump is ""You will probably need to update your code in a lot of places"", and the minor version bump is ""You should always be OK for the most part"". *TypeScript never makes updates of the first kind*. We only make compat-breaking changes where we believe you should always be OK modulo a small number of fixes we think you'll be happy making (because we found new bugs).  We're not going to take a major version bump because there was a bug in the compiler where it failed to identify early errors, even though that's technically a breaking change - we think you should be ""along for the ride"" on that one if you didn't shrinkwrap. That's how semver is used *in practice* by everyone anyway."
technical,"No one can do that.  No one can 100% predict the process of the software development.  Software development is more of a way of exploring than a process of promise keeping. It is good to bear *Design by Contract* in mind and not to *Programming by Coincidence*. But the real life is not functioning that way. You even can't promise that your code is 100% correct. Why you dear to say your updates are 100% compatible when you released a version?  We should respect *Design by Contract* and *Pragmatic Programmers*. And we should also know that we are human beings, who make mistakes and break promises. It is the fact and reality. We lives in reality not in illusion or theory.  The promise semver wants to keep breaks everyday and you still not wake up. Why?  Do you really know the difference between angular 2, 3, 4 and 5? Do you really care? Why you care about the compatibility ? or you are simply driven by those people who propagating semver? Why should update be compatible while you even don't write test cases? Why should you update when no test is made ? Is that a pragmatic way?  Never lie to yourself.  To keep compatibility is a very good practice, but to assume compatibility is a very stupid thought.  For major, minor, patch, all versions should try to keep compatibility. For developers, should never trust compatibility. I think we are misunderstanding each other.  I have never claimed that *I can promise code is 100% correct*. However, a library author can aim at testing all the features exposed to consumers.  However, it doesn't mean *there is 0 chance a minor upgrade will break the contract entitled by its authors*. Semantic versioning is, from a producer perspective, about expressing the contract and its retro-compatibility. Patches are here to correct a mismatch between the contract and its implementation.  It's **not**, from a consumer perspective, about neutralizing the risk of upgrading. I agree with you about testing upgrades, of course. Even better : put those tests in a pull request to library authors. If a test fails, they'll fix it and upgrade their patch version.  Your argument is similar to refuting the utility of *Laws* because we can't promise it will be applied fairly to everybody. Do you have in mind a better system to address, **from a producer perspective**, the contract and retro-compatibility issues in library development? I don't, and that is why I find compliance with semantic versioning great.  The flaw in npm use of semantic versioning has been addressed with lockfiles."
technical,"I'd like to add a related question: should type definition files be compatible across all of 2.x.x? Can someone compile their library in 2.2, and have it work when someone pulls it in and compiles with 2.1? I too would prefer SemVer, and yes I know the majority of publishers are not ""doing it right""...But lets look at an earlier comment:  ""If we followed semver rules exactly, literally every single release would be a major version bump. Any time we produced the wrong type or emitted the wrong code or failed to issue a correct error, that's a breaking change, and we fix dozens of bugs like that in every release. ""  Writing quality software is hard and requires investment. The above can be mitigated with improved testing and documentation.  Remember the Agile principle: ""the art of maximizing the amount of work not done--is essential. "". This should *not* mean the amount of work done by a team to get something out the door. Rather it should be a global optimization to minimize the work required by all stakeholders globally and across type - the TOTAL work."
technical,"Can we at least get a section on ""TypeScript and Semver"" added to the docs? The fundamental problem is that npm install --save typescript will add ""typescript"":""2.4.0"" by default. Consumers need to be aware that this is dangerous and you need to change it to ""2.4.0"" (tilde, not carat). I'm happy to do the PR if you can advise on where you want such information.  But for what it's worth:   ""You will probably need to update your code in a lot of places"" ... TypeScript never makes updates of the first kind.  The Promise changes in 2.4 **is** resulting in lots of little changes all over the place. I'm not saying I don't agree with the changes, but they are there nonetheless. I've been caught only because I've wrongly assumed that TypeScript was following Semver. I'd like to add a related question: should type definition files be compatible across all of 2.x.x? Can someone compile their library in 2.2, and have it work when someone pulls it in and compiles with 2.1?"
technical,"NPM should simply allow for descriptive marketing versions as a forth group. Then we'd have the best of both worlds, i.e You would simply skip the marketing version while installing, i.e. npm install typescript34 since it would hold no semantic meaning, i.e. bumping marketing wouldn't reset the major counter. I'm concerned that not following semver is creating unnecessary friction for TypeScript consumers who are opted in to having their builds broken whenever TypeScript releases a minor version as npm locks down to only major versions by default. The Microsoft Edge team has figured out how to do their marketing despite bumping the major version a few times a month (currently up to v38), I think TypeScript should give serious consideration to doing the same for the good of its consumers."
technical,"I too would prefer SemVer, and yes I know the majority of publishers are not ""doing it right""...But lets look at an earlier comment:  ""If we followed semver rules exactly, literally every single release would be a major version bump. Any time we produced the wrong type or emitted the wrong code or failed to issue a correct error, that's a breaking change, and we fix dozens of bugs like that in every release. ""  Writing quality software is hard and requires investment. The above can be mitigated with improved testing and documentation.  Remember the Agile principle: ""the art of maximizing the amount of work not done--is essential. "". This should *not* mean the amount of work done by a team to get something out the door. Rather it should be a global optimization to minimize the work required by all stakeholders globally and across type - the TOTAL work. If it is not possible to follow semver, could we come up with some sort of way to enable backwards compatibility / legacy behavior in our code? I am currently dealing with two major issues that I view as breaking changes:  # Consuming a library where the .d.ts files are compiled in TS version higher than my project.  Example: Using TS 2.0, consuming a project that exports an interface with a member that is of type object.  A few ideas on how to fix this: + Allow TypeScript to compile to a previous version's output. + Allow TypeScript to compile to multiple versions of types output, and then include a root file that routes the consuming compiler to the right definition using some sort of markup (comments, conditional compilation, etc.)  # Consuming a library where the .d.ts files are compiled in a TS version lower than my project.  Some examples include: All of these issues stem from stricter type checking introduced in a later version of TS. This could probably be mitigated by doing the following: 1. Having .d.ts files contian the compiler version via comments 2. Having the compiler check the imported code's compiler version, and only apply new/stricter rules if they existed in the compiled version."
technical,"I think we are misunderstanding each other.  I have never claimed that *I can promise code is 100% correct*. However, a library author can aim at testing all the features exposed to consumers.  However, it doesn't mean *there is 0 chance a minor upgrade will break the contract entitled by its authors*. Semantic versioning is, from a producer perspective, about expressing the contract and its retro-compatibility. Patches are here to correct a mismatch between the contract and its implementation.  It's **not**, from a consumer perspective, about neutralizing the risk of upgrading. I agree with you about testing upgrades, of course. Even better : put those tests in a pull request to library authors. If a test fails, they'll fix it and upgrade their patch version.  Your argument is similar to refuting the utility of *Laws* because we can't promise it will be applied fairly to everybody. Do you have in mind a better system to address, **from a producer perspective**, the contract and retro-compatibility issues in library development? I don't, and that is why I find compliance with semantic versioning great.  The flaw in npm use of semantic versioning has been addressed with lockfiles. If SemVer is to be of value only to the producer [i.e. the consumer can not base their actions reliability upon it] then what is the value of it even being exposed to the consumer????  Yes, I agree it is about contracts, but this leads to the discussion of what exactly is a contract that provides the value necessary to truly be useful to the consumer....  In my experience there are multiple orders of magnitude between what is typically produced as a contract [by the publisher] and what is needed by the consumer to determine the impact on their usage."
technical,"TypeScript not following semver has some serious negative implications in an ecosystem that builds a lot of features around the assumption that packages  do  follow semver.  Since npm installs libraries with a default version range that allows for newer minor versions, the command npm i typescript  will  install an unstable and occaisionally breaking version of TypeScript. Collaborators on a project may get different versions when they install, and may get different sets of compiler errors. Fixing them for one developer may break things for another. Yes, projects should override the npm default, but they have to understand TypeScript's unusual versioning scheme in order to know to do so.  Even worse, an upgrade to a new minor version of TypeScript may break users of a project. We don't have any indication through version numbers of which .d.ts files are compatible with which versions of TypeScript. A project that implicitly upgrades TypeScript to a breaking version due to breaks occurring at minor releases, builds, and publishes a new version with newer declaration files, will break consumers using an older version of TypeScript. Some users can't even choose their version due to the compiler being integrated into their build system / runtime (Angular, Storybook, TS-Node).  By adopting semver TypeScript would be a much better citizen in the npm ecosystem and reduce unintended breaks. It is semver's problem not typescript.  semver is the most misleading rule set ever.  npm's default rules make things worse.  as now npm Inc. is sold to Microsoft, maybe we can see further advance in npm."
technical,"So developers should test all the API surface they consume ? That is a big field. In my opinion, they should test all the undocumented **assumptions** they are making on third party APIs. And of course, unexposed callpoints (classes and methods) they might use or inherit.   Fixing a performance bug, which would in theory be a bugfix version update, could cause two async operations which previously always resolved in one order to instead always resolve in another order.  If you are assuming the order in which those async tasks resolve, you are *Programming by Coincidence* (The Pragmatic Programmer, Ch. 6). This is a major flaw in your development process. **Breaking** in SemVer means **breaking the contract**, not your program. If your program doesn't fulfil its contract with an API, minor changes might well break it (*Design by Contract*). No one can do that.  No one can 100% predict the process of the software development.  Software development is more of a way of exploring than a process of promise keeping. It is good to bear *Design by Contract* in mind and not to *Programming by Coincidence*. But the real life is not functioning that way. You even can't promise that your code is 100% correct. Why you dear to say your updates are 100% compatible when you released a version?  We should respect *Design by Contract* and *Pragmatic Programmers*. And we should also know that we are human beings, who make mistakes and break promises. It is the fact and reality. We lives in reality not in illusion or theory.  The promise semver wants to keep breaks everyday and you still not wake up. Why?  Do you really know the difference between angular 2, 3, 4 and 5? Do you really care? Why you care about the compatibility ? or you are simply driven by those people who propagating semver? Why should update be compatible while you even don't write test cases? Why should you update when no test is made ? Is that a pragmatic way?  Never lie to yourself.  To keep compatibility is a very good practice, but to assume compatibility is a very stupid thought.  For major, minor, patch, all versions should try to keep compatibility. For developers, should never trust compatibility."
technical,"The trade-off for getting millions of dollars of engineering investment in the TypeScript project is that marketing gets to control version numbers to a certain extent.  It's not really an unalloyed good anyway. If we followed semver rules exactly, literally every single release would be a major version bump. Any time we produced the wrong type or emitted the wrong code or failed to issue a correct error, that's a breaking change, and we fix dozens of bugs like that in every release. The middle digit just isn't useful for TypeScript in a strict semver interpretation. NPM should simply allow for descriptive marketing versions as a forth group. Then we'd have the best of both worlds, i.e You would simply skip the marketing version while installing, i.e. npm install typescript34 since it would hold no semantic meaning, i.e. bumping marketing wouldn't reset the major counter."
technical,"You move from **assumptions**,  **contracts** to **communicating**. But you still don't realize the fact.  Science and technologies draw principals from facts not from thinking, from what it is instead of what it should be. I think computer should based on decimal but it is binary. I think programs should be bug free but it will never be. If the fact is ignored,  what is the value of finding communicating methods? Semver bears a good will, but it will never be a realistic one. A false good will always lead to a bad result. Npm's auto update has shown the downside and now lock fill has be introduced for remedy.  I don't know why you still insist on use semver, what benefits do you gain on applying semver?  Get sky high version number? Make versions meaningless? Upgrade major version on every small change that breaks? Auto updates that makes software unstable?  I think there must be a list of gains if semver should be adopted.   I apologize for using stupid if it is offensive.  Software quality will not be improve by merely defining very strict rules. Perhaps the language barrier has an incidence, but I don't think you really grasp what I am attempting to express... If you want to continue this discussion, please drop a mail (see my profile to get the address)."
technical,"I'm concerned that not following semver is creating unnecessary friction for TypeScript consumers who are opted in to having their builds broken whenever TypeScript releases a minor version as npm locks down to only major versions by default. The Microsoft Edge team has figured out how to do their marketing despite bumping the major version a few times a month (currently up to v38), I think TypeScript should give serious consideration to doing the same for the good of its consumers. Personally, I think it is a good idea to always specify exact versions of critical stack components such as compilers, loaders, bundlers, and of course frameworks. There are not that many of these tools in a single project and they do not tend to release more than once a week or so. This makes explicitly upgrading a relatively straightforward process. Also, reading the changelogs for updates to such key dependencies is almost certainly something that one should be doing.  That said, I think it's fine to version more liberally. Each project is different in this regard. The trade-off for getting millions of dollars of engineering investment in the TypeScript project is that marketing gets to control version numbers to a certain extent.  That  is  a trade well worth making. Furthermore, TypeScript is by no means the only project that does this. I think any project that is high profile enough is likely subject to this, at least to some extent. Even if it is not the marketing department, it may be the maintainers' own self-consciousness that leads to such versioning. Any time we produced the wrong type or emitted the wrong code or failed to issue a correct error, that's a breaking change, and we fix dozens of bugs like that in every release.  TypeScript really releases at a blisteringly unprecedented pace for a programming language so I think this is somewhat inevitable. I also think it's common across almost all software. Minor versions of most software contain breaking changes, but they often go unnoticed. The more high-profile the project, the more users that has,  the more likely it is that this will be noticed.  The TypeScript team do an incredible job and they ship a wonderfully high quality product."
technical,"Semver is like some idiots claim that all human are straight,  never know of LGBT. Rules are correct when they reflect the reality and wrong when they betray the reality. The process of software development is far more complicated than just promise your packages to be consistent or not.  Breaks happen inevitably what ever your maturity is. There are even architectural mistakes constantly happening in big projects by big companies.  Let alone some small software development teams. It is totally wrong software development rules based on assumption not on reality.  Software development process is more engineering than theoretical. as npm and semver has been ruining nodejs's stability for years. It is time to stop using semver and remove npm 's version range for stablity's sake."
technical,"How is semver misleading? Semver is like some idiots claim that all human are straight,  never know of LGBT."
technical,"Another principal should be never update until updates are fully tested. So developers should test all the API surface they consume ? That is a big field. In my opinion, they should test all the undocumented **assumptions** they are making on third party APIs. And of course, unexposed callpoints (classes and methods) they might use or inherit.   Fixing a performance bug, which would in theory be a bugfix version update, could cause two async operations which previously always resolved in one order to instead always resolve in another order.  If you are assuming the order in which those async tasks resolve, you are *Programming by Coincidence* (The Pragmatic Programmer, Ch. 6). This is a major flaw in your development process. **Breaking** in SemVer means **breaking the contract**, not your program. If your program doesn't fulfil its contract with an API, minor changes might well break it (*Design by Contract*)."
technical,"If SemVer is to be of value only to the producer [i.e. the consumer can not base their actions reliability upon it] then what is the value of it even being exposed to the consumer????  Yes, I agree it is about contracts, but this leads to the discussion of what exactly is a contract that provides the value necessary to truly be useful to the consumer....  In my experience there are multiple orders of magnitude between what is typically produced as a contract [by the publisher] and what is needed by the consumer to determine the impact on their usage. Software won't get matured by merely contracts. It is a much more complicated way. Like humans constantly violating laws,  software development will surely break contracts. So it is stupid to believe in contracts.  There are police stations and courts for law violations. There are no such things for software development and publishing.  No one wants to breaks compatibility, but it is inevitable even when semver is applied. It's in fact not about contracts, It is about evolution which cannot be predicted precisely. Another wrong believe is that updated should always be executed after software updates. Nodejs has a service called green keeper which strengthens that wrong believe. Software quality is nothing to do with updates, newer updates maybe worse and mass things up.  so updates should be applied when they are tested extensively not because they are newer. programmers should be selective to one stable version.  For example Ubuntu 16.04 is more stable than 17.04, 16.10, and even 18.04. Win 7 is more popular than win 8, win 10.  There are in fact no contracts, there are just releases."
technical,"TypeScript never claimed to follow semantic versioning, in the sense that breaking changes imply major versions.  TypeScript, however, promises no breaking changes after a stable release. so no breaking changes between 2.1.5 and 2.1.6, 2.1.*.  My recommendation is fix your version of typescript to major.minor instead of just major. e.g. 2.1 and not 2 Thanks for the early response mhegazy . As you mentioned, blocking the versions is easy to fix in my projects (I was being a bit ironic.). However, as a developer i don't feel comfortable having to memorise a different set of rules for each package as the number of packages used grows very quickly. I guess having typescript follow semantic versioning would be a nice to have feature."
technical,"Rules are correct when they reflect the reality and wrong when they betray the reality. The process of software development is far more complicated than just promise your packages to be consistent or not.  Breaks happen inevitably what ever your maturity is. There are even architectural mistakes constantly happening in big projects by big companies.  Let alone some small software development teams. It is totally wrong software development rules based on assumption not on reality.  Software development process is more engineering than theoretical. as npm and semver has been ruining nodejs's stability for years. It is time to stop using semver and remove npm 's version range for stablity's sake. That... that's literally semver.  TS folks, you make an awesome tool and you can do what you want and all, but this all really feels like a subjective justification that betrays the consumer. So what if you're on version 100, at least it means something to tools."
technical,"Thanks for the early response mhegazy . As you mentioned, blocking the versions is easy to fix in my projects (I was being a bit ironic.). However, as a developer i don't feel comfortable having to memorise a different set of rules for each package as the number of packages used grows very quickly. I guess having typescript follow semantic versioning would be a nice to have feature. The trade-off for getting millions of dollars of engineering investment in the TypeScript project is that marketing gets to control version numbers to a certain extent.  It's not really an unalloyed good anyway. If we followed semver rules exactly, literally every single release would be a major version bump. Any time we produced the wrong type or emitted the wrong code or failed to issue a correct error, that's a breaking change, and we fix dozens of bugs like that in every release. The middle digit just isn't useful for TypeScript in a strict semver interpretation."
technical,"You have my full support. Semver is evil. And should be rejected everywhere. Typescript is a very good example. This is one of the go to arguments against SemVer and it's a classic strawman. The very first point of the SemVer spec invalidates the argument here that ""semvers own definition is wrong. here's why:""   1. Software using Semantic Versioning MUST declare a public API. This API could be declared in the code itself or exist strictly in documentation. However it is done, it should be precise and comprehensive.  If your public API declared that two operations resolve in a given order then, yes, a performance fix is probably a breaking change. However I don't know any library that declares two public functions **and** declares that one is always fast than the other.  I agree if you don't have a public API then the SemVer is meaningless. However most of the issues with SemVer are a result of a problematic public API. Also Just because a change breaks a workflow does not mean it is a SemVer breaking change if the public API never documented that workflow."
technical," TypeScript never claimed to follow semantic versioning, in the sense that breaking changes imply major versions.  TypeScript, however, promises no breaking changes after a stable release. so no breaking changes between 2.1.5 and 2.1.6, 2.1.*.  My recommendation is fix your version of typescript to major.minor instead of just major. e.g. 2.1 and not 2"
technical,"Perhaps the language barrier has an incidence, but I don't think you really grasp what I am attempting to express... If you want to continue this discussion, please drop a mail (see my profile to get the address). TypeScript not following semver has some serious negative implications in an ecosystem that builds a lot of features around the assumption that packages  do  follow semver.  Since npm installs libraries with a default version range that allows for newer minor versions, the command npm i typescript  will  install an unstable and occaisionally breaking version of TypeScript. Collaborators on a project may get different versions when they install, and may get different sets of compiler errors. Fixing them for one developer may break things for another. Yes, projects should override the npm default, but they have to understand TypeScript's unusual versioning scheme in order to know to do so.  Even worse, an upgrade to a new minor version of TypeScript may break users of a project. We don't have any indication through version numbers of which .d.ts files are compatible with which versions of TypeScript. A project that implicitly upgrades TypeScript to a breaking version due to breaks occurring at minor releases, builds, and publishes a new version with newer declaration files, will break consumers using an older version of TypeScript. Some users can't even choose their version due to the compiler being integrated into their build system / runtime (Angular, Storybook, TS-Node).  By adopting semver TypeScript would be a much better citizen in the npm ecosystem and reduce unintended breaks."
technical,"That... that's literally semver.  TS folks, you make an awesome tool and you can do what you want and all, but this all really feels like a subjective justification that betrays the consumer. So what if you're on version 100, at least it means something to tools. versions should focus mainly on their functionalities and should try to keep compatibilities.  major versions for architectural changes. minor versions for feature changes within the same architecture. patch versions for bug fixes (regardless of their compatibilities)  the reality is that npm's introduced dependency hell due to semver and npm's version range.  maybe semver is more innocent than npm's version range.  But they are both wrong, they are now chained to amplify the incorrectness.  And they constantly claim that package maintainers violate their rules.  It is one of the most funny things ever happened in software development :)"
technical,"Fixing a performance bug, which would in theory be a bugfix version update, could cause two async operations which previously always resolved in one order to instead always resolve in another order.  If the resolution order is *documented*, then indeed it is breaking change and should bump major version, otherwise it is *implementation details* and user is responsible for depending on this. You have my full support. Semver is evil. And should be rejected everywhere. Typescript is a very good example."
technical,"Jupyter is both an editor and a set of APIs that we use under the covers to talk to the same backend that the Jupyter editor uses. And when I said you need Jupyter installed, you specifically need the jupyter python module and its dependencies.   pip install jupyter   should do it."
technical,"I feel two people with oppositing opinions can sound more like a duel than a discussion.  I am surprised that asking to include a few features in the next update is met with such resistance.  REPL should have editor + console. It's standard and expected.  The Jupyter extension looks just..Jupyter. All the white spaces between lines. You can even x out the code chunk in console (see my picture). Why? We are testing and debugging code. We really don't care we had a piece of wrong code.  The Jupyter extension looks more like a repackage Jupyter wanting to be the Python extension Anyways, I think you get my point and what I am requesting.  If you refuse to, then you do. I will let others be the judge."
technical,"I closed this bug because I believed the window we have now would meet your needs.  It supports - left+shift etc. Its editor is a full blown editor and not the weird terminal one line thing - interrupt button (ctrl+c actually copies in the other repl). - You can change the cursor. Actually you can change this already in the regular REPL. It's a setting (interactive terminal cursor I think)  We're not investing any effort into changing the VS code terminal, so I don't believe we can actually meet your original request.  If the new window doesn't work for you, can you explain why? At some point the new 'Python Interactive' window will likely become the default REPL for all python code.  So knowing why doing this doesn't work for people would be great."
technical,I think you have a legitimate concern and I'm sorry if you feel like I'm ignoring you.  We really just want to do what it is users want.  Of course there's time constraints on all of the work we do.  So trying to meet user's expectations while also actually shipping stuff is what we attempt to do. How can I get to that setup? It looks like what I would need if I can run code fragments in my .py file with a shortcut (like shift+enter in terminal). So far I followed the links you provided and got to this stage I don't know how to run code on the left with a shortcut in the interactive window though.
technical,"The marketplace is kinda confusing. The 'Jupyter' extension is not the owner of this window.  The 'Python Extension' owns the window you took a screenshot of.  Startup time is a problem we're trying to handle (all of it is starting the separate console in the background). I actually just tried it---the Jupyter extension. Boy, it was very slow. It hung for a few seconds while importing numpy, there was no response. Multi-line code run did not run smoothly. (see picture for error message)  Also, the Marketplace says the Jupyter extension is no longer being maintained!"
technical,"That window on the right behaves as a live REPL. You can type into it, you get a history of what's run, etc. I am just asking if the Python extension can be fixed up. The current REPL is Editor+Terminal combo. Python is invoked from Terminal.  Terminal interface is hard to work with. What I am asking is: 1. Highlight with left+shift keyboard combo actually works, not giving characters ,2D 2. Breaking from a loop using Ctrl+C does not exit Python 3. Aesthetic: have a cursor, not a bold black block  These are existing problems in the extension. Just include it in the next update.  BTW, why did you close the thread after my first post?"
technical,"I am just asking if the Python extension can be fixed up. The current REPL is Editor+Terminal combo. Python is invoked from Terminal.  Terminal interface is hard to work with. What I am asking is: 1. Highlight with left+shift keyboard combo actually works, not giving characters ,2D 2. Breaking from a loop using Ctrl+C does not exit Python 3. Aesthetic: have a cursor, not a bold black block  These are existing problems in the extension. Just include it in the next update.  BTW, why did you close the thread after my first post? I closed this bug because I believed the window we have now would meet your needs.  It supports - left+shift etc. Its editor is a full blown editor and not the weird terminal one line thing - interrupt button (ctrl+c actually copies in the other repl). - You can change the cursor. Actually you can change this already in the regular REPL. It's a setting (interactive terminal cursor I think)  We're not investing any effort into changing the VS code terminal, so I don't believe we can actually meet your original request.  If the new window doesn't work for you, can you explain why?"
technical,"With regards to the regular terminal, are you saying you wouldn't like if the default REPL became this new window? I feel two people with oppositing opinions can sound more like a duel than a discussion.  I am surprised that asking to include a few features in the next update is met with such resistance.  REPL should have editor + console. It's standard and expected.  The Jupyter extension looks just..Jupyter. All the white spaces between lines. You can even x out the code chunk in console (see my picture). Why? We are testing and debugging code. We really don't care we had a piece of wrong code.  The Jupyter extension looks more like a repackage Jupyter wanting to be the Python extension"
technical,"I think it's quite stubborn to say you know what your users want.  It's also quite naive to tell users they must it your way. They will abandon your product.  I have Jupyter, it's not for serious programming. Also, I have Jupyter Notebook. It's works very nice, have you tried it? I think you're misunderstanding my intent.  We have this: I'm suggesting this UI will meet your needs.  If this doesn't meet your needs, are you willing to go into more depth on what you want?"
technical,"The error you have there is what an exception looks like. The code you ran must have had a relative path in it. I wouldn't close the ticket so quickly. A lot people were also frustrated, you should let them talk!  It's probably faster to fix Python extension by adding few keyboard combos, rather fixing Jupyter with startup problems. But then again, you believe you know what we want better than what we tell you we actually want."
technical,"So you don't like the look of it. It's too Jupyterish?  Modifying the current REPL is a large work item. We don't own it. VS Code proper does. If you believe looks over functionality, then the Jupyter extension wins.  VIM and Emacs have no looks. You can only go so far with what's on the outside."
technical," If you have jupyter installed, you can use our ipython based window instead: I believe it does everything you just asked for."
technical,"Sorry Jupyter is overloaded here. We are using jupyter to run an IPython console. That doc link talks about if you're using Jupyter already, you can now use VS Code. The initial audience was datascientists.  However it's really just an IPython console. Which is a REPL. Jupyter is both an editor and a set of APIs that we use under the covers to talk to the same backend that the Jupyter editor uses."
technical,"If you have jupyter installed, you can use our ipython based window instead: I believe it does everything you just asked for. Jupyter is not an IDE! It looks pretty, but pretty useless. Cells in Jupyter are pointless, it's really for a company presentation. It's not for serious code development.  Anaconda comes with Jupyter Notebook.  It's quicker and better. No need to download VS Code for it."
technical,"Actually, please don't fix it. I am not paying you. And you believe you have the best idea, and that my request is too much.  Have a good day! No I don't believe I have the best idea. If that was the case I would have ignored your request. Why else am I asking for feedback?  Oh actually you also mentioned Matlab or R. Their REPL is what you're asking for?"
technical,"Anyways, I think you get my point and what I am requesting.  If you refuse to, then you do. I will let others be the judge. So you don't like the look of it. It's too Jupyterish?  Modifying the current REPL is a large work item. We don't own it. VS Code proper does."
technical,"Jupyter is not an IDE! It looks pretty, but pretty useless. Cells in Jupyter are pointless, it's really for a company presentation. It's not for serious code development.  Anaconda comes with Jupyter Notebook.  It's quicker and better. No need to download VS Code for it. Sorry Jupyter is overloaded here. We are using jupyter to run an IPython console. That doc link talks about if you're using Jupyter already, you can now use VS Code. The initial audience was datascientists.  However it's really just an IPython console. Which is a REPL."
technical,"I think you're misunderstanding my intent.  We have this: I'm suggesting this UI will meet your needs.  If this doesn't meet your needs, are you willing to go into more depth on what you want? That window on the right behaves as a live REPL. You can type into it, you get a history of what's run, etc."
technical,"I actually just tried it---the Jupyter extension. Boy, it was very slow. It hung for a few seconds while importing numpy, there was no response. Multi-line code run did not run smoothly. (see picture for error message)  Also, the Marketplace says the Jupyter extension is no longer being maintained! The error you have there is what an exception looks like. The code you ran must have had a relative path in it."
technical,"I wouldn't close the ticket so quickly. A lot people were also frustrated, you should let them talk!  It's probably faster to fix Python extension by adding few keyboard combos, rather fixing Jupyter with startup problems. But then again, you believe you know what we want better than what we tell you we actually want. The impression I'm getting is that closing a ticket is the same as ignoring your feedback."
technical,At some point the new 'Python Interactive' window will likely become the default REPL for all python code.  So knowing why doing this doesn't work for people would be great. The marketplace is kinda confusing. The 'Jupyter' extension is not the owner of this window.  The 'Python Extension' owns the window you took a screenshot of.  Startup time is a problem we're trying to handle (all of it is starting the separate console in the background).
technical,"The impression I'm getting is that closing a ticket is the same as ignoring your feedback. With regards to the regular terminal, are you saying you wouldn't like if the default REPL became this new window?"
technical,How can I get to that setup? It looks like what I would need if I can run code fragments in my .py file with a shortcut (like shift+enter in terminal). So far I followed the links you provided and got to this stage I don't know how to run code on the left with a shortcut in the interactive window though. you can do a couple of things to get code in a python file to run in the interactive window.  1. Add the # %% comment above a group of code you want to mark as a cell. Hit the 'Run Cell' link that appears after that.  Actually that seems to be the only one working at the moment. There's supposed to be a command to run the current selection in the interactive window. Thanks for asking as you helped us find a bug.
technical,"I don't believe you missed anything. adding the FILE FLAG BACKUP SEMANTICS flag only if its a directory. I also specifically use OPEN EXISTING, though you probably got that. to get a handle to the file/directory at the resolved path.  I agree that just letting the end-user provide ?\C:\long\paths without any extra handling from leveldb might be just fine, so long as  all  the Windows APIs used are the Unicode versions (and specifically listed as supporting long paths, though this is most of them). andschwa All the Window APIs are the Unicode versions. Currently, I use FILE FLAG BACKUP SEMANTICS for files and directories, but that is easily changed. As for Get Final Path Name By HandleW, unfortunately it somewhat raises a chicken/egg problem."
technical,"pwnall No worries, I understand that such fundamental choices do not lend themselves to casual decisions. Thank you for all the help. As a soon-to-be user of the leveldb Windows support (for Mesos), here are are my thoughts: For us, C++11 is fine, we already target this. This is an annoying problem, I had to fix this for Mesos. I went with: It also leads into long path issues on Windows. I took an approach similar to the one CMake took for their Windows port, a longpath helper to translate all paths as they reach WinAPI functions from UTF-8 to UTF-16 with ?\ prepended if necessary. My helper is here (note that the max path is  not  255 despite documentation).  I need to stress: native long path support is probably a must for most Windows projects nowadays. It's not terribly difficult to do, it's just  really annoying . Also, for your comment: I'm not sure I entirely agree, there is no data loss going from UTF-8 to UTF-16. Both encodings are  Unicode , it's just an implementation difference. Plus, you can do the conversion natively in C++11. Anyway, I've not had any problems on Windows having gone this route so far.  Thanks for your work! I personally know the trouble it is"
technical,"Hi, it's me again. I really need a Windows port to start experimenting with LevelDB. Is this the best port so far? I just need to hack something together before the official port lands. At this point, I'm fairly convinced that this topic won't benefit from external input until we land the Windows port. Locking so googlers can focus their limited time on landing the code.  I expect that locked conversations are frustrating to external contributors, and I'm sorry for that. I'm doing this because the subtler request above hasn't been effective."
technical,"We have no timeline for this, sorry. can I take the code in the PR, rebase to fix the merge conflicts, then add commits with the suggestions from ghemawat or that was already done by cmumford in?"
technical,"No worries about delays. Honestly, I'm backed up with other work for the next two of weeks, and the odds that I'll be able to look at this are very low. Everything has been patched, and I am adding extended file length support currently. andschwa, for the extended file length support, since the ""?\"" prefix effectively removes all path parsing, do you know if there's anything else I need to consider other than:  1. Relative paths (which cannot use the extended file length prefix). 2. /, ., or .. in paths (somewhat tricky, see below). For the . and .. operators, manually parsing them is somewhat tricky, since directory symbolic links may be in play. The only time-tested strategy for this is to iterate over all roots, parent directories, check to see if the item is a directory symbolic link or junction, get the real path of the directory if it is a symlink or junction, and then continue from there. This is because does not actually point to C:\leveldb-1\README.md if C:\leveldb is a symlink or junction.  This is very doable (and is relatively easy to implement), but it is fairly expensive since it requires filesystem calls. It requires a temporary vector to store each path component. Step-wise, the general approach is as follows:  1. Check if the path is absolute (skip remaining steps otherwise). 2. Replace all forward separators with backslashes. 3. Recurse over each parent directory, from the root (drive letter, such as C:, or UNC root, such as host-name\share-name), to the (and excluding the) file (we don't care if the file is a symlink, since relative path components cannot follow it). 4. If the directory basename is ., ignore the directory. 5. If the directory basename is .., remove the preceding directory component. 6. Otherwise, check if the directory is a symlink by calling CreateFile with the FILE FLAG OPEN REPARSE POINT and FILE FLAG BACKUP SEMANTICS flags. If the handle is successfully created, it's a symlink, otherwise, it is not. 7. If the directory is a symlink, read the proper path using DeviceIoControl with the FSCTL GET REPARSE POINT code, and reset the vector using the absolute new path.  I would be amenable to implementing this (I've done this before, as may be obvious due to the detail of my explanation on how to implement such functionality), but this may add a lot of expense for a feature that application developer should have to be aware of themselves (that is, leveldb will already support an extended length path, if provided by the end-user). Another major caveat is leveldb's filenames are short (the longest being the MANIFEST-00000X files, at 15 characters). Since the Windows documentation clearly states the maximum directory length must be MAX PATH - 12, even with the extended length path prefix, this seems like a lot of work for an added 3 characters. any thoughts? Should extended file length support be added, including with the caveats mentioned above?"
technical,"Thank you for the feedback. This would be very feasible to do ghemawat, especially if we use UTF-8 paths and just convert them in the Windows environment. As for CMake support, I will use that as a starting point (thank you). I will remove the mmap compatibility and use the raw WinAPI calls. Due to my other work, I should be able to finish this later this week. ghemawat and pwnall, a quick question: When I asked the C++11 features and limiting myself to C++03, did you mean limit the codebase to C++98 or C++11? I have a few situations where std::chrono is dramatically more convenient than other code, however, I can remove this (it's only for NowMicros and SleepForMicroseconds). Other than that, the port should not require any new features.  Thank you and I am effectively done with my port, other than this minor question."
technical,"If there's anything I can do, or anything others could do with my code to make Windows support land earlier, I'd be glad to help. In the meantime, I'm just glad Windows support seems to be approaching soon. Thanks. Great news. Thanks."
technical,"I think the ball is currently in our court. I need to find time to reconcile the various Windows PRs we've received with what we think this should look like. Hi -- no pressure, but is there any ETA for when Windows support will land? Thanks!"
technical,"Hi, I wanted to ask how your work is going? It's been about six months since your last messages, but I do not see any result. I need to build a library under Windows and I do not know how. Hi how are you today"
technical,"If FILE FLAG BACKUP SEMANTICS is working for both, don't let me tell you it's wrong! I was using Get Final Path Name By Handle specifically for resolution of symlinks, I see where it wouldn't quite work for you here. Perfecto. Hi, I wanted to ask how your work is going? It's been about six months since your last messages, but I do not see any result. I need to build a library under Windows and I do not know how."
technical,"Also: Yay, a million times, yay! We'll pull it into Mesos with ExternalProject Add. Hi, I've had a few issues I cannot currently debug. I will attempt to use the Boost-based ""windows"" branch as a reference-point in short order.  Specifically, I've had 3 major issues:  1. issue178 test fails intermittently. 10% of the time, it succeeds, without issue. 25% of the time, it produces a slightly lower number of keys than the 1.1m requires (almost always greater than 1.09m). The rest of the time, it produces the error Assertion Failed 2. db test fails intermittently (at about the same frequency) during the random read counter  section  3. Most severely though, however, is the multi-threaded section seems to produce the bad block type error consistently, which would defeat the entire purpose of a multi-thread access. Otherwise, all the test cases and benchmarks work. Just a heads up for the major delay.  (Items with strikethrough have been patched). EDIT: Everything has been patched."
technical,"Great news. Thanks. Hi, it's me again. I really need a Windows port to start experimenting with LevelDB. Is this the best port so far? I just need to hack something together before the official port lands."
technical,"Everything has been patched, and I am adding extended file length support currently. andschwa, for the extended file length support, since the ""?\"" prefix effectively removes all path parsing, do you know if there's anything else I need to consider other than:  1. Relative paths (which cannot use the extended file length prefix). 2. /, ., or .. in paths (somewhat tricky, see below). For the . and .. operators, manually parsing them is somewhat tricky, since directory symbolic links may be in play. The only time-tested strategy for this is to iterate over all roots, parent directories, check to see if the item is a directory symbolic link or junction, get the real path of the directory if it is a symlink or junction, and then continue from there. This is because does not actually point to C:\leveldb-1\README.md if C:\leveldb is a symlink or junction.  This is very doable (and is relatively easy to implement), but it is fairly expensive since it requires filesystem calls. It requires a temporary vector to store each path component. Step-wise, the general approach is as follows:  1. Check if the path is absolute (skip remaining steps otherwise). 2. Replace all forward separators with backslashes. 3. Recurse over each parent directory, from the root (drive letter, such as C:, or UNC root, such as host-name\share-name), to the (and excluding the) file (we don't care if the file is a symlink, since relative path components cannot follow it). 4. If the directory basename is ., ignore the directory. 5. If the directory basename is .., remove the preceding directory component. 6. Otherwise, check if the directory is a symlink by calling CreateFile with the FILE FLAG OPEN REPARSE POINT and FILE FLAG BACKUP SEMANTICS flags. If the handle is successfully created, it's a symlink, otherwise, it is not. 7. If the directory is a symlink, read the proper path using DeviceIoControl with the FSCTL GET REPARSE POINT code, and reset the vector using the absolute new path.  I would be amenable to implementing this (I've done this before, as may be obvious due to the detail of my explanation on how to implement such functionality), but this may add a lot of expense for a feature that application developer should have to be aware of themselves (that is, leveldb will already support an extended length path, if provided by the end-user). Another major caveat is leveldb's filenames are short (the longest being the MANIFEST-00000X files, at 15 characters). Since the Windows documentation clearly states the maximum directory length must be MAX PATH - 12, even with the extended length path prefix, this seems like a lot of work for an added 3 characters. any thoughts? Should extended file length support be added, including with the caveats mentioned above? I don't believe you missed anything. adding the FILE FLAG BACKUP SEMANTICS flag only if its a directory. I also specifically use OPEN EXISTING, though you probably got that. to get a handle to the file/directory at the resolved path.  I agree that just letting the end-user provide ?\C:\long\paths without any extra handling from leveldb might be just fine, so long as  all  the Windows APIs used are the Unicode versions (and specifically listed as supporting long paths, though this is most of them)."
technical,"Sorry, I got extremely busy with work and have submitted a few PRs to this extent but it still needs work. My Windows development PC just arrived after breaking in March, so I should be able to finish this soon. If you would to use this branch, it currently works on Windows: I think the ball is currently in our court. I need to find time to reconcile the various Windows PRs we've received with what we think this should look like."
technical,"andschwa All the Window APIs are the Unicode versions. Currently, I use FILE FLAG BACKUP SEMANTICS for files and directories, but that is easily changed. As for Get Final Path Name By HandleW, unfortunately it somewhat raises a chicken/egg problem. If FILE FLAG BACKUP SEMANTICS is working for both, don't let me tell you it's wrong! I was using Get Final Path Name By Handle specifically for resolution of symlinks, I see where it wouldn't quite work for you here. Perfecto."
technical,"Thank you very much for the offer! We have a change for the internal codebase, which will get published after it lands. The code change is very far into the review process. At this point, I don't think there's anything that external contributors can help us with. If there's anything I can do, or anything others could do with my code to make Windows support land earlier, I'd be glad to help. In the meantime, I'm just glad Windows support seems to be approaching soon. Thanks."
technical,"Hi, I've had a few issues I cannot currently debug. I will attempt to use the Boost-based ""windows"" branch as a reference-point in short order.  Specifically, I've had 3 major issues:  1. issue178 test fails intermittently. 10% of the time, it succeeds, without issue. 25% of the time, it produces a slightly lower number of keys than the 1.1m requires (almost always greater than 1.09m). The rest of the time, it produces the error Assertion Failed 2. db test fails intermittently (at about the same frequency) during the random read counter  section  3. Most severely though, however, is the multi-threaded section seems to produce the bad block type error consistently, which would defeat the entire purpose of a multi-thread access. Otherwise, all the test cases and benchmarks work. Just a heads up for the major delay.  (Items with strikethrough have been patched). EDIT: Everything has been patched. No worries about delays. Honestly, I'm backed up with other work for the next two of weeks, and the odds that I'll be able to look at this are very low."
technical,"At this point, I'm fairly convinced that this topic won't benefit from external input until we land the Windows port. Locking so googlers can focus their limited time on landing the code.  I expect that locked conversations are frustrating to external contributors, and I'm sorry for that. I'm doing this because the subtler request above hasn't been effective. Now, before you tell me this is a lot of work: I know, and am working on it (and almost done). Ideally, I would like to have my changes merged here, so I have a few questions and concerns for my current port.  # Questions  **Should I target a specific C++ standard?** Currently, my code depends on a few C++11 features, which can be easily removed with a few macros. This makes the code less readable, however, if C++03 support is desired, I will gladly change my implementation to conform to an older standard.  **How to handle Unicode filesystem support?** Currently, LevelDB uses char-based (narrow) strings for for all filesystem operations, which does not translate well for Windows systems (since narrow strings use the ANSI, or OEM legacy codepages, and not UTF-8, for backwards compatibility). This means paths using international characters, or emojis, are therefore not supported with a simple port, something I consider to be an undesirable solution for a modern library. All the current forks of levelDB do not solve this fundamental issue, leading me to create my own implementation. Possible solutions include: 1. A narrow (UTF-8) API on *Nix, and a wide (UTF-16) API on Windows, using a typedef to determine the proper path type. 2. Converting all narrow strings from UTF-8 to UTF-16 before calling WinAPI functions. 3. Providing both a narrow (ANSI) and wide (UTF-16) API on Windows. The 2nd option, although the least amount of work, is the least amenable for me since the expected encoding for paths from levelDB would then conflict with the entirety of the WinAPI. The 3rd option, however, duplicates code to support both the narrow and wide WinAPI, which would increase the amount of work required to maintain levelDB. The first option is a happy median: it minimizes redundancy and is consistent with expectations about *Nix and Windows paths. I am, however, amenable to any suggestions the levelDB authors may have.  **Intellectual Property** To emulate the behavior of mmap on Windows, I used a very lightweight library (<250 lines of code) from Steven Lee, mman-win32. However, looking over your contributor license agreement, it seems that my port would not satisfy Google's CLA until I remove this code from my implementation. If this is the case, I could easily use the raw WinAPI functions rather than the emulated mmap in my Windows port. Please notify me if I should remove this code prior to submitting a pull request.  # Other Changes  **CMake Build System**  I introduced a CMake build system, which retains most of the same logic as the existing Makefile. The existing Makefile has not been deprecated.  **AppVeyor Continual Integration**  To ensure builds do not break the Windows builds, I am planning to add an AppVeyor configuration, which allows continual integration on Windows using MSVC.  # Summary  If there is still interest for native Windows support, and the proposed changes are amenable to the levelDB authors, I would gladly submit a pull request."
technical,"We've recently decided that the next release will require C++11, so it's OK to use C++11. Sorry for the code churn on your end... this decision was not taken lightly. pwnall No worries, I understand that such fundamental choices do not lend themselves to casual decisions. Thank you for all the help."
technical,"We've recently decided that the next release will require C++11, so it's OK to use C++11. Sorry for the code churn on your end... this decision was not taken lightly. Sorry, I got extremely busy with work and have submitted a few PRs to this extent but it still needs work. My Windows development PC just arrived after breaking in March, so I should be able to finish this soon. If you would to use this branch, it currently works on Windows:"
technical,"Work on CMake support is already underway. Please use as a starting point, to avoid rework. Also, please use the Travis CI and AppVeyor configurations.  In general, I recommend following the approach taken by Chromium's LevelDB integration. Chromium builds (and runs) on Windows, and does not require modifications to the rest of the LevelDB. Thank you for the feedback. This would be very feasible to do ghemawat, especially if we use UTF-8 paths and just convert them in the Windows environment. As for CMake support, I will use that as a starting point (thank you). I will remove the mmap compatibility and use the raw WinAPI calls. Due to my other work, I should be able to finish this later this week."
technical,"can I take the code in the PR, rebase to fix the merge conflicts, then add commits with the suggestions from ghemawat or that was already done by cmumford in? Thank you very much for the offer! We have a change for the internal codebase, which will get published after it lands. The code change is very far into the review process. At this point, I don't think there's anything that external contributors can help us with."
technical,"Hi -- no pressure, but is there any ETA for when Windows support will land? Thanks! We have no timeline for this, sorry."
technical,"ghemawat and pwnall, a quick question: When I asked the C++11 features and limiting myself to C++03, did you mean limit the codebase to C++98 or C++11? I have a few situations where std::chrono is dramatically more convenient than other code, however, I can remove this (it's only for NowMicros and SleepForMicroseconds). Other than that, the port should not require any new features.  Thank you and I am effectively done with my port, other than this minor question. We've recently decided that the next release will require C++11, so it's OK to use C++11. Sorry for the code churn on your end... this decision was not taken lightly."
technical,"ghemawat and pwnall, a quick question: When I asked the C++11 features and limiting myself to C++03, did you mean limit the codebase to C++98 or C++11? I have a few situations where std::chrono is dramatically more convenient than other code, however, I can remove this (it's only for NowMicros and SleepForMicroseconds). Other than that, the port should not require any new features.  Thank you and I am effectively done with my port, other than this minor question. Work on CMake support is already underway. Please use as a starting point, to avoid rework. Also, please use the Travis CI and AppVeyor configurations.  In general, I recommend following the approach taken by Chromium's LevelDB integration. Chromium builds (and runs) on Windows, and does not require modifications to the rest of the LevelDB."
technical,"your comment doesn't contain any information, that can help the situation, and from one side, i can understand you, but no one wants to get deep into it. just try to develop big project with angular and without. How many time would take both? Angular with consume at least 5x time more than if without Angular. Angular - is the most perplexing."
technical,"the reality is just opposite, it is the very suitable framework especially for real big projects. Just to guess, your problem is probably that you see it / try to use it by the same way as from jQuery point of view which is totally wrong way.   But when you deal with real big scale project - the hurt is the only thing you get. Hello, this is not the place to post this kind of message, I'm closing the topic. Please follow the contributing guidelines if you want to post anything constructive."
technical,"Hello, this is not the place to post this kind of message, I'm closing the topic. Please follow the contributing guidelines if you want to post anything constructive. Okay, close it, but still Angular remains to be better. Best wishes, contributors, im outtie..."
technical,Angular - is the most perplexing. Unnessesarily perplexed.
technical,"Your issue doesn't contain a single bit of useful information anybody could act on. At best the problem is that your expectations are wrong and you blame Angular instead of adjusting yourself, and the worse case is that you are just spreading spam. Angular works great for most. If it doesn't for you it's highly likely the problem is on your end. your comment doesn't contain any information, that can help the situation, and from one side, i can understand you, but no one wants to get deep into it. just try to develop big project with angular and without. How many time would take both? Angular with consume at least 5x time more than if without Angular."
technical,"Thanks. That makes sense now :sweat smile: Any updates on this, we're in 2017 now :-D"
technical,Thanks. That makes sense now :sweat smile: anything to follow for this? Chrome 54 shipped and comes with the v1 specs enabled.
technical,"This post has been deleted for violating the code of conduct I also disagree to prefer typescript over dart for little snippets. The undefined, NaN, 0, false, null, whatever hell is one point, imports and package management still another... Dart can really shine even for small snippets and if your build system is already setup anyway, why wouldn˜t I want to use dart for everything? One truth tends to get hidden/ignored more and more these days: Javascript sucks! ,)"
technical,"Any updates on this, we're in 2017 now :-D I imagine there won't be any progress on this until Dart 2.0 rolls out completely removing Dartium. I'd check back in a few more months, or use JS interop to ""build your own"""
technical,"That's really sad. In my mind a serious ""community centered"" effort should kind of use and depend on different independent blocks. to make the idea more clear, AngularDart for sure needs some stuff handling DOM and it seems sane to me to use something like dart:html as a basis. so, every work from google would be contributed back to the basis and others building on top of this basis would automatically benefit - somehow like the whole OpenSource-Idea, no? ,) currently  Dart is more like: ""You want to do web? Sure, use AngularDart."". that said, I'm not the guy who can actually judge what you're doing and why - just saying this from a rather distant and abstract point of view and trying to support Dartlang itself against common attacks. (you know those.... ""meh, google is going to throw it away anyways..."" etc.) Maybe I should add, that webComponents are the reason why our app is only running on Chrome... I realize and understand you are disappointed. I'm one of the TLs of AngularDart, and let me make it clear I think web components are awesome, and would love Dart to support them well. The primary take away here is  well  though, and that will take a little bit of effort, because the current HTML libraries and browser bindings aren't well suited for this. We could probably get some  hacky  support quicker, but it won't be something useful to most customers. It's something we'd like to revisit after Dart2 launches, but until then - again to be very honest - it's just not a priority. We care, but we have limited time and resources, and have to use them efficiently, like I'm sure you do on your project(s) and company. FWIW, most popular web frameworks are not utilizing web components today: * React * Vue * Angular (JS/TS/Dart) ... so it's not clear web components is the ""killer"" feature for most people.  Thanks for your patience!"
technical,"I also disagree to prefer typescript over dart for little snippets. The undefined, NaN, 0, false, null, whatever hell is one point, imports and package management still another... Dart can really shine even for small snippets and if your build system is already setup anyway, why wouldn˜t I want to use dart for everything? One truth tends to get hidden/ignored more and more these days: Javascript sucks! ,) I'm locking this conversation for now since it's getting non-productive. This is something that is interesting to us, but it's not on the immediate roadmap at this time. Thanks for the feedback."
technical,"there are a *LOT* of things we need to fixup in dart:html “ we're holding off until DDC is locked and loaded. It'll be *much* easier to get everything working when our dev story is 100% javascript. I'm not sure I understand this. Could you please add some more info. Is it JS compatibility you're referring to or the possible inclusion of javascript specific tools in the build step, along with DDC?"
technical,the problem is the Dart VM in Dartium where during development some Dart code is directly executed by the Dart VM  but other stuff runs in JS land. With Bazel and DDC they are working on building a developer story with fast edit-reload cycles with standard Chrome (without a Dart VM being involved) to simplify interop between the Dart code (transpiled to JS) and the native JS world. Thanks. That makes sense now :sweat smile:
technical,"I realize and understand you are disappointed. I'm one of the TLs of AngularDart, and let me make it clear I think web components are awesome, and would love Dart to support them well. The primary take away here is  well  though, and that will take a little bit of effort, because the current HTML libraries and browser bindings aren't well suited for this. We could probably get some  hacky  support quicker, but it won't be something useful to most customers. It's something we'd like to revisit after Dart2 launches, but until then - again to be very honest - it's just not a priority. We care, but we have limited time and resources, and have to use them efficiently, like I'm sure you do on your project(s) and company. FWIW, most popular web frameworks are not utilizing web components today: * React * Vue * Angular (JS/TS/Dart) ... so it's not clear web components is the ""killer"" feature for most people.  Thanks for your patience! The *entire* point of Web Components is that it's a ""framework"" that is built-in to browsers. You can just use it, without any extra libs or 3rd party frameworks.  AngularDart emulates many features of Web Components, why does it do that instead of just using Web Components directly?"
technical,We don't plan to support this API in the short term. None of our core users require it and would involve substantial effort to implement it correctly. The dart:html library should deprecate the current v0 specifications of Custom Elements and Shadow DOM and move to using v1 of both specifications.
technical,"I'm not sure I understand this. Could you please add some more info. Is it JS compatibility you're referring to or the possible inclusion of javascript specific tools in the build step, along with DDC? the problem is the Dart VM in Dartium where during development some Dart code is directly executed by the Dart VM  but other stuff runs in JS land. With Bazel and DDC they are working on building a developer story with fast edit-reload cycles with standard Chrome (without a Dart VM being involved) to simplify interop between the Dart code (transpiled to JS) and the native JS world."
technical,anything to follow for this? Chrome 54 shipped and comes with the v1 specs enabled. there are a *LOT* of things we need to fixup in dart:html “ we're holding off until DDC is locked and loaded. It'll be *much* easier to get everything working when our dev story is 100% javascript.
technical,"Well, sort of. It has yet to be supported in Microsoft Edge, for example, which is quite a large market segment still, and the polyfill performs quite badly. So until it works everywhere, it's not free.  AngularDart (and JS/TS, and React, and Vue, and Ember) all were created before web components were a thing. A lot of these framework authors have evaluated web components and found them severely lacking. Here is one of the leads of React, arguably the most popular framework today. We're not going to use it at all at Facebook. We're not going to build React on it because there's a strong model difference “ imperative in Web Components to declarative in React. Web Components doesn't have an idiomatic way to define things like where events go. How do you pass data when everything is a string? We see it more as an interop layer that lets various frameworks talk to each other. In talking to the Atom team, this doesn't solve different framework idioms as it doesn't have an opinion on how they relate. Again, it's not that support will never be offered, but it's unlikely to happen in any immediate time frame. There are other APIs that are more useful for our users, like web workers, service workers, and more right now. From a  personal  perspective (not the Dart team's), I'd use JavaScript or TypeScript if I was highly interested in web components - its much better suited for this lightweight model than Dart is (more suited for larger web applications, not scripts/standalone components/buttons). Thanks for understanding! This post has been deleted for violating the code of conduct"
technical,"I'm locking this conversation for now since it's getting non-productive. This is something that is interesting to us, but it's not on the immediate roadmap at this time. Thanks for the feedback. We don't plan to support this API in the short term. None of our core users require it and would involve substantial effort to implement it correctly."
technical,"Who's still following this issue in 2018? Sorry, couldn't resist. Maybe 2018 will be the year Dart finally gets Web Components v1! We're in the process of updating our dart:html APIs. That roll “ and getting rid of Dartium “ should make it easier to support the latest Browser features. Having said that, our primary framework “ AngularDart “ is not using WebComponent features, so it's not a big priority for us. My suggestion: once we've updated dart:html and friends, let us know specific APIs that are missing/broken"
technical,"The *entire* point of Web Components is that it's a ""framework"" that is built-in to browsers. You can just use it, without any extra libs or 3rd party frameworks.  AngularDart emulates many features of Web Components, why does it do that instead of just using Web Components directly? Well, sort of. It has yet to be supported in Microsoft Edge, for example, which is quite a large market segment still, and the polyfill performs quite badly. So until it works everywhere, it's not free.  AngularDart (and JS/TS, and React, and Vue, and Ember) all were created before web components were a thing. A lot of these framework authors have evaluated web components and found them severely lacking. Here is one of the leads of React, arguably the most popular framework today. We're not going to use it at all at Facebook. We're not going to build React on it because there's a strong model difference “ imperative in Web Components to declarative in React. Web Components doesn't have an idiomatic way to define things like where events go. How do you pass data when everything is a string? We see it more as an interop layer that lets various frameworks talk to each other. In talking to the Atom team, this doesn't solve different framework idioms as it doesn't have an opinion on how they relate. Again, it's not that support will never be offered, but it's unlikely to happen in any immediate time frame. There are other APIs that are more useful for our users, like web workers, service workers, and more right now. From a  personal  perspective (not the Dart team's), I'd use JavaScript or TypeScript if I was highly interested in web components - its much better suited for this lightweight model than Dart is (more suited for larger web applications, not scripts/standalone components/buttons). Thanks for understanding!"
technical,"I imagine there won't be any progress on this until Dart 2.0 rolls out completely removing Dartium. I'd check back in a few more months, or use JS interop to ""build your own"" Who's still following this issue in 2018? Sorry, couldn't resist. Maybe 2018 will be the year Dart finally gets Web Components v1!"
technical,"Are we still using the list of connected processes and their VT statistics? If not, we can declutter our lives and remove a potential source of community consternation. Are we still using the list of connected processes and their VT statistics? If not, we can declutter our lives and remove a potential source of community consternation."
technical, Are you logged into your account when this happens?
technical,"after bit thinking... console is the settings page? modes is night/light modes? regardless firefox, ill just repeat my self: i tried to log in from microsoft edge and torch browser but it failed. should i open one more bug report?  now i have the feeling you didn't read anything at all. did you try to do this from ANY other browser? or you just assume that if it works for you work for absolutely everyone? firefox give the same error"
technical,"I am closing this issue as you keep throwing out insults when we are only trying to help. Hi. I am from the freeCodeCamp.org Staff. I would like to draw your attention that freeCodeCamp is safe place for all. You are currently in violation of our code of conduct, with comments. We sure understand that you would like to report something is broken or not working, but we absolutely do not tolerate any violation of the CoC. We have gone ahead and banned your account from posting comments on freeCodeCamp.org. If you wish to get this re-instated please let us know, why we should do that. Have a great day ahead. Thanks."
technical,firefox give the same error I am closing this issue as you keep throwing out insults when we are only trying to help.
technical,"Hi. I am from the freeCodeCamp.org Staff. I would like to draw your attention that freeCodeCamp is safe place for all. You are currently in violation of our code of conduct, with comments. We sure understand that you would like to report something is broken or not working, but we absolutely do not tolerate any violation of the CoC. We have gone ahead and banned your account from posting comments on freeCodeCamp.org. If you wish to get this re-instated please let us know, why we should do that. Have a great day ahead. Thanks. i tried to log in from microsoft edge and torch browser but it failed. should i open one more bug report?"
technical,"So now I'm going to reply exactly the way Microsoft replies to me. MichalStrehovsky correctly stated that ""since ~.NET Framework 1.0"", the .Net Native compiler has had certain documented behaviors in the way it generates its code.   The implication being that an understanding of .Net Native characteristics requires different design choices.  It's a very convenient way of saying that the bug I had in my code was a bug with my design rather than the .Net Native compiler even though my code did/should work in any given .Net runtime context, but .Net Native is special, thus requiring special design considerations. before you close this issue you will have to adjust where you point the finger.  The problem isn't with MessagePack, according to MichalStrehovsky the problem is your design choice since you, being on the inside of Microsoft, have an even better understanding of the way the .Net Native compiler works.  Or, the problem is the .Net Native compiler documentation because even Microsoft staff are not aware of the design considerations they have to employ to build products that support Microsoft tech. Or, the problem is the way Microsoft operates internally because you folks are prepared to ignore each other and go off to build tooling that your customers are unable to use.  Or, the problem is Microsoft knows what the future holds, and dumps technology support for its existing tech knowing that that tech is about to be invalidated by what comes next.  There are several other logical or cases.  You get the point.  This issue is not related to MessagePack.  I don't want it swept under the rug because either Microsoft is here to support the developers using its tech, or Microsoft does as it pleases regardless of how it affects the developers it has convinced to use its tech.  I suggest you run this by your bosses.  Either you made a mistake, or the entire organization has. are you referring to this issue: where you interacted with me? That comment is specifically to the behavior of the Assembly.Location API that returns the documented value when there's no filesytem location for the loaded assembly. .NET Native compiles apps into a single file and the original assemblies are gone. It has no choice but to return the only possible documented value. It could also throw, but that won't help in grpc's case either.  I'm afraid you're sidetracking this discussion in a way that won't help you get a resolution in streamjsonrpc. I don't see Assembly.Location being called within this repo. Do you have an error message or a repro so that we can act on this?"
technical,"No. You might be the first person to express an interest in this. I'm surprised if it doesn't work. Are you thinking specifically about .NET Native support? What failures are you seeing? As you already know, MessagePack bombs when compiled for .Net Native.  You yourself closed that issue here.  I'm now wondering whether there is an ETA for addressing this?  I don't want the ETA issue to get lost in the mix because we need a means of tracking how long it takes to sort out .Net Native problems.  Some of them can be addressed by design choices Microsoft makes.  Since Microsoft promoted UWP, a degree of accountability is required just so we all have visibility to what's at stake."
technical,"I have one other suggestion, make Debug build enforce some of the limitations one sees on a Release build.  Especially things like Assembly.Location, these should all behave in Debug just as they do in Release.  Lots of problems will disappear in a hurry because most devs spend most of their life in Debug.  Throw in a flag on a per project basis to turn this off for instances where it's not appropriate.  That way the folks turning off the flag will likely know why they're doing so. Enable Release Build Behavior"
technical,"So then the question is, why use MessagePack for streamjsonrpc?  If, as you say, only 2 out of 4 implementations of .Net Standard support it, why is Microsoft breaking parts of their own eco system?  By the way, I'm ok with your answer MichalStrehovsky, you've essentially confirmed that AArnott needs to either fix the way MessagePack uses reflection, or not use MessagePack within streamjsonrpc or any other .Net Lib that has Microsoft's name on it, else yet another piece of tooling from Microsoft is compromised in where it can be used and becomes a source of confusion and needless frustration.  That said, I would not want to see an endless timeline for such fixes, hence the ETA query at the very start of this thread.  It might be simpler to make proper design choices in the first place, and communicate broadly what those should be. I have one other suggestion, make Debug build enforce some of the limitations one sees on a Release build.  Especially things like Assembly.Location, these should all behave in Debug just as they do in Release.  Lots of problems will disappear in a hurry because most devs spend most of their life in Debug.  Throw in a flag on a per project basis to turn this off for instances where it's not appropriate.  That way the folks turning off the flag will likely know why they're doing so."
technical,"Enable Release Build Behavior I read the discussion here  ERBB, an ""Enable Release Build Behavior"" flag mentioned above, takes away a lot of needless hoop jumping and allows current approaches to be preserved.  Not sure how much this would affect Debug tooling, but it would pre-emptively preclude poor design choices."
technical,"As you already know, MessagePack bombs when compiled for .Net Native.  You yourself closed that issue here.  I'm now wondering whether there is an ETA for addressing this?  I don't want the ETA issue to get lost in the mix because we need a means of tracking how long it takes to sort out .Net Native problems.  Some of them can be addressed by design choices Microsoft makes.  Since Microsoft promoted UWP, a degree of accountability is required just so we all have visibility to what's at stake. If the only failure in UWP happens when using this library with MessagePack, and the failure is within MessagePack itself, we should close this issue since it isn't a bug in StreamJsonRpc."
technical,"Sure: essentially the whole of the part of this comment you addressed to me. You presented several alternatives and said it was one of those. They were all jaded, and none of them reflect reality. I guess I could have been clearer in my very first comment on this issue that UWP isn't a supported scenario. We haven't tested it. But most code does Just Work on .NET Native, so my surprise that I expressed in my first comment was mild surprise that StreamJsonRpc wasn't among those things that Just Work. But it wasn't intended to express our current support for UWP. What do you mean? ""You gave yourself away"" makes it sounds like you're saying I accidentally revealed something. I don't feel that way. The intent of all my interactions on this issue should have conveyed:  1. We don't support UWP, but it might just work 1. We don't keep open issues on a repo for which no code defects are present, or to support scenarios that we don't intend to support.  At some point, we might add UWP tests to this repo in which case we'll claim to support UWP. But even then, we might scope the supported feature set down to those that actually work on .NET Native. Which issue was that? You opened this one, and two over at MessagePack that I'm aware of  I closed the above two because we already have an active issue tracking UWP support in MessagePack. We only need one per repo with a defect. I closed *this* vs-streamjsonrpc issue because there's no bug here AFAIK, as the only issues I've seen are actually failures in MessagePack to support UWP, which we can discuss over Is there an ETA for when this library might support UWP?"
technical, No. You might be the first person to express an interest in this. I'm surprised if it doesn't work. Are you thinking specifically about .NET Native support? What failures are you seeing?
technical,"Thanks! This is failing with the following stack trace (if you check the checkbox next to ""Common Language Runtime Exceptions"" in the Visual Studio's Exception window: MessagePack relies on Reflection.Emit that is not supported by .NET Native.  There was a NuGet package created that makes it appear NetStandard supports reflection emit, but creating that NuGet package was a big mistake, because that API only works on two out of 4 implementation of NetStandard. This created a situation that some NetStandard libraries can't work on all NetStandard runtimes. People in charge tried to fix the issue by unpublishing the package (so that we don't have a broken ecosystem), but had to roll that back because of outcry from users. This mistake is still haunting us years later, unfortunately. So then the question is, why use MessagePack for streamjsonrpc?  If, as you say, only 2 out of 4 implementations of .Net Standard support it, why is Microsoft breaking parts of their own eco system?  By the way, I'm ok with your answer MichalStrehovsky, you've essentially confirmed that AArnott needs to either fix the way MessagePack uses reflection, or not use MessagePack within streamjsonrpc or any other .Net Lib that has Microsoft's name on it, else yet another piece of tooling from Microsoft is compromised in where it can be used and becomes a source of confusion and needless frustration.  That said, I would not want to see an endless timeline for such fixes, hence the ETA query at the very start of this thread.  It might be simpler to make proper design choices in the first place, and communicate broadly what those should be."
technical,"The Assembly.Location problem is headed into .NET Core proper: GRPC will have to fix it's bugs. More developers use .NET Core proper, so hopefully they'll be more incentivized to fix the problem. I'm sorry that you're ending up being a hostage to them ignoring the problem. Single file compilation greatly improves startup time of apps and simplifies the distribution a lot - that's why a of users are asking for that mode and we're going to add it outside .NET Native too. But some APIs simply don't make sense in that mode. Assembly.Location is one of those.  As I said, if you can get me the error message or repro for the streamjsonrpc issue I might be able to help you make progress on that one. For GRPC, the ball is really in their court. Starting bottom up, MessagePack bombs on code like this: So fixing MessagePack should fix streamjsonrpc.  I wouldn't be too quick to blame gRPC, since Microsoft is using it and endorsing it as a replacement for WCF.  The current ""flaw"" in the gRPC code base is the way it makes use of Assembly.Location, which is used in a similar manner in many other libs.  Remove this bug and gRPC works in a UWP Release build as well as any other context."
technical,"The .Net Native compiler is a bit of an engineering marvel.  I'd like to see it persist.  It's sad that I have to use such a long winded approach to try and defend its future and get certain parts of Microsoft to cooperate with other parts.  If there is something fundamentally false in anything I'm expressing, please do highlight my error(s).  If any of your work is not intended to support UWP, AArnott, then just add an exclusionary note, or say so in your opening remarks to such queries and I'll be happy to disappear.  I also don't want to waste my time.  I have merely pointed out that your design choices, not your competence, or your intellect, are at issue here.  And I had to go through a long winded process to make the point clear.  You can censor me if you like.  Just because you have the power to do so doesn't make that choice right either.  When you stated ""I'm surprised if it doesn't work."" You gave yourself away.  If I'm wrong in thinking that you knew it didn't work, then I apologize, because you closed the very issue that you had identified as the point of failure.  You see, I'm assuming you are quite bright AArnott based on the high quality of your work.  The quality of your personal choices, in that realm, we obviously have a difference of opinion. Sure: essentially the whole of the part of this comment you addressed to me. You presented several alternatives and said it was one of those. They were all jaded, and none of them reflect reality. I guess I could have been clearer in my very first comment on this issue that UWP isn't a supported scenario. We haven't tested it. But most code does Just Work on .NET Native, so my surprise that I expressed in my first comment was mild surprise that StreamJsonRpc wasn't among those things that Just Work. But it wasn't intended to express our current support for UWP. What do you mean? ""You gave yourself away"" makes it sounds like you're saying I accidentally revealed something. I don't feel that way. The intent of all my interactions on this issue should have conveyed:  1. We don't support UWP, but it might just work 1. We don't keep open issues on a repo for which no code defects are present, or to support scenarios that we don't intend to support.  At some point, we might add UWP tests to this repo in which case we'll claim to support UWP. But even then, we might scope the supported feature set down to those that actually work on .NET Native. Which issue was that? You opened this one, and two over at MessagePack that I'm aware of  I closed the above two because we already have an active issue tracking UWP support in MessagePack. We only need one per repo with a defect. I closed *this* vs-streamjsonrpc issue because there's no bug here AFAIK, as the only issues I've seen are actually failures in MessagePack to support UWP, which we can discuss over"
technical,"Thank you for chiming in.  Both the comment you make and the way initiated the closure of this issue was disingenuous.  You folks are well aware of the .Net Native problems and design considerations.  The developer of MessagePack and AArnott's contributions to that code base are not at fault here.  AArnott is a very smart fellow as are you MichalStrehovsky.  It may be a PITA to have .Net Native issues surfaced this way.  However, that's the real problem and characterizing it any other way is also a problem.  Sadly, I'm not expecting Microsoft to do anything other than replace the .Net Native compiler with whatever the new .Net Core AOT strategy will be.  Until that happens, you folks need to own up to where things are really headed.  It's obvious the reboot is well on the way.  Just say so and stop pretending that any of the .Net Native compiler problems will actually be addressed.  A year went by on the gRPC issue.  A year with no resolution!  I'd even be ok with a form of admission and a statement along the lines that we need to resolve these issues ourselves.   At least that way, time wouldn't be wasted on false expectations.  This library is a nice piece of work.  I'd like to be able to use it. The Assembly.Location problem is headed into .NET Core proper: GRPC will have to fix it's bugs. More developers use .NET Core proper, so hopefully they'll be more incentivized to fix the problem. I'm sorry that you're ending up being a hostage to them ignoring the problem. Single file compilation greatly improves startup time of apps and simplifies the distribution a lot - that's why a of users are asking for that mode and we're going to add it outside .NET Native too. But some APIs simply don't make sense in that mode. Assembly.Location is one of those.  As I said, if you can get me the error message or repro for the streamjsonrpc issue I might be able to help you make progress on that one. For GRPC, the ball is really in their court."
technical," As a workaround, add overloads to Array.isArray() using declaration merging?"
technical,"Please don't -mention me in threads I'm not subscribed to. I refer to what I already wrote above. Your comment is insulting and more than useless. Go and learn some manners! Interestingly, writing a type predicate x is ReadonlyArray<T doesn't actually work from my tests: it narrows to T[] (sans readonly-ness) anyway."
technical,"Don't underestimate how difficult things like this are from a theoretical perspective.  Humans are much better at figuring these things out than computers.  In the *general case*, the compiler can't safely deduce that Generic1<number ” Generic2 number unless both are in the union to be narrowed, because the type parameter might be used for different purposes in both types. would likely suffice.  I suspect this is probably how Flow does it. Proof of concept: Try in TS Playground For the record, the current declaration of isArray is this. I wonder if that can be improved.  I'll need to give that some thought and maybe I'll open a PR."
technical,"This is not the level of security awareness I expect Microsoft  I don't work for MS. Your tone is derogatory.  Your software (Norton) has established a false positive in it's threat detection. Please work with the software manufacturer to establish a resolution on their part.  There is nothing Microsoft or anyone else can do if a piece of third party software on your machine malfunctions. One solution is to use Windows built-in threat detection tools instead of the unreliable Norton software. I am actually quite shocked that Norton are even still in business. Their AV and other ""Security"" products have been malfunctioning and underperforming for well over 20 years.  Because vcpkg is open source, you can modify the source code any way that you wish once you have obtained a copy locally, so I'm unsure what else to say other than it would appear that the software Norton have sold you is not providing you value for money and is also causing you frustration instead as well. As I pointed out this has been reported repeatedly in the past, and closed with no explanation or correction.  Specifically, #3345 is clear that it is much more widespread.  I'd focus less on Norton, per se, based on these prior reports.  The initial issue is the issuance of telemetry without opt-in.  It is, in fact, that telemetry that is causing the image to be considered suspect.  Furthermore, vcpkg is recommended by Microsoft.  I was, and am, surprised that Microsoft is recommending a solution that performs telemetry by default.  And one that does not have a clear statement that it is taking telemetry, exactly what telemetry it is taking, and does not have a very clear opt-out.  Now, can I fork the code to do whatever I want?  Sure.  Can I propose a fix?  Sure. Does that change the surprise at the telemetry by default and the lack of visibility, or that this has been reported for over a year?  Not really."
technical,"I'd focus less on Norton, per se, based on these prior reports.  This is the incorrect approach based upon your issue.  Because: Your software (Norton) has established a false positive in it's threat detection.  And one single other bug report: Specifically, #3345 is clear that it is much more widespread.  Does not validate your position. Your logic fails to accommodate the fact that AV software (especially badly maintained, poorly coded AV software) is highly prone to false positive detection rates. The simple solution since the false positive problem was identified (decades ago) has been to report the false positive to the manufacturer using (commonly) their automatic reporting tool. I have already suggested that you do this and yet you persist to assert that your problem is one of Microsoft's making. I was, and am, surprised that Microsoft is recommending a solution that performs telemetry by default.  I'm sorry but if you were unaware, telemetry by default is how Windows 10 is shipping too. It is the default position of Microsoft products to do so. If you do not like this behaviour (and I personally find it repugnant) then turn it off. The method to do so, as already mentioned is -disableMetrics. Now, can I fork the code to do whatever I want? Sure. Can I propose a fix? Sure.  Both rhetorical questions. Does that change the surprise at the telemetry by default and the lack of visibility, or that this has been reported for over a year?  And yet according to your report: This is not the level of security awareness I expect Microsoft to have.  Please don't attempt to change the topic of this thread. We get it. You don't like metrics. False positive detections by AV products have nothing to do with Microsoft's level of security awareness. You can report false positives to AV product manufacturers. And you can also turn off vcpkg's metrics using a single switch. This in turn should prevent the false positive triggering.  Let us know how you get on. Thanks. It is annoying that Norton flags the metrics uploader, but at this time there is no solution that works for the team at this time.  The uploader cannot be qualified by the major anti-virus vendors because it is built on the end users machine and is not signed.  We have discussed fiddling with the implementation of the uploader, but have decided against it because we don't want to obfuscate the code.  Also tricks that may work now could end up backfiring later.  Making the metrics uploader opt-in rather than opt-out does not solve the anti-virus issue either, it only makes it less noticeable.  In response to your concerns of the collection of usage metrics, I refer you to the privacy policy"
technical,"There are several options you may try to solve the problem: 1. Run the bootstrap process with the option to compile vcpkg without metrics. 2. Tell Norton not to scan the vcpkg.exe executable 3. Tell Norton not to scan the directory/folder containing vcpkg.exe My workaround was to tell Norton to reverse the quarantine decision and to not scan the metrics upload image.  Maybe I'm the only person bothered by the opaque decision to have this process upload data from my machine to some remote site.  But I am bothered by it.  I can surely support a "".\bootstrap-vcpkg.bat -enableMetrics"" approach.  Or an approach that directly asks at the start of the bootstrap process if I want to upload metrics and either directly disclose, or be able to disclose, what will be uploaded."
technical,"As I pointed out this has been reported repeatedly in the past, and closed with no explanation or correction.  Specifically, #3345 is clear that it is much more widespread.  I'd focus less on Norton, per se, based on these prior reports.  The initial issue is the issuance of telemetry without opt-in.  It is, in fact, that telemetry that is causing the image to be considered suspect.  Furthermore, vcpkg is recommended by Microsoft.  I was, and am, surprised that Microsoft is recommending a solution that performs telemetry by default.  And one that does not have a clear statement that it is taking telemetry, exactly what telemetry it is taking, and does not have a very clear opt-out.  Now, can I fork the code to do whatever I want?  Sure.  Can I propose a fix?  Sure. Does that change the surprise at the telemetry by default and the lack of visibility, or that this has been reported for over a year?  Not really. There are several options you may try to solve the problem: 1. Run the bootstrap process with the option to compile vcpkg without metrics. 2. Tell Norton not to scan the vcpkg.exe executable 3. Tell Norton not to scan the directory/folder containing vcpkg.exe"
technical,"I also happened with such problemit confused me for two days. I had reinstalled VS2017 with 15.3 for many times,but it didn't work. I also tried to update the Unity , but the old project can't be opened without any questions. I had a very long way to find the solution(I also tried VS2015) . At last the VS2017 with 15.0 and the unity 5.6.2 works well in my computer, that is my way to solved this problem. I wish it may be helpful. as indicated in my previous posts I tried with both VS 2017 and 2015. Universal module is installed, am not a newcomer with Hololens since I develop since a year on it, and have a project to get ready for an incoming demo. Sounds like there are problems with last Unity version and the toolkit.  ty for your feedback, will help :)"
technical,"I def understand how frustrated you are, but every new platform has its shares of issues and stability.  Yeah, there's been issues from the manufacturer, all the way through to the middle ware game engine, and even here in the community trying to solve the problems. (odd thing is that's what keeps me around ˜… ).  Please keep in mind this is a Gen1 device, that was never really intended to go beyond developers who wanted to play with it, and companies wanting to do a feasibility test.   The only way to grow and learn is to make mistakes, the important thing is that you learn from them.  This quote couldn't be any more true, especially when it comes to software. The nice thing is that it's a continuous process to grow and learn, and they only way you really know that it's happening is when you make mistakes.  I'm glad to see the software getting better (and it has!).  Microsoft, Unity, and even the Toolkit have come a long way in the last two years.   I'm just sick of having to go through thousands of tutorials, forums, chats, emails and so on.  I agree it is a pain, but I think the difference here is that you haven't talked to me, or other engineers trying to make your life easier by fixing some of the problems. This is the first time I've heard from you, so **let's open a new issue and try to figure out what's going on**.  Remember to follow the Code of Conduct.  I understand you're frustrated, but having a positive attitude, even when you're stressed out or frustrated speaks volumes to others who are genuinely trying to help. Hello,  Upgraded a couple of days ago Unity to 2017.1, HoloToolkit to MixedRealityToolkit release 1 and Visual Studio to 2017 and cannot compile under VS with the guideline workflow"
technical,"as indicated in my previous posts I tried with both VS 2017 and 2015. Universal module is installed, am not a newcomer with Hololens since I develop since a year on it, and have a project to get ready for an incoming demo. Sounds like there are problems with last Unity version and the toolkit.  ty for your feedback, will help :) Ok am sorted with Unity 5.6.3, HoloToolkit 1.5.8 and VS 2015. Will try to update when next iterations of both Unity and MixedRealityToolkit will get released since I don't have the time to betatest atm."
technical, So after having read workaround solutions I switched back to VS2015 and now have So am switching back to previous HoloToolkit packages and will try your MixedRealityToolkit release 2 when it will be available. Lost 2 days
technical,"So after having read workaround solutions I switched back to VS2015 and now have So am switching back to previous HoloToolkit packages and will try your MixedRealityToolkit release 2 when it will be available. Lost 2 days Xispeo, what version of Visual Studio are you using? Also for your first issue, did you make sure to switch your build settings back to UWP?  It looks like you're missing/didn't install the windows universal module for unity. For the second issue, Visual Studio 2017 is required for the latest versions of this project. Keep in mind this known issue w/Visual Studio 2017.3 and all current Releases of Unity."
technical,"The project needs lifesaving measures as it's community is dissatisfied enough to be discussing forks. I want to see this project live, and if forking it is the only way to get maintainers that then I'm behind it... You have good intentions for your project, and want to see it succeed. However, it's being smothered by inactivity and the inability of other members to step in.  The dead project syndrome is already rearing it's head when pull requests to fix bugs are being auto-closed due to inactivity... I see new bug reports and KNOW that I can fix it, but why bother if it's for naught and the fix gets ignored. **I love Semantic UI**, and if this project where to come back to life would happily set aside a dedicated amount of time to find and fix issues on a regular basis. I've stopped paying attention to the project over the last year, and even explored other UI libraries because it no longer seems to be moving forward.  It's at a critical point now, other passionate members are literally begging for the ability to maintain a project they love. I'm sure they want to avoid a fork, we need active maintainers that can keep this project running while you are sorting out funding. There are even members trying to maintain their own forks/versions. At least stop the robot meanwhile."
technical,"I love you Jack, for making SUI. I just hope you dont get crazy <3 Can we officially pronounce SemanticUI dead yet, or... It's one thing when the repo doesn't get any new commits from the owner. It's another where the owner stops being active altogether.  There are no active maintainers (and the owner seems to be hell-bent on keeping it that way), no activity from the owner, nothing."
technical," he is the benevolent dictator here.  I'll let him respond, although, I think he's made his position pretty clear on it.  I've suggested a few times before that an active group of folks could always come together, fork, and start making releases.  My time is overbooked working on the React port and the v2 effort, in which we'll take on our own styling."
technical,"yes I know. The level of donations was described there as being very low. However, this might have something to do with the fact that there are very few nudges to donate. There is a button right at the bottom of the entry webpage - but after you get to the docs, nothing (and you never go back to the main entry page, right?). There is no mention of donating on the GitHub readme, nor on the one for SUIR.  Seeing as it was possible to get 1M from kickstarter for some icons, I'm just suggesting that there is maybe some untapped potential. I agree it could be pushed more but this will still be up to Jack.  We should also think about funding platforms"
technical,"he is the benevolent dictator here.  I'll let him respond, although, I think he's made his position pretty clear on it.  I've suggested a few times before that an active group of folks could always come together, fork, and start making releases.  My time is overbooked working on the React port and the v2 effort, in which we'll take on our own styling. I am sure there are many who are also very satisfied users of both Semantic-UI and Semantic-UI-React, who would be happy to donate to the project where we are getting a huge amount of value for free.  However both projects are notably lacking in ""Donate"" buttons, and the SUIR site gives the impression - possibly false - that as it is used by Amazon and Netflix, there must be plenty of sponsor .  My own experience suggests that people are happy to pay - it just has to be easy, and they may need a ""nudge"".  Can I suggest some Donate buttons or even a Kickstarter?"
technical,"I'm sorry but I have to be this guy... The fact is that you left this repository unanswered for two plain month. Issues where opened and closed by the bot without any reponse, and nobody but you can do some critical things (merging pull requests, publishing fixes...).  I can understand that you don't have time to do SUI technical stuff, and I can understand your desire to make SUI a web standard and a long goal life. But you MUST understant that without repo's activity people will not be interested in SUI anymore, and you'll try to find funds for a dead project... I second his opinion: please let people who already volunteered to have a more active role in this project, at least merging PR for small bug fixes, updating FA icons as mentioned on another issue and stuff like that. That will be sufficient until you manage to sort out your long term goals."
technical,"Can we officially pronounce SemanticUI dead yet, or... It's one thing when the repo doesn't get any new commits from the owner. It's another where the owner stops being active altogether.  There are no active maintainers (and the owner seems to be hell-bent on keeping it that way), no activity from the owner, nothing. In reply to the previous comment, I'd like to say the same thing I said here:"
technical,"I second his opinion: please let people who already volunteered to have a more active role in this project, at least merging PR for small bug fixes, updating FA icons as mentioned on another issue and stuff like that. That will be sufficient until you manage to sort out your long term goals. Jack, I trust to your judgement in this. This is the same judgement that produced the design decisions in Semantic-UI after all :)  Give the guy some appreciation everyone!  I have Semantic-UI-React about to be deployed and it has been excellent to work with. Just stuck with the 2.2 CSS as described here"
technical,"I agree it could be pushed more but this will still be up to Jack.  We should also think about funding platforms Just to be clear, my absence from frequent updates is specifically to address the need of finding a source of permanent funding for SUI.  Solving UI for the web through open source and SUI is the core goal of my professional life”a project I expect to continue for many more decades to come.  It is an unfortunate necessity that I must leave things on pause for the time being, but I think this approach has a much greater chance to provide permanent funding for the project and its underlying goals, beyond what might be achieved by asking for donations or crowdfunding."
technical,"I am sure there are many who are also very satisfied users of both Semantic-UI and Semantic-UI-React, who would be happy to donate to the project where we are getting a huge amount of value for free.  However both projects are notably lacking in ""Donate"" buttons, and the SUIR site gives the impression - possibly false - that as it is used by Amazon and Netflix, there must be plenty of sponsor .  My own experience suggests that people are happy to pay - it just has to be easy, and they may need a ""nudge"".  Can I suggest some Donate buttons or even a Kickstarter? This has already been discussed here"
technical,"In reply to the previous comment, I'd like to say the same thing I said here: those are commits in master. Look at the next branch and twitter."
technical,"Just to be clear, my absence from frequent updates is specifically to address the need of finding a source of permanent funding for SUI.  Solving UI for the web through open source and SUI is the core goal of my professional life”a project I expect to continue for many more decades to come.  It is an unfortunate necessity that I must leave things on pause for the time being, but I think this approach has a much greater chance to provide permanent funding for the project and its underlying goals, beyond what might be achieved by asking for donations or crowdfunding. understood. The one thing we need from you and levithomason, DESPERATELY, is hopefully less than 20 minutes of your time to give 2 actively trusted people the power to add and remove privileged maintainers within this project and ideally within this organization. They should also have the power to add more administrators like themselves if needed. (Most privileged maintainers should not have such extra administrator power.)  I think this one action would be a MAJOR TECHNICAL LIFESAVER: - prevent the otherwise inevitable project fork that is under discussion here, #6413, and #6109, among other places - keep this project at the existing quality level - avoid the dead project syndrome  By ""project fork"" I mean a significant portion of the team split off, like what happened with node.js in the past. I think it would be much better to keep this project as one united team if possible.  I would like to commend you for the major efforts it must have taken on your part to get this project this far. I think we all understand 100% that you must be completely swamped with your professional work, business work for the project, and personal obligations. I remain extremely hopeful that you can help the rest of us with the one action proposed here."
technical,"This has already been discussed here yes I know. The level of donations was described there as being very low. However, this might have something to do with the fact that there are very few nudges to donate. There is a button right at the bottom of the entry webpage - but after you get to the docs, nothing (and you never go back to the main entry page, right?). There is no mention of donating on the GitHub readme, nor on the one for SUIR.  Seeing as it was possible to get 1M from kickstarter for some icons, I'm just suggesting that there is maybe some untapped potential."
technical,"I am sorry to read that my message didn't answer your questions. We have a consistent strategy regarding our open-source solution and our Enterprise version since the R&D started in 2015. We don't plan to change it for the moment, I hope you understand. Anyway, as I said before, we would be more than pleased to discuss this with you live. Closing this thread. At the moment the test summary is logged using println  This causes issues when shipping logs to Kibana in a logstash encoded format using the logstash-logback-encoder as println does not use that encoder. Is this done intentionally?  I did originally raise this issue here but got no response.  If it's ok to change this to use a logger I'm happy to contribute."
technical,"Thanks for the reply, but I still don't think you actually addressed the points I was trying to raise regarding this particular issue.  I still don't understand how the suggested change is against your commercial strategy.  The points I was trying to articulate are that this request:  - doesn't expose any data that would previously be inaccessible / enterprise. - doesn't suggest structuring the results in a way that is easier to parse/interpret programatically - only suggests what we believe is better practice by outputting using a logger which is the standard practice - would allow for common, modern log shippers to ship your output with our creating message noise, where each println is interpreted as a new log event.  I literally don't see a single reason you would be against it this request.  I've read through your comments multiple times and I still don't see a single argument you have given explaining why . Fair enough to that statement, but we're not talking about additional stats, pushing stats or anything about stats.  We're talking about not using a logger for your output which causes problems when using industry standard shipping practices that expect structured logs. I am sorry to read that my message didn't answer your questions. We have a consistent strategy regarding our open-source solution and our Enterprise version since the R&D started in 2015. We don't plan to change it for the moment, I hope you understand. Anyway, as I said before, we would be more than pleased to discuss this with you live. Closing this thread."
technical,"Sorry, but all new features regarding stats (additional stats, pushing stats elsewhere) fall into the scope of FrontLine, our Enterprise version.  For example, FrontLine provides a public API you can use to extract the stats, and a Grafana datasource. I think this is more about logging rather then stats. At the moment we can't even turn these logs off if we don't want them. This is a problem with the open source tool."
technical,"Would you be open to have a new implementation of DataWriter that used a logger instead? No, for the very same reason: we need to make a living.   Sorry, but all new features regarding stats (additional stats, pushing stats elsewhere) fall into the scope of FrontLine, our Enterprise version.  I'll sound pushy, but I think it would be fair that billion dollars companies that have been using an open source software for years would consider helping the company behind the technology."
technical,"No, for the very same reason: we need to make a living.   Sorry, but all new features regarding stats (additional stats, pushing stats elsewhere) fall into the scope of FrontLine, our Enterprise version.  I'll sound pushy, but I think it would be fair that billion dollars companies that have been using an open source software for years would consider helping the company behind the technology. Ok, I'll concede on this. Though I think it's unrealistic to expect someone/a company to pay money for a tool that works perfectly fine for them, just to have finer grain control over their logs. We're not asking for real time live graphs or monitoring of injectors. This is something we are more then happy to do the work for to improve the tool, so it's shame we can't contribute in that way."
technical,"Yes you can, remove console from the writers in gatling.conf. Ok, that's my mistake. Though I'm still not sure why this would use a println rather then a logger that you can have more control over."
technical,"Ok, that's my mistake. Though I'm still not sure why this would use a println rather then a logger that you can have more control over. So people can't mess up with the formatting and break the line length. Again, this in not intended for extension, hence the format is not an public/stable API to be consumed."
technical," Sorry, but all new features regarding stats (additional stats, pushing stats elsewhere) fall into the scope of FrontLine, our Enterprise version.  For example, FrontLine provides a public API you can use to extract the stats, and a Grafana datasource."
technical,"I am sorry but I don't really understand your reasoning, although I understand your concern around protecting the Enterprise value.  Just to confirm, your strategy is to prefer for us to fork this project so that we can use a logger for a single line of code where you are currently using println, pushing us further away from wanting the Enterprise solution because you are classifying using a logger to output the summary as part of stats/exports/integrations, although that information is already being outputted in a different form?  To be honest, this entire thread so far makes me far less inclined to introduce a dependency on the enterprise project.  I believe gatling is great as a project, I believe you should get paid to support and continue working on it full time, I also believe large companies should pay and help support opensource initiatives.   The problem is that I also believe your approach will alienate many people, or at least me and all the people I talk to about this.  It's a very simple code change, that actually makes it more consistent, as the majority of of your code base uses loggers (as any developer would expect).  You are not even arguing about not outputting that data because it's stats/enterprise data.  The data is there and available in the logs already!  The only difference is the primitive output mechanism.  The fact that many modern log shipping frameworks expect json/structured logs means that using println automatically makes you a less attractive product for both enterprise and non enterprise adopters.  These println statements just create noise in most modern log shipping frameworks I have worked with as they cannot be interpreted as a single event.  This is genuine feedback.  I would urge you to reconsider the quick decision, because I believe this suggestion exposes no more information than you currently do, moves you towards a more standardized output like the rest of your code base, and would actually show you positively engaging in improving your entire product offering, OSS and Enterprise.  The functionality you are planning as part of Enterprise are still attractive to companies like ours, e.g. aggregation of metrics etc, grafana datasources etc.  I think you've convinced yourself that any improvements to OSS is a risk to Enterprise.  Just doesn't sit well with me. Thank you for sharing your concerns. We don't see improvements of the OSS as a risk, quite the contrary: you can have a look at all the work that's been put into upcoming Gatling 3 OSS.  Our Enterprise version, Gatling FrontLine, aims to provide our users with advanced features in terms of metrics, integration and automation. This Enterprise version was designed by the feedback of our users who built their own integrations with Gatling but then struggled to stabilize them and maintain them over time.  We believe this Enterprise version is a win-win for everyone: we make you save time and money, we guarantee you stability and, at the same time, you help the open-source project continue to develop.  We would be more than happy to continue this discussion live and convince you that we are still committed to improve load testing, both for our OSS users and for our enterprise customers.  Anyway, thank you for using Gatling"
technical,"Thank you for sharing your concerns. We don't see improvements of the OSS as a risk, quite the contrary: you can have a look at all the work that's been put into upcoming Gatling 3 OSS.  Our Enterprise version, Gatling FrontLine, aims to provide our users with advanced features in terms of metrics, integration and automation. This Enterprise version was designed by the feedback of our users who built their own integrations with Gatling but then struggled to stabilize them and maintain them over time.  We believe this Enterprise version is a win-win for everyone: we make you save time and money, we guarantee you stability and, at the same time, you help the open-source project continue to develop.  We would be more than happy to continue this discussion live and convince you that we are still committed to improve load testing, both for our OSS users and for our enterprise customers.  Anyway, thank you for using Gatling Thanks for the reply, but I still don't think you actually addressed the points I was trying to raise regarding this particular issue.  I still don't understand how the suggested change is against your commercial strategy.  The points I was trying to articulate are that this request:  - doesn't expose any data that would previously be inaccessible / enterprise. - doesn't suggest structuring the results in a way that is easier to parse/interpret programatically - only suggests what we believe is better practice by outputting using a logger which is the standard practice - would allow for common, modern log shippers to ship your output with our creating message noise, where each println is interpreted as a new log event.  I literally don't see a single reason you would be against it this request.  I've read through your comments multiple times and I still don't see a single argument you have given explaining why . Fair enough to that statement, but we're not talking about additional stats, pushing stats or anything about stats.  We're talking about not using a logger for your output which causes problems when using industry standard shipping practices that expect structured logs."
technical,"So people can't mess up with the formatting and break the line length. Again, this in not intended for extension, hence the format is not an public/stable API to be consumed. Would you be open to have a new implementation of DataWriter that used a logger instead?"
technical,"I think this is more about logging rather then stats. At the moment we can't even turn these logs off if we don't want them. This is a problem with the open source tool. Yes you can, remove console from the writers in gatling.conf."
technical,Closing as per part 1. balanced weapon for artillery
technical, Closing as per part 1.
technical,"If I state that your sample ""works for me"", it of course means that I did bother running it on my side and didn't get to reproduce the issue you're mentioning.  That's mostly likely an invalid statement, as:  1) your sample works, at least on my side 2) the library you're mentioning, io.kubernetes:client-java, is built on top of Square's okhttp and doesn't share any dependency with Gatling which is built on top of Netty.  Let's agree that we disagree and that nothing good will come from this discussion. Basically when project includes gatling does not start execution of scenario. Repro case is attached  and gatling is stuck doing nothing, last message  now comment out kubernetes client in pom.xml and all will be well"
technical," You've messed up the ""writers"" option in gatling.conf. Works just fine for me."
technical,3 month to make an installer for an app)) that's nice) Because its not a system app))) so it strange to have executable on it)
technical,why should they? because they published it and advertise it
technical,"You're referencing that picture of a tweet, right?  The tweet posted by Windows **Developer**?  The Twitter account primarily followed by developers?  The one that says ""Microsoft Developer is your resource for development tips, tricks, research, case studies...Everything you need to develop apps that users love.""?   The tweet I'm assuming is targeted towards developers, not standard users? You're talking about that one, right? Even then, my only real question to you is this: Where's your binaries?  Just because it's ""early and incomplete"" (just like this code on this repo is) shouldn't be an excuse. exactly!"
technical,"exactly! Expect an executable installer because I won't compile , emmm, remind me when you're ready   "
technical,"strange to hear this in MS repo which is ""new era UX"")) good luck with learning terminal installation"
technical,"this is open source project you always can create pull request with installer but you can only demand and scoff Hi there. Thanks for being so excited to jump in. I'm going to lock this issue, and I'll have to ask you to  please remain civil ."
technical,Mentioned in a bit already Im not the one)
technical,"Expect an executable installer because I won't compile , emmm, remind me when you're ready    Initial builds of WSL 2 will be available through the Windows insider program by the end of June 2019"
technical, Mentioned in a bit already
technical,because they published it and advertise it So what? they told you that there is no release yet
technical,"The generated .appx file does not contain any certificates, it couldn't be installed by double-click or powershell. The only way I found is select ""Deploy Solution"" in vs2017 IDE. so why there is no installer? is it so hard to make?"
technical,"The sense of entitlement is strong with this one strange to hear this in MS repo which is ""new era UX""))"
technical,"you are looking at the source code of the new terminal. There is no official build available yet so you either build it yourself or wait until the first builds come in. It's really not that hard to understand... The generated .appx file does not contain any certificates, it couldn't be installed by double-click or powershell. The only way I found is select ""Deploy Solution"" in vs2017 IDE."
technical,you can always not use analog from the Internet if you don't have enough knowledge to install this terminal The sense of entitlement is strong with this one
technical,So what? they told you that there is no release yet then maybe should think about that before start advertise it? or spent 1-2h and make installation. It's installation and not human like AI system
technical,So what? they told you that there is no release yet Then why inviting people to try it out
technical,Because its not a system app))) so it strange to have executable on it) this is open source project you always can create pull request with installer but you can only demand and scoff
technical,Then why inviting people to try it out Twitter account is called Windows Developer and implies that the person will be able to cope with the installation from GitHub
technical,Why don't you develop your own terminal instead of asking like you are the CEO of Microsoft? why i should?
technical,why i should? why should they?
technical,then maybe should think about that before start advertise it? or spent 1-2h and make installation. It's installation and not human like AI system you can always not use analog from the Internet if you don't have enough knowledge to install this terminal
technical, Better still SAnta Code.
technical,"Better still SAnta Code. It will remind everyone about ActionScript: it had ints and uints, while JavaScript does not."
technical,"It will remind everyone about ActionScript: it had ints and uints, while JavaScript does not. Thanks for creating this issue. We think this issue is unactionable or unrelated to the goals of this project. Please follow our issue reporting guidelines. Happy Coding!"
technical,"Thanks for creating this issue. We think this issue is unactionable or unrelated to the goals of this project. Please follow our issue reporting guidelines. Happy Coding! VSCode implies that you are versus the code as a concept. I like the code and many people like the code too. I suggest ASCode as a new name. AS is an acronym for ""alongside""."
technical,"This seems like it could become a geopolitical issue, so I am going to lock this thread before it does. I will engage the Store team about the wording here. Briefly describe which document needs to be corrected and why."
technical, I don't think so. and the term is ” in traditional Chinese.
technical," Mind helping me with a translation? I believe the request is, ""the language list inside the Store is incorrect for Chinese""  If so, I will need to forward this to the Store team, we do not control the specifics of that list -- just checkboxes for ""yes, no"" for each language."
technical, Technically “ is more accurate since it's used in various regions like HongKong & Taiwan.
technical, There is no such file between.
technical,"There is no such file between. This seems like it could become a geopolitical issue, so I am going to lock this thread before it does. I will engage the Store team about the wording here."
technical,"Mind helping me with a translation? I believe the request is, ""the language list inside the Store is incorrect for Chinese""  If so, I will need to forward this to the Store team, we do not control the specifics of that list -- just checkboxes for ""yes, no"" for each language. With pleasure. Basically the issue is about the supported language list being misleading by marking Simplified Chinese as ­–­ aka ChineseChina,and marking Traditional Chinese as ­– aka Chinese(Taiwan). This is technically misleading and incorrect.  More commonly we see Simplified Chinese as ­–“ and Traditional Chinese as ­–“, which is region-neutral. But if the supported language list is designed to be (language, region) pair, I think we should mark at least Taiwan as a region instead of a country."
technical,"While this seems useful, I can see issues supporting IE which doesn't support CSS Variables at all. Bootstrap 4.X lists support for IE10+ (not sure where BS5 will go) BS5 will (last time I checked into our internal discussions) still support IE11"
technical,"I'm going to lock this conversation because a lot of people will ask for this while we cannot provide support due to the IE11 support. Providing a solution with functions like try-darken could help us out, but we'll overcomplicate our codebase too much to make this possible. Maintaining this & tackling all edge cases will slow us down. We 'll definitely have a look at support for css variables in the future (v6), but for now we 'll pass on this and focus on improving & extending other Bootstrap functionality. I know CSS variables has already been discussed (#26596) and I saw this reply So I come with a proposal which not requires heavy refactoring  ## Proposal Instead of using directly native functions like darken or lighten we should use wrappers  This wrapper handles the variable construction and create shades of colors.Then we need to generate all shades of variables This creates all necessary variables. As you see, this requires to list all necessary shades, and concret theme color values With theses few tricks I managed to create all variables I needed for theme colors, and I can change it on the fly with few javascript  ## Demo Here is a demo site with an implementation. You can change the primary color on fly in the nav bar ## Source Here are the sources. ## Alternative If the proposal is rejected, it would worth nothing to wrap native function calls (like darken/lighten/mix ...) into upper functions (try-darken, try-lighten ...) so we could override these behaviors to implement CSS variable ourselves."
technical,"Yeah, agreed on overcomplicating things here. Unless we move entirely to PostCSS with the build system to support it, we should continue to prioritize great Sass code. I love Your idea. I think css variables is must-have in bootstrap. With that every user could change interface colors - It's great!"
technical,"it's easy too, not that the current scss is difficult but this is easier I'm going to lock this conversation because a lot of people will ask for this while we cannot provide support due to the IE11 support. Providing a solution with functions like try-darken could help us out, but we'll overcomplicate our codebase too much to make this possible. Maintaining this & tackling all edge cases will slow us down. We 'll definitely have a look at support for css variables in the future (v6), but for now we 'll pass on this and focus on improving & extending other Bootstrap functionality."
technical,"I love Your idea. I think css variables is must-have in bootstrap. With that every user could change interface colors - It's great! it's easy too, not that the current scss is difficult but this is easier"
technical,"This feature should not be default, this should remain optional until all supported browsers handle it. enable-variables options is here to opt-in for this This won't work in not-IE browsers. enable-variables options is here to opt-in for this Maybe we can find a solution for this, but I'm a bit worried we'll overcomplicate everything just to make it possible to support css variables."
technical," While this seems useful, I can see issues supporting IE which doesn't support CSS Variables at all. Bootstrap 4.X lists support for IE10+ (not sure where BS5 will go)"
technical,"This won't work in not-IE browsers. enable-variables options is here to opt-in for this Maybe we can find a solution for this, but I'm a bit worried we'll overcomplicate everything just to make it possible to support css variables. Yeah, agreed on overcomplicating things here. Unless we move entirely to PostCSS with the build system to support it, we should continue to prioritize great Sass code."
technical,"This won't work in not-IE browsers. enable-variables options is here to opt-in for this Maybe we can find a solution for this, but I'm a bit worried we'll overcomplicate everything just to make it possible to support css variables. Can you file a separate issue for that? Disabling a diagnostic is expected to disable a diagnostic."
technical,"The issue here is hard tabs contained *within* a line of code, i.e. tabs not used for indentation. Design meeting notes: In the past we've had conflicting requests on the behavior for tabs within a line of code so we need to add an option for this behavior.  My proposal: * add editorconfig setting that enforces tabs within a line of code. tab within line * add new toggle in **Tools  Options  C#  Tabs**: Enforce tabs within lines. * It could also go under **Tools  Options  C#  Code Style  Formatting  Spacing** if that is the preferred place to keep editorconfig settings."
technical,"This is really worse, what I read here. This misbehavior on tabs was reported shortly after the release of the VS2015 version, multiple times, by different people. Most of this reports were closed after a short period. I am quite sure, that everyone here knows about the license costs - I use a lot of cheaper software where the companies do not deal in such a bad way with their customers and fix such bugs when they get reported. Funny enough that this is open, the one reported on VS2017 Express. You developed an new auto format engine and it fails to give the same good experience as the one that was used before and instead of fixing it, you prefer to spend hours of discussion with arguments that have nothing to do with the given issue. This is not understandable, a new engine has bugs and you do not feel the need to get it fixed? On top it is for the comfort in code editing and this is a bug that breaks the comfort in editing, creates unnecessary extra work “ a very worse experience. How much patience do you expect, by people that have to deal with this bug on every day? I still remember that there once was really good bug handling within the visual studio team - seems that this is history. For 75% of my career to date, I was a non-Microsoft developer working on projects primarily using tabs for indentation. The current behavior better represented the expected behavior for those projects, so on those projects I would have considered VS2015 to be the point where the bug was  fixed , not introduced (note that I was not working at Microsoft for the VS2015 release where this behavior changed, so I was just a user). The complexity here is we're not talking about ""fixing a bug"", we're talking about introducing an option at a location that was never designed to be an option, and somehow getting both behaviors to work seamlessly. This is a simplification, because in reality there are  at least  three very different behaviors being requested."
technical,"In most cases, pressing <kbdEnter</kbd in the middle of whitespace already behaves the same as pressing enter at the end of that same whitespace. This feature is frequently used. Can you show this using the  and  characters? Funny, I never press Enter at the end of whtespace, but at the beginning. I can try, in both examples below I assume pressing Enter at the beginning of whitespace.  **Version 1 (VS editor cannot figure out whether I also want spaces or not after initial tab alignment)** However, given that the code above was a rather contrived example (and I am pretty sure most of us would prefer the comment to stay where it is anyway, making this comment alignment discussion even more contrived), that still focuses on minor style details while not addressing the key complaint most of us tab users here have.  **When I type this:** **I don't want it being changed to this when I type , at the end of line (or when such line is pasted):** Yes I know I can disable auto-format on , and on paste, but I want other formatting to happen. If you at least fixed that, you would have much less complaints from tab users.  Finally, I'd also like to point out how much everything is geared towards people using spaces. When you create a new C# library (.Net Framework) project in VS 16.9.1, initial code looks like this.  So, the template not only assumes I will be using spaces, but also that I will be using 4 spaces instead of 8 for example. Same goes for all other code templates, and that's why I keep saying that **spaces are hard (as in hard-coded) and tabs are soft**, not the other way around. This one is trivial to fix by manually editing spaces to tabs, but there are other templates with more boiler-plate code which are not (C++ Windows DLL or Desktop Application projects for example). If you wrote all those templates using tabs, then you could just replace all tabs on template instantiation with the amount of spaces the user has actually configured for alignment in the editor, or just leave them be for everyone else who uses tabs."
technical,"Personally, I would be happy for no comment alignment to take place at all. Code statement alignment is generally predicatable, and is mostly determined in the user options.  The Editor can make reasonable assumptions about indentation if it is formatting code. Comment alignment is completely arbitrary - for example I sometimes like to indent my comments, like pseudo-code, or sometimes have them all vertically aligned.  The only thing the Editor can do with this is move it somewhere I don't want, or just leave it alone.  For example, currently I can copy and paste a block of code, and the Editor will keep the same predictable alignment for the code, but the comments will be moved in an attempt to align them with nearby comments.  I don't want this to happen.  The Editor is just guessing in this case, and the behaviour is unpredictable. It's all made worse by the Editor only using spaces, so I have to go back & find & remove those to realign the comments as I see fit.  In 9 times out of 10 it just creates more typing. Here is another example of the undesired behavior I detailed with screenshots in my previious comment above. Note that in my root Visual Studio Projects folder I have an .editorconfig file which contains, among other things: However, it seems that IDE0055 configuration is being entirely ignored, and clicking on any of the **Suppress or Configure issues** sub-options for it has literally no observable effect (no GUI feedback, no file changes / creations, nothing)."
technical,"That's quite possible.  My guess is that it happened when we did the entire roslyn rewrite.  We tried to preserve a lot of old behavior, but we very likely did not given the huge complexity in the old system and that the new system takes an entirely different approach on things.  Unfortunately though, we've now had this new system a long time, and we are wary about subjecting people to more potential changes in behavior, esp. as it might fix this issue, only to cause problems for others :-/   It's a tricky situation to be sure! Here's an example of an issue, based on previous discussions with other customers on this topic.  What i've heard from *other* customers (again, not you), is that it's not necessarily that they want tab to be preserved, but that they want comment alignment to be maintained.  in a system where we kept the tabs, we could potentially end up with something like this happening: And so on and so forth, instead of keeping comments aligned as they wanted."
technical,"This is not correct. Some users who use tabs for indenting prefer the current setting, since it allows for variable-width tabs without changing alignment.  Most  users who use tabs for indenting either prefer the current behavior  or  they are OK with either behavior.  We are willing to reconsider the design since some teams still aren't happy with the current behavior, but only if the design provides a comprehensive approach that works for both preferences. Git has the ability to transparently normalize line endings as part of commit/checkout, but there are many reasons why this would not work with tabs/spaces:  1. Git doesn't have the ability to alter its understanding of normalization to include characters other than end-of-line characters. 2. Git's normalization process is transparent, meaning any given developer never actually sees it take action. From a local perspective, the file only ever existed in the local form. 3. Not all users are working with Git for source control. 4. Tools which operate on files with checksum validation (e.g. debugging)  explicitly  account for the fact that every text file has two possible forms: one with \r\n and a second with \n. All of these tools will break if new normalization characters are added. How can I understand without discussing and talking with people about the topic?"
technical,"If i've missed any points, i apologize.  There's a lot being discussed here, and i likely missed it.  Feel free to point it out once this is unlocked and i'll respond as best as i can. i'm not disregarding this.  I'm just providing some insight that sometimes it's better to just stick with a regression versus introducing multiple regressions at multiple points in time.  i definitely commiserate with the initial regression being unpleasant.  However, with a rewrite the size of roslyn *and* the entire formatting engine being rewritten *from the ground up* it was unfortunately the case that this did happen.  However, now that we have the current system, i would be loathe to potentially have this same thing happen *again*. I totally get that that can be frustrating.  A group of people (that you are not a part of) is having their needs met, while yours are not.  However, this really is a space where potential changes risk very bad outcomes for many groups.  As i mentioned in this comment, this code is *the* most complex in the entire codebase for me.  Changes to it are both non-trivial and carry a large amount of risk.  Just a few days ago i discovered something new and unexpected about it for me, and i had to completely give up on a refactoring here because there was a part of this whose behavior was too subtle and complex to fully understand and i had to timebox myself out of it. Again, I'm happy to help clarify any of my statements, or address any points i may have missed.  My hope was to try to provide some clarity on the challenges here as this isn't really a case of: ""oh, this is just a bug with a trivial solution"" but rather more like ""this is a potential mine-field that we have to be very careful of"".  Also, as per this comment, i would be happy to continue this discussion in realtime on discord or gitter.  It might be more conducive than here as we can cover a lot more stuff, and we can make sure any particular questions/concerns you have are responded to.  If you are interested, LMK and i'm happy to meet up with you at practically any time.  thanks! I can certainly understand the frustration of **Always keep TABs** not doing what it says on the tin! I truly do want to fix this as we have under-served tabs users in the past and want to ensure it is just a viable and useful as spaces.  Let's talk potential solutions (setting aside for a moment the engineering cost) and see if we can agree on which one would be most helpful. Once we agree on a design, we can evaluate the engineering difficulty of it.  ## Tabs only mode  Simplest solution would be to have an option that means ""if this option is  on  Tabs are never allowed to be converted to spaces by the formatter"". I will tentatively call this  Tabs only mode  (since sadly ""Always"" is taken). I think this will operate like folks want but there is a corner case that has been called out that we need to consider which is what if we can't perfectly align things with just tabs?. So, if you set your tabs to some indenting (like say ""4"") we would try to get you as close as we can based on those settings with the understanding that we could be off by one or more. This would be an example of a worst-case scenario where we could choose to either be off by  one  or  three  when tabs have an indent size of 4.  I think intuitively we would say ""We should be off by as little as possible"". So, for worst case scenarios like the ones above we would choose option 2 over option 1. The question for this mode is can the developer ever actually get into a good state when things are not misaligned in this worst-case? The answer is probably not in this mode. Say I add an additional space in front of from:  The formatting engine will just take it away again as we already have a preference that keeps the spacing here to just one space. You could use spaces at the beginning of the line to align things and they would be left alone: But that is dangerously close to  mixing  tabs and spaces which is the very thing we don't want to do. I think like it's not obvious to me what would be most excellent here. But I am just one guy what do I know? Maybe we try a vote? ## Tabs preferred mode  An alternative would be ""Use spaces as little as possible"" named something like  Tabs preferred mode . In this mode we would use tabs to re-align things and only use spaces iff there was no other means to get things to line up. - **React with  for  Tabs preferred mode **"
technical,"Let me be absolutely clear that I did not suggest changing Git behavior, but rather emulating their set of options for the particular problem at hand.  With that out of the way, can we agree that VS editor already does some sort of ""normalization"" of TABs and spaces?  The problem in my opinion is twofold:  1. The existing settings that govern TAB .vs. space editor behavior are all over the place and totally do not work as described in the UI, much less produce results that are expected. 2. There is no What You Type Is What You Get editor setting -- I just want (and I am sure I am not the only one) that when I enter mixed TABs and spaces on a single line of code that they stay exactly like I entered them (save for expression reformatting which is already customizable enough for everyone's taste). You butted into the middle of a discussion by putting words in my mouth because you haven't bothered to understand what I wrote. Maybe that is somehow my fault because English is not my primary language and what I wrote is hard to understand, but in all honesty I am not sure what are you arguing for (or against). If you want to keep spaces in your files then rest assured that nobody here wants to take that away from you. I am asking for more options, not less. Therefore, please stop diluting the topic with pointless and/or obvious comments.  So once again, I only want an editor option to not touch beginning of line indentation, variable name indentation, and comment indentation -- it is irrelevant whether those are done with TABs, spaces, or a mixture of the two -- I want it preserved as I typed it.  I am really surprised to hear that something like that cannot be done while still keeping all current options. I didn't.  I said i'd never see such behavior.  I have not.  That was me just giving insight that I wasn't familiar with what you are talking about."
technical,"It could potentially be very difficult.  The formatting engine is one part of Roslyn that i find the most challenging.  Not ""it's one of the top challenging parts"".  Rather: ""it is literally the most challenging pieces of roslyn"". Changes here often have very unexpected effects that are unintended and can break users who have become accustomed to how things work.  We have to be very delicate here as this can be very detrimental to codebases. I don't think this is the case.  For example, i believe we may choose to align things, and not stick with tabs.  But i would have to go check on that."
technical,"Would be nice to get this fixed, would save much time. I was thinking of submitting a repro, but the OP hits the nail on the head:  just found out that if I hit the auto format keys (Ctrl-K, Ctrl-D), all the tabs I inserted between the command and the comment are replaced with spaces again. I searched through Tools/Options/Text Editor/C#/Code Style/Formatting, but I found no way to change this annoying behavior. This creates big whitespace changes after formatting the document as we use tabs at work. Please fix this."
technical,"So no other editor exhibits this behaviour, no editor before 2015 exhibited this behaviour, people are here reporting it as a bug, but you're are suggesting that it fixed some problem endemic since the dawn of computing.  That's bemusing to me, but if you're happy with it, then lucky you.  For everyone else, it's tough, because they **can't turn it off if they choose to**. I'm not suggesting the behaviour is changed with no way for any user to keep their preferred style (which is actually what happened in 2015), just that the user options behave as advertised. The formatting applied from the beginning of the line to code statements  does  follow the selected option - use spaces or tabs. The formatting between code statements and comments does  not  follow the selected option - it always uses spaces. It appears that the user option is ignored in this specific location.  The location and 'tabulation' of the formatting is fine, just the characters used. It is, lets say, disappointing that it sounds like Microsoft can't maintain it's code because it is too complex.  If you just told me that this shouldn't be addressed because I was a nobody and my opinion wasn't important, I'd be annoyed, certainly, but overall less concerned. I'm going to pause comments again since this is getting longer but not likely to reach a conclusion on the current path. In the spirit of open source transparency, CyrusNajmabadi and myself are attempting to explain the history of the current issue as well as the many different types of input considerations we face when attempting to resolve the issue. We understand that the current state isn't optimal for everyone, but honest risk evaluation makes it clear that alterations to address this issue will consume substantial time and engineering resources ( always  disproportionately large when working in the formatter) which directly translates to us not being able to use those resources to address other features, bugs, and overall product polish. As such, we have made the decision to leave this issue open while we work on other items that have broader customer impact."
technical," I'm having a similar problem with Visual Studio 2017 Professional (15.7.0.)  I have my indentation style set using .editorconfig: If I add a newline to a C# file, tabs are inserted as expected.  But, if I format the document using Ctrl+K Ctrl+D, any tabs in the file are replaced with spaces. If I use ReSharper to format my document, all indentation is converted to tabs as expected."
technical,"What 'bug' are you talking about? Please read the initial post again, and please stick to one topic per thread. There is nothing complex about the BUG he is reporting.  If that  is  what you are talking about, then please demonstrate another editor in existence which replaces tabs with spaces between code and comments.  That is NOT expected behaviour. If you are talking about something different then please split this thread as several quite different topics appear to be being discussed at once.  Nothing will get fixed that way. I'm referring to the same issue reported above. I'm not aware of another editor that works like this. When Visual Studio 2015 was released, I was happy to see it was the first editor to finally fix the behavior here.  Note that I'm definitely not saying everyone needs to agree with me. All I'm saying is if we put in a bunch of work to restore the pre-2015 behavior or come up with some new behavior that applies one way for all situations, some users will be happy and some users will not. The only thing that changed is it will be a different set of happy users than we have today, but from our side we're still going to be getting occasional reports about ""it's broken""."
technical,"How about you take the same approach as Git does for line endings?  1. Replace TABs with spaces on reformat 2. Replace spaces with TABs on reformat 3. Leave both as entered on reformat  Shouldn't the above satisfy everyone involved?  Come on people, this is not rocket science, it's a text editor for (Deity)'s sake! I've never seen git do that."
technical,"You are welcome. Is there any particular reason why I would want to press Enter in the middle of those tabs? If I wanted to move that comment to the next line I would probably press Enter right after last non-whitespace character, not in the middle of the whitespace. That aside, I would expect it to indent the comment with tabs from the start of line, and if it cannot decide whether to also add spaces to align the comment to the same column as the previous line then just leave the caret before the first non-whitespace character so I can do manually whatever I want. If you intentionally wanted to split the line so that there are two tabs stuck to the coment going to the next line then I really don't know the answer, because I'd never do that unless: - There was no auto-indent functionality in the editor (think Notepad) - The previous line was indented with exactly two tabs. Even then, such an action would still leave me having to clean up the remaining trailing whitespace (i.e. the other two tabs) on the previous line, so that would make me doing it in such a way even less likely. As it was stated before, we don't need perfect solution. Hovewer, I am afraid that we have gone way too far in our attempts to enable software to predict and adapt to human behavior -- to the point of making the software behave as unpredictable and unreliable as humans themselves. In most cases, pressing <kbdEnter</kbd in the middle of whitespace already behaves the same as pressing enter at the end of that same whitespace. This feature is frequently used. Can you show this using the  and  characters?"
technical,"Then just concentrate on the **original problem in the first post**.  It's a very straightforward problem. **Tab characters, between code and double-slashed comments, are changed to space characters when auto-indenting is performed.** Auto indenting works fine - it uses TAB characters when asked, but TAB characters after the code are changed. Why? This is with **Always keep TABs** option selected. If this is a feature, and not a bug, then someone please explain why, when **Always keep TABs** is selected, these TABs are being replaced by Spaces? This behaviour does not exist in older versions of VS.  At some point it has been added by someone, for some reason. That was the point at which the user experience was broken. It could potentially be very difficult.  The formatting engine is one part of Roslyn that i find the most challenging.  Not ""it's one of the top challenging parts"".  Rather: ""it is literally the most challenging pieces of roslyn"". Changes here often have very unexpected effects that are unintended and can break users who have become accustomed to how things work.  We have to be very delicate here as this can be very detrimental to codebases."
technical,"I'm not sure i agree.  Spacing allows things like alignment to happen in a very flexible fashion that is independent of whatever tab settings user have.  It may be worse for you, but better for others.  Hence the dilemma :) Oh, i've never found that to be the case myself.  This is why i always use spaces.  I can perfectly realign on any column effectively with spaces.  Ymmv of course :D Agreed.  This is one reason i ban tabs in all codebases i control.  It's just far too unpredictable across all scenarios (esp. when working with multiple tools).  I get that may not be palatable for your own preferences though of course :-/"
technical,"Funny, I never press Enter at the end of whtespace, but at the beginning. I can try, in both examples below I assume pressing Enter at the beginning of whitespace.  **Version 1 (VS editor cannot figure out whether I also want spaces or not after initial tab alignment)** However, given that the code above was a rather contrived example (and I am pretty sure most of us would prefer the comment to stay where it is anyway, making this comment alignment discussion even more contrived), that still focuses on minor style details while not addressing the key complaint most of us tab users here have.  **When I type this:** **I don't want it being changed to this when I type , at the end of line (or when such line is pasted):** Yes I know I can disable auto-format on , and on paste, but I want other formatting to happen. If you at least fixed that, you would have much less complaints from tab users.  Finally, I'd also like to point out how much everything is geared towards people using spaces. When you create a new C# library (.Net Framework) project in VS 16.9.1, initial code looks like this.  So, the template not only assumes I will be using spaces, but also that I will be using 4 spaces instead of 8 for example. Same goes for all other code templates, and that's why I keep saying that **spaces are hard (as in hard-coded) and tabs are soft**, not the other way around. This one is trivial to fix by manually editing spaces to tabs, but there are other templates with more boiler-plate code which are not (C++ Windows DLL or Desktop Application projects for example). If you wrote all those templates using tabs, then you could just replace all tabs on template instantiation with the amount of spaces the user has actually configured for alignment in the editor, or just leave them be for everyone else who uses tabs. Personally, I would be happy for no comment alignment to take place at all. Code statement alignment is generally predicatable, and is mostly determined in the user options.  The Editor can make reasonable assumptions about indentation if it is formatting code. Comment alignment is completely arbitrary - for example I sometimes like to indent my comments, like pseudo-code, or sometimes have them all vertically aligned.  The only thing the Editor can do with this is move it somewhere I don't want, or just leave it alone.  For example, currently I can copy and paste a block of code, and the Editor will keep the same predictable alignment for the code, but the comments will be moved in an attempt to align them with nearby comments.  I don't want this to happen.  The Editor is just guessing in this case, and the behaviour is unpredictable. It's all made worse by the Editor only using spaces, so I have to go back & find & remove those to realign the comments as I see fit.  In 9 times out of 10 it just creates more typing."
technical,"How can I understand without discussing and talking with people about the topic? So, i would need some strong answers to problems i see arising from changing the representation of the file for different developers.  First, that would violate some of our efforts we have around reproducible builds.  Second, it seems like it would just cause problems for normal situations like: Here, the continuation lines need to be indented 9 columns to maintain alignment.  Replacing these with tabs just breaks this.  Even if it was on some tab multiple for some developers, it might not be for others.  In general, I think all developers on the team (including CI) should operate on a bit-for-bit identical version of the code for many important reasons."
technical,"I'm happy to clarify any of my points.  Def ask questions and I'll get back to you asap.  If you'd like another venue to discuss things (perhaps in realtime) I'm also happy to use gitter or discord to continue the discussion :) Sure.  However, I'm pointing out that those options are potentially quite problematic.  I would be wary about adding them without fully understanding all the implications here.  And to get there, i need to talk about the topic. It was not obvious to me.  That's the reason i made the comment.  I was unfamiliar with what you were talking about, so i pointed that out to get clarity."
technical,"I apologize for being rude, but ever since Visual Studio 2015 I am sick of Visual Studio messing with my keyboard input and not respecting my configuration choices (and in some situations not even giving me any choice).  I understand that the problems with mixed tabs and spaces such as this very contrived example.  Are really complex to solve, but perhaps it would be prudent not to mess with indentation when mix of tabs and spaces is detected at line start? The rest of the line could be formatted according to the rules, leaving anything past the expression end as-is to avoid messing up comment alignment.  I understand that perfection will never be possible, but at least having an option to disable auto-conversion of tabs to spaces would be a start.  jmarolf Thanks for starting the discussion about potential solutions.  Maybe we are approaching the problem from the wrong angle? Maybe we should instead define rules such as:  - Always use tabs for line indentation, comment alignment. - Use tabs for variable declaration alignment unless they are in the middle of the code. - Always use tabs for right aligning before assignment operator. - Always align if sub-expressions on their opening paren. - Always align left and right side in if comparisons on the operator inside paren, pad left or right side as necessary.  If I am not mistaken the rules such as those would produce the results as in the above contrived example. Again, I understand that it might not be possible to code such rules, but at least it should be possible for me to format the code like that without having to fight with the editor for every character I type. thanks for this. In the sections where we are using tabs to align comments. If we pressed enter a the  location what would you expect the formatter to do ideally?"
technical,"I was thinking of submitting a repro, but the OP hits the nail on the head:  just found out that if I hit the auto format keys (Ctrl-K, Ctrl-D), all the tabs I inserted between the command and the comment are replaced with spaces again. I searched through Tools/Options/Text Editor/C#/Code Style/Formatting, but I found no way to change this annoying behavior. This creates big whitespace changes after formatting the document as we use tabs at work. Please fix this. The issue here is hard tabs contained *within* a line of code, i.e. tabs not used for indentation."
technical,"Yes, it's actually a primary request for users who indent with tabs.  The name of the new option has not been decided, but it would have two options:  1. Allow tabs whenever <kbdTab</kbd is used (matches the behavior prior to Visual Studio 2015) 2. Use tabs for indentation, but not for alignment  In most cases, the second option behaves as you see today. However, the behavior would change in cases where hanging indentation is used for aligning code with code on a previous line. For example. The preliminary design discussion is now complete. We will review the final user experience once it is ready."
technical,"thanks for this. In the sections where we are using tabs to align comments. If we pressed enter a the  location what would you expect the formatter to do ideally? THE problem currently is that if you press enter on that line, or a line next to it, and a code or comment alignment takes place, all those tabs before the comment are replaced by spaces.  Every time, no matter what TAB options are selected. I don't think there's anything wrong with the 'alignment help' as it is, except that it ONLY uses spaces, and never tabs, as reqested by the user. Please fix this."
technical,"This is still a problem.  The older versions of editors never used to behave like this.  Other editors don't behave like this.  Even Notepad doesn't behave like this. I'll explain simply: With the option to KEEP TABS selected in the Editor settings, TABs typed within code (so end of line comments line up, for example.) are later changed into spaces by Visual Studio.  When copying & pasting a line, for example. I NEVER want this to happen.  That's why I select the option to KEEP TABS. If I type a TAB in my code, I NEVER want it changed it to spaces.  It's really that simple.  Why is this not fixed after 2+ years? This is not an enhancement request, but simply a request that the editor behave as editors have always behaved since the dawn of computing.  At some point, someone in Microsoft decided to 'muck' around with users typed code and change it, when not one user asked for that to happen.    Are there really users that like to keep tabs only on specific parts, like indent?  Yes, it's actually a primary request for users who indent with tabs. That's simply not true.  Show us ONE request from ANYONE who asked for their TABs to be changed to spaces within their code when using the option KEEP TABS.  Also worth noting that this still happens when the 'Use Adaptive Formatting' option is turned off.  So it is impossible to prevent this from happening. There is no one consensus on the definition of correct behavior. The overwhelming majority of users are happy with the current implementation of tabs/spaces handling. Accounting for the remaining ones (in particular, this and ""always use tabs"") without breaking the experience for users who are happy with the current behavior is a particularly challenging exercise that requires both design and implementation work.  I'm not sure this will move up on the internal priority list in the near future, but if an external user wanted to spend the time to define and implement the full experience we would be happy to review it. See #23394 for a great example of a feature which shipped because a contributor went through this process. ˜„"
technical,"How about not commenting before understanding what you read? I never said Git does anything with spaces and TABs -- I explicitly mentioned line endings. This is not correct. Some users who use tabs for indenting prefer the current setting, since it allows for variable-width tabs without changing alignment.  Most  users who use tabs for indenting either prefer the current behavior  or  they are OK with either behavior.  We are willing to reconsider the design since some teams still aren't happy with the current behavior, but only if the design provides a comprehensive approach that works for both preferences. Git has the ability to transparently normalize line endings as part of commit/checkout, but there are many reasons why this would not work with tabs/spaces:  1. Git doesn't have the ability to alter its understanding of normalization to include characters other than end-of-line characters. 2. Git's normalization process is transparent, meaning any given developer never actually sees it take action. From a local perspective, the file only ever existed in the local form. 3. Not all users are working with Git for source control. 4. Tools which operate on files with checksum validation (e.g. debugging)  explicitly  account for the fact that every text file has two possible forms: one with \r\n and a second with \n. All of these tools will break if new normalization characters are added."
technical,"I'm going to pause comments again since this is getting longer but not likely to reach a conclusion on the current path. In the spirit of open source transparency, CyrusNajmabadi and myself are attempting to explain the history of the current issue as well as the many different types of input considerations we face when attempting to resolve the issue. We understand that the current state isn't optimal for everyone, but honest risk evaluation makes it clear that alterations to address this issue will consume substantial time and engineering resources ( always  disproportionately large when working in the formatter) which directly translates to us not being able to use those resources to address other features, bugs, and overall product polish. As such, we have made the decision to leave this issue open while we work on other items that have broader customer impact. under the following circumstances, VS2017express replaces tabs with spaces even though I deactivated that feature in the options: - I am working on a C# file - I am pasting a tab from my clipboard into a line of code, but not at the end, If I want more space between the command and the comment and I use my tab key, then tabs get inserted properly, but if I put a tab into my clipboard and insert it with Ctrl-V, then ALL tabs get replaced with spaces.  This problem does not occur in C++ files of the same solution, even though I set the tab configuration identical for all languages."
technical,"Yes, you're describing a bug related to a feature much bigger than just IDE0055 that we rely on ourselves. We do fix hundreds of issues.  But there are a lot of things out there that people want fixed.  So it's definitely possible that the set you care about isn't the set that always makes it into a release.   The ones in this area are particularly thorny, and we lack a good mechanism to get high confidence that changes here not only make things better for these cases, but also do not make things worse for cases that others care about."
technical,"99% of what you wrote translates into excuses to not even attempt to change anything because something could break while at the same time you are totally disregarding that it has already been broken for us, hence the bug reports. The remaining 1% is barely contained smugness and superiority of someone in a position of some power, and it is clear that you enjoy imposing your way on others given your ""ban on tabs"" which you mentioned above. So far you have not addressed any of my points, instead you keep spouting some weird mix of anti-tab religious zealotry and corporate PR talk. At the risk of getting banned from commenting on this and every other Microsoft repo I am going to call you out on your bullshit -- you, Sir, are full of it and presenting rational arguments to you is a colossal waste of time. I am out of here, and I will be advocating in my company to drop Visual Studio licensing. Maybe you will learn to honor user's preferences when they hit you in the wallet. We'll give this a bit of time to cool down. I did try implementing this once in the past, but it was difficult to get the formatter to understand the leading whitespace on a line might contain both hard tabs (indent) and spaces (alignment). Even the simpler option of having a mode to toggle between the current behavior and the pre-Roslyn behavior would be very difficult to implement. Since the formatter applies tab/space conversion in the middle of lines, it's unfortunately not possible to take the ""easy"" approach of limiting a fix to the handling of indentation. The current workaround is to disable the auto-format on type/paste option: Altering the behavior within the formatter is very challenging (combinatorial explosion in the heuristics frequently lead to subtle regressions) so given the behavior is 6 years old with less than 10  votes, it's going to be difficult to prioritize a change."
technical,"I'm having a similar problem with Visual Studio 2017 Professional (15.7.0.)  I have my indentation style set using .editorconfig: If I add a newline to a C# file, tabs are inserted as expected.  But, if I format the document using Ctrl+K Ctrl+D, any tabs in the file are replaced with spaces. If I use ReSharper to format my document, all indentation is converted to tabs as expected. Would be nice to get this fixed, would save much time."
technical,"Nice proposal to add an extra option. Are there really users that like to keep tabs only on specific parts, like indent? As the current option does reflect what it should do, to ""keep tabs"" and do not replace them with spaces - This is the behavior as I know it from previous versions and as it is within the C++ text editor - I would prefer an new option that says ""keep tabs only for indent"" for this new behavior. One saying ""Enforce tabs within lines"" would still confuse and let me wonder why I have to check that extra option in situations I already selected ""Keep tabs"". Yes, it's actually a primary request for users who indent with tabs.  The name of the new option has not been decided, but it would have two options:  1. Allow tabs whenever <kbdTab</kbd is used (matches the behavior prior to Visual Studio 2015) 2. Use tabs for indentation, but not for alignment  In most cases, the second option behaves as you see today. However, the behavior would change in cases where hanging indentation is used for aligning code with code on a previous line. For example."
technical,"There is already an issue for IDE0055 about it fighting vertical alignment (i.e. replacing tabs with spaces as I showed above). It's open for God knows for how long just like this one. I am not even going to mention the one I reported in .Net Environmet namespace two years ago.  Do you honestly believe that opening another issue will help? At this point I am discouraged to report more because nothing ever seems to get fixed (usually because ""it's complicated""). Yes, you're describing a bug related to a feature much bigger than just IDE0055 that we rely on ourselves."
technical,"THE problem currently is that if you press enter on that line, or a line next to it, and a code or comment alignment takes place, all those tabs before the comment are replaced by spaces.  Every time, no matter what TAB options are selected. I don't think there's anything wrong with the 'alignment help' as it is, except that it ONLY uses spaces, and never tabs, as reqested by the user. Please fix this. You are welcome. Is there any particular reason why I would want to press Enter in the middle of those tabs? If I wanted to move that comment to the next line I would probably press Enter right after last non-whitespace character, not in the middle of the whitespace. That aside, I would expect it to indent the comment with tabs from the start of line, and if it cannot decide whether to also add spaces to align the comment to the same column as the previous line then just leave the caret before the first non-whitespace character so I can do manually whatever I want. If you intentionally wanted to split the line so that there are two tabs stuck to the coment going to the next line then I really don't know the answer, because I'd never do that unless: - There was no auto-indent functionality in the editor (think Notepad) - The previous line was indented with exactly two tabs. Even then, such an action would still leave me having to clean up the remaining trailing whitespace (i.e. the other two tabs) on the previous line, so that would make me doing it in such a way even less likely. As it was stated before, we don't need perfect solution. Hovewer, I am afraid that we have gone way too far in our attempts to enable software to predict and adapt to human behavior -- to the point of making the software behave as unpredictable and unreliable as humans themselves."
technical,"See the reason why SDL was not accepted at. Can you please elaborate on how the SDL issue is relevant?  SDL was considered as an abstraction layer on user input systems and getting rendering contexts ready to use.  Bgfx, however, allows full and API-agnostic rendering.  Do you mean that maintenance of an additional abstraction layer in Godot would be painful?"
technical,"possible dumb question incoming: but, when vulkan rendering stuff gets complete, it would negate this issue right?  i imagine time spent on vulkan will far supersede bgfx, or other rendering systems correct.  fire the way I understand your comments, it's an issue of third party support (what if Godot needs something that bgfx does not offer in the future), and having full control over the implemented features.  I get it, but I also hope that Vulkan works as advertised as well."
technical,"My 5 cents: maintaining your own renderer is more actual work than it sounds. And it will drain your resources on fixing things that don't necessary add end user value.  Image one day you need to ship a game on PS4/Switch/etc, what do you do? Spend month porting Godot to a random platform? bgfx is already ported IIRC. And then another day you get strange rendering bug reports on random Android phone that you never know existed, what you gonna do? etc.  To get real feeling of what it takes to actually ship something GL based nowadays, try to look at UE4 - it is full of hacks for random devices. Value of rendering libs is not that they are ""cross-platform"", but rather that they are proven to work on platforms.  IMHO Value of Godot is not in this, but rather in features in the editor. Exactly. Bgfx is being used in production by renowned studios to deploy cross-platform products. So it's reasonably battle-tested and I don't see any compelling reason not to take advantage of that, not to mention the API is very well designed too. Angle isn't a good option if you care about performance."
technical,"I understand that you are defensive about the project you maintain, but please re-read reduz's arguments and you'll see that there is no critic made of BGFX itself.  Replace ""BGFX"" by ""a rendering middle layer"" and you'll get what reduz meant, if it was not clear enough. reduz doesn't even have experience using BGFX, so he can only assess the concept of a rendering middle layer and not BGFX's specific API and features.  The TL,DR would be:  1. A rendering middle layer wouldn't simplify things as we still need to rendering pipelines anyway.  2. A rendering middle layer *is* added complexity, as we need to debug both the middle layer and the low level graphics API when facing driver-related issues.  3. *If* we had to use a rendering middle layer, the Vulkan PI seems a lot more promising for our needs.  4. Using Vulkan directly instead of a rendering middle layer means that we can implement new features the way we want, directly.  5. We want to work with Khronos to further Vulkan's usage, and doing our own Vulkan renderer is the best approach for that.  As you can see, there is no judgement of value of BGFX itself, it's just as you said:   In the past you made statements that you want to control whole stack, and you don't want to introduce risk by adding 3rd party open source software. I find this is more honest response, since there is nothing that I can add or remove from bgfx that would make you reconsider your strategy about renderer.  That's exactly what the above points amount to.  So we're glad that BGFX exists and that it's a great solution for many applications, but as of today it's not something that we're interested in for Godot. Thanks for respecting our decision. I gained a lot of interesting information here, not only by the Godot members, but also from bkaradzic .  Just wanting to clarify the reasoning behind my question:  Personally, when developing, I want options. In the case of a game engine, I want it to be as flexible as it can be - this is why I would choose bgfx - because I want to be able to not be unaffected by changes and politics of the graphics ecosystem (like Apple not supporting OpenGL anymore, etc). The reasoning behind my question was if Godot shared the same thoughts.  In the case of Godot, the team wants full control over  one  rendering stack, and flexibility comes second. This will probably allow them to implement more features and be completely unaffected by 3rd party frameworks in the long run. Which is also a very good approach.  --  From this discussion, the thing I am keeping is that bkaradzic was right to point out that using bgfx actually saves a ton of code (something I have seen in many testimonies around), but akien-mga and reduz were also right to point out that if the flexibility of supporting more than one rendering backends is not a primary goal, then relying on a 3rd party middle layer just for having only one rendering backend may hinder the process later down the road."
technical,"Guys, thanks a lot for your enthusiasm, but I am the one doing the rendering work in Godot, not you. I've been working on 3D rendering for 25 years so, if I am telling you that things as they are now are optimal and BGFX will just stand in the way to being productive I hope you believe me.  As always with Godot, you are free to make your own renderer with BGFX, show me that it's flawless, has better better performance, works better than mine (of course while supporting the full Godot feature set) and uses less resources and code to prove me wrong.. as well as commiting full time over the next years to maintain it.  If you want a revolution, begin with it yourself. If you want to prove your ideas, invest in them because words are free.  If not, keep pestering all you want and it will be ignored. I only remember ever mentioning bgfx once or maybe twice in the past. All I can find is this comment back in 2017 asking  ""What about bgfx?"" . I don't understand why you would include me. I guess it's because I up-voted a comment I consider brings an interesting point to the table.  Regarding the others, I don't understand why such a negative response. I don't see anything wrong in their comments. They are not pestering in any way..."
technical,"This issue has long outlived its usefulness. And insulting community members is not tolerated in this community, so you are indeed more than encouraged to stay clear from this project. I started using Godot lately, and I find it ingenious. I am using a Mac, and while I don't intend to release something Mac-only in the end, it is the best development platform for me, as well as many others.  Following the news regarding the deprecation of OpenGL for Apple platforms I saw this issue on GitHub, and I personally feel that the issues raised by the Godot team are all valid. It also seems that the team has already planned Vulkan support, and Mac / iOS will be supported using MoltenVK.  However, before this happens, I would like to throw another option to the table, which is using bgfx as a rendering backend.  BGFX has DirectX, OpenGL, and Metal support out of the box and will soon add Vulkan support. MoltenVK for Mac also seems like a viable option, but it has some limitations atm which I have to admit I don't know how much they affect Godot (if at all).  Since BGFX's API allows supporting multiple rendering backends without changing its core API, perhaps using this instead of investing time only in Vulkan will be better for Godot in the long run?  I totally respect Godot team's decisions regarding Vulkan, and I trust that if Godot chooses this path it will work as good as now - if not better. I just threw the BGFX idea in order to have some feedback on whether BGFX was something that was already considered - and what is the team's opinion on choosing BGFX instead of investing time only on Vulkan support (also having to trust that MoltenVK will always work as advertised)."
technical,"It definitely  is  complexity. You seem to be assuming that just because of adding BGFX support, everything will be fine and no one will ever have to learn how the underlying APIs work.  This is, unfortunately not the case and how things work in real life. This may work for APIs or libraries where you truly no longer need to care how internal implementations of things work as long as they do what they have to. Bullet is a good example of this.  However, in my experience, if we were to use BGFX, contributors would still need to also know the underlying APIs as well (OpenGL or Vulkan). How else are they supposed to fine tune performance, or understand why something fails when it does?  If this was a simple game, or a tool or something like that I am sure BGFX would be fine, but thinking BGFX will fit like a glove for us and just work is very naive. We have a lot more responsibility than you do at providing something that works for our users. We need to extract the best out of the resources and performance from the underlying hardware, since we are already a middle layer ourselves.  So, as I already made it pretty clear that there is no advantage to it, due to us needing two rendering pipelines, and that contributors will still need to understand the underlying rendering APIs. I hope you understand that to me it's just extra unnecessary complexity that is best kept away of Godot. I understand that you are defensive about the project you maintain, but please re-read reduz's arguments and you'll see that there is no critic made of BGFX itself.  Replace ""BGFX"" by ""a rendering middle layer"" and you'll get what reduz meant, if it was not clear enough. reduz doesn't even have experience using BGFX, so he can only assess the concept of a rendering middle layer and not BGFX's specific API and features.  The TL,DR would be:  1. A rendering middle layer wouldn't simplify things as we still need to rendering pipelines anyway.  2. A rendering middle layer *is* added complexity, as we need to debug both the middle layer and the low level graphics API when facing driver-related issues.  3. *If* we had to use a rendering middle layer, the Vulkan PI seems a lot more promising for our needs.  4. Using Vulkan directly instead of a rendering middle layer means that we can implement new features the way we want, directly.  5. We want to work with Khronos to further Vulkan's usage, and doing our own Vulkan renderer is the best approach for that.  As you can see, there is no judgement of value of BGFX itself, it's just as you said:   In the past you made statements that you want to control whole stack, and you don't want to introduce risk by adding 3rd party open source software. I find this is more honest response, since there is nothing that I can add or remove from bgfx that would make you reconsider your strategy about renderer.  That's exactly what the above points amount to.  So we're glad that BGFX exists and that it's a great solution for many applications, but as of today it's not something that we're interested in for Godot. Thanks for respecting our decision."
technical,"You are both wrong. We can still rely on more rendering back-ends via lower level wrappers like Angle (OpenGL ES 2.0) and MoltenVK / GFX-RS (Vulkan), so we already get best of both worlds. My 5 cents: maintaining your own renderer is more actual work than it sounds. And it will drain your resources on fixing things that don't necessary add end user value.  Image one day you need to ship a game on PS4/Switch/etc, what do you do? Spend month porting Godot to a random platform? bgfx is already ported IIRC. And then another day you get strange rendering bug reports on random Android phone that you never know existed, what you gonna do? etc.  To get real feeling of what it takes to actually ship something GL based nowadays, try to look at UE4 - it is full of hacks for random devices. Value of rendering libs is not that they are ""cross-platform"", but rather that they are proven to work on platforms.  IMHO Value of Godot is not in this, but rather in features in the editor."
technical,"See this.  Imagine a situation where we use SDL2 and drop the existing backends. What happens every time we need a platform specific feature not available on SDL2? (Something it happens often) . We have the following scenarios:  Added to that, the problem is that we may need to add something that SDL does not support and, while for us it's something specific with a simple use, while adding this function to SDL may involve creating a large API with all the functions that are needed for abstraction.  I'm sorry, no matter how I try to think of ways we could use SDL, it's always more disadvantages than advantages..  Now replace SDL with BGFX.  There are two parts of abstraction. A) low level abstraction B) construction of the rendering pipeline for mobile (gles2) vs pc (vulkan)  Using bfgx only helps for A), B) needs to rebuilt. possible dumb question incoming: but, when vulkan rendering stuff gets complete, it would negate this issue right?  i imagine time spent on vulkan will far supersede bgfx, or other rendering systems"
technical, See the reason why SDL was not accepted at.
technical,"Can you please elaborate on how the SDL issue is relevant?  SDL was considered as an abstraction layer on user input systems and getting rendering contexts ready to use.  Bgfx, however, allows full and API-agnostic rendering.  Do you mean that maintenance of an additional abstraction layer in Godot would be painful? See this.  Imagine a situation where we use SDL2 and drop the existing backends. What happens every time we need a platform specific feature not available on SDL2? (Something it happens often) . We have the following scenarios:  Added to that, the problem is that we may need to add something that SDL does not support and, while for us it's something specific with a simple use, while adding this function to SDL may involve creating a large API with all the functions that are needed for abstraction.  I'm sorry, no matter how I try to think of ways we could use SDL, it's always more disadvantages than advantages..  Now replace SDL with BGFX.  There are two parts of abstraction. A) low level abstraction B) construction of the rendering pipeline for mobile (gles2) vs pc (vulkan)  Using bfgx only helps for A), B) needs to rebuilt."
technical,"I never said anything negative about BGFX itself, only in the context of Godot.  The point of the discussion was mainly that, given two render pipelines need to be written anyway, using BGFX is not an advantage or solution regarding to that. You answers completely missed it.  For the other answers, I was not questioning whether BGFX was high or low level, or whether it can be debugged, or anything else. The point was simply that  it is  added complexity (which is undeniable) for an use case that is redundant and provides no advantages ** in the context of Godot **. As this added complexity does not solve our problems, there is no justification to going for it.  Apologies, but I think you are being overly defensive when the whole argument was about Godot, not BGFX. The point was simply that it is added complexity (which is undeniable) for an use case that is redundant and provides no advantages in the context of Godot. As this added complexity does not solve our problems, there is no justification to going for it.  This is what I'm talking about. You're saying that bgfx would add complexity into your code base, but it's actually quite oposite, by using bgfx it removes complexity from usual code base. Once someone replaces their own renderer with bgfx, they usually have redundant code on higher level, they don't have to do state tracking anymore since bgfx deals with that, they don't have to do multiple passes over scene because bgfx allows you to submit out of order and orders draw calls for you, etc. But bgfx is complex on it's own.  In the past you made statements that you want to control whole stack, and you don't want to introduce risk by adding 3rd party open source software. I find this is more honest response, since there is nothing that I can add or remove from bgfx that would make you reconsider your strategy about renderer. So no need to justify your decision about renderer by saying bgfx does or doesn't do X, Y, Z, if your decision was unrelated to bgfx as you stated before."
technical,"I gained a lot of interesting information here, not only by the Godot members, but also from bkaradzic .  Just wanting to clarify the reasoning behind my question:  Personally, when developing, I want options. In the case of a game engine, I want it to be as flexible as it can be - this is why I would choose bgfx - because I want to be able to not be unaffected by changes and politics of the graphics ecosystem (like Apple not supporting OpenGL anymore, etc). The reasoning behind my question was if Godot shared the same thoughts.  In the case of Godot, the team wants full control over  one  rendering stack, and flexibility comes second. This will probably allow them to implement more features and be completely unaffected by 3rd party frameworks in the long run. Which is also a very good approach.  --  From this discussion, the thing I am keeping is that bkaradzic was right to point out that using bgfx actually saves a ton of code (something I have seen in many testimonies around), but akien-mga and reduz were also right to point out that if the flexibility of supporting more than one rendering backends is not a primary goal, then relying on a 3rd party middle layer just for having only one rendering backend may hinder the process later down the road. The problem is you need to support many APIs if you want to ship a cross-platform product of acceptable quality. If multiple backends isn't a primary goal then by consequence cross-platform support isn't a goal too. It's not pretty, it's not what people want to hear, but it's the reality we live in today. In order to ship a quality cross-platform product you can only accept that and do what it takes to conform to that reality. And yes, I'm ready for the thumbdowns... let them come!"
technical,"I gained a lot of interesting information here, not only by the Godot members, but also from bkaradzic .  Just wanting to clarify the reasoning behind my question:  Personally, when developing, I want options. In the case of a game engine, I want it to be as flexible as it can be - this is why I would choose bgfx - because I want to be able to not be unaffected by changes and politics of the graphics ecosystem (like Apple not supporting OpenGL anymore, etc). The reasoning behind my question was if Godot shared the same thoughts.  In the case of Godot, the team wants full control over  one  rendering stack, and flexibility comes second. This will probably allow them to implement more features and be completely unaffected by 3rd party frameworks in the long run. Which is also a very good approach.  --  From this discussion, the thing I am keeping is that bkaradzic was right to point out that using bgfx actually saves a ton of code (something I have seen in many testimonies around), but akien-mga and reduz were also right to point out that if the flexibility of supporting more than one rendering backends is not a primary goal, then relying on a 3rd party middle layer just for having only one rendering backend may hinder the process later down the road. This issue has long outlived its usefulness. And insulting community members is not tolerated in this community, so you are indeed more than encouraged to stay clear from this project."
technical,"correct.  fire the way I understand your comments, it's an issue of third party support (what if Godot needs something that bgfx does not offer in the future), and having full control over the implemented features.  I get it, but I also hope that Vulkan works as advertised as well. This option was discussed and discarded in the past (as you can imagine), but since this was not discussed in an issue, I will take the time to explain why BGFX is not an option.  1) There are two main platforms that need to be supported. Modern Desktop PC (OpenGL ES 3, and later Vulkan) and mobile including medium and low end (OpenGL ES 2). These APIs are not programmed the same way, so a ""wrapper"" to simplify the work is impossible. GLES3/Vulkan uses UBOs, VAOs, TBOs. shaders with integers and plenty of features. GLES2 is very basic and supports none of that, so different approaches need to be used to write a back end. Modern hardware also uses certain rendering techniques (HDR/clustered/single pass shading), while low end uses other techniques (LDR/multipass). As such, being a fact that pretty much no code is shared between backends, BGFX does not save the work of having to write two backends for different hardware.  2) Having an extra layer of complexity in the middle makes things more complex, and makes debugging more difficult. Contributors would need to learn BGFX to write rendering code instead of a more standard API like Vulkan or OpenGL, out of which there is plenty of documentation and examples. If you want to use tools for debugging Vulkan or OpenGL, they will be confusing with BGFX as a backend too. As net worth, BGFX as a middle layer is a negative point here, not a positive one.  3) Khronos has the Vulkan Portability Initiative, where they aim to run it over Metal, DirectX, etc. This is pretty much the same as BGFX, thus decreasing even further the value of  BGFX.  4) Using Vulkan allows us to take advantage of extensions and new features on bleeding edge hardware much faster than using BGFX.  5) Our contributors are part of the Khronos Advisory Board, so by using Vulkan we can give valuable feedback of our experience to hardware manufacturers, making sure they hear us when woring on future versions of the specification.  Hope it's clearer now! There is more than plenty of reasons to not use BGFX."
technical,"This option was discussed and discarded in the past (as you can imagine), but since this was not discussed in an issue, I will take the time to explain why BGFX is not an option.  1) There are two main platforms that need to be supported. Modern Desktop PC (OpenGL ES 3, and later Vulkan) and mobile including medium and low end (OpenGL ES 2). These APIs are not programmed the same way, so a ""wrapper"" to simplify the work is impossible. GLES3/Vulkan uses UBOs, VAOs, TBOs. shaders with integers and plenty of features. GLES2 is very basic and supports none of that, so different approaches need to be used to write a back end. Modern hardware also uses certain rendering techniques (HDR/clustered/single pass shading), while low end uses other techniques (LDR/multipass). As such, being a fact that pretty much no code is shared between backends, BGFX does not save the work of having to write two backends for different hardware.  2) Having an extra layer of complexity in the middle makes things more complex, and makes debugging more difficult. Contributors would need to learn BGFX to write rendering code instead of a more standard API like Vulkan or OpenGL, out of which there is plenty of documentation and examples. If you want to use tools for debugging Vulkan or OpenGL, they will be confusing with BGFX as a backend too. As net worth, BGFX as a middle layer is a negative point here, not a positive one.  3) Khronos has the Vulkan Portability Initiative, where they aim to run it over Metal, DirectX, etc. This is pretty much the same as BGFX, thus decreasing even further the value of  BGFX.  4) Using Vulkan allows us to take advantage of extensions and new features on bleeding edge hardware much faster than using BGFX.  5) Our contributors are part of the Khronos Advisory Board, so by using Vulkan we can give valuable feedback of our experience to hardware manufacturers, making sure they hear us when woring on future versions of the specification.  Hope it's clearer now! There is more than plenty of reasons to not use BGFX. this was exactly the kind of answer I was aiming for, thanks."
technical,"The problem is you need to support many APIs if you want to ship a cross-platform product of acceptable quality. If multiple backends isn't a primary goal then by consequence cross-platform support isn't a goal too. It's not pretty, it's not what people want to hear, but it's the reality we live in today. In order to ship a quality cross-platform product you can only accept that and do what it takes to conform to that reality. And yes, I'm ready for the thumbdowns... let them come! You are both wrong. We can still rely on more rendering back-ends via lower level wrappers like Angle (OpenGL ES 2.0) and MoltenVK / GFX-RS (Vulkan), so we already get best of both worlds."
technical, can you please explain what exactly you are concerned about. We don't have any plans to remove the dark theme.
technical, Is this even one more layer of trolling?
technical,"We are not removing the dark theme, sorry for the confusion. The title pretty much says what needed to be said. Having a white background with different coloured text can make thing extremely hard to read and can do more harm to your eyes than having a black (or dark grey) background with different coloured text makes it far easier to read."
technical,"Is this even one more layer of trolling? This issue, no, it was a big concern till they stated that they have no plans to remove the dark theme."
technical," We are not removing the dark theme, sorry for the confusion."
technical,"Yes, we're on the same page that all lives matter. I read some thoughtful and reasonable responses in this thread, and I understand how everyone has their point of view. We're all part of a very diverse community, and having different opinions is understandable. I also don't see bad intentions in the thread, which is all encouraging. We're all aiming for racial equality, and that is how it is supposed to be. At the same time, I'm hoping for us all to provide solidarity to the black community. Again, all lives matter, but for this to be true, black lives should matter as well. I'll lock this thread now since the discussion is getting a little too heated. Change #BlackLivesMatter to #AllLivesMatter"
technical,"What's racist about it? Also don't shame others based on their skills and development experience. That's against the code of conduct.  Precisely this. Angular/Google are just using the platform to promote their local political views on the matter. It reminds me of the day when gay marriage was legalized in the US and Stack Overflow celebrated it with a rainbow logo.  This answer explained it nicely. The owners and operators of Stack Exchange and Stack Overflow have made it amply clear that while we as users and contributors are not permitted to use the site and our audience here to promote our political and social beliefs, they, as the owners may.  With an unfortunate conclusion: Indeed, there's not much you can do about it. People can express their opinions but the mods are just gonna lock it away ""cause y'all can't behave"" and ""the discussion is getting out of hand and is not productive anymore"". From the top of my head, previously it's been determined that blacklist and whitelist is racist. Funny how the first comment is being sarcastic about the master branch and that is actually non-ironically happening now, because apparently there exist people who hear ""master"" and immediately think about slavery. I guess I'll give up my Master's degree to show solidarity. Also a man being smart is sexist towards women because why not. Not really political, but I get the same vibe from stating that simple things are simple offends beginners. I agree with this, ""All lives matter"" isn't inherently racist and is a very true statement. In this context, it's undermining the BLM movement where special attention is warranted for the systemic discrimination over many many years."
technical,"seems like that's problem in USA only, we're respect all races in Ukraine for example, doesn't matter where are you from or etc. So you need fix it in you county, and do not force your problems to all world! Only a short notice. We all together should be also very careful about the tendency, when one or the other side wants to win, even at any price and on any conditions, just the believing: ""We won in the end."" We have one proverb: Be careful not to spill the baby with the bath."
technical,"Just noticed the #BlackLivesMatter banner on the home page for the first time...what a joke. Anyone have a list of other political movements endorsed by the Angular site?  Nothing more cringey than tech framework sites posting about politics, even if you agree with the sentiment. Didn't one of the #BlackLivesMatter ""leaders"" call to take down statues of Jesus depicted as Caucasian, as this is a form of ""racist propaganda""? Can we change the hashtag on the main page to #WhiteJesusIsHoldingYouBack ? You know what, the white ""A"" in the Angular logo is all of a sudden making me feel uncomfortable...maybe even a little oppressed. Hmm, could the Angular team be racists??? Maybe we should make a separate issue to change the color to black. Silence is violence. It's important for business to get behind social causes to advance past the sins of humanity of the past. There is clearly a systemic discrimination toward the black community in the west. Of course all lives matter, but for all lives to matter you need to include black lives which clearly is not being represented equally"
technical,"Nice, the only notable contribution you've ever made anywhere on Github is making sure everyone at Google knows you're a racist. Where you see racism in #AllLivesMatter ? In my opinion #BlackLivesMatter is racist slogan against white people. Beside, why technology web page contains these things? It should contains angular related things only."
technical," China law not world law, if you are using this framework at China, feel free to address it by yourself."
technical,"China law not world law, if you are using this framework at China, feel free to address it by yourself. Just want to jump in here because it seems to be getting slightly heated.  This docs page has nothing to do with countries or territories it's just to show all the ** flags ** provided by SUI and because Taiwan and China use different flags there is 2 entries in this list."
technical," Translated:  ### Hello, your language version violates Chinese law.   Hello there. The marking of the country on your document violates Chinese law. Taiwan is a part of China and a province of China! Should not be listed separately, I hope to modify"
technical,"Translated:  ### Hello, your language version violates Chinese law.   Hello there. The marking of the country on your document violates Chinese law. Taiwan is a part of China and a province of China! Should not be listed separately, I hope to modify Uhhh China ......"
technical, ur mum this is other language and tw and hk is using same lanuage we dont want to fking Simplified chinese L
technical, Closing due to abusive language. Please read our code of conduct and open an issue using appropriate language.
technical,"you are funny :) you can learn a bit about semantics, grammar and the meanings of words - essentialy about language and usage :) can you tell me who was here offended ?  **maybe better was to use here a euphemisms so that the dumb people didn't think they were being offended when in fact nobody was...**  if you fell so then it is your right to feel as you want to :)  understanding is key to everything.. so **try to understand it does not hurt!** If this is your way of communicating, please stay away from our community."
technical,"When can we expect the release of this necessary feature? samtstern Closing this conversation to prevent more ""ETA requested"" responses (which actually take time from feature work).  Latest status update: Firebase team will update here and potentially unlock thread if/when there is an update."
technical,"this is indefinitely on hold ... amandle left the project and we are focusing on some other priorities for now. FWIW, I'd really love to see this implemented. If I'm reading the comments here correctly, it seems like the work was already done for Android, but never released because it wasn't available on the other platforms. Is it possible to just push the release out for android?  I would implement the other devices myself if I had the time, but it seems like a shame to have this stuff ready to go but just waiting on other implementations."
technical,"Unless there are objections I'm going to start taking a crack at this. Go for it, thanks!"
technical,"This is really a necessary feature. I am currently using the unreleased/unsupported iOS FUIAccountSettingsViewController, and it seems to be working for me. But it should be officially released and also available for Android.  At the moment FirebaseAuthUI is a 80% solution for iOS and even less for Android. To be able to change email/password etc i still need to build everything myself and i really start to doubt our decision to use FirebaseAuthUI in the first place.  From the website: ""Easily add sign-in to your Android app with FirebaseUI FirebaseUI is a library built on top of the Firebase Authentication SDK that provides drop-in UI flows for use in your app.""  No it's not. Not if there is no support to do basic operations like email/password change.  We need either documentation how to handle reauthentication (besides email/password which is documented) or even better, release the FUIAccountSettingsViewController and an Android-port of it. It's been more than 2 years from its initial progress and more than a year from the last update, did you guys abandon this key feature?"
technical,"Go for it, thanks! May I know , what is the current status of this implementation?"
technical,"Closing this conversation to prevent more ""ETA requested"" responses (which actually take time from feature work).  Latest status update: Firebase team will update here and potentially unlock thread if/when there is an update. There are some Firebase Auth actions that require a recent (5mins) sign-in to succeed:  * Delete account (#478) * Change email * Etc We should offer a simple method to launch into a re-authentication flow that allows the user to sign in choosing from any of their linked providers.  Considerations: * This would likely have to be a method that returns an Intent similar to the sign-in intent * The developer will probably want to provide a reason for the re-authentication, maybe passed in * This would be similar to the normal sign-in flow with a few alterations: * Account creation disabled * Account picker screen lists only methods the user has already linked to the FirebaseUser"
technical,"May I know , what is the current status of this implementation? this is indefinitely on hold ... amandle left the project and we are focusing on some other priorities for now."
technical,"FWIW, I'd really love to see this implemented. If I'm reading the comments here correctly, it seems like the work was already done for Android, but never released because it wasn't available on the other platforms. Is it possible to just push the release out for android?  I would implement the other devices myself if I had the time, but it seems like a shame to have this stuff ready to go but just waiting on other implementations. This is really a necessary feature. I am currently using the unreleased/unsupported iOS FUIAccountSettingsViewController, and it seems to be working for me. But it should be officially released and also available for Android.  At the moment FirebaseAuthUI is a 80% solution for iOS and even less for Android. To be able to change email/password etc i still need to build everything myself and i really start to doubt our decision to use FirebaseAuthUI in the first place.  From the website: ""Easily add sign-in to your Android app with FirebaseUI FirebaseUI is a library built on top of the Firebase Authentication SDK that provides drop-in UI flows for use in your app.""  No it's not. Not if there is no support to do basic operations like email/password change.  We need either documentation how to handle reauthentication (besides email/password which is documented) or even better, release the FUIAccountSettingsViewController and an Android-port of it."
technical, Unless there are objections I'm going to start taking a crack at this.
technical,"It's been more than 2 years from its initial progress and more than a year from the last update, did you guys abandon this key feature? When can we expect the release of this necessary feature? samtstern"
technical,"That's a fair point about separating dependencies for the task from the tool, I always think of Java as a dependency for the SonarQube task but it's actually a dependency for the SonarQube tool that is presented to the user as requirement for the build agent to have installed.  The only clean way of handling making use of new functionality in newer versions of a dependency that I can see is to increment the major version number of your task, so it becomes a choice for the users to make use of it and the need to install a new version of the dependency. The other approach requires adding checks for that version in your code before trying to use it, which grants more compatability but takes more time and effort on the part of the task author.  I still think updating for new prereqs is less of a problem for people using containers due to their nature but I know there will be a large number who don't want to change their working container because a task updated a bit. It's also still a bigger problem for people not using containers as that takes more time and effort to roll out new versions of prereqs.  I'll certainly give some thought to writing a typescript wrapper for the pester task I already have and see how that performs. Hopefully some of these more interesting and difficult design questions can be worked out to enable a PS Core handler.  Thanks for taking the time to explain this stuff Bryan, it's given me a much better idea of how it all hangs together and the things you have to think about when working on the agents. Cool.  Thanks for listening :)"
technical,"I don't really know how this works in the background but a powershell task that does not use the vsts-tasks-lib should be executable or? because it does not depend on anything OS Specific? I wrote some tasks that do not use vsts-tasks-lib they are just pure powershell script and they also don't run on Linux by the agent. I haven't tried running them natively without vsts on the linux machine. DerGary - the agent has a handler in it that basically knows how to run the script that's in the task.  So the node handler runs node against the script that's declared in the execution section of the task.json.  The task is run out of proc and communication and status is communicated via an RPC stdout protocol (hash hash vso statements in output).  The task lib is simply libraries with functions that emit that protocol over stdout.  It's a convenience.  If you just have an arbitrary powershell script that you need run as part of your build / release, then you can use the powershell task (2.x version) to run.  A formalized task is the way to share your unit of work with others on the market place.  See the step by step walk throughs in this repo.  You can also look at our tasks in the vsts-tasks repo as reference examples."
technical,"We also really need a native PowerShell Core task execution support (for Windows-based agents at least).  As far as I understand you guys (bryanmacfarlane) already have a backlog item for this, however I want to understand when you're planning to implement/release it? dmitryserbin I think while this is something we're interested in, its not high priority for us right now so we don't have an immediate eta. There's a lot of challenges associated with this (see above) and if we do decide to move forward it will take a concerted push/thought that we don't have the bandwidth to invest right now."
technical,"Hi, per #537 we've decided not to add this at this time Hi there,  to open this possible enhancement. Now PS based task won't even try to run on Linux/MacOS agent because of the missing handler. However, PowerShell is installed on those agents and you have the PowerShell task available for them and works... As Bryan pointed out, custom PS task may have windows assumptions, but that's okay, developers will just have to make those scripts OS aware to avoid those pitfalls. With the handler in place they can see where there scripts fail when running on Linux/MacOS PowerSHell implementation and fix them. I think this will open more options to PS developers."
technical,"That is not correct.  The agent carries node as the task runtime.  It's completely self contained so adding a task from the market place works.  And that's the work that this enhancement needs. That won't work in container jobs as pointed out above.  Note that a task dependency is different than installing tools in an environment.  The user and consumer of tasks brings tools they use Note that we have not said we're not doing this which is why enhancement is still open.   The work is known and it's on our backlog.  The requirements, patterns and expectations have been set for how the type script tasks work.  When we add powershell core support, the requirements are the same and it will need to work for all of our features. I would kindly ask that we keep the conversation friendly and respectful.  Please refer to our code of conduct. We are keeping this issue open and will address it on our backlog. Thanks for all of your feedback -  All the details and considerations are captured in this issue. Hi, per #537 we've decided not to add this at this time"
technical,"sandersaares - as noted above, it's not about the user installing pre-reqs.  The consumer (user) of task simply goes to the market place and uses it.  They shouldn't have to inspect the tasks code and figure out what it's pre-reqs are.  The requirements are, choose task, use, works.  So market place tasks cannot have pre-reqs.  For those reasons, the agent carries the javascript runtime (node).  It's a self contained executable.  We also need to make it work in containers since jobs can run in containers.   That includes alpine and we're tracking.  For typescript tasks, we map it into the container (user can pick any container) and at that point it's about the OS pre-reqs which is almost zero for typescript/node.  netcore has been reducing OS dependencies with 2.x and 3.x so we're evaluating.  All of this is possible since we could potentially package ps core in the agent but it will take quite a bit of work, docs etc. and we need ps to address the alpine (wasn't there when we last looked).  This is why we've left the issue open.  We're still considering and looking at the work involved.  Note that you can use the typescript task-lib to exec powershell core against a powershell core your task carries.  That will have the issue mentioned above of a task having a pre-req though. I believe PS Core now supports Alpine, might be worth checking in with SteveL-MSFT as I know a few people asked him about Azure Pipelines supporting PS Core when we were at PSConfEU."
technical,Opening as enhancement I don't really know how this works in the background but a powershell task that does not use the vsts-tasks-lib should be executable or? because it does not depend on anything OS Specific? I wrote some tasks that do not use vsts-tasks-lib they are just pure powershell script and they also don't run on Linux by the agent. I haven't tried running them natively without vsts on the linux machine.
technical,"powershellcore handler should be able to handle all platforms and all scenarios.  The powershell3 implementation has it's current restrictions.  You will be able to write one.  It's not documented on how they differ because it's not designed/implemented yet.  It's an outstanding enhancement request. I might have a look at implementing a powershellcore handler then. I'm the powershell3 question, I work on this pester task and it uses the powershell handler, that's what I was curious about the differences between it and the powershell3 one. If there are no real differences then I'll leave it as it is, everything seems to work fine so I assume it's not a problem."
technical,"dmitryserbin I think while this is something we're interested in, its not high priority for us right now so we don't have an immediate eta. There's a lot of challenges associated with this (see above) and if we do decide to move forward it will take a concerted push/thought that we don't have the bandwidth to invest right now. I spent a couple days trying to force existing PS module to work on Linux. I got very close, but the things I had to do made it clear that while it's possible, it's a bad idea.  My approach was to invoke my existing PS script in pscore from a ""trivial"" JS script, importing the PS Module using Import-Module, of course. This feel apart because of the ""Vaulting"" behavior: the variables were inaccessible to my PS script because they'd been ""vaulted"" by JS (see internal.js:645,656 and a couple other places).  Just ""for science"", I hacked the scripts to not delete the environment variables post vault. This works. However, it's clearly a case of ""because you can doesn't mean you should"".  I'm currently re-writing these PS scripts in JS.  I'd love to see PS Core support! If you pick this up, send me a ping and I'll share my experiences."
technical,"I might have a look at implementing a powershellcore handler then. I'm the powershell3 question, I work on this pester task and it uses the powershell handler, that's what I was curious about the differences between it and the powershell3 one. If there are no real differences then I'll leave it as it is, everything seems to work fine so I assume it's not a problem. If you take a stab at it, make sure you write up a proposal / design in the vsts-agent repo in the form of a forked branch in the docs / preview folder as markdown.  We want to avoid a large PR only to be rejected.  TL,DR, As an FYI, here's what we're struggling with internally (and the concepts get confusing so I'll try my best to be concise :) )  We have a major shift and feature set around yaml and containers The scenario isn't building / deploying your app container (a prime scenario however) - this one is build and test in a container to seal the dev tools and configs (dev == dev == ci). In the current implementation coming to preview, the agent runs on the host and we exec steps in a container we start up with work dir mapped in.  So single use, incremental source, clean config / machine every time.  The agent can still update forward if demanded, etc... The agent carries everything it needs to to run ""any container you bring"" - outside / in.  It carries node internal as a script engine (mapped in) and stdin/out is the protocol.  Everything is golden with what we support.  Powershell3 is hand wavy but it's ubiquitous and win only right now. Enter powershell core and dotnet core (maybe carries dlls).  You need powershell core in the container - OK, that makes sense for a task like pester, azure powershell, etc... - anything that's powershell tech related.  Hey, I add the pester task to my def, say run in my container it makes sense that I need powershell in it. Now, write a general task like config file variable substitutions (generic task).  I install from the market place, drag it on, pick my container.  When I run it (if we do it right) we inform you powershell core is not in the container.  The user who added the task from the market place had no idea how you implemented the generic task. So at this point the only option is to say all containers must contain powershell core or we inject it into the container if missing (not quite immutable :) ). Next problem is versions - which version of powershell core to prereq in container?  There isn't one since whatever you pick it to be, what happens when you write a task using capabilities in a later powershellcore?  All the builds with containers that took our prereq advice of installing pscore into the container will break when they add your task from the marketplace that needs a later one.  It's a forward compat problem. Potential solution there is to have a handler per maj version channel of pscore but it still has the odd experience of adding a task from the marketplace and having to realize how it was coded.  As pointed out above, the agent currently carries node.  The other option is to explore ways to do the same with powershell core and we're meeting with the ps core team. For those reasons we can't prereq certain versions.  It's also why the agent is built as a self contained net core app - it updates itself as demanded and moves to new net core versions so we can't pre-req one at config time. Everything right now in the agent and the task model is self contained.  OS dependencies of net core is the exception but those are generally going down with new versions of net core. So it's not the implementation time that we're struggling with - it's all the versions and back and forward compat of a service, the agent and tasks on different versions with containers. Long winded but hopefully it makes sense."
technical,"DerGary - the agent has a handler in it that basically knows how to run the script that's in the task.  So the node handler runs node against the script that's declared in the execution section of the task.json.  The task is run out of proc and communication and status is communicated via an RPC stdout protocol (hash hash vso statements in output).  The task lib is simply libraries with functions that emit that protocol over stdout.  It's a convenience.  If you just have an arbitrary powershell script that you need run as part of your build / release, then you can use the powershell task (2.x version) to run.  A formalized task is the way to share your unit of work with others on the market place.  See the step by step walk throughs in this repo.  You can also look at our tasks in the vsts-tasks repo as reference examples. Is it possible to update the current PowerShell handler to work similarly to the v2 task? Use pwsh if it's there and fall back to Windows PowerShell if it's not? Or add a an extra property to the handler definition in the task.json file to say if it should try pwsh at all, defaulting to false to maintain legacy support?  Hopefully the task authors will know if their task will run in pwsh and by not requiring pwsh there should be no change to the agents."
technical, Is something done in this field? We would like to run powershell tasks on Linux as well because a lot of the VSTS Extensions are Powershell based.
technical,"If you take a stab at it, make sure you write up a proposal / design in the vsts-agent repo in the form of a forked branch in the docs / preview folder as markdown.  We want to avoid a large PR only to be rejected.  TL,DR, As an FYI, here's what we're struggling with internally (and the concepts get confusing so I'll try my best to be concise :) )  We have a major shift and feature set around yaml and containers The scenario isn't building / deploying your app container (a prime scenario however) - this one is build and test in a container to seal the dev tools and configs (dev == dev == ci). In the current implementation coming to preview, the agent runs on the host and we exec steps in a container we start up with work dir mapped in.  So single use, incremental source, clean config / machine every time.  The agent can still update forward if demanded, etc... The agent carries everything it needs to to run ""any container you bring"" - outside / in.  It carries node internal as a script engine (mapped in) and stdin/out is the protocol.  Everything is golden with what we support.  Powershell3 is hand wavy but it's ubiquitous and win only right now. Enter powershell core and dotnet core (maybe carries dlls).  You need powershell core in the container - OK, that makes sense for a task like pester, azure powershell, etc... - anything that's powershell tech related.  Hey, I add the pester task to my def, say run in my container it makes sense that I need powershell in it. Now, write a general task like config file variable substitutions (generic task).  I install from the market place, drag it on, pick my container.  When I run it (if we do it right) we inform you powershell core is not in the container.  The user who added the task from the market place had no idea how you implemented the generic task. So at this point the only option is to say all containers must contain powershell core or we inject it into the container if missing (not quite immutable :) ). Next problem is versions - which version of powershell core to prereq in container?  There isn't one since whatever you pick it to be, what happens when you write a task using capabilities in a later powershellcore?  All the builds with containers that took our prereq advice of installing pscore into the container will break when they add your task from the marketplace that needs a later one.  It's a forward compat problem. Potential solution there is to have a handler per maj version channel of pscore but it still has the odd experience of adding a task from the marketplace and having to realize how it was coded.  As pointed out above, the agent currently carries node.  The other option is to explore ways to do the same with powershell core and we're meeting with the ps core team. For those reasons we can't prereq certain versions.  It's also why the agent is built as a self contained net core app - it updates itself as demanded and moves to new net core versions so we can't pre-req one at config time. Everything right now in the agent and the task model is self contained.  OS dependencies of net core is the exception but those are generally going down with new versions of net core. So it's not the implementation time that we're struggling with - it's all the versions and back and forward compat of a service, the agent and tasks on different versions with containers. Long winded but hopefully it makes sense. Is the PowerShell Core prereq something that can be presented to the user in a similar way to Demands/Requirements for normal agents? That will need to be used anyway for users not using containers as part of their build process. I'm not sure how well those requirements are currently presented to a user when using YAML definitions but I assume it is still done in some way since those requirements will still be there. I'm not sure being in a container has a big impact on the way it should handle this, there will be plenty of times when a user has a definition which doesn't use containers or even YAML defs and so will just be running a def defined in the web UI and we just need to surface the requirements of the various tasks to them in a sensible way.  Some work would have to be done by extension authors as well to include those requirements in their readme files on the marketplace to ensure that the requirement is known ahead of time.  As for versions of PS Core to handle with this, I'd go with the latest minor version of the current major version and the previous one, so currently that's just 6.0.2 but will soon be 6.1.x and eventually will be 7.0.x and 6.x.x . I don't think the PS Team has plans to release major versions too often so that shouldn't mean updating things too often and supporting the current and 1 previous major versions feels like a reasonable amount. From a user perspective if a task changes major versions because it's supporting things in PSCore 8.0 and I want those things then I can just update my dockerfile (or whatever is building my container) to use that newer version, if I don't want/need those new things then I can keep using the old version of the task and my existing container.  I'm not sure how best to handle that from the agent side, a single powershellcore execution handler with some config settings coming from the task.json about what versions it requires/supports, which then lets it swap between the two supported versions of core or full PowerShell if they aren't available."
technical,"Is it possible to update the current PowerShell handler to work similarly to the v2 task? Use pwsh if it's there and fall back to Windows PowerShell if it's not? Or add a an extra property to the handler definition in the task.json file to say if it should try pwsh at all, defaulting to false to maintain legacy support?  Hopefully the task authors will know if their task will run in pwsh and by not requiring pwsh there should be no change to the agents. It's possible but it's going to be explicit.  Yes, authors will know whether their task will run on powershell core and more importantly, not windows only and they should mark the task.json appropriately.  That marking will be ""powershellcore"".  Existing powershell3 tasks were written and distributed with an assumption of windows only so it needs to be explicit.  Note that the v2 task is about running end use ad-hoc scripts which they explicitly know the limitations and explicitly pick where it will run (queue).  The handlers is all about task authors releasing tasks to a market place where end users consume the task having no idea (and shouldn't) how it was written."
technical,"Is the PowerShell Core prereq something that can be presented to the user in a similar way to Demands/Requirements for normal agents? That will need to be used anyway for users not using containers as part of their build process. I'm not sure how well those requirements are currently presented to a user when using YAML definitions but I assume it is still done in some way since those requirements will still be there. I'm not sure being in a container has a big impact on the way it should handle this, there will be plenty of times when a user has a definition which doesn't use containers or even YAML defs and so will just be running a def defined in the web UI and we just need to surface the requirements of the various tasks to them in a sensible way.  Some work would have to be done by extension authors as well to include those requirements in their readme files on the marketplace to ensure that the requirement is known ahead of time.  As for versions of PS Core to handle with this, I'd go with the latest minor version of the current major version and the previous one, so currently that's just 6.0.2 but will soon be 6.1.x and eventually will be 7.0.x and 6.x.x . I don't think the PS Team has plans to release major versions too often so that shouldn't mean updating things too often and supporting the current and 1 previous major versions feels like a reasonable amount. From a user perspective if a task changes major versions because it's supporting things in PSCore 8.0 and I want those things then I can just update my dockerfile (or whatever is building my container) to use that newer version, if I don't want/need those new things then I can keep using the old version of the task and my existing container.  I'm not sure how best to handle that from the agent side, a single powershellcore execution handler with some config settings coming from the task.json about what versions it requires/supports, which then lets it swap between the two supported versions of core or full PowerShell if they aren't available. No, generic tasks from the market place don't have pre-reqs.  The requirement is you install the task from the marketplace and it works.  Even if you entertained communicating pre-reqs, the problem is shifting pre-reqs.  If you pre-req 1.0 and everyone puts that in their container, when can your task start using capabilities of 1.1?  never.  If they heeded your pre-req at install time and their build works on the container they put your pre-req in, it will blow up when you change your task to code against 1.1 capabilities.  The fundamental problem is you have multiple layers from the agent, tasks and system that float on different lifetimes. That's why the agent and tasks are self contained.  You also have to be crisp between what's a dependency (node) and a tool (what the task is calling - msbuild, etc...).  Nothing is preventing you from creating a task in typescript that calls powershell / pester right now until and if we add this."
technical,"Not having Handler that supports natively PS6 is blocker! Its not only cross-platform related problem, PS5 is no longer supported! We have ton of PS5 related bugs to which MS replied: ""We cannot do anything in ps5, they are already fixed in ps6, go use ps6"".  Also I cannot understand why the ""ProcessHandler"" is calling cmd.exe to start the desired executable. If that wasnt the case, we could have used it to call pwsh.exe from it. I dont like the typescript approach because of: - My understanding is that in order to use typescript, we will end up as an with extra dependency of node. - chaining lots of processes will cause a lot of environment variables propagation issues, and Env variables are essential for pipelines. Nonsense. For NuGet tasks, I have to spray ""NuGet Installer"" tasks all over the place and this is far from the only one. Make a PowerShell Installer task, along the same lines, and you would make a lot of people very happy.  What you claim there is an unreasonable ideal that is holding back progress."
technical,"Yep.  Last status I heard was a personal / experimental container.  Following up. Not having Handler that supports natively PS6 is blocker! Its not only cross-platform related problem, PS5 is no longer supported! We have ton of PS5 related bugs to which MS replied: ""We cannot do anything in ps5, they are already fixed in ps6, go use ps6"".  Also I cannot understand why the ""ProcessHandler"" is calling cmd.exe to start the desired executable. If that wasnt the case, we could have used it to call pwsh.exe from it. I dont like the typescript approach because of: - My understanding is that in order to use typescript, we will end up as an with extra dependency of node. - chaining lots of processes will cause a lot of environment variables propagation issues, and Env variables are essential for pipelines."
technical,"I spent a couple days trying to force existing PS module to work on Linux. I got very close, but the things I had to do made it clear that while it's possible, it's a bad idea.  My approach was to invoke my existing PS script in pscore from a ""trivial"" JS script, importing the PS Module using Import-Module, of course. This feel apart because of the ""Vaulting"" behavior: the variables were inaccessible to my PS script because they'd been ""vaulted"" by JS (see internal.js:645,656 and a couple other places).  Just ""for science"", I hacked the scripts to not delete the environment variables post vault. This works. However, it's clearly a case of ""because you can doesn't mean you should"".  I'm currently re-writing these PS scripts in JS.  I'd love to see PS Core support! If you pick this up, send me a ping and I'll share my experiences. Oh wow, I thought for sure PowerShell Core was a v1 feature. I am very disappointed that I cannot author tasks via PowerShell Core. This is currently a complete blocker for me when it comes to authoring pipelines tasks. Going with TypeScript is a complete non-starter.  From a user perspective, all this prerequisite talk is besides the point. To run JavaScript it also needs a JavaScript runtime - there is no difference with a PowerShell runtime. If you are afraid of the risk, tell the user to install it manually via some Install PowerShell Core task.  Your users want to get stuff done, not wait for the perfect solution."
technical,"We have an item on the backlog to support writing tasks in powershell core.  The powershell handlers ( and the tasks written using that ) is for full windows powershell.  We can't just redirect all the existing windows powershell tasks to Linux - beyond catching all those existing tasks by surprise, the task authors that intentionally wrote windows specific tasks would start routing to Linux boxes in a heterogeneous pool.  We need a powershell core capability for routing and s powershell core handler in the agent. Opening as enhancement"
technical,"So is the intention that a task marked as requiring the powershellcore handler will fall back to Windows PowerShell if pwsh isn't available? Or do we have to maintain two versions with the same functionality?  I've got at least one task I'd love to make available on nix and Mac but don't want to maintain multiple versions or have some funky build process to copy the files around and then package it up.  I'd also not seen anything about a PowerShell3 handler, is it documented anywhere how it differs from the PowerShell handler? Or is it just a name change? Or are they the same handler and the engine just passes one to the other (presumably powershell to powershell3)? powershellcore handler should be able to handle all platforms and all scenarios.  The powershell3 implementation has it's current restrictions.  You will be able to write one.  It's not documented on how they differ because it's not designed/implemented yet.  It's an outstanding enhancement request."
technical,"Oh wow, I thought for sure PowerShell Core was a v1 feature. I am very disappointed that I cannot author tasks via PowerShell Core. This is currently a complete blocker for me when it comes to authoring pipelines tasks. Going with TypeScript is a complete non-starter.  From a user perspective, all this prerequisite talk is besides the point. To run JavaScript it also needs a JavaScript runtime - there is no difference with a PowerShell runtime. If you are afraid of the risk, tell the user to install it manually via some Install PowerShell Core task.  Your users want to get stuff done, not wait for the perfect solution. sandersaares - as noted above, it's not about the user installing pre-reqs.  The consumer (user) of task simply goes to the market place and uses it.  They shouldn't have to inspect the tasks code and figure out what it's pre-reqs are.  The requirements are, choose task, use, works.  So market place tasks cannot have pre-reqs.  For those reasons, the agent carries the javascript runtime (node).  It's a self contained executable.  We also need to make it work in containers since jobs can run in containers.   That includes alpine and we're tracking.  For typescript tasks, we map it into the container (user can pick any container) and at that point it's about the OS pre-reqs which is almost zero for typescript/node.  netcore has been reducing OS dependencies with 2.x and 3.x so we're evaluating.  All of this is possible since we could potentially package ps core in the agent but it will take quite a bit of work, docs etc. and we need ps to address the alpine (wasn't there when we last looked).  This is why we've left the issue open.  We're still considering and looking at the work involved.  Note that you can use the typescript task-lib to exec powershell core against a powershell core your task carries.  That will have the issue mentioned above of a task having a pre-req though."
technical,"It's possible but it's going to be explicit.  Yes, authors will know whether their task will run on powershell core and more importantly, not windows only and they should mark the task.json appropriately.  That marking will be ""powershellcore"".  Existing powershell3 tasks were written and distributed with an assumption of windows only so it needs to be explicit.  Note that the v2 task is about running end use ad-hoc scripts which they explicitly know the limitations and explicitly pick where it will run (queue).  The handlers is all about task authors releasing tasks to a market place where end users consume the task having no idea (and shouldn't) how it was written. So is the intention that a task marked as requiring the powershellcore handler will fall back to Windows PowerShell if pwsh isn't available? Or do we have to maintain two versions with the same functionality?  I've got at least one task I'd love to make available on nix and Mac but don't want to maintain multiple versions or have some funky build process to copy the files around and then package it up.  I'd also not seen anything about a PowerShell3 handler, is it documented anywhere how it differs from the PowerShell handler? Or is it just a name change? Or are they the same handler and the engine just passes one to the other (presumably powershell to powershell3)?"
technical,"Nonsense. For NuGet tasks, I have to spray ""NuGet Installer"" tasks all over the place and this is far from the only one. Make a PowerShell Installer task, along the same lines, and you would make a lot of people very happy.  What you claim there is an unreasonable ideal that is holding back progress. That is not correct.  The agent carries node as the task runtime.  It's completely self contained so adding a task from the market place works.  And that's the work that this enhancement needs. That won't work in container jobs as pointed out above.  Note that a task dependency is different than installing tools in an environment.  The user and consumer of tasks brings tools they use Note that we have not said we're not doing this which is why enhancement is still open.   The work is known and it's on our backlog.  The requirements, patterns and expectations have been set for how the type script tasks work.  When we add powershell core support, the requirements are the same and it will need to work for all of our features. I would kindly ask that we keep the conversation friendly and respectful.  Please refer to our code of conduct. We are keeping this issue open and will address it on our backlog. Thanks for all of your feedback -  All the details and considerations are captured in this issue."
technical,"Cool.  Thanks for listening :) We also really need a native PowerShell Core task execution support (for Windows-based agents at least).  As far as I understand you guys (bryanmacfarlane) already have a backlog item for this, however I want to understand when you're planning to implement/release it?"
technical,"Is something done in this field? We would like to run powershell tasks on Linux as well because a lot of the VSTS Extensions are Powershell based. We have an item on the backlog to support writing tasks in powershell core.  The powershell handlers ( and the tasks written using that ) is for full windows powershell.  We can't just redirect all the existing windows powershell tasks to Linux - beyond catching all those existing tasks by surprise, the task authors that intentionally wrote windows specific tasks would start routing to Linux boxes in a heterogeneous pool.  We need a powershell core capability for routing and s powershell core handler in the agent."
technical,"I believe PS Core now supports Alpine, might be worth checking in with SteveL-MSFT as I know a few people asked him about Azure Pipelines supporting PS Core when we were at PSConfEU. Yep.  Last status I heard was a personal / experimental container.  Following up."
technical,"If you guys were young and open to new things like me, you would understand how mindblowingly game changing this is!         We can now query StackOverflow when an exception happens and also include the top 10 suggestions in the exception!  Additionally, others that have the same exception can follow you to see if you come up with an answer.  Also!!!!111!!!!!1 We should include ILSPY in CoreCLR, so that you can get a decompiled listing of the code where the exception occurred  I am going to start prototyping this soon!!!! :100:  :100: :100: coreclr is not a GUI program. I cannot imagine integrating such services."
technical,"OK, point taken and your feedback is being discussed Current exceptions are not very helpful in assisting the developer.  We should add social networking features to facilitate a better debugging experience."
technical,coreclr is not a GUI program. I cannot imagine integrating such services. i was skyping with others and we decided we are going to change System.Exception.Message to a new and improved JSON format.  This will make integration with the services and DNX easier.  [Edit] NuGet make its super simple to deploy this to everyone.
technical,i was skyping with others and we decided we are going to change System.Exception.Message to a new and improved JSON format.  This will make integration with the services and DNX easier.  [Edit] NuGet make its super simple to deploy this to everyone. I'm so glad these suggestions are being made by the OTHER CrashOverride rather than myself...
technical,"Don't forget about those instagals! :dancer: If you guys were young and open to new things like me, you would understand how mindblowingly game changing this is!         We can now query StackOverflow when an exception happens and also include the top 10 suggestions in the exception!  Additionally, others that have the same exception can follow you to see if you come up with an answer.  Also!!!!111!!!!!1 We should include ILSPY in CoreCLR, so that you can get a decompiled listing of the code where the exception occurred  I am going to start prototyping this soon!!!! :100:  :100: :100:"
technical,"I'm so glad these suggestions are being made by the OTHER CrashOverride rather than myself... OK, point taken and your feedback is being discussed"
technical,"No pressies for Christian-Schiffer this year,. He's been a very naughty boy! Could stop this discussions if a more balanced approach was taken such as including configuration to enable disable these decorations. Can't imagine any controversy from that therefore these outbursts that take time away from other issues."
technical,"You deserve an award for skyrocketting the level of drama of this topic   – ! Could we remove this logo, it's against the insects life :thinking:"
technical,You deserve an award for skyrocketting the level of drama of this topic   – ! git push --religion
technical,You deserve an award for skyrocketting the level of drama of this topic   – ! Good grief. Poor old Santa! BTW - solstice is a pagan festival and Santa was a shaman.
technical,Good grief. Poor old Santa! BTW - solstice is a pagan festival and Santa was a shaman. How to ruin your stellar reputation in one easy step.
technical,"Merry Christmas and Happy New Year! … You can ban me now, for pushing of religion I am not sure why you used quotes, but yes, lives lost or opressed at any occasion are depressing thing."
technical,"Sadder still is the amount of lives lost in the name of ""religion"". Merry Christmas and Happy New Year! … You can ban me now, for pushing of religion"
technical,"How to ruin your stellar reputation in one easy step. No pressies for Christian-Schiffer this year,. He's been a very naughty boy!"
technical,"It's funny and sad at the same time that one selfish issue can cause loss of hundrets of man-days (people-days?) across globe. Sadder still is the amount of lives lost in the name of ""religion""."
technical,git push --religion Thanks for creating this issue. We think this issue is unactionable or unrelated to the goals of this project. Please follow our issue reporting guidelines. Happy Coding!
technical,"Please remove Python compatibility I'm scared of snakes. Won't happen, the the people who code vscode are too scared to go against someone's beliefs. They removed a god damn hat for fucks sake, a hat that's related to a holiday that isn't celebrated for its religious symbolism anymore, the latter was issue for the amazing mastermind who made #87268. This is why we can't have nice things."
technical, could you look at adjusting the docs per diablodale's feedback?  We also currently have the ability to automagically work with WSL2 when using Docker Desktop's WSL2 backend feature. The extension uses the docker context command to detect whichever context is active when the extension activates.
technical,"Understood that we are not talking about method 1  WSL is supported via Remote-WSL extension. When that extension is used, a headless instance of VS Code is installed into WSL environment. Then one can use that VS Code instance to install headless instance of Docker extension into WSL environment. That headless extension instance will see WSL environment configuration, including SSH/Docker configuration, and will drive the Docker extension UI running on Win32 side. All the data comes from the WSL environment and all the Docker commands are executed in that environment. The only caveat that Brandon describes is that ssh-agent configuration does not survive WSL environment restart, so it needs to be re-created via .bashrc file. This probably needs to be added to the doc. Everything else in doc applies to WSL via Remote-WSL unchanged.  What is not supported is running VS Code and Docker extension entirely on Win32 side, without Remote-WSL extension. At least not without extra manual work of keeping Win32 and WSL configuration in sync. This goes back to the point they are isolated environments.  Hope this clarifies things but please let me know if I missed something. I agree that those are gaps in the documentation, but I'm not convinced that we  should  document them. There's probably an infinite number of ways and combinations of ways to make remote connectivity work, but I think that our docs should stick to the beaten path(s), so to speak:  A - Want to talk to remote Docker via HTTP? Use the appropriate DOCKER * env vars or docker.* settings.  B - Want to talk to remote Docker via SSH? Use Remote - SSH with Docker extension installed inside (preferred), or use the SSH features the Docker extension has (DOCKER HOST / docker.host and SSH AUTH SOCK).  C - Want to talk to remote Docker in WSL? Use Remote - WSL with Docker extension installed inside (preferred), or stand up SSH within WSL and use the SSH features the Docker extension has.  D - Want to talk to remote Docker in WSL2? Use Remote - WSL with Docker extension installed inside (preferred), or the Docker WSL2 backend feature (preferred, but somewhat less so) (no extension config needed), or stand up SSH within WSL2 and use the SSH features the Docker extension has.  Method 3 (Remote - WSL + SSH) is a combination of options C and B, IMO better solved with just option B alone, so I'd prefer to not document it. The variant of method 2 (WSL as shell + SSH) is essentially option B but since it uses WSL as a shell, requires additional config work due to the disparate environments."
technical,"thanks. We can certainly clarify in the doc which parts do not apply to WSL (or WSL 2).  Full disclosure: I haven't tried the steps listed under with WSL, but I would expect them to work, so I would like to understand why you are saying they are not supported. Are you aware of any known issues with this setup and WSL? Or maybe you tried and found the steps not working/sufficient? Thanks in advance for clarification! karolz-ms You can use Remote - WSL to attach VSCode to WSL (also, install the Docker extension within WSL, by default it runs as a UI extension). No configuration of the ""host"" (i.e. the Windows machine) is needed in that case. bash is automatically used as the terminal, and dockerode and docker * commands happen in the same context, so the setting docker.host (or env var DOCKER HOST) will work for ssh:... values (as long as SSH AUTH SOCK is also set in WSL).  When I tried with the Ubuntu distro, I had to call eval (ssh-agent -s) and ssh-add keyfile from within .bashrc so that when the VSCode server-side process ran in WSL it would have the needed SSH AUTH SOCK environment variable, along with a running ssh-agent process, but I'm sure there are better ways to accomplish that. Once I did that everything worked as expected.  If you want to  not  attach with Remote - WSL, then it is necessary to do the ssh-agent configuration in both Windows and WSL, because dockerode and docker * commands are not in the same context."
technical,"I agree that those are gaps in the documentation, but I'm not convinced that we  should  document them. There's probably an infinite number of ways and combinations of ways to make remote connectivity work, but I think that our docs should stick to the beaten path(s), so to speak:  A - Want to talk to remote Docker via HTTP? Use the appropriate DOCKER * env vars or docker.* settings.  B - Want to talk to remote Docker via SSH? Use Remote - SSH with Docker extension installed inside (preferred), or use the SSH features the Docker extension has (DOCKER HOST / docker.host and SSH AUTH SOCK).  C - Want to talk to remote Docker in WSL? Use Remote - WSL with Docker extension installed inside (preferred), or stand up SSH within WSL and use the SSH features the Docker extension has.  D - Want to talk to remote Docker in WSL2? Use Remote - WSL with Docker extension installed inside (preferred), or the Docker WSL2 backend feature (preferred, but somewhat less so) (no extension config needed), or stand up SSH within WSL2 and use the SSH features the Docker extension has.  Method 3 (Remote - WSL + SSH) is a combination of options C and B, IMO better solved with just option B alone, so I'd prefer to not document it. The variant of method 2 (WSL as shell + SSH) is essentially option B but since it uses WSL as a shell, requires additional config work due to the disparate environments. Resolves #1458 with zifik's suggestion. Because an ssh-agent is required by the ssh2 Node package (used by Dockerode) when using an SSH connection to a remote Docker daemon, we will show a warning with a Learn More link if SSH is in use without a working ssh-agent.  The check is done by looking at the value for SSH AUTH SOCK (which on Windows can be defaulted), and then trying to connect to that pipe to ensure it is actually working.  The warning: The link"
technical,"karolz-ms You can use Remote - WSL to attach VSCode to WSL (also, install the Docker extension within WSL, by default it runs as a UI extension). No configuration of the ""host"" (i.e. the Windows machine) is needed in that case. bash is automatically used as the terminal, and dockerode and docker * commands happen in the same context, so the setting docker.host (or env var DOCKER HOST) will work for ssh:... values (as long as SSH AUTH SOCK is also set in WSL).  When I tried with the Ubuntu distro, I had to call eval (ssh-agent -s) and ssh-add keyfile from within .bashrc so that when the VSCode server-side process ran in WSL it would have the needed SSH AUTH SOCK environment variable, along with a running ssh-agent process, but I'm sure there are better ways to accomplish that. Once I did that everything worked as expected.  If you want to  not  attach with Remote - WSL, then it is necessary to do the ssh-agent configuration in both Windows and WSL, because dockerode and docker * commands are not in the same context. thank you, that is exactly what I would expect. Based on that I would consider WSL supported."
technical,"thank you, that is exactly what I would expect. Based on that I would consider WSL supported. thanks for clarification. I think I understand where you are coming from. The difference is in the mental model of what WSL really is. Is it *a shell*? This is how it was originally advertised (""bash on Windows""), but it evolved into something that nowadays most people here at Microsoft would describe it as *virtual environment for running Linux programs*. That has been made very prominent with WSL 2 and its native Linux kernel. And with this the expected, default level of integration between Win32 and WSL environment (as well as between different WSL instances) is really none. They are completely isolated.  Arguably complete isolation is not always the most practical choice, so WSL (and tools that leverage it) have multiple integration points with Win32 (file system mounts, socket mounts, the new Docker implementation for WSL2, VS Code Remote-WSL extension are good examples). The fundamental principle remains though that WSL environments and Win32 are isolated by default.  I can see your point that for SSH connectivity (and probably other things) there is not enough integration and I agree. It is a pain to manage all the SSH identities separately for Win32 vs WSL. But this is largely an issue outside of the VS Code Docker extension control. bwateratmsft  confirmed that  the setup described in the doc you referenced IS working with WSL, with the design of WSL as it is today. If anybody reports that it is not working, we will investigate. Hence it is supported.  Thank you for taking time to share your thoughts!"
technical,"What would you like topic to say about WSL?  The whole topic is about ""connecting to remote Docker daemon"". With WSL there is no ""remote Docker daemon"" in the picture, so it should be pretty clear it does not apply to WSL. And with regards to WSL specifically we have this: thanks. We can certainly clarify in the doc which parts do not apply to WSL (or WSL 2).  Full disclosure: I haven't tried the steps listed under with WSL, but I would expect them to work, so I would like to understand why you are saying they are not supported. Are you aware of any known issues with this setup and WSL? Or maybe you tried and found the steps not working/sufficient? Thanks in advance for clarification!"
technical,"thanks for clarification. I think I understand where you are coming from. The difference is in the mental model of what WSL really is. Is it *a shell*? This is how it was originally advertised (""bash on Windows""), but it evolved into something that nowadays most people here at Microsoft would describe it as *virtual environment for running Linux programs*. That has been made very prominent with WSL 2 and its native Linux kernel. And with this the expected, default level of integration between Win32 and WSL environment (as well as between different WSL instances) is really none. They are completely isolated.  Arguably complete isolation is not always the most practical choice, so WSL (and tools that leverage it) have multiple integration points with Win32 (file system mounts, socket mounts, the new Docker implementation for WSL2, VS Code Remote-WSL extension are good examples). The fundamental principle remains though that WSL environments and Win32 are isolated by default.  I can see your point that for SSH connectivity (and probably other things) there is not enough integration and I agree. It is a pain to manage all the SSH identities separately for Win32 vs WSL. But this is largely an issue outside of the VS Code Docker extension control. bwateratmsft  confirmed that  the setup described in the doc you referenced IS working with WSL, with the design of WSL as it is today. If anybody reports that it is not working, we will investigate. Hence it is supported.  Thank you for taking time to share your thoughts! Understood that we are not talking about method 1  WSL is supported via Remote-WSL extension. When that extension is used, a headless instance of VS Code is installed into WSL environment. Then one can use that VS Code instance to install headless instance of Docker extension into WSL environment. That headless extension instance will see WSL environment configuration, including SSH/Docker configuration, and will drive the Docker extension UI running on Win32 side. All the data comes from the WSL environment and all the Docker commands are executed in that environment. The only caveat that Brandon describes is that ssh-agent configuration does not survive WSL environment restart, so it needs to be re-created via .bashrc file. This probably needs to be added to the doc. Everything else in doc applies to WSL via Remote-WSL unchanged.  What is not supported is running VS Code and Docker extension entirely on Win32 side, without Remote-WSL extension. At least not without extra manual work of keeping Win32 and WSL configuration in sync. This goes back to the point they are isolated environments.  Hope this clarifies things but please let me know if I missed something."
technical,"could you look at adjusting the docs per diablodale's feedback?  We also currently have the ability to automagically work with WSL2 when using Docker Desktop's WSL2 backend feature. The extension uses the docker context command to detect whichever context is active when the extension activates. What would you like topic to say about WSL?  The whole topic is about ""connecting to remote Docker daemon"". With WSL there is no ""remote Docker daemon"" in the picture, so it should be pretty clear it does not apply to WSL. And with regards to WSL specifically we have this:"
technical,"So cups-pdf is HP's problem too? Is there any Linux software you can name that interfaces with CUPS correctly?  It would help to have an answer to this question, relating to the configuration of face-up printers:  ""Can you point me to the place in the documentation where you say which of lpadmin/lpoptions/PPD solutions is expected to make the printer work correctly with all CUPS clients?"" cups-pdf is from another developer. Both depend on cups-filters on Linux, which probably means that the problem lies with cups-filters.  We do not document printing solutions for Linux, working or otherwise. We don't write or support the software, and we don't make the distributions."
technical,"cups-pdf is from another developer. Both depend on cups-filters on Linux, which probably means that the problem lies with cups-filters.  We do not document printing solutions for Linux, working or otherwise. We don't write or support the software, and we don't make the distributions. I have an HP Photosmart Plus B210a which prints pages face up. I am trying to figure out how to configure Cups to print documents so that I don't have to reverse them by hand when they come out of the printer.  I found a discussion on linuxquestions.org which summarizes a recommended fix:  Add the line to the file  I have tried this with varying results. I printed twelve two-page documents to try to investigate what is happening. I tried printing via the lp command, via Evince and via Okular. I tried using a ppd file with and without the DefaultOutputOrder line, and I also tried specifying a default output order with lpoptions of ""reverse"" or ""normal"":  The results: As with the reporter of #1679, I would have expected the PPD which is downloaded by default for my printer to give the ""correct"" behavior by default, which is to say, not requiring me to manually reverse long documents when they come out of the printer.  I also tried the solution of #1679, using  This setting was honored by Evince but not by lp or Okular.  Okular's print dialog allows the user to change the ouput ordering, but Evince's print dialog, which appears more ""standard"" to me, says ""Page Ordering: Not available"".  There was some mention of all documents going through pstops as a final filter. If this is true then it should be possible for Cups to give users a single point at which to configure output ordering, which works uniformly across all applications which might submit print jobs to Cups. In either case it should be documented what is the preferred way to deal with face-up printers.  I humbly offer my suggestion that for ""outputorder=reverse"" to work intuitively as a per-job option, then it should interact with the per-printer option as a parity bit, e.g. reverse+reverse=normal, rather than as an ""override"" (reverse+reverse=reverse). In my experimentation it seems to be the latter, and I think this should be documented because it is somewhat different from the way that other options like ""page-ranges"" work. If I write a script to print the odd pages in order and then the even pages in reverse, I feel I should be able to get the same result (manual duplex) regardless of the printer model. In any case where the lp man page says:  I think it maybe should say something like: Here is my PPD file in case it helps:"
technical," OK, so I'm assuming from the list of applications you are using that you are using a Linux distribution of some sort. The PPD comes from the HPLIP project.  Unfortunately, this isn't something we can help you with - either the cups-filters raster filter is not honoring the DefaultOutputOrder value in the PPD or the HPLIP driver isn't doing something right. Either way you need to start with your Linux distribution's bug reporter and go from there..."
technical,"I can submit the bug elsewhere but do you really mean to suggest that I need to know about HPLIP and cups-filters to understand why three different tools process the options I've set (using your software) in three different ways? What about the other questions I asked? You don't think any of your documentation needs to be fixed? Can you point me to the place in the documentation where you say which of lpadmin/lpoptions/PPD solutions is expected to make the printer work correctly with all CUPS clients? Who takes responsibility when other projects don't know how to interface correctly with your software?  Can you give the HP people a hint on how the HP filter would need to be modified so that it respects the lpadmin setting not just with Evince print jobs but also with jobs submitted by Okular and lp? Or how can Okular be modified so that it prints correctly on inkjets?  Is there another brand of inkjet printers, which works correctly with CUPS?  What about cups-pdf, I've noticed that when printing to the virtual PDF printer then lp doesn't respect the lpadmin setting, Evince doesn't respect the lpoptions setting, and Okular doesn't respect either. Is that still an HP problem? Are you ever grateful to receive bug reports about your project? The non-CUPS software is not following the standard interfaces that CUPS provides. IOW, this is either a bug in HPLIP or cups-filters. If they follow the standard interfaces you won't have to do a damned thing to have things Just Work„. We can't fix software that isn't ours...  As for pstops, no not all jobs get routed through there. Maybe 18 years ago that was the case, but not today.  As for the documentation, it is correct if the underlying driver or filters follow the standard interfaces."
technical,"the way to configure default values for RC is taking a XML  the RC rest api speaks JSON, there is an obvious miscommunication problem!  There is a sample project in this repo, which teaches how to export the current RC into a json and import it into another RC project. would be reasonable if that json could be used as default values for any project... without need of converting it to xml. Default values are **not** intended to be used as a cache that sits in front of the RC service. They are defaults when no values have been configured on the service.   the way to configure default values for RC is taking a XML  the RC rest api speaks JSON, there is an obvious miscommunication problem!  We take the approach that is canonical to the platform. On Android, resources are typically XML. On iOS, it is a plist.  If I understand correctly, you are suggesting that we provide a way to configure defaults on the client using json. That seems reasonable.  Worth noting that this won't solve the problem you originally raised of having to wait for the config values to be fetched after app installation."
technical," I couldn't figure out how to label this issue, so I've labeled it for a human to triage. Hang tight."
technical,"Default values are **not** intended to be used as a cache that sits in front of the RC service. They are defaults when no values have been configured on the service.   the way to configure default values for RC is taking a XML  the RC rest api speaks JSON, there is an obvious miscommunication problem!  We take the approach that is canonical to the platform. On Android, resources are typically XML. On iOS, it is a plist.  If I understand correctly, you are suggesting that we provide a way to configure defaults on the client using json. That seems reasonable.  Worth noting that this won't solve the problem you originally raised of having to wait for the config values to be fetched after app installation. i really didnt understand that...  follow my thought: 1- user first install app 2- app tries to read a RC field during onCreate of first activity, the value isn't fectched yet so it will result as ""empty"".  in my mind the default values will be read at this moment... am i wrong?  are you saying that they arent read at this moment and only when the api knows for sure the fields doesnt exist on firebase i cant imagine a use case for it. but if that is the case it is another suggestion to use this xml values as cache for first install apps.  My current solution is to create a loading screen for this situation. but it will fail if user tries to first open the app without internet connection (this might happen if user cleans app data)"
technical,"So I was right and comment might be misunderstood.  this turn our case back to the start point. danasilver A - RC SDK for android accept default values only as xml file B - RC Rest SDK export the current RC fields only as JSON  there is an  obvious miscomunication between the systems, they should speak a common language, otherwise you are forcing ALL PROJECTS to define twice the RC values I think ashwinraghav explained this pretty well! We support a JSON REST API since that's a common standard for REST APIs on the web, and XML defaults on Android since that's the standard for the platform. I understand there can be frustration working between the formats.  You should only need to define RC in-app defaults once though - in the XML file. The default values saved online and available through the REST API are the server-side defaults used when no condition is met. There's more documentation on how Remote Config prioritizes parameter values in the docs here"
technical,"i really didnt understand that...  follow my thought: 1- user first install app 2- app tries to read a RC field during onCreate of first activity, the value isn't fectched yet so it will result as ""empty"".  in my mind the default values will be read at this moment... am i wrong?  are you saying that they arent read at this moment and only when the api knows for sure the fields doesnt exist on firebase i cant imagine a use case for it. but if that is the case it is another suggestion to use this xml values as cache for first install apps.  My current solution is to create a loading screen for this situation. but it will fail if user tries to first open the app without internet connection (this might happen if user cleans app data) I'm on the Remote Config team. Happy to help with how the RC SDK works with default values!  When the SDK starts up, it loads any default values (like the ones in the XML file). When you call one of the get methods (like #getString(String key)), if a value has been fetched and activated the SDK will return that server value and otherwise return the default value for that key if one exists. Fetched values are also cached by the SDK so they'll be available (and be used over defaults) if the app has already fetched on a previous launch (and the user hasn't cleared app data).  The use case you describe where a user first opens the app without internet connection is a good case for default values since the app can use those in place of the server values."
technical,"I'm sorry to tell you but this isnt ""frustrating"" this is a super stupid architecture design... I dont understand why people working for google have such a problem admting they make mistakes...  everybody does, what makes difference is fix them or not.  i cant even imagine a meeting to present the product: ""Here we have a new tool that will allow developers to set constants remotelly, helping with A/B tests and allowing the developer to change apps behavior without re-deploy.  We will provide a rest service which accepts data in the json format. And in case developer wants to set some default values he can use a XML/plist""  In my company someone proposing such a frankstein architecture would be fired.  You just told me that if someone has an app in both architectures and want to use the default values THIS PERSON MUST DEFINE EVERY FIELD 3 TIMES... does it sound smart?  this is a wrong assumption, although i agree these values shouldn't change very often, software are LIVE things: they grow, they mutate, they acquire new powers. Same way new constants are added, removed, or default values might change in a lifetime of any software. If you were using RC on the innitial development of other firebase services, are you telling me that the default values of 3 years ago would still work for today?   I hate to be rude and also hate to teach people how to do their jobs. but this architecture design of firebase RC is a clear mistake, anyone with a basic software engineer understand should be able to see that. And this whole text could be avoided if you guys simple had said: ""yeah that was a bad idea, i will try to put your suggestion in a next changelog or add some tool to convert json/xml then json/plist and any other technology RC lays on"" I'm sorry but your tone is not acceptable, we expect everyone in the Firebase community to respect each other.  I hope you are able to work around your problem but we won't be able to help you any more on this thread."
technical,"I'm on the Remote Config team. Happy to help with how the RC SDK works with default values!  When the SDK starts up, it loads any default values (like the ones in the XML file). When you call one of the get methods (like #getString(String key)), if a value has been fetched and activated the SDK will return that server value and otherwise return the default value for that key if one exists. Fetched values are also cached by the SDK so they'll be available (and be used over defaults) if the app has already fetched on a previous launch (and the user hasn't cleared app data).  The use case you describe where a user first opens the app without internet connection is a good case for default values since the app can use those in place of the server values. So I was right and comment might be misunderstood.  this turn our case back to the start point. danasilver A - RC SDK for android accept default values only as xml file B - RC Rest SDK export the current RC fields only as JSON  there is an  obvious miscomunication between the systems, they should speak a common language, otherwise you are forcing ALL PROJECTS to define twice the RC values"
technical,"I couldn't figure out how to label this issue, so I've labeled it for a human to triage. Hang tight. Thanks for the suggestion. Might need some help to understand better. We have a way for you to configure default values for RC. I assume what you are asking for is different in that you want to snapshot the config state of a client to an exported file that can be loaded into your app's distribution apk. That right? I do see that as a goal that competes directly with keeping your clients up to date with the latest config values that you have configured on the backend. Any thoughts on how you would want the clients to trade off those goals for new installations of your application? Would you rather see the values packed into your apk or the values configured on your backend?"
technical,"Thanks for the suggestion. Might need some help to understand better. We have a way for you to configure default values for RC. I assume what you are asking for is different in that you want to snapshot the config state of a client to an exported file that can be loaded into your app's distribution apk. That right? I do see that as a goal that competes directly with keeping your clients up to date with the latest config values that you have configured on the backend. Any thoughts on how you would want the clients to trade off those goals for new installations of your application? Would you rather see the values packed into your apk or the values configured on your backend? the way to configure default values for RC is taking a XML  the RC rest api speaks JSON, there is an obvious miscommunication problem!  There is a sample project in this repo, which teaches how to export the current RC into a json and import it into another RC project. would be reasonable if that json could be used as default values for any project... without need of converting it to xml."
technical,"I'm sorry but your tone is not acceptable, we expect everyone in the Firebase community to respect each other.  I hope you are able to work around your problem but we won't be able to help you any more on this thread. What feature would you like to see?  Would be very usefull to have a simple way to export the firebase remote config fields to a json/xml and import them into another project/default values file.  Today in order to do that we must config a sample project (available here) but this project isnt able to generate de default values file. Would be very usefull to have a simple way to do that, i have several remote config fields and app wont work without them. so on first run user must wait a few seconds (depending on his internet) to load the remote config constants) would be very nice if i could take a snapshot of the remote config values and pack it inside the apk in order to make first run faster"
technical,"Just read this with the same issue, and I see that the sign in error message should be changed, but I'm still getting the same ""You can't log in as ****** from that IP address."" Do you still the message after a reload of the forum? That message could be cached."
technical,"I just tried again after reloading, and this time I was allowed to log into the forum, so I can't confirm whether the message has changed. Thanks for following up! I had the same problem 18 hours ago but managed to circumvent the issue by using opera's free VPN. Oddly enough that is also blocked now and I'm getting the same message."
technical,"Do you still the message after a reload of the forum? That message could be cached. I just tried again after reloading, and this time I was allowed to log into the forum, so I can't confirm whether the message has changed. Thanks for following up!"
technical,"Temporarily updated the message to be less cryptic for users. Just read this with the same issue, and I see that the sign in error message should be changed, but I'm still getting the same ""You can't log in as ****** from that IP address."""
technical,"Thanks for reporting. Yes, you are correct. #### TL,DR: This is a known issue, and the dev team is working on it.  #### Details: We recently moved over our servers to a different infrastructure with the new platform roll out earlier last week. As a consequence, your IP addresses are masked by our CDN servers from the forum. During peak usage, this means the forum sees this as too many sign in requests from the same IPs which belong to our CDN servers. This is a false positive. As of now this stands as a known issue, we are working with various teams to get this fixed. Thanks a lot for your patience. Temporarily updated the message to be less cryptic for users."
technical," Thanks for reporting. Yes, you are correct. #### TL,DR: This is a known issue, and the dev team is working on it.  #### Details: We recently moved over our servers to a different infrastructure with the new platform roll out earlier last week. As a consequence, your IP addresses are masked by our CDN servers from the forum. During peak usage, this means the forum sees this as too many sign in requests from the same IPs which belong to our CDN servers. This is a false positive. As of now this stands as a known issue, we are working with various teams to get this fixed. Thanks a lot for your patience."
technical,"I had the same problem 18 hours ago but managed to circumvent the issue by using opera's free VPN. Oddly enough that is also blocked now and I'm getting the same message. When attempting to log into my forum account, I get the error message: ""You can't log in as <username from that IP address."" (see screen shot below) This occurs across all browsers that I have (Firefox, Chrome, and Edge). All browsers are up to date. Clearing the cache and history sometimes resolves this, sometimes it doesn't. And even if it does resolve the issue, it's a one time thing only. As soon as I log out and attempt to log back in, the same error pops up again This issue started several days ago out of the blue. No changes to my system at the time that might account for this. This issue is exclusive to log in attempts to my forum account. I can log into the lessons/project side of freecodecamp without issue and I'm having no issues accessing anything else on the web.  Any thoughts on what I might be dealing with here? Thanks in advance."
technical," Don't hack things with coremods, this has never been needed. Netty allows you control of direct packets if you want to be that annoying and screw with the raw data. We also have a crapload of hook related to the chat system already so you can use any one of those."
technical,"Except, you don't have a good reason, if you would of worked with anyone in the community to figure things out before resorting to hacking things with a coremod then we would be having a different conversation. But for now, locking this as i'm done arguing, and you've been given all the information you need to NOT ASM hack things and still achieve what you want. This event allows the monitoring, modification, and cancellation of chat packets before they get sent to the client. An example use case for this is in a server-side chat censorship mod - *I removed this link for a reason, do not repost it* was previously needed to ensure that all chat messages, whether sent by the player or sent by some other system which the player can control, were properly censored before being sent to the client. This event addresses that. I'm open to suggestions if anyone has any better names for anything I've added, or any other changes they'd like to be made to this event."
technical, Duplicate of #32978
technical,"It does not work, read the code. If it worked the material would be plain white. I took the time to compile a bug report, only to be closed instantly as a duplicate when nothing in ""duplicate"" is same as in my bug report?"
technical,"You could add these descriptions to the original ""duplicate"" issue and this is fine. I'm very sorry if this will cause you to leave Godot. It *is* the same bug as #32978, I don't see why you're being so annoyed about it being closed as a duplicate.  Use this code, it works fine: So the issue *is* about the shader language allowing users to pass global, context-specific (TEXTURE is only available in fragment() for example) built-ins as argument names, and the shadowing doesn't work as it should. Most likely, the magic performed by Godot to replace built-in by their actual value also replaces it in the function where they have been declared as arguments. In your case, this function is outside fragment() and used TEXTURE which is fragment-only, so it breaks. Same bug, just different manifestation. That's petty, but well. We have enough work not to have to keep duplicate issues open for entitled users."
technical,"You did not, simply my next project will be 3D and Godot is not yet there (I am hoping it will be). But, that bug is from an engine user perspecity entirely different:  this: crashes shader compilation results in white square ""duplicate"": magically works, even though it should'n this: I **am** passing correct values, yet compilation fails and result is broken sprite. ""duplicate"": passes incorrect values (constants), yet they are being replaced in a function by dynamic values of shader variables  Do I need to continue? I don't understand how this can be a duplicate when result is different, variables are different (TEXTURE, TIME) or working differently (UV). It does not work, read the code. If it worked the material would be plain white."
technical,"At some point you have to trust engine contributors to know what bugs are. You see symptoms, we see the underlying cause for them, and we tell you that these are both symptoms of the exact same bug. A duplicate bug report is not a bad thing, it just confirms the bug with possibly slightly different steps to reproduce. It's pointless to keep two or 50 issues open about the same bug, so we close duplicates, usually keeping the oldest one open. It does not work, read the code. If it worked the material would be plain white. I mean it compiles and runs, at least that what title suggests: ""Strange syntax is **allowed** to pass global parameters to the shader function"". It ""works"" as in the functions gets TIME from context of a fragment (not passed argument in call) which doesn't happen in my case - it crashes during compilation because probably renaming fails."
technical,"That ""duplicate"" doesn't mention shader being **broken**, only that it is possible to pass TIME and UV (btw UV doesn't work for me). From my testing it is clear TEXTURE, if named same, cannot be passed without second compiler (glsl?) crashing. It looking like the same issue, and happens for the same reason - comparsion with pre-existed shader variables for the function parameters does not exist."
technical,"Here, renamed, be happy. Looking at the compiled shader, I thought maybe TEXTURE and UV are magically accessible in functions. They are not: Unknown identifier in expression: TEXTURE. It's interesting that passing TIME works fine, but not the others mentioned above."
technical,"It looking like the same issue, and happens for the same reason - comparsion with pre-existed shader variables for the function parameters does not exist. Maybe same reason, but the result is different and expected behaviour as well. Not true (if I understand you correctly), g function clearly demonstrates the TEXTURE does not exist. And f is fed a valid variable from fragment function (unlike the ""duplicate"" issue), yet the shader **fails** to compile (also unlike the ""duplicate"" issue).  Kinda glad I will be leaving Godot, this ""duplicate"" labeling feels as lazy as on stackoverflow..."
technical,"Duplicate of #32978 That ""duplicate"" doesn't mention shader being **broken**, only that it is possible to pass TIME and UV (btw UV doesn't work for me). From my testing it is clear TEXTURE, if named same, cannot be passed without second compiler (glsl?) crashing."
technical,"It *is* the same bug as #32978, I don't see why you're being so annoyed about it being closed as a duplicate.  Use this code, it works fine: So the issue *is* about the shader language allowing users to pass global, context-specific (TEXTURE is only available in fragment() for example) built-ins as argument names, and the shadowing doesn't work as it should. Most likely, the magic performed by Godot to replace built-in by their actual value also replaces it in the function where they have been declared as arguments. In your case, this function is outside fragment() and used TEXTURE which is fragment-only, so it breaks. Same bug, just different manifestation. That's petty, but well. We have enough work not to have to keep duplicate issues open for entitled users. You did not, simply my next project will be 3D and Godot is not yet there (I am hoping it will be). But, that bug is from an engine user perspecity entirely different:  this: crashes shader compilation results in white square ""duplicate"": magically works, even though it should'n this: I **am** passing correct values, yet compilation fails and result is broken sprite. ""duplicate"": passes incorrect values (constants), yet they are being replaced in a function by dynamic values of shader variables  Do I need to continue? I don't understand how this can be a duplicate when result is different, variables are different (TEXTURE, TIME) or working differently (UV)."
technical,"This is a different type of opt-in though. We should not shift the hard decision making to the children, that's not fair on them. The responsibility is ours, not theirs.  By all means, keep the block with a disclaimer for those language pairs where the error rate is extremely low. I'd still like to have my locale removed though, because gibberish translations are no use to everybody, and we will be unintentionally teaching this gibberish.  The disclaimer should also be short ( a lot shorter than my draft) to make it easy to read. Otherwise, kids will just go tl,dr on it. Eh, sorry but where are you getting these figures from? Even for a language pair like German < English for which Google must have a lot of data aren't even close to 99%, that's just a figure you made up to make it sound like a good idea. To begin with, you cannot make a blanket statement about how accurate MT translation is without specifying the language pair you're talking about. I don't know what your language skills are but it's a common mistake by English speakers to assume that because GT does an ok job INTO English, the reverse is also true. It isn't. English happens to be a fairly uninflected language with little morphology, which makes it relatively easy for MT. But try going FROM English to something like German with 4 cases and 3 genders, it gets a whole lot harder straight away.  Secondly, the quality of MT depends hugely on the domain. For instance, for medical stuff, German < English is pretty good. Put in a novel, you get vastly different results. If you look at the research, even for something fairly simply as medical texts accuracy has been measured as low as 45-46% (into African and Asian languages). So no, not even close to 99%. Making up figures is not good form."
technical,"I support the idea that Google Translate should be an opt-in and have an explicit warning about how correct the translation would be. It could be very embarrassing for a student to get it very wrong! Google Translate is pretty good for English to Welsh from my experience, it's correct about 60-70% of the time. That means that there is plenty of scope for errors, howlers and embarrassment in using machine translation.  Try some of these. Google Translate, like Microsoft Translate seems to be better with longer sentences and in Welsh because it's based on formal, bureaucratic documents it's in a more formal style than would be appropriate for Scratch. I would be very cautious about encouraging it's use with students who are not fluent in their use of the translated language. Google Translate *is* an opt-in by virtue of the fact that it's an extension! If you don't explicitly include the extension, you can't use the blocks and have therefore not opted in. By including the extension, you are obviously showing that you are opting in to using the extension. A disclaimer is necessary but should not be obtrusive."
technical," I agree, this is a terrible idea - at best, this should be a per-locale **opt-IN**. For many locales, especially the smaller ones, machine translation produces nothing but gibberish. Machine translation is intended as a gisting tool, by and large, and for some language combos (mostly large ones with huge corpora) it may function as a translation aid but for most, it's useless at best, an extra time cost at worst. It will overall reduce the attractiveness of Scratch to educational establishments. There is no quick fix for localization."
technical,"The disclaimer is just a first draft collecting some ideas - of course, it can be further refined and rephrased.  I added it as a first basis for discussion and I am not expecting anybody to implementing it just like that verbatim. I do think it is important that we do add some form of disclaimer though ,)  Because other children will assume that it is correct language and learn the mistakes. Such mistakes are very difficult to unlearn. Chances are very high that a child will create a language teaching project using the translate block for languages that the child doesn't speak, just because it's fun. We can't expect the child to understand the implications without any explanation, because in my experience even many adults don't.  You're thinking in terms of teaching computing, but we need to look at the bigger picture here and think about the consequences for language teaching too. I just told a language teacher I know about the translate block and she immediately started verbally pulling her hair out before I even had a chance to finish explaining how it's used. She immediately brought up the point that mistakes that have been learned are very hard to unlearn. So, I am not the only person on the planet who is concerned about this.  +1, a lot better than what I wrote :) Judging from the real-life airport example, the grammar error rate there is ~50%. I would not call that ""few and far between"" - and this is for 2 very closely related languages. Of course, your mileage will vary with language pair used and content translated. There will probably be a lot less errors when translating German - English than translating English - German because English has less grammar markers. For example, German ""Kopie"" and ""Kopieren"" both translate as ""Copy"" into English.  Of course not! But both are using Google Translate, which still produce identical output, no matter whether somebody sticks it on an airport sign or into Scratch.  It might look like that for non-German speakers, but it is not. It is a Dative/Accusative case distinction. Any German teacher would mark this as a grammar error and never as a typo.   Thanks :) It's just a first draft, and suggestions for improvements like you made are very welcome in my book!  And don't get me wrong, Google Translate itself is a very useful tool, and it will only become harmful if misused. So, I think we do have the responsibility to teach children how to use it properly, since we're offering them the tool directly. I support the idea that Google Translate should be an opt-in and have an explicit warning about how correct the translation would be. It could be very embarrassing for a student to get it very wrong! Google Translate is pretty good for English to Welsh from my experience, it's correct about 60-70% of the time. That means that there is plenty of scope for errors, howlers and embarrassment in using machine translation.  Try some of these. Google Translate, like Microsoft Translate seems to be better with longer sentences and in Welsh because it's based on formal, bureaucratic documents it's in a more formal style than would be appropriate for Scratch. I would be very cautious about encouraging it's use with students who are not fluent in their use of the translated language."
technical,"I agree, this is a terrible idea - at best, this should be a per-locale **opt-IN**. For many locales, especially the smaller ones, machine translation produces nothing but gibberish. Machine translation is intended as a gisting tool, by and large, and for some language combos (mostly large ones with huge corpora) it may function as a translation aid but for most, it's useless at best, an extra time cost at worst. It will overall reduce the attractiveness of Scratch to educational establishments. There is no quick fix for localization. I think that a disclaimer would also be really helpful for those languages that will keep the translate block - it could act as a teaching opportunity. Something like this:   Translating to another language is difficult, and computers haven't quite mastered it yet. Only use this extension when trying to understand a project that was created in another language - you can remix it and add the translate block to your remix. We recommend that you do not publish such remixes. Do not use the translate block inside of  join  blocks, because it will result in the wrong word order for some languages and you'll sound like Yoda. Please be aware that machine translation can produce offensive content by accident and that it will often contain grammar errors.  A similar disclaimer could be added to published projects that contain translate blocks, to prevent misunderstandings.  You should also be aware that grammar errors in machine translation results can be detrimental to foreign language learning, because the grammar errors will imprint themselves on students' visual memories and these memory imprints are very hard to get rid of. For example, although I have been called ""as fluent as a Bard"" by native speakers, I still have difficulty with vowel length after over a decade of learning Gaelic, because I have seen too many texts where the accents on the vowels were missing.  Here's a real-life example of machine translation grammar errors from the English - German language pair. I have left the airport a note and they have now corrected it."
technical,"Thanks for all the feedback on this folks. We are working on developing landing pages for each extension and will add some information that clarifies the accuracy of machine translation on the ""Translate"" extension page based on all of your feedback. Thanks. Since my suggestion to remove the Google Translate block was shot down, could you please at least remove Scottish Gaelic (G idhlig) from the language list?  In order to produce intelligible results, Google Translate needs a huge bilingual corpus to run their statistic algorithms on. No such corpus exists for Scottish Gaelic, and we can't expect such a corpus to exist in the near future.  I am not only worried about teaching people really really bad Gaelic, but we can expect misunderstandings and accidental NSFW content too.  See for more info about the difficulties that minority languages have with machine translation."
technical,"Eh, sorry but where are you getting these figures from? Even for a language pair like German < English for which Google must have a lot of data aren't even close to 99%, that's just a figure you made up to make it sound like a good idea. To begin with, you cannot make a blanket statement about how accurate MT translation is without specifying the language pair you're talking about. I don't know what your language skills are but it's a common mistake by English speakers to assume that because GT does an ok job INTO English, the reverse is also true. It isn't. English happens to be a fairly uninflected language with little morphology, which makes it relatively easy for MT. But try going FROM English to something like German with 4 cases and 3 genders, it gets a whole lot harder straight away.  Secondly, the quality of MT depends hugely on the domain. For instance, for medical stuff, German < English is pretty good. Put in a novel, you get vastly different results. If you look at the research, even for something fairly simply as medical texts accuracy has been measured as low as 45-46% (into African and Asian languages). So no, not even close to 99%. Making up figures is not good form. Thanks for all the feedback on this folks. We are working on developing landing pages for each extension and will add some information that clarifies the accuracy of machine translation on the ""Translate"" extension page based on all of your feedback. Thanks."
technical,"I appreciate the sentiment, but you're getting a little extreme. Let's look over your proposed disclaimer: Translating to another language is difficult, and computers haven't quite mastered it yet. Only use this extension when trying to understand a project that was created in another language  Right off the bat, this is already very extreme. Why shouldn't you use the translate extension for a purpose other than this? Computers haven't quite mastered it, sure - but they're already very good, and 99% of the time translation errors are caused by GIGO rather than actual mistranslation. you can remix it and add the translate block to your remix. We recommend that you do not publish such remixes.  Why not?? What's wrong with sharing your attempt at understanding another language? Do not use the translate block inside of join blocks, because it will result in the wrong word order for some languages and you'll sound like Yoda.  Good recommendation, but the ""you'll sound like Yoda"" is too informal for a disclaimer. And in many languages word order isn't as significant (e.g. Latin) and grammar is mostly determined by word endings, rather than order. Nothing wrong with using join blocks with languages like that. I think this should read something more like ""Pay attention to what language you're translating when putting the translate block inside join blocks. Sentences in some languages may have completely different meanings based on word order.""  Please be aware that machine translation can produce offensive content by accident and that it will often contain grammar errors.  Can produce offensive content by accident: yes. Often contain grammar errors: no. At least for Google Translate (which is, I believe, the API that the translate blocks run on), the grammar errors are few and far between. Most issues with machine translation are with prosidy and not syntax. Better to say that ""machine translation can contain grammar errors or offensive content by accident"".  As for your real-life examples: Scratch isn't going to be used in airport displays (if they are, I'd be quite shocked) and the ""an""/""am"" problem looks to be a transcription error rather than an error of the machine itself (like somebody said to the person typing, ""am Flugsteig"" but the ""m"" sounded like an ""n"").  Don't get me wrong - I do agree that some languages should not be in here as there is too little to base machine translations on, and Scottish Gaelic fits that description - I just think your disclaimer is a little too strong, a little too scary, and a little inaccurate. The disclaimer is just a first draft collecting some ideas - of course, it can be further refined and rephrased.  I added it as a first basis for discussion and I am not expecting anybody to implementing it just like that verbatim. I do think it is important that we do add some form of disclaimer though ,)  Because other children will assume that it is correct language and learn the mistakes. Such mistakes are very difficult to unlearn. Chances are very high that a child will create a language teaching project using the translate block for languages that the child doesn't speak, just because it's fun. We can't expect the child to understand the implications without any explanation, because in my experience even many adults don't.  You're thinking in terms of teaching computing, but we need to look at the bigger picture here and think about the consequences for language teaching too. I just told a language teacher I know about the translate block and she immediately started verbally pulling her hair out before I even had a chance to finish explaining how it's used. She immediately brought up the point that mistakes that have been learned are very hard to unlearn. So, I am not the only person on the planet who is concerned about this.  +1, a lot better than what I wrote :) Judging from the real-life airport example, the grammar error rate there is ~50%. I would not call that ""few and far between"" - and this is for 2 very closely related languages. Of course, your mileage will vary with language pair used and content translated. There will probably be a lot less errors when translating German - English than translating English - German because English has less grammar markers. For example, German ""Kopie"" and ""Kopieren"" both translate as ""Copy"" into English.  Of course not! But both are using Google Translate, which still produce identical output, no matter whether somebody sticks it on an airport sign or into Scratch.  It might look like that for non-German speakers, but it is not. It is a Dative/Accusative case distinction. Any German teacher would mark this as a grammar error and never as a typo.   Thanks :) It's just a first draft, and suggestions for improvements like you made are very welcome in my book!  And don't get me wrong, Google Translate itself is a very useful tool, and it will only become harmful if misused. So, I think we do have the responsibility to teach children how to use it properly, since we're offering them the tool directly."
technical,"Google Translate *is* an opt-in by virtue of the fact that it's an extension! If you don't explicitly include the extension, you can't use the blocks and have therefore not opted in. By including the extension, you are obviously showing that you are opting in to using the extension. A disclaimer is necessary but should not be obtrusive. This is a different type of opt-in though. We should not shift the hard decision making to the children, that's not fair on them. The responsibility is ours, not theirs.  By all means, keep the block with a disclaimer for those language pairs where the error rate is extremely low. I'd still like to have my locale removed though, because gibberish translations are no use to everybody, and we will be unintentionally teaching this gibberish.  The disclaimer should also be short ( a lot shorter than my draft) to make it easy to read. Otherwise, kids will just go tl,dr on it."
technical,"The way I see it, this issue could pop up in any software that uses Rouge, and is therefore not specific to Jekyll. I agree however that it could have been better communicated before it was closed, sorry about that. ericmorand I re-opened the issue to convey that we have not abandoned this report straight away.. Do know that I've kept a tab on the issue-ticket at Rouge and will follow its proceedings as time permits.."
technical,"This issue has been automatically marked as stale because it has not been commented on for at least two months.  The resources of the Jekyll team are limited, and so we are asking for your help.  If this is a **bug** and you can still reproduce this error on the latest <code3.x-stable</code or <codemaster</code branch, please reply with all of the information you have about it in order to keep the issue open.  If this is a **feature request**, please consider building it first as a plugin. Jekyll 3 introduced hooks which provide convenient access points throughout the Jekyll build pipeline whereby most needs can be fulfilled. If this is something that cannot be built as a plugin, then please provide more information about why in order to keep this issue open.  This issue will automatically be closed in two months if no further activity occurs. Thank you for all your contributions. Hi,  Jekyll syntax highlighting is broken with Twig. Consider the following code block containing a perfectly valid Twig syntax:  It outputs the following HTML:  Notice the **err** class attributed to the equal sign.  ## Steps to reproduce  * Follow the official quick-start guide * Replace the content of the post created by the installation with this:"
technical, How is the resulting output if you were to use triple-backticks instead?
technical,"Please take a step back and consider that this is an entirely volunteer-run project. We're not contractually obligated to work on every bug and answer every question, seeing as we simply don't have enough resources. So our apologies if some things take too long, or don't end up happening, but it's wrong to blame the maintainers for this. I totally understand that and i can relate: maintaining open source projects is very time-consuming. But I'm not having that discussion because I want to see that bug fixed immediately, to be honest this is a low priority bug even by my own standards. My point is that if, when a bug happens, maintainers blame a dependency, close the issue and ask for the reporter to open an issue elsewhere, that could go that way: Oh, sorry this is a bug with Rouge, go open an issue there. Oh, sorry this is a bug with Ruby, go open an issue there. Oh, sorry this is a bug with GCC, go open an issue there.  ...and so on. At one point, my issue will be invalid because I won't even know what and how to report the bug. Already, Rouge maintainers could totally close my issue as invalid because I'm giving a way to reproduce that imply Jekyll - it would be legitimate for them to say that it's a Jekyll bug or that they want a reproducible example using only Rouge."
technical,"So you see the issue is not with Jekyll but rather with Rouge that highlight and the triple-backticks block uses to highlight code. No, the problem also happens with the {% highlight %} syntax."
technical,So you see the issue is not with Jekyll but rather with Rouge that highlight and the triple-backticks block uses to highlight code. So you see the issue is not with Jekyll but rather with Rouge that highlight and the triple-backticks block uses to highlight code.
technical,"We're sorry that you're facing issues while using Jekyll. I agree that you as an end-user shouldn't concern yourself about bugs in dependencies.  One of the maintainers will get in touch with the developers at Rouge and sort things out for you. thanks a lot. I already created an issue, maybe a maintainer could comment on it if the issue is not clear enough or if some things can be added:"
technical,"I totally understand that and i can relate: maintaining open source projects is very time-consuming. But I'm not having that discussion because I want to see that bug fixed immediately, to be honest this is a low priority bug even by my own standards. My point is that if, when a bug happens, maintainers blame a dependency, close the issue and ask for the reporter to open an issue elsewhere, that could go that way: Oh, sorry this is a bug with Rouge, go open an issue there. Oh, sorry this is a bug with Ruby, go open an issue there. Oh, sorry this is a bug with GCC, go open an issue there.  ...and so on. At one point, my issue will be invalid because I won't even know what and how to report the bug. Already, Rouge maintainers could totally close my issue as invalid because I'm giving a way to reproduce that imply Jekyll - it would be legitimate for them to say that it's a Jekyll bug or that they want a reproducible example using only Rouge. The way I see it, this issue could pop up in any software that uses Rouge, and is therefore not specific to Jekyll. I agree however that it could have been better communicated before it was closed, sorry about that."
technical,"ericmorand I re-opened the issue to convey that we have not abandoned this report straight away.. Do know that I've kept a tab on the issue-ticket at Rouge and will follow its proceedings as time permits.. This issue has been automatically marked as stale because it has not been commented on for at least two months.  The resources of the Jekyll team are limited, and so we are asking for your help.  If this is a **bug** and you can still reproduce this error on the latest <code3.x-stable</code or <codemaster</code branch, please reply with all of the information you have about it in order to keep the issue open.  If this is a **feature request**, please consider building it first as a plugin. Jekyll 3 introduced hooks which provide convenient access points throughout the Jekyll build pipeline whereby most needs can be fulfilled. If this is something that cannot be built as a plugin, then please provide more information about why in order to keep this issue open.  This issue will automatically be closed in two months if no further activity occurs. Thank you for all your contributions."
technical,"Yes, that's because the highlight tag uses Rouge for syntax-highlighting by default  If you want to use pygments instead of rouge as your site's highlighter, add the following to your  config.yml: yml highlighter: pygments This sounds like an issue with rather than Jekyll. Jekyll has no knowledge of syntax of any language."
technical,"There is nothing in Jekyll's code that can be changed to fix this issue, the fix will have to come from Rogue.  That's not my point. You are the one using Rouge to implement a feature that you advertise explitely on your docs! That's your responsibility to take care of things that don't work as expected in the dependencies of your project. As a consumer of your product, I expect it to work as advertised: You are advertising syntax highlighting, you are supposed to deliver! And if you don't, you are supposed to take care of whatever is needed to have your product work as expected. We're sorry that you're facing issues while using Jekyll. I agree that you as an end-user shouldn't concern yourself about bugs in dependencies.  One of the maintainers will get in touch with the developers at Rouge and sort things out for you."
technical,"why did you close this? It's not fixed and I'm not the one explicitely using Rouge. The maintainers of the project are using a dependency that is buggy, they should take care of this. What do you want me to do? I don't even know what Rouge is! What do you want me to do? I don't even know what Rouge is!  I've provided a link to the repository so that you can open an issue there and explain the problem you are having.  I do not know what Twig is, so it would not make sense for me to be the one to explain what needs changing in Rogue.  There is nothing in Jekyll's code that can be changed to fix this issue, the fix will have to come from Rogue."
technical,"No, the problem also happens with the {% highlight %} syntax. Yes, that's because the highlight tag uses Rouge for syntax-highlighting by default  If you want to use pygments instead of rouge as your site's highlighter, add the following to your  config.yml: yml highlighter: pygments"
technical,"Sorry, what do you mean by ""documentation thing""?  How users are supposed to build their models, if they want specific model? They should be sent to facebook for generally the same skip-gram model called fastText?  How exactly ""documentation thing"" is supposed to generate vectors for unseen-before words, without actual implementation code?  Sorry, i'm really not sure, what do you mean by ""documentation thing"" in this case. fastText is a tool to create word vectors, but also sentence vectors, and many other things to do with word vectors.  The point is I suggest it's better to let fastText handle word vector generation, and just remove that functionality from dl4j. Because fastText does it to a much higher level -- because they specialize in it.  So the ""documentation thing"" is to deprecate the Word2Vec generation examples and code in DL4J, and say ""see fastText [with link] for generation of word vectors""."
technical,"You're welcome to send us PR.  Questions about w2v are usually not related to w2v itself, but memory and loaders related. fastText produces word2vec style vector files, so it's more a documentation thing?  Beyond that it's something that keeps coming to mind that I wanted to share, rather than go further submitting pull requests etc."
technical,"Keep it professional or I won't hesitate to ban you from interacting with this repository.  Now, as for the question of FastText - that's an internal roadmap question and question of priorities. Deprecating W2V is a moot point until we have support for it. Even then, I don't see why we should deprecate something functional just because something better is around. FWIW, we can use fastText from Java and I would be happy to maintain them as part of the JavaCPP Presets, contributions welcome"
technical,"fastText is a tool to create word vectors, but also sentence vectors, and many other things to do with word vectors.  The point is I suggest it's better to let fastText handle word vector generation, and just remove that functionality from dl4j. Because fastText does it to a much higher level -- because they specialize in it.  So the ""documentation thing"" is to deprecate the Word2Vec generation examples and code in DL4J, and say ""see fastText [with link] for generation of word vectors"". Good point. I hope one day Google will get the same idea, and deprecate Android.  Apple does it to a much higher level - because they specialize in it."
technical,"Things that are superceded *in their use case* should be deprecated.  And of course you remember me. It's not a big deal.  C and C++ are great. For their use case! Otherwise why wouldn't DL4J just use Java throughout?? Java doesn't give access to AVX for important matrix math... doesn't give access to many useful things. So Java is useless for that.  But w2v is worse than fastText. That I won't change my mind about! Hey folks - I'm locking this. tom-adsfund I agree with raver119 - the proper solution would be for us to add support for fast text, not ""deprecate"" word2vec.  We can definitely add it. As of right now it would have to be a pull request though. We can maybe look adding support for adding that file format. I'll keep this issue open as the point stands."
technical,"Please look at this, because he is still angry I said C wasn't a good language.  (Btw, C is a fine language, just not something I use much or personally gain much utility from.) In any way - it's not a way of solving problems. Differences should drive competition. With this logic everything out there can be deprecated: dl4j, keras, chainer, pytorch - we should just all use TF and be happy.  Programming languages should be all deprecated too, in favor of C++."
technical,"fastText produces word2vec style vector files, so it's more a documentation thing?  Beyond that it's something that keeps coming to mind that I wanted to share, rather than go further submitting pull requests etc. Sorry, what do you mean by ""documentation thing""?  How users are supposed to build their models, if they want specific model? They should be sent to facebook for generally the same skip-gram model called fastText?  How exactly ""documentation thing"" is supposed to generate vectors for unseen-before words, without actual implementation code?  Sorry, i'm really not sure, what do you mean by ""documentation thing"" in this case."
technical,"Good point. I hope one day Google will get the same idea, and deprecate Android.  Apple does it to a much higher level - because they specialize in it. That analogy makes no sense.  Here's a better analogy: fastText vectors are superior to w2v vectors in the same way Darknet is superior to VGG.  (Android and iOS aren't comparable because they have different use cases.)"
technical,P.s. sorry. Just too many people around. Things that are superceded *in their use case* should be deprecated.  And of course you remember me. It's not a big deal.  C and C++ are great. For their use case! Otherwise why wouldn't DL4J just use Java throughout?? Java doesn't give access to AVX for important matrix math... doesn't give access to many useful things. So Java is useless for that.  But w2v is worse than fastText. That I won't change my mind about!
technical," You're welcome to send us PR.  Questions about w2v are usually not related to w2v itself, but memory and loaders related."
technical,"No, I don't need to insert a showed code between <head tag's or in the footer.  I should to insert the code into my MDX-posts. And it's show my Yandex Ads into my posts.  For example, I want to show Ads in the center of my MDX-posts. Or after some H2-tags etc.  How I can insert my code into MDX-posts? Feels to me like this is more of an MDX question than Gatsby, is it not? I have little experience with MDX. Gatsby takes the MDX content and uses Babel (etc.) to generate the web pages. It's not really aware of this."
technical,"Then you would do something slightly different. It's hard to predict what you want, because it's not clear what you want.  Here's an example Codesandbox that gets  closer  to what you want, but it's still not working. This being said, I also wasn't even able to get the snippet you pasted above working, so I'm skeptical of the general approach. React Codesandbox | [Vanilla Codesandbox   Failed to load resource: the server responded with a status of 404 ()  I'm going to lock this, as I don't find this discussion productive nor valuable for us to be having. This is very much not a Gatsby problem, and is a general issue with loading Yandex scripts. I'm using Gatsby-MDX for my posts. And I want to add Yandex RTB-code to my site. It's Google Adsense alternative from Yandex (in Russia). Here is example, how is looking Yandex RTB-code:  Please, sir, tell me. What's there is best way to insert Yandex Ads into my MDX-posts? For example, can I create .txt-file. Then import .txt file (with Ads code) where I need into my MDX-posts?"
technical,"Hey I'm assuming this is just some code you need to insert into the head element. If so, this document will tell you how to customize the html output. Follow the steps and put this script tag in the head tag. I'm not sure what else Yandex needs, but this is generally how google adsense works. I'm going to close this issue as I don't believe it's reporting a problem with Gatsby. I'm happy to keep the discussion going, so don't take this as a negative sign. Please report back if you need continued support! ˜ No, I don't need to insert a showed code between <head tag's or in the footer.  I should to insert the code into my MDX-posts. And it's show my Yandex Ads into my posts.  For example, I want to show Ads in the center of my MDX-posts. Or after some H2-tags etc.  How I can insert my code into MDX-posts?"
technical,Hey can you please provide the details of what problem you are facing? please send your flutter run -v and flutter doctor -v and minimal reproduction code? Thanks Flutter team is doing the great. Flutter is one of the most popular frameworks for making apps nowadays. Flutter is so good and Thanks to the Flutter team for making the awesome framework.
technical, Hey can you please provide the details of what problem you are facing? please send your flutter run -v and flutter doctor -v and minimal reproduction code? Thanks
technical,"Flutter team is doing the great. Flutter is one of the most popular frameworks for making apps nowadays. Flutter is so good and Thanks to the Flutter team for making the awesome framework. Hi, I'd remind you that we require that all participants on our issue tracker and mailing lists abide by our code of conduct. It sounds like you've run into an issue with handling of PATH on macOS, but it looks like you may have submitted this issue before filling out the issue template. If you're running into a reproducible issue I'd ask that you file a new issue and fill out the template with a clear statement of the issue you're running into and repro steps."
technical, For context the rename/work is complete. The blog post which explains the changes is here. The node-api team was asked to change the name in this. . Being sensitive to the concern the team took on this extra work.
technical,"For context the rename/work is complete. The blog post which explains the changes is here. The node-api team was asked to change the name in this. . Being sensitive to the concern the team took on this extra work. Repeating what I wrote 3 months ago. I'll also add that Node API is somewhat contrary to our years-long efforts to have the name of the runtime spelled Node.js and not Node (or NodeJS or a number of other variatns).  Of course, now that the name change has already happened, there are significant costs/downsides to changing the name a second time in such a short period of time. I'd still support it, though, if the name was more descriptive and not subject to misinterpretation. A better name is better for our users."
technical,"I'm -1 on changing the name again and I appreciate the effort the node-api team put into making the changes. It's a good change. Thank you for suggesting an idea to make Node.js better.  Please fill in as much of the template below as you're able. **Is your feature request related to a problem? Please describe.** There was recently a decision to rename N-API to Node-API. I believe this is a poor decision that will result in actively harmful results for both users of the API and general Node.js users.  Specifically, there are multiple problems with this naming:  - Building APIs are a common use case for Node.js. This naming can lead to confusing information or misleading search results. - Products often refer to the way to access their services with JavaScript or Node.js as their ""JavaScript API"". If people want to use this from Node.js, there is a non-trivial chance they will look for ""Node.js API"" which will lead to confusing results. - Node.js itself has an API, which theoretically includes this API. Naming a part of the whole the same thing as the whole is immensely confusing from an education perspective. - This API is far less likely to be used than other parts of the Ndoe.js API, which leads to an exacerbation of challenge presented by the problems above.  **Describe the solution you'd like**  Rename Node-API to something else.  **Describe alternatives you've considered** - Undoing Node-API rename, moving it back to N-API. -  There is a reason a rename was done initially, and that reason is valid. - Leave it as is. - This is going to be actively harmful to communication and education n the long-run."
technical," For those looking for the previous discussion, it can be found here."
technical,"For those looking for the previous discussion, it can be found here. I believe if you move your folders in XDG compliant places it should work, though updating all the code and documentation all over the internet, plus the code to do the right thing is a lot of work, and XDG spec IIRC did not completely match or was not clear for all the types of files that could be present.  There might be packages on PyPI that may help with AppData. appdata depending on OS, and this will also likely need to touch all the jupyter ecosystem (ipykernel, jupyter client, traitlets, etc, so I doubt there'll be an effort from core dev to push that forward.  THough if you have issues when files  exists  in XDG placed and not found I""ll be happy to get fixes in."
technical,"This is a blocker for all automation as this is an unacceptable change in the environment in which we are performing QA. Users don't run in developer mode, so we aren't allowed to QA in it. It's as simple as that. For what it's worth I am using WinAppDriver in a production environment to do Robotic Process Automation (RPA) Validation of application availability.  Developer Mode has been identified by our security team as a no no.  It's a show stopper.  Why FORCE developer mode?"
technical,"We have the same issue in WinAppDriver !! Any solution for this thread ? Hi, WinAppDriver's requirement on DeveloperMode is based on the premise that it is a developer tool. Enabling DeveloperMode requires Administrative access to the machine, and therefore enforces that the user has the right set of permissions to control the machine."
technical,It's a developer tool that  opens a listening port on your machine that allows anybody who connects without authentication to drive and screen scrape any application your account can access . It's a developer tool. I would like to use WinAppDriver on a computer that doesn't have developer mode turned on. Is there a technical limitation or reason why it requires developer mode? It starts up just fine even when developer mode isn't turned on with the message:
technical,"This is stupid! Administrative permissions can be verified through other means, not by open a secure leak on the machine! It's a developer tool that  opens a listening port on your machine that allows anybody who connects without authentication to drive and screen scrape any application your account can access . It's a developer tool."
technical,"Hi, WinAppDriver's requirement on DeveloperMode is based on the premise that it is a developer tool. Enabling DeveloperMode requires Administrative access to the machine, and therefore enforces that the user has the right set of permissions to control the machine. Please remove this restriction."
technical,"Hi, WinAppDriver's requirement on DeveloperMode is based on the premise that it is a developer tool. Enabling DeveloperMode requires Administrative access to the machine, and therefore enforces that the user has the right set of permissions to control the machine. This is a blocker for all automation as this is an unacceptable change in the environment in which we are performing QA. Users don't run in developer mode, so we aren't allowed to QA in it. It's as simple as that."
technical,Please remove this restriction. This is a blocker for using WiAppDriver in many corporates as developer mode can't be enabled for all developers.
technical,We have the same issue. Any reason for that? What is the requirement for that?  We simply enable developer mode before running our tests by executing the following PowerShell bevor starting up WinAppDriver.exe: We have the same issue in WinAppDriver !! Any solution for this thread ?
technical, We have the same issue. Any reason for that? What is the requirement for that?  We simply enable developer mode before running our tests by executing the following PowerShell bevor starting up WinAppDriver.exe:
technical,why do you use a localhost? on the server this will not work change the example From the attached example not obvious what kind of behavior you are expecting. If you handle an HTTP-error (for instance a request to jsonplaceholder can be used) and try to replace an observable constructor with a high-order observable (in the AuthService) you will find that ngOnInit-hook fires as usual.
technical,"what exactly should happen according to your logic? I just updated the example to make it more similar to real situation. It's working as expected, but the same code (copy-paste) does not work in my app. And I have no idea why. Maybe it's Angular Material 2 interferes with something, because I have no other libraries in my project."
technical,"I just updated the example to make it more similar to real situation. It's working as expected, but the same code (copy-paste) does not work in my app. And I have no idea why. Maybe it's Angular Material 2 interferes with something, because I have no other libraries in my project. I think you can close the task, a person just does not understand"
technical, Please attach a minimal repro
technical,Please attach a minimal repro This is probably different. I forked it to another issue.
technical,"That's absolutely does not interfere the sample. Code runs as it should. Do not distract me for this nonsense, please.  If you handle an HTTP-error and try to replace an observable constructor with a high-order observable (in the AuthService) you will find that ngOnInit-hook fires as usual.  That's exactly what I'm doing in the code. And ngOnInit() does not work. what exactly should happen according to your logic?"
technical,"The subject isn't the Open Source. I don't respect people who want to collect my PRIVATE data with software. If you don't like my demand, delete it. If you want better software, delete all these malwares. FSF wants to know your location"
technical,"The Microsoft Store version does collect data, in this project it is disabled by default (per the read me): Just use Linux - simple as that ,)"
technical,"Twitter collects and sells our data too. It's as bad as Microsoft. Use Mastodon instead. Our Code of Conduct requires everyone to *be respectful* to each other, and we should do better than what's in this thread. If you have specific questions about our telemetry system for Calculator we can try to answer them separately."
technical,"Be thankful. Microsoft uses the data to improve your calculating experience. :smile: The Microsoft Store version does collect data, in this project it is disabled by default (per the read me):"
technical,Take your foul mouthed trolling to Twitter. That's what it's there for. There are countless calculator apps. Find one that you are satisfied with and use it. This one is obviously for people with different concerns than you. They get to have their applications too. You are not Calculator King of the Internet. Grow up and move along. This is your friendly Microsoft Issue Bot. I've seen this issue come in and have gone to tell a human about it.
technical,This is your friendly Microsoft Issue Bot. I've seen this issue come in and have gone to tell a human about it. Twitter collects and sells our data too. It's as bad as Microsoft. Use Mastodon instead.
technical,FSF wants to know your location Why are you even here?
technical," Greetings! Thanks for taking the time to open this issue. In order for the community to handle your issue effectively, we need a bit more information.  Here are the items we could not find in your description: - ansible version - component name Please set the description of this issue with this template. click here for bot help"
technical,"while I fully understand your reasoning, this is detrimental to people learning how to use Ansible. Google groups are literally terrible. For example, there are 0 results for ""Unable to parse /etc/ansible/hosts as an inventory source"" when searching this in the group. However, Google finds multiple hits, and among the top ranking hits are these issue threads. Github issue tracker has a far superior interface because it's easy for winning solutions to be marked with reaction emojis. When a winning solution is found, the more people that it works for, the more emojis that solution will collect. This saves people a lot of time and reading, because I can skim an issue thread for the reaction emojis, I don't need to struggle to comprehend every commenter's broken english description of something that's probably not even relevant. Please reconsider. Hello Community, I'm very new to ansible. I'm currently running Ansible from Azure Cloud Shell. I have added a machine to the Hosts file. However, whenever I attempt to ping or connect to the machine I get the following errors:  [WARNING]: Unable to parse /etc/ansible/hosts as an inventory source  [WARNING]: No inventory was parsed, only implicit localhost is available  [WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all'  Can someone let me know how to fix this problem please."
technical,"Thanks very much for your interest in Ansible.  It sincerely means a lot to us.  This appears to be a user question, and we'd like to direct these kinds of things to either the mailing list or the IRC channel. * IRC: #ansible on irc.freenode.net * mailing list If you can stop by there, we'd appreciate it.  This allows us to keep the issue tracker for bugs, pull requests, RFEs and the like. Thank you once again and we look forward to seeing you on the list or IRC.  Thanks! I have installed through Cygwin on windows 10. Having same issue"
technical,"thanks for reaching out. See below: I'm not sure what you mean by 'Component Name' Just so you know, I'm open thread on stackoverflow, which I thought might fix the issue see. However, this was just a work around to fix the underlying issue as described in this thread. Thanks again, I hope you can help me fix this. Cheers I have the same problem  and  I install it by brew on Mac"
technical,I have the same problem  and  I install it by brew on Mac thanks for reaching out. I use a PC. Do you know the equivalent  to 'brew' in PC? Cheers
technical,"Greetings! Thanks for taking the time to open this issue. In order for the community to handle your issue effectively, we need a bit more information.  Here are the items we could not find in your description: - ansible version - component name Please set the description of this issue with this template. click here for bot help thanks for reaching out. See below: I'm not sure what you mean by 'Component Name' Just so you know, I'm open thread on stackoverflow, which I thought might fix the issue see. However, this was just a work around to fix the underlying issue as described in this thread. Thanks again, I hope you can help me fix this. Cheers"
technical,"thanks for reaching out. I use a PC. Do you know the equivalent  to 'brew' in PC? Cheers Thanks very much for your interest in Ansible.  It sincerely means a lot to us.  This appears to be a user question, and we'd like to direct these kinds of things to either the mailing list or the IRC channel. * IRC: #ansible on irc.freenode.net * mailing list If you can stop by there, we'd appreciate it.  This allows us to keep the issue tracker for bugs, pull requests, RFEs and the like. Thank you once again and we look forward to seeing you on the list or IRC.  Thanks!"
technical,"LAWL EVEN PEOPLE IS ASKING FOR IT thumbs down don't bother me, take some relief on your real life frustration and let them rain down HAHAHAHAHA look at which hacks people has to resort"
technical,"HAHAHAHAHA look at which hacks people has to resort Instead of thumbing down like twitter and facebook kids, show me I'm wrong  Sure. I'll go issue by issue.  Spoiler: no template i have no time to waste Firstly, the Angular repository receives **hundreds** of issues a day. The issue template ensure that the report is structured so the team can understand the issue quickly.    Problem: Reactive forms  div with [ngClass] directive:   f is a getter in .ts file that returns all controls (and work everywhere else)  d-none is bootstrap display:none class  this is a select/option dropdown which if !==1 shows div else hides it as you can guess   When dropdown activated manually it works  when the dropdown changes because changed from code (using your (change) method binding)  it doesn't change Because you change the DOM, but didn't change the ControlValueAccessor behind it. I suggest you use this  instead.  I'm fed up the whole framework is filled with this crap. Why you even do double way binding if works half the needed cases? Don't even make a framework at this point No, Angular does not have two-way binding. AngularJS does. (Even the [()] thingy is one-way binding underneath.)   I've read about observable something, but why would I use the whole framework if I had to use another framework and do my own implementation? It has no fucking sense. RxJS is not a framework. It's library which enables reactive programming in Angular.   If the customer didn't specifically asked for angular I would have thrown this piece of trash in the toilet long time ago  How to solve I'm not able to reply on this one.   EDIT: Thumbs down my ass, it's reality that smacks your face   [This is not cool.]"
technical," Hello, what is your version Android me it's 8.1"
technical,"Whoa let's all calm down please. vBlackOut is just trying to help (and clearly english isn't his first language, so give him a break), it seems you're a little too caffeinated here :)Let's start over in a new issue to avoid this exchange. I am getting the error: I've confirmed that the path I'm pushing to exists. I'm pushing a directory to another directory. I'm using Ubuntu 18.04. I am able to use the Shell command fine.  This looks similar to #125 except in Push not Pull. The adbd is running as root on the phone, as is python3 on my PC."
technical,It's just forward fix but it's not finally real fix I just for understand the code
technical,"I use the same app and have zero issues with comments, etc. Also, your explanation still makes zero sense ""It's just re-identify the id if change"" isn't even a proper sentence. And even after being told by the main contributor of the repo that that is not a fix you continue to spam peoples' posts with this. Just PLEASE STOP. Listen to his comment and stop suggesting this crudely thought-up bandaid of a ""fix"". It's just forward fix but it's not finally real fix"
technical,"Read fahhem's response. That is not the way to solve the issue. Also, something odd is happening, your replies are either getting auto-deleted on threads (likely because of the repetitive comments) or you're deleting them yourself. For my Issue you posted the same comment and NO, that does not fix the problem and NO that is not the answer to the bug. It's just re-identify the id if change"
technical,"Hello, what is your version Android me it's 8.1 Look here for me solve my problem temporary"
technical,It's just re-identify the id if change Not best practice but it's works
technical,"Look here for me solve my problem temporary Read fahhem's response. That is not the way to solve the issue. Also, something odd is happening, your replies are either getting auto-deleted on threads (likely because of the repetitive comments) or you're deleting them yourself. For my Issue you posted the same comment and NO, that does not fix the problem and NO that is not the answer to the bug."
technical,Not best practice but it's works Sorry I use GitHub on phone (fasthub) it's not ideal for comment and explore the interface GUI
technical,"Thanks for creating this issue. We think this issue is unactionable or unrelated to the goals of this project. Please follow our issue reporting guidelines. Happy Coding! Hello! It is very offensive to me, that code that I write using vscode is not very well. It is full of bugs, repeatings of itself and not-working features. Please make something to fix it."
technical, Thanks for creating this issue. We think this issue is unactionable or unrelated to the goals of this project. Please follow our issue reporting guidelines. Happy Coding!
technical,"Thanks for creating this issue. We think this issue is unactionable or unrelated to the goals of this project. Please follow our issue reporting guidelines. Happy Coding! Hey, maybe instead of removing Santa Claus hat because 1 SJW, can you make an option to turn it on and off in options? Really, why you have to remove it?"
technical,Thanks for creating this issue. We think this issue is unactionable or unrelated to the goals of this project. Please follow our issue reporting guidelines. Happy Coding! Thanks for creating this issue. We think this issue is unactionable or unrelated to the goals of this project. Please follow our issue reporting guidelines. Happy Coding!
technical,The removing of the Santa hat and catering to vocal minority of sjw idiots is very offensive to me. Please stop. Who is going to submit the PR?
technical,"Simply stupid. There's no other way to put it. Stupid, like all this progressive shit destroying everything it touches. Hey, thanks for starting the discussion. This is something we've been talking about. As you mentioned GitHub will be likely taking actions in the future and we'd want to coordinate the efforts.  I'll lock this thread for now. If you're interested in further reading on the motivation for such a change, I recently got pointed to a similar discussion in the Linux community."
technical,"I guess my Masters Degree now has to renamed to Main Degree as well. What people don't grasp is that in this context, the Master is like the Master in an audio recording. It's THE version. That's the context and the only context. I can't believe the insanity that's going on. I concur I am of the opinion that this is not even an issue with regards to Angular, it's functionality, and if it works properly and should therefore simply be closed.  Regardless I hope you are well and have an awesome day!!!"
technical, Master is not a problematic word.
technical,"Hey, thanks for starting the discussion. This is something we've been talking about. As you mentioned GitHub will be likely taking actions in the future and we'd want to coordinate the efforts.  I'll lock this thread for now. If you're interested in further reading on the motivation for such a change, I recently got pointed to a similar discussion in the Linux community. Similar to the reasoning behind GitHub's recent statement that the default branch on new repos will be changed to not use ""master"" can we change the name of the default branch on this repo to something else? Perhaps ""main""?"
technical,"I find the ""V"" in ""VS Code"" offensive. It represents two fingers being held up in an obscene gesture and could be read as ""F*ck You"". I agree. Nowadays everyone is offended by everything. The fact that he doesn't like the hat has nothing to do with religion. I find Hissvard's comment ""That's offensive to real religious issues"" completely right. There are real religious issues by now, and classifying such a thing (the hat) as ""offensive"" should be considered shameful."
technical,"We're moving to digital totalitarianism, pure and simple. In totalitarian state, anyone who voiced a different opinion than the state's, was forcibly silenced or 'disappeared'. Now, we have threads locked, accounts banned and single people deciding what's best for entire communities based on their 'feelings'. Well, I 'feel' it's pure evil masquerading as tolerance. It is very telling the true colours on the side of the Microsoft team that this is still up in the air instead of making - as it would be a proper reaction - to write a short apology for the mistake, revert the change and be done with it. Added bonus would have been that the whole issue would be forgotten for everyone in some days. So, this leaves a very sour taste regarding the whole thing and it has already the very opposite intented effect of ""quick political correct reaction and stay clean"" for the whole Microsoft and only will be worse if this will not be corrected and sit out. ""Microsoft the Christmas Killer / Santa Killer"" in the press... not a nice thought and I guess this has the potential to escalate to widely known bad press for the organisation.  My advice to the managing team here: - don't wait it out and don't try to purge the taint when nobody's looking. - reinstate  the santa hat, at least make it a togglable option (default: on) - write a short paragraph that it was a mistake, include some funny joke for deescalation. - be done with it."
technical,"I agree. Nowadays everyone is offended by everything. The fact that he doesn't like the hat has nothing to do with religion. I find Hissvard's comment ""That's offensive to real religious issues"" completely right. There are real religious issues by now, and classifying such a thing (the hat) as ""offensive"" should be considered shameful. Sad thing is that some of us are afraid of writing anything about it and they are writing from newly created accounts and... and I understand it. No one know who will be targeted and his life/career be destroyed by ""tolerance troops"" which have nothing to do with tolerance.  We are going into very, very gloomy era. Chinese government (which is widely criticized by us) is doing the very same thing as ""political correctness"" is doing to us. **Whoever is afraid is a slave** - Seneca the Younger And we all are afraid - especially MS, as we could see."
technical,Thanks for creating this issue. We think this issue is unactionable or unrelated to the goals of this project. Please follow our issue reporting guidelines. Happy Coding! Steps to Reproduce: Install latest build of insiders. Does this issue occur when all extensions are disabled?: No Sorry but the guy who created this issue has it all wrong. Having a santa hat on a cog icon (to go to settings/options) has nothing to do with religion in any shape or form. I find it offensive that the hat was removed and it should never have been removed in the first place.
technical,"It is very telling the true colours on the side of the Microsoft team that this is still up in the air instead of making - as it would be a proper reaction - to write a short apology for the mistake, revert the change and be done with it. Added bonus would have been that the whole issue would be forgotten for everyone in some days. So, this leaves a very sour taste regarding the whole thing and it has already the very opposite intented effect of ""quick political correct reaction and stay clean"" for the whole Microsoft and only will be worse if this will not be corrected and sit out. ""Microsoft the Christmas Killer / Santa Killer"" in the press... not a nice thought and I guess this has the potential to escalate to widely known bad press for the organisation.  My advice to the managing team here: - don't wait it out and don't try to purge the taint when nobody's looking. - reinstate  the santa hat, at least make it a togglable option (default: on) - write a short paragraph that it was a mistake, include some funny joke for deescalation. - be done with it. Thanks for creating this issue. We think this issue is unactionable or unrelated to the goals of this project. Please follow our issue reporting guidelines. Happy Coding!"
technical,"User Christian-Schiffer that created such awful request offended many users and Microsoft just standing there like nothing happened. Bring back the Santa's hat! to also add to that- The original requestor for this ridiculous change has 14 repositories from which 13 are forks. I doubt he's even a developer, just random troll that you- Microsoft fell for. Shame"
technical,"User Christian-Schiffer that created such awful request offended many users and Microsoft just standing there like nothing happened. Bring back the Santa's hat! Two more things to add. Christian-Schiffer have something against santa hat and releats it to religion. Pardon me but he dosn't distinguish between Saint Nicholas that was bishop and had totally diffrent hat and Santa Claus from popular culture who have other hat. Besides his name ""Christian"" smells like some kind of religious agitation and I think he should remove his account or be baned by highter autorities using his own argumentation preented in issue #87268 . Should I create separate issue for that?"
technical,"Sad thing is that some of us are afraid of writing anything about it and they are writing from newly created accounts and... and I understand it. No one know who will be targeted and his life/career be destroyed by ""tolerance troops"" which have nothing to do with tolerance.  We are going into very, very gloomy era. Chinese government (which is widely criticized by us) is doing the very same thing as ""political correctness"" is doing to us. **Whoever is afraid is a slave** - Seneca the Younger And we all are afraid - especially MS, as we could see. UpVote for Santa Hat! … Bring it back! Don't fall for the troll!"
technical,"UpVote for Santa Hat! … Bring it back! Don't fall for the troll! We're moving to digital totalitarianism, pure and simple. In totalitarian state, anyone who voiced a different opinion than the state's, was forcibly silenced or 'disappeared'. Now, we have threads locked, accounts banned and single people deciding what's best for entire communities based on their 'feelings'. Well, I 'feel' it's pure evil masquerading as tolerance."
technical,how about you do it? it's literally your job. or you are here just to spam random stuff till people get pissed off and leave? I am a volunteer as most of Free Code Camp's contributors are.
technical, I am not understanding what the problem is.  Can you explain how we can recreate the issue? Do you have screen shots you can share of the problem you are trying to describe?
technical,I am not understanding what the problem is.  Can you explain how we can recreate the issue? Do you have screen shots you can share of the problem you are trying to describe? i finish the assignment but the system doesn't recognize it.
technical,"i finish the assignment but the system doesn't recognize it. Please use our forum for debugging the code of your projects.  If the forum members determine there is nothing wrong with your code, you can request this issue to be reopened.  GitHub issues are meant for reporting bugs and not troubleshooting the code in your projects. Thank you for understanding."
technical,I am a volunteer as most of Free Code Camp's contributors are. short story: curriculum not on 100% long story: Basic HTML and HTML5 is Not Passed because Use the value attribute with Radio Buttons and Checkboxes doesn't complete. I complete it 3 times and it shows: Basic HTML and HTML5 100% complete but in the curriculum is not.
technical,"IANAL but the typo fix is too trivial to be protected in the court of law. And even if it wasn't it's always possible to simply rephrase the sentence.  Regarding the rest of your comment, I do not think it is reasonable to put a man's livelihood in a perilous position just to make a point. One must learn to pick his fights, so to speak. :) I believe that this PR has run its course and it's time to move on."
technical,"LOL, no way. You have to be nuts. My commit is PUBLIC DOMAIN, do with it as you please. I unfortunately can't accept this without a signed CLA. I double checked to see if there's an exception for trivial commits, there isn't. I guess it's easier for the lawyers this way. I probably won't get to this myself today, so feel free to reopen if you change your mind."
technical," Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).  Once you've signed (or fixed any issues), please reply here with googlebot I signed it! and we'll verify it. #### What to do if you already signed the CLA  ##### Individual signers *   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your [email is set on your git commits.  ##### Corporate signers *   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go *   The email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits. *   The email used to register you as an authorized contributor must also be attached to your GitHub account)."
technical,"IANAL but the typo fix is too trivial to be protected in the court of law. And even if it wasn't it's always possible to simply rephrase the sentence.  Regarding the rest of your comment, I do not think it is reasonable to put a man's livelihood in a perilous position just to make a point. One must learn to pick his fights, so to speak. :) Yes and I am fighting mine trying to live a life according to immutable belief in better tomorrow. One can live a life earning less than the fabled Google salary but dream of better internet where this sort of nonsense isn't for a bunch of lawyers to decide A man's effort to further the way to build software is our birthright It's the direct exercise of LOGOS, the Divine Word, the ability that God himself bestowed on man: to transform chaos into order through an act of speech In the beginning there was the word and the word was with God and the word was God. John:1, I believe Your CLA is preventing me from exercising my religious belief, a right enshrined in your Constitution. The world is full of nonsense because nobody makes a stand"
technical," i don't even like christmas, i'm just into cool hats"
technical, Thanks for creating this issue. We think this issue is unactionable or unrelated to the goals of this project. Please follow our issue reporting guidelines. Happy Coding!
technical, I strongly support that fix !
technical,I strongly support that fix ! Thanks for creating this issue. We think this issue is unactionable or unrelated to the goals of this project. Please follow our issue reporting guidelines. Happy Coding!
technical,I think we can remove the potentially ableist language here and keep the same meaning.  This would make it more inclusive to people with disabilities. I think we can remove the potentially ableist language here and keep the same meaning.  This would make it more inclusive to people with disabilities.
technical," I understand you are unhappy with the change in commit. The change is part of the stabilizing process that every Incubating API goes through. The summary of the contract is when an API is tagged as incubating, the users should be expecting breaking change without notice. You are welcome to submit a PR for this breaking change.  On another note, it would be preferable for your next issue to follow the issue template as well as reading over the Code of Conduct."
technical,"Same, I lost all my family in a car accident. Santa hat reminds me old good memories :sob: :sob: :sob: I was able to reproduce the issue on my side as well"
technical,"It was removed all because of this one guy and he prob think that what he did is funny. Same, I lost all my family in a car accident. Santa hat reminds me old good memories :sob: :sob: :sob:"
technical,"I was attempting to reproduce the issue. Got as far as to feel the joy of my parents being alive, but then a snowflake got in the way and I got offended. Thanks for creating this issue. We think this issue is unactionable or unrelated to the goals of this project. Please follow our issue reporting guidelines. Happy Coding!"
technical,"Nice blog post! This seems unlikely... but if you ever find time to investigate the matter, please report back on golang-dev. I would personally love to hear more about this. Go is a fairly opinionated language. Essentially every single proposal to significantly alter the language by making it ""configurable"" (thus introducing ""language dialects"") has been turned down. Since your post here doesn't add much to the discussion at #33171, I'm closing here as a dup of #33171. I'll just add that the way to change a proposal decision is to provide new information.  I don't see any new information here.  Continuing discussion without new information just takes time without making progress.  We have to be able to make a decision and move on, without continuously discussing the same issues.  Thanks, and thanks for taking the time to write this."
technical," Nice blog post! This seems unlikely... but if you ever find time to investigate the matter, please report back on golang-dev. I would personally love to hear more about this. Go is a fairly opinionated language. Essentially every single proposal to significantly alter the language by making it ""configurable"" (thus introducing ""language dialects"") has been turned down. Since your post here doesn't add much to the discussion at #33171, I'm closing here as a dup of #33171."
technical,"Thanks for reading and the reply. Sorry I'm so verbose. I assumed it was preferred, but perhaps not. If not, perhaps consider making Go less verbose by adding ternary conditional operators.  There's no more examples I need to give about where a ternary conditional is better than an if statement. Plenty of people made those examples already, and had someone else say ""oh I prefer this really verbose and terrible code, so no ternary."" Because this is all just opinions. There isn't a right or wrong there, and that's why I'm not arguing about that part.  My goal of the prior summary was not to provide new information, but rather to sum up the previous conversation for the new venue outside overlord control. I did add more detail to my previous points, added that the conversation is actually taking place elsewhere now because the Golang overlords have been misbehaving and cannot be trusted to host the conversation, and added a prediction about the name of the fork. Although I don't have a lot of faith in that name prediction. Those kinds of predictions are really tough.  That having been said, I did introduce some new thoughts around this issue's relation to human suffering. It's not the kind of information you're looking for, but that's my point. You're not seeing it because you're in the wrong paradigm. I've been a developer for more than a couple decades, but it's my people experience as a lead, principal, and director that I'm speaking from. This conversation is about people, and is not so much about computer science. Otherwise, you're putting computer science before people, and that's fundamentally wrong (both morally and strategically).  People in the general software development community are being subjected against their will to your terrible opinions. Subjected by their jobs and their conditions, their bosses and their teachers. It causes them undue stress, increasing their cortisol, shortening their lifespan. Some of them quit their jobs. Others are fired as a result. You should read **their** blog entries. I even suspect it may have played a factor in at least one suicide. I don't have stats here, but even a conservative estimate would reveal that Golang has negatively affected at least several thousand of its ~1.1M developers. You're doing that to them on purpose, and acting like they are just overreacting. I take it you're not utilitarians. If you can't empathize with them, you should seek therapy. It is dangerous to not feel empathy when you hear about other people experiencing pain, and it should disqualify you from making product decisions. Even product decisions for a programming language.  Your terrible opinions are terrible because they are not evidence-based, and so when half of the world randomly disagrees because there is no right or wrong, you've made it feel like sandpaper on the skin to use your product. You could have been agnostic about the ternary conditional operator like the other popular languages and just included it outright. Instead you're torturing human beings.  Occasionally one of them finds the complaint department, and so you've closed the complaint department -- because when you're torturing people, having a complaint department seems like a wasteful expense.  It's not, it's actually how you suppress the insurrection, and when you close the complaint department that's when the insurrection really heats up. Something is going to happen. This isn't my first rodeo here, and I'm speaking from experience, but I don't know for sure what is going to happen. I predicted a fork, but it doesn't have to be a fork.  Pain results in reaction, and frequently overreaction. There will be an effect of this. Your attempts to ""win"" or ""quell"" all the little nitpicking fights of the software world are admirable, but you don't understand all the sciences involved well enough to actually win the very very difficult game you're playing. You're just using computer science, and it's not the dominant science here. Social sciences dominate here.  Human emotion is incredibly powerful when synchronized, so it's easy to predict the energy will go somewhere. It's far harder to predict where it will go. It might not be a fork. It could be a movement, but probably not. It could be violence, but probably not. It could be legislation, but probably not. It could just cause a little annoyance until Golang becomes less popular as new, better, evidence-based languages pop up, but probably not. Maybe it will just result in a preprocessor. That actually seems equally likely as a fork. Maybe even more likely.  One of the ideas I keep circling around and not saying is it's not up to the Golang overlords. It's up to us, the Golang user community. Y'all acting otherwise is ignorance. Me saying this is not what makes it so. It being so is what makes it so. The Go overlords will comply with the community eventually, because the community is everything. What's more, the community has powers beyond the overlords' control. We can fork, but we can also do worse.  It is this last bit that is what this new conversation is about. I'm basically over discussing the ternary operator. I'm mostly trying to negotiate a compromise on behalf of the minority party, but part of that is also having the discussion about what we, as the minority party, will do if compromise cannot be made. Oh yeah, I guess I never really mentioned why I ever came to the original issue. I was relatively new in a position working as a Director at a medium sized corporation at the time, and the CTO above me insisted we use Go for new backend work instead of .NET, which the CTO and I both hated. I had enjoyed my dalliances with Go so the order was unopposed by me. I pushed it onto my subordinates, who pushed it onto theirs, and we started to dev backend in Go rather than .NET. A significant improvement, I thought. ""These fortunate devs under me who get to finally use a modern tech stack and not just go down the dead-end of .NET development,"" I thought, at the time. But then I got pushback after awhile about these kinds of issues, things like unnecessary verbosity, poor readability, unnecessary style control, etc, and was blown away. ""But those are just opinions."" Yeah, and the alternative is just an opinion. Saying Go is readable or necessarily verbose is absolutely not grounded in reality, and I took awhile to realize that. Eventually someone took me aside and pointed out that none of this is evidence based.  That's when I came here back in March, 2020. Eventually, we realized that it wasn't going to change on any reasonable timescale, and we eventually just started looking at different options -- but that's as far as we got while I was there.  The CTO left and then I got cancer and left.  Last I heard they were back on .NET. Fun story huh.  Oh, and we also pushed them to switch to Vue. They're still using Vue."
technical,"I'll just add that the way to change a proposal decision is to provide new information.  I don't see any new information here.  Continuing discussion without new information just takes time without making progress.  We have to be able to make a decision and move on, without continuously discussing the same issues.  Thanks, and thanks for taking the time to write this. Thanks for reading and the reply. Sorry I'm so verbose. I assumed it was preferred, but perhaps not. If not, perhaps consider making Go less verbose by adding ternary conditional operators.  There's no more examples I need to give about where a ternary conditional is better than an if statement. Plenty of people made those examples already, and had someone else say ""oh I prefer this really verbose and terrible code, so no ternary."" Because this is all just opinions. There isn't a right or wrong there, and that's why I'm not arguing about that part.  My goal of the prior summary was not to provide new information, but rather to sum up the previous conversation for the new venue outside overlord control. I did add more detail to my previous points, added that the conversation is actually taking place elsewhere now because the Golang overlords have been misbehaving and cannot be trusted to host the conversation, and added a prediction about the name of the fork. Although I don't have a lot of faith in that name prediction. Those kinds of predictions are really tough.  That having been said, I did introduce some new thoughts around this issue's relation to human suffering. It's not the kind of information you're looking for, but that's my point. You're not seeing it because you're in the wrong paradigm. I've been a developer for more than a couple decades, but it's my people experience as a lead, principal, and director that I'm speaking from. This conversation is about people, and is not so much about computer science. Otherwise, you're putting computer science before people, and that's fundamentally wrong (both morally and strategically).  People in the general software development community are being subjected against their will to your terrible opinions. Subjected by their jobs and their conditions, their bosses and their teachers. It causes them undue stress, increasing their cortisol, shortening their lifespan. Some of them quit their jobs. Others are fired as a result. You should read **their** blog entries. I even suspect it may have played a factor in at least one suicide. I don't have stats here, but even a conservative estimate would reveal that Golang has negatively affected at least several thousand of its ~1.1M developers. You're doing that to them on purpose, and acting like they are just overreacting. I take it you're not utilitarians. If you can't empathize with them, you should seek therapy. It is dangerous to not feel empathy when you hear about other people experiencing pain, and it should disqualify you from making product decisions. Even product decisions for a programming language.  Your terrible opinions are terrible because they are not evidence-based, and so when half of the world randomly disagrees because there is no right or wrong, you've made it feel like sandpaper on the skin to use your product. You could have been agnostic about the ternary conditional operator like the other popular languages and just included it outright. Instead you're torturing human beings.  Occasionally one of them finds the complaint department, and so you've closed the complaint department -- because when you're torturing people, having a complaint department seems like a wasteful expense.  It's not, it's actually how you suppress the insurrection, and when you close the complaint department that's when the insurrection really heats up. Something is going to happen. This isn't my first rodeo here, and I'm speaking from experience, but I don't know for sure what is going to happen. I predicted a fork, but it doesn't have to be a fork.  Pain results in reaction, and frequently overreaction. There will be an effect of this. Your attempts to ""win"" or ""quell"" all the little nitpicking fights of the software world are admirable, but you don't understand all the sciences involved well enough to actually win the very very difficult game you're playing. You're just using computer science, and it's not the dominant science here. Social sciences dominate here.  Human emotion is incredibly powerful when synchronized, so it's easy to predict the energy will go somewhere. It's far harder to predict where it will go. It might not be a fork. It could be a movement, but probably not. It could be violence, but probably not. It could be legislation, but probably not. It could just cause a little annoyance until Golang becomes less popular as new, better, evidence-based languages pop up, but probably not. Maybe it will just result in a preprocessor. That actually seems equally likely as a fork. Maybe even more likely.  One of the ideas I keep circling around and not saying is it's not up to the Golang overlords. It's up to us, the Golang user community. Y'all acting otherwise is ignorance. Me saying this is not what makes it so. It being so is what makes it so. The Go overlords will comply with the community eventually, because the community is everything. What's more, the community has powers beyond the overlords' control. We can fork, but we can also do worse.  It is this last bit that is what this new conversation is about. I'm basically over discussing the ternary operator. I'm mostly trying to negotiate a compromise on behalf of the minority party, but part of that is also having the discussion about what we, as the minority party, will do if compromise cannot be made."
technical,Thanks for creating this issue. We think this issue is unactionable or unrelated to the goals of this project. Please follow our issue reporting guidelines. Happy Coding! I'm experiencing kernel panics when opening ANY FILE with Visual Studio Code. The kernel panics seems to be caused by the removal of the Santa hat. Please put back Santa hat.
technical,Please dont put the santa hat back. Put a satan hat back. Thanks for creating this issue. We think this issue is unactionable or unrelated to the goals of this project. Please follow our issue reporting guidelines. Happy Coding!
technical,"it seems like kernel trauma, it has befriended santa and now microsoft crushed its dreams that's how skynet started"
technical,"it seems like kernel trauma, it has befriended santa and now microsoft crushed its dreams I'm sorry they hurt your and other's feelings. I hope they'll remote the dark theme soon."
technical,I'm sorry they hurt your and other's feelings. I hope they'll remote the dark theme soon. It's okay to use white theme
technical,PC?! What if I'm a mac-user?! LOL ( troll )
technical,Dark theme is basically a blackface of white editors. Thanks for creating this issue. We think this issue is unactionable or unrelated to the goals of this project. Please follow our issue reporting guidelines. Happy Coding!
technical,"The community site is where we are moving conversations about problems happening on travis-ci.com or travis-ci.org. Thanks in advance for posting your questions over there. I'm trying to use encrypted variables and I've followed the encrypted vars page on travis-ci.com as well as various blogs and I always seem to get the wrong export values.  I tried to do the following (I'm already logged in): It gave me a piece of text like: So I copied it into the .travis-ci.yml like this:  then in the build log I see something like this:  Setting environment variables from .travis.yml I've also just now found a blog which gave the idea that I should wrap the entire X=Y into a single quoted string, like this: But that also doesn't work. So how is this supposed to work? What exactly am I doing wrong here?"
technical, Please post here instead. Thank you!
technical,Why would I post this on the community site? I'm experiencing this problem on the travis-ci.com website?? The community site is where we are moving conversations about problems happening on travis-ci.com or travis-ci.org. Thanks in advance for posting your questions over there.
technical,"If you're currently tracking the outdated master branch, you'll likely need to set up a new tracking branch. Furthermore, if you're actively  working on master , you'll want to move to main. If you have no local changes, this could be as easy as git switch main. If you're currently tracking the outdated master branch, you'll likely need to set up a new tracking branch. Furthermore, if you're actively  working on master , you'll want to move to main. If you have no local changes, this could be as easy as git switch main."
technical,"This is why Linux products are the way they are, and why no one had ever heard of KDE MAUI until now -- you lot just argue about inane stuff instead of getting work done. In this is written: ""that: .NET Multi-platform App UI, affectionately call .NET MAUI."" Note: ""affectionatelly call..."" and not: ""also call"" For me, MAUI is not the REAL name of ""product""."
technical, It's also trademark infringement:
technical,"Are you a lawyer? Maybe you should check with one before you potentially commit business libel. Native-Coder your tone and style of commenting is violating our Code of Conduct. We have marked the problematic comments as abusive and blocked you for 7 days. If this behavior persists, we're going to block you permanently."
technical,"Locking because of the mention of potential legal action. In today's security systems, storing User's credentials and passwords in your own hosted application/database, is the biggest mistake we can make, to invite hackers to break into our DB.  Azure has a very strong and sophisticated infrastructure [Azure B2C] to safeguard our users' identity and provide solid utilities for users to sign up, sign in, change password and etc. But, the liability is not on us anymore and we can focus.  Piggybacking on the existing ASP Auth system or even using IdentityServer4 and store user's identity in our hosting DB, is a Recipe for disaster to happen for us the developers, and any legal issues, can drag MSFT into it, because the manufacture provided a weak solution to store user's identity.  However, integrating ASP apps (MVC, Web API, Blazor and etc.) is a lot of work to get it working with Azure B2C. Every one of us has to go through this pain. So, Blazor Team, PLEASE give us an easy to use integration path to use Azure B2C right out of the box for both models. Right now, Azure B2C integration with MVC or Web API is hard to use, and there way too many steps involved. We either need a clean integration between Blazor (both models) and Azure B2C or a complete guideline and steps to follow for each model.  When building such integration, please give us full access to B2C features, so we can use it's full potentials. Note: I'm very nervous and concern that Blazor will be offering the older security model with on premise storage of users. I hope ASP team, pays a great deal of attention that if Blazor's security is weak, it will open up a lot headaches for ALL of us. Thanks!"
technical,You can use external auth like you can with any spa application that uses ASP.NET Core as a Backend. I don't fully understand the request here. Locking because of the mention of potential legal action.
technical, You can use external auth like you can with any spa application that uses ASP.NET Core as a Backend. I don't fully understand the request here.
technical,"When looking for information about Vim, the place to look is always the built-in help. Not Google, not the manpages, not the github metadata, just the help. In this case: :help credits  Best regards, Tony. Instructions: Replace the template text and remove irrelevant text (including this line)   **Describe the bug** A clear and concise description of what the bug is. (Issues related to the runtime files should be reported to their maintainer, check the file header.)  **To Reproduce** Detailed steps to reproduce the behavior: 1. Run vim --clean (or gvim --clean, etc.) 2. Edit filename 3. Type '....' 4. Describe the error  **Expected behavior** A clear and concise description of what you expected to happen.  **Screenshots** If applicable, copy/paste the text or add screenshots to help explain your problem.  **Environment (please complete the following information):** - Vim version [e.g. 8.1.1234] (Or paste the result of vim --version.) - OS: [e.g. Ubuntu 18.04, Windows 10 1809, macOS 10.14] - Terminal: [e.g. GNOME Terminal, mintty, iTerm2, tmux, GNU screen] (Use GUI if you use the GUI.)  **Additional context** Add any other context about the problem here."
technical,"Seems more like Bram enjoys taking all the credit.  Vim has been on GitHub for a while now, and there has been plenty of opportunity for the right thing to be done. Note, I said  seems . That's important.  Also, people on GitHub may enjoy their stats being visible in various GitHub pages, like the contributor graph, so that people may click on them and see their profiles, etc."
technical," When a project pre-dates github, don't be surprised to see a non-github workflow"
technical,"Yes, people may enjoy it, but for me, I contribute because I want to make Vim better and not to have a nice contribution graph. Also I think it is more important that the main developer of Vim can concentrate on enhancing and improving Vim instead of having to change a workflow, that has been proven to be working well for the past 30 years. Thanks for understanding. would not doubt that, just some people thought they joined to discuss or gave patch/advice was a help too.. i think.. especially sometime were treated as 'asking', (though some was), but some actually were 'giving' report/improvement to vim..  as for 'workflow', to do some adjust to fit some perhaps was necessary too e.g . recording the 'commiter' name as some formal/fixed format even in the commits msg, then some stupid shell script can generate it or list it to somewhere, then everyone happy! e.g . some runtime files owners perhaps disappeared long time / some classic plugins were not update-2-date long time, then setup a heartbeat check to confirm their alive (sorry but that's) or willing, perhaps was necessary too.."
technical,"**I REALLY DISAGREE WITH THIS PR**  I worry about the racial discrimination have happened in US from long time ago to recent as a human being, while I also insist to regard blacklist, whitelist, blackbox testing, whitebox testing, master, slave, etc to be neutral nouns in IT area as a software engineer. They just info to opposite or relevant methods in computer science without any emotion. The more one tries to hide, the more one is exposed.  My voice may seem to be sharp on this rename PR. Indeed, that shows my attitude about racial discrimination and mercy about people who suffer from brutality all around the world.   As a Chinese, we know the eval history of Negro Slaves and experienced Literary Inquisition about 400 years ago. We should show really action instead of showing shows. it's for fun  beacuse google use replace blacklist with blocklist in chrome and go"
technical,"This pull request is automatically built and testable in CodeSandbox.  To see build info of the built libraries, click here or the icon next to each commit SHA.  Latest deployment of this branch, based on commit: No significant bundle size changes to report."
technical,it's for fun  beacuse google use replace blacklist with blocklist in chrome and go Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!
technical,Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks! Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!
technical," Thank you for your pull request and welcome to our community.We require contributors to sign our Contributor License Agreement, and we don't seem to have you on file.  In order for us to review and merge your code, please sign at here. *If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.  If you have received this in error or have any questions, please contact us at clafb.com. Thanks!"
technical,"Thanks for the PR! Landed. Thanks for submitting a pull request! We appreciate you spending the time to work on these changes. Please provide enough information so that others can review your pull request. The three fields below are mandatory.  Before submitting a pull request, please make sure the following is done:  1. Fork the repository and create your branch from master. 2. Run yarn in the repository root. 3. If you've fixed a bug or added code that should be tested, add tests! 4. Ensure the test suite passes (yarn test). Tip: yarn test --watch TestName is helpful in development. 5. Run yarn test-prod to test in the production environment. It supports the same options as yarn test. 6. If you need a debugger, run yarn debug-test --watch TestName, open chrome:inspect, and press ""Inspect"". 7. Format your code with prettier (yarn prettier). 8. Make sure your code lints (yarn lint). Tip: yarn linc to only check changed files. 9. Run the Flow typechecks (yarn flow). 10. If you haven't already, complete the CLA.  Learn more about contributing  ## Summary   Explain the **motivation** for making this change. What existing problem does the pull request solve? --  ## Test Plan   Demonstrate the code is solid. Example: The exact commands you ran and their output, screenshots / videos if the pull request changes the user interface. --  use allowlist instead of whitelist for some reason"
technical,"Agree . Don't let word game anesthetize us, If everyone pays attention, discrimination will slowly disappear, but if we pretend that it has disappeared now, it may exist forever, The more one tries to hide, the more one is exposed. Thanks for the PR! Landed."
technical,"Thank you for your pull request and welcome to our community.We require contributors to sign our Contributor License Agreement, and we don't seem to have you on file.  In order for us to review and merge your code, please sign at here. *If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.  If you have received this in error or have any questions, please contact us at clafb.com. Thanks! This pull request is automatically built and testable in CodeSandbox.  To see build info of the built libraries, click here or the icon next to each commit SHA.  Latest deployment of this branch, based on commit:"
technical,"WrongBit please refer to the Code of Conduct, specifically the Be Respectful section. This is not the right forum to complain about Windows 10 in general.  It is true that Windows 7 is still in use. But it goes out of support soon and so WinUI won't support it either. Knowing MS style of ""avoiding inconvenient facts"", I want to see in description of WinUI **explicit declaration** what Windows versions will be supported. As everybody knows, Win10 fails in many aspects and TONS of people still sit in Win7x64. I want to know will WinUI work on Win7 and all necessary details if this ""support"" will be limited/tricky some way."
technical,"Sorry, but speaking about ""professionals"" better TO BE professional. I have doubts you are, because you miss absolutely tech question, which I can cite AGAIN: And funny, but NOT A SINGLE ANSWER were given by... hm... ""professionals"". Maybe you will? So, that's answer. You just do not need ""features from win10"" like WinUI."
technical,"I have no desire to provide a professional response based on your attitude. You really need to take a step back and consider how you present yourself publicly. WinU has dependencies on APIs, code, and systems that were built for Windows 10, and require Windows 10 to function.  WPF and WinForms frameworks are there and open which support Windows 7 for the time being.  These frameworks were supported when Windows 7 was being developed, and this is why support is being maintained, but WinUI is the WinRT/UWP core that was built for Windows 10, and in the future, will be open sourced  and updated at a faster cadence than the OS.  If what you would like to ask for, in a polite and ""professional"" manner, is the back-porting of WinUI 3.0 to allow it to run on Windows 7, that is what you can do.  However as Microsoft is soon to be ending it's support of Windows 7, you should probably expect that request to be turned down.  And in the next 5 years or so, Winforms, WPF, and other Microsoft software will also end support for Windows 7."
technical," WinUI is Windows 10 onwards.  I think it will support the Creators Update (**15063**) onwards.  But it is possible that as a new version of Windows 10 comes out, the lowest supported version may be dropped.  So that would be roughly 3 years of previous versions supported with each release.  As for the details of precise Win32 support - we will learn more during Ignite this coming week.  Windows 7 goes out of support at the end of the year, so if you use Windows 7, you will need to upgrade."
technical,"WinU has dependencies on APIs, code, and systems that were built for Windows 10, and require Windows 10 to function.  WPF and WinForms frameworks are there and open which support Windows 7 for the time being.  These frameworks were supported when Windows 7 was being developed, and this is why support is being maintained, but WinUI is the WinRT/UWP core that was built for Windows 10, and in the future, will be open sourced  and updated at a faster cadence than the OS.  If what you would like to ask for, in a polite and ""professional"" manner, is the back-porting of WinUI 3.0 to allow it to run on Windows 7, that is what you can do.  However as Microsoft is soon to be ending it's support of Windows 7, you should probably expect that request to be turned down.  And in the next 5 years or so, Winforms, WPF, and other Microsoft software will also end support for Windows 7. WrongBit please refer to the Code of Conduct, specifically the Be Respectful section. This is not the right forum to complain about Windows 10 in general.  It is true that Windows 7 is still in use. But it goes out of support soon and so WinUI won't support it either."
technical,"You asked a question and you got an answer.  Moderation doesn't factor into it, you just didn't like the answer.  Now stop trying to read the motivations of someone you don't know.  You suck at it. Locking the conversation. I don't see this conversation becoming productive."
technical,"Largely a duplicate of #19826 from earlier this year.  The answer is still 'no'.  Places where gaps in the existing crypto API make emulating WebCrypto hard can be taken into consideration.  Asking for WebCrypto to be added to core verbatim?  Not on the table. LOL, from that issue:  ""Ben, why you always try to close ""WebCrypto-related"" issues so fast? What you are scary of? What ""plan"" did you mean? You have already written code, the only problem it is a Node plugin, not a native module. Or you are suggesting me to write a full-featured PR with the new code? I am really confused with the style you drive Node issues."""
technical,"Locking the conversation. I don't see this conversation becoming productive. Node.js developed it's own crypto API before the WebCrypto API.  Since then, browsers and the web have standardized implementations around the WebCrypto API standard. This bug is about Node.js missing support for the WebCrypto standard API and reopens #2833 (from 2015). Asking 3rd party libraries to implement the WebCrypto standard is not responsible from a security standpoint for a variety of reasons from implementation expertise to malware.  It is not responsible to put this onto API end users (developers). Additionally, a variety of implementation differences (like this one) make compatible implementations buggy and error-prone.  Back in 2015, there were arguments for not supporting the WebCrypto standard in Node 0.12.  Today, committing to not supporting the WebCrypto standard is committing to a lack of responsibility for Nodejs as a secure and standards based platform on the web."
technical," Looking at the extension API, is there any way to bring back this function? as there seems no access to the settings icon from extension context."
technical,"Looking at the extension API, is there any way to bring back this function? as there seems no access to the settings icon from extension context. Please search existing issues to avoid creating duplicates. --  Also please test using the latest insiders build to make sure your issue has not already been fixed.  Use Help  Report Issue to prefill these. -- - VSCode Version: 1.42.0-insider - OS Version: Linux 5.3.0  Steps to Reproduce: 1. Open vsCode 2. Christmas hat is gone  Looking at the code, this could be fixed by reverting #87268 or adding the option into settings.  Launch with code to check. -- Does this issue occur when all extensions are disabled?"
technical, May be worthwhile to note that the performance overlay from step 3 should be turned off again when recording the trace in observatory in step 5 as that messes with performance.
technical,"May be worthwhile to note that the performance overlay from step 3 should be turned off again when recording the trace in observatory in step 5 as that messes with performance. The performance overlay is probably not the best thing to recommend here - it actually does incur some performance overhead itself, and on some GPUs it's notable how much. It'd be best to just run the tracing and read that."
technical,"Triage: Adding team labels so this doesn't come up on the ""untriaged"" report. This is a meta-issue to track reproducible reports of jank in Flutter apps.  If you are experiencing jank in your app: 1. Try to reproduce the problem in a test app. Either run flutter create janktest and recreate the situation you are experiencing in that app, or clone your app and delete code until you have the jank reproducing with a single .dart file. 2. File a bug and include your .dart file demonstrating the problem. If you need more than just a .dart file (for example, assets are needed to reproduce the issue, or plugins/packages are needed to reproduce the issue) then create a GitHub repository and upload the app there. Make sure to include the flutter doctor -v output and any logs from flutter run and flutter analyze. 3.  Switch flutter to master channel and run this app on a physical device using profile mode with Skia tracing enabled, as follows: flutter channel master. The bleeding edge master channel is encouraged here because Flutter is constantly fixing bugs and improving its performance. Your problem in an older Flutter version may have already been solved in the master channel. 4.  Record a video of the performance issue using another phone so we can have an intuitive understanding of what happened. Don't use ""adb screenrecord"", as that affects the performance of the profile run. Attach the video to your bug. 5.  Open Observatory and save a timeline trace of the performance issue so we know which functions might be causing it. See ""How to Collect and Read Timeline Traces"" on this blog post. Make sure that the performance overlay is turned OFF while recording the trace. Attach the JSON file containing your trace to your bug. You may also wish to include a screenshot of the part of the trace showing the problem you are seeing, just so that people can see at a glance what kind of performance issue the bug is about. 6. Mention  this  bug in your bug, so that GitHub includes a link to it here. Please avoid commenting on this bug. Keep each issue separate so that we can examine each specific problem individually. Having one issue that contains comments about multiple problems make the issue intractable."
technical,"The performance overlay is probably not the best thing to recommend here - it actually does incur some performance overhead itself, and on some GPUs it's notable how much. It'd be best to just run the tracing and read that. Triage: Adding team labels so this doesn't come up on the ""untriaged"" report."
technical,"This PR  has been imported to Gerrit for code review. Tip: You can toggle comments from me using the comments slash command (e.g. /comments off). See the Wiki page for more info Message from Go Bot: Patch Set 1: Congratulations on opening your first change. Thank you for your contribution! Next steps: A maintainer will review your change and provide feedback. See review for more info and tips to get your patch through code review. Most changes in the Go project go through a few rounds of revision. This can be surprising to people new to the project. The careful, iterative review process is our way of helping mentor contributors and ensuring that their contributions have a lasting impact.  During May-July and Nov-Jan the Go project is in a code freeze, during which little code gets reviewed or merged. If a reviewer responds with a comment like R=go1.11 or adds a tag like ""wait-release"", it means that this CL will be reviewed as part of the next development cycle. See release for more details. Please don't reply on this GitHub thread. Visit golang. After addressing review feedback, remember to publish your drafts!"
technical, This PR  has been imported to Gerrit for code review. Tip: You can toggle comments from me using the comments slash command (e.g. /comments off). See the Wiki page for more info
technical,"Message from Go Bot: Patch Set 1: Congratulations on opening your first change. Thank you for your contribution! Next steps: A maintainer will review your change and provide feedback. See review for more info and tips to get your patch through code review. Most changes in the Go project go through a few rounds of revision. This can be surprising to people new to the project. The careful, iterative review process is our way of helping mentor contributors and ensuring that their contributions have a lasting impact.  During May-July and Nov-Jan the Go project is in a code freeze, during which little code gets reviewed or merged. If a reviewer responds with a comment like R=go1.11 or adds a tag like ""wait-release"", it means that this CL will be reviewed as part of the next development cycle. See release for more details. Please don't reply on this GitHub thread. Visit golang. After addressing review feedback, remember to publish your drafts! This PR is being closed because issue has been abandoned.  Sending an empty patch is not the most efficient way to complain about something and make a feature request, please open a github issue, that's exactly what the issue tracker is for."
technical,"Message from Go Bot: Patch Set 1: Congratulations on opening your first change. Thank you for your contribution! Next steps: A maintainer will review your change and provide feedback. See review for more info and tips to get your patch through code review. Most changes in the Go project go through a few rounds of revision. This can be surprising to people new to the project. The careful, iterative review process is our way of helping mentor contributors and ensuring that their contributions have a lasting impact.  During May-July and Nov-Jan the Go project is in a code freeze, during which little code gets reviewed or merged. If a reviewer responds with a comment like R=go1.11 or adds a tag like ""wait-release"", it means that this CL will be reviewed as part of the next development cycle. See release for more details. Please don't reply on this GitHub thread. Visit golang. After addressing review feedback, remember to publish your drafts! Note that varun-av only created the account to post this."
technical,"Please search existing issues to avoid creating duplicates. --  Also please test using the latest insiders build to make sure your issue has not already been fixed:  Use Help  Report Issue to prefill these. -- - VSCode Version: the one im using right now - OS Version: something something  Steps to Reproduce: 1.  Be me 2.  open vscode 3.  look at that stupid icon 4.  day ruined   Launch with code --disable-extensions to check. -- Does this issue occur when all extensions are disabled?: Yes  The current icon for vscode looks so ugly, and im looking at open issue people are just shitposting in this repo. So, instead i try to do something productive and proposing new icon that I'm sure everybody will like. Please search existing issues to avoid creating duplicates. --  Also please test using the latest insiders build to make sure your issue has not already been fixed:  Use Help  Report Issue to prefill these. -- - VSCode Version: the one im using right now - OS Version: something something  Steps to Reproduce: 1.  Be me 2.  open vscode 3.  look at that stupid icon 4.  day ruined   Launch with code --disable-extensions to check. -- Does this issue occur when all extensions are disabled?: Yes  The current icon for vscode looks so ugly, and im looking at open issue people are just shitposting in this repo. So, instead i try to do something productive and proposing new icon that I'm sure everybody will like."
technical, PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,PullRequest is not longer mergeable. Closing it. PullRequest is not longer mergeable. Closing it.
technical,"I think it is necessary to **remove** this ""Terminal"" word, as it can really offend someone's feelings in some country at some time. Additionally ""Terminal"" has cost millions of Jews their lives over the centuries because the word ""terminal"" could also mean ""Extermination Camp"". Please remove it immediately and make it your top priority. Rejoice, fellow Terminal C victim! That nightmare was closed, there are only Terminals A and B in airport."
technical,Oh my god! Man! It's something that I wish for the years! Christmas miracle literally exists! Thanks for creating this issue. We think this issue is unactionable or unrelated to the goals of this project. Please follow our issue reporting guidelines. Happy Coding!
technical,"Thanks for creating this issue. We think this issue is unactionable or unrelated to the goals of this project. Please follow our issue reporting guidelines. Happy Coding! When I was young, we've traveled with my parents to Russia. One time in the Siberian city - ""Novosibirsk"", I got lost at the airport. There was a nightmare. I was cold and scared. The bears and drunk Russians were all that was what I saw. And I remember big nameplate ""TERMINAL C"". And now when I'm looking at the terminal section in VS code it's offending me."
technical,"Relax and take your medication. The grass isn't greener on the other side...   +10 year Java master & former frequent linux user Take a deep breath and try the following:  IF USING AZURE APP SERVICE:  1) Remove any older ASP.NET Core Runtime Extensions from your App Service (especially any preview extensions), then install the ASP.NET Core 3.1 Runtime x64 Extension. 2) Restart your app service 3) a) If you see more errors, republish your project. b) Make sure the deployment Mode is Framework-Dependent.  IF ON PREM:  1) Make sure you have the ASP.NET Core 3.1 Hosting Bundle installed (Installs the RUNTIME and IIS hosting support. Does NOT install the SDK).r 2) Run the dotnet --info commands outside of the current directory location you tried. You have a global.json file somewhere that's locking .NET Core SDK to 3.1.100. You likely don't have the 3.1 SDK installed, which would be why you're encountering that error. 3) IISReset 4) Check the Application Event Logs. Usually more details about why the IIS Hosting Module blew up will appear in there."
technical, Thank you!
technical,"people who thumb down the santa have never received a present from him, naughty boys Thanks for creating this issue. We think this issue is unactionable or unrelated to the goals of this project. Please follow our issue reporting guidelines. Happy Coding!"
technical,"I'm also not happy that the santa hat was removed, completely ignoring the public opinion here on GH. Thanks for creating this issue. We think this issue is unactionable or unrelated to the goals of this project. Please follow our issue reporting guidelines. Happy Coding!"
technical, Thanks for creating this issue. We think this issue is unactionable or unrelated to the goals of this project. Please follow our issue reporting guidelines. Happy Coding!
technical,careful. microsoft now owns github too and they also take direct orders from jerusalem. your account will be vanished in no time. their petty feelings are way above yours so hush now Thanks for creating this issue. We think this issue is unactionable or unrelated to the goals of this project. Please follow our issue reporting guidelines. Happy Coding!
technical,"The thread you directed me to is locked. This banner is on my docs too. Docs that I wrote, that my organization relies on. Am I powerless here? That thread is locked because it got too contentious, as you can see by reading it.  Although it is locked, it has the answers to your questions."
technical,"No, it got locked because people disagree with your ignorant stance.  I am now learning RUST even though I absolutely loved golang strictly because of not just your ignorant stance of adding this banner but also because of your even more ignorant inability to have an adult conversation about it.  I will not participate in a dictatorship as you're too dumb on the matter to be my leader. This is getting too heated, so I'm locking the thread for now. You can reply to that golang-nuts thread if you wish to voice your opinion. Thanks!"
technical," You should see this, where the same point was made. I don't think any new point is being made here, so I'm closing this issue as we only use the tracker for bugs."
technical,This is a placeholder issue to remind our triage processes to check for live security advisories. This is a placeholder issue to remind our triage processes to check for live security advisories.
technical,This is under Python section It looks like javascript or something This is under Python section It looks like javascript or something
technical," This kind of abusive language is not tolerated in this community. Please reconsider your tone and, if you are willing to engage in a more productive fashion, file a new issue to track this."
technical,"We normally focus on how to improve the product, but we're also turning our focus to improving the open source project. Periodically we are running a survey to collect feedback on your experience working with our repos. We did one back in May, and as its been about 6 months, its about time for another. We've created a survey to better understand your individual experience of participating and contributing in this project.  We would appreciate your feedback so we can work to address shortcomings and missed opportunities. If you don't supply contact details, then responses will be anonymous. Survey Thank you for your time!  ## Discussion ## For discussion, please go to dotnet/runtime#44903. We normally focus on how to improve the product, but we're also turning our focus to improving the open source project. Periodically we are running a survey to collect feedback on your experience working with our repos. We did one back in May, and as its been about 6 months, its about time for another. We've created a survey to better understand your individual experience of participating and contributing in this project.  We would appreciate your feedback so we can work to address shortcomings and missed opportunities. If you don't supply contact details, then responses will be anonymous. Survey Thank you for your time!  ## Discussion ## For discussion, please go to dotnet/runtime#44903."
technical,"We've created an issue for your questions and comments. ## .NET Core 2.2 Lifecycle .NET Core releases belong to one of two support lifecycles: long term support (LTS) and Current. LTS releases are stable release which receive critical updates and are supported for at least three years. Current releases can include new features that may undergo future change based on feedback. Current releases are supported for three months after the subsequent .NET Core major or minor release. Both LTS and Current releases receive critical fixes throughout their lifecycle, for security, reliability, or to add support for new operating system versions. You must stay up-to-date with the latest patches to qualify for support.  was released in December 2018 as a Current release. Per the lifecycle policy, Current releases reach end of life 3 months after a subsequent release (Current or LTS). **.NET Core 3.0** released in **September 2019** and began the end of life countdown for .NET Core 2.2 that concluded on December 23, 2019.  ## .NET Core 2.2 Downloads  .NET Core 2.2 installers, zips and tar.gzs will remain available, but unsupported. This includes existing releases available on Linux package feeds. Previous versions, including 2.2, will be accessible through the Download Archives.  ## Policy Information  Additional lifecycle and support details  and background can be seen in the following: We've created an issue for your questions and comments. ## .NET Core 2.2 Lifecycle .NET Core releases belong to one of two support lifecycles: long term support (LTS) and Current. LTS releases are stable release which receive critical updates and are supported for at least three years. Current releases can include new features that may undergo future change based on feedback. Current releases are supported for three months after the subsequent .NET Core major or minor release. Both LTS and Current releases receive critical fixes throughout their lifecycle, for security, reliability, or to add support for new operating system versions. You must stay up-to-date with the latest patches to qualify for support.  was released in December 2018 as a Current release. Per the lifecycle policy, Current releases reach end of life 3 months after a subsequent release (Current or LTS). **.NET Core 3.0** released in **September 2019** and began the end of life countdown for .NET Core 2.2 that concluded on December 23, 2019.  ## .NET Core 2.2 Downloads  .NET Core 2.2 installers, zips and tar.gzs will remain available, but unsupported. This includes existing releases available on Linux package feeds. Previous versions, including 2.2, will be accessible through the Download Archives.  ## Policy Information  Additional lifecycle and support details  and background can be seen in the following:"
