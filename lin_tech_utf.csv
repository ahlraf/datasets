quotation,code
"When controller is working as Device then host part of controller is held in reset and vice versa, so driver has restricted access to registers. ",technical
,
Switching role form user space will limited driver only to selected mode. Only for USB DR MODE OTG driver should base on ID pin. That's my intension.,technical
,
"Yes, I know and I agree with you. I have plan to replace this with trace points. But I need some time for this. Currently I'm focusing on testing. Probably  I will change It by the end of this month, so it should be in RFC PATCH v3. ",technical
,
User space setting should override ID if it was OTG mode. At least this is how it is for dwc3. That way it is useful for testing role swap.,technical
,
I think you should send it to ASoC maintainer and list for applying. Shawn,technical
,
" It's still quite new for me to submit patch, but if this patch should be sent to ASoC maintainer maybe there is a line missing in the MAINTAINERS file no ? SOUNDM: ",technical
,
"Mark, comment? ",technical
,
"Adding a file pattern for this seems sensible, yes.",technical
,
"OK, I will send a v2 for the IMX6 Audmux bindings with ASoC List and Maintainers. ",technical
,
"This is v2 patch. I'm sorry that I forgot to add the word ""v2"" to the subject. ",technical
,
"AFAICS this will break e.g. x86 which can have both ZONE DMA andZONE DMA32, and now you would make kmalloc(  GFP DMA) return objects from ZONE DMA32 instead of   ZONE DMA, which can break something. Also I'm probably missing the point of this all. In patch 3 you use  get dma32 pages() thus   get free pages(  GFP DMA32), which uses alloc pages, thus the page allocator directly, and there's no slab caches involved. It makes little sense to involve slab for page table allocations anyway, as those tend to be aligned to a page size (or high-order page size). So what am I missing?",technical
,
"Oh, I was not aware that both ZONE DMA and ZONE DMA32 can be defined at the same time. I guess the test should be inverted, something like this (can be simplified...) 32 pages fixes level 1 page allocations in the patch 3.This change fixes level 2 page allocations(kmem cache zalloc(data->l2 tables, gfp  GFP DMA)), by transparently remapping GFP DMA to an underlying ZONE DMA32.The alternative would be to create a new SLAB CACHE DMA32 when CONFIG ZONE DMA32 is defined, but then I'm concerned that the callers would need to choose between the 2 (GFP DMA or GFP DMA32...), and also need to use some ifdefs (but maybe that's not a valid concern?).Level 2 tables are ARM V7S TABLE SIZE(2) => 1kb, so we'd waste 3kb if we allocated a full page.",technical
,
"Oh, I see. Well, I think indeed the most transparent would be to support SLAB CACHE DMA32. The callers of kmem cache zalloc() would then need not add anything special to gfp, as that's stored internally upon kmem cache create(). Of course SLAB BUG MASK would no longer have to treat   GFP DMA32 as unexpected. It would be unexpected when passed to kmalloc() which doesn't have special dma32 caches, but for a cache explicitly created to allocate from ZONE DMA32, I don't see why not. I'm somewhat surprised that there wouldn't be a need for this earlier, so maybe I'm still missing something.",technical
,
Do you have any numbers for how much difference this change makes with various different workloads?,technical
,
"Yep, I got some non KVM numbers, Formulas: Percentage - (pages sharing - pages shared)/pages unshared Memory saved - (pages sharing - pages shared)*4/1024 MiB- My working laptop: 5% - ~100 MiB saved ~2GiB used  Many different chrome based apps + KDE- K8s test VM:  40% - ~160 MiB saved ~920MiB used  With some small running docker images- Ceph test VM: 20% - ~60MiB saved ~600MiB used  With ceph mon, osd. Develop cluster servers:- K8s server backend: 72%, ~5800 MiB saved ~35.7 GiB used  (With backend apps: C, java, go & etc server apps)- K8s server processing: 55%, ~2600 MiB saved ~28 GiB used  (90% of load many instance of one CPU intensive application)- Ceph node: 2%, ~190 MiB saved ~11.7 GiB used  (OSD only) So numbers, as always depends on the load. Thanks!- - -P.S.On recent kernels (4.19) i see BUG ON message, that ksmd scheduled while in critical section/atomic context, not sure how to properly fix that. (If i understood correctly, i can use preempt disable(); but that looks more like hack, not a fix). Any feedback are welcome.",technical
,
"Isn't it more straight-forward to use ""rcu-rscs^-1"" other than""rcu-rscsi"" in the definition of ""rcu-fence"", is it? The introduction of ""rcu-rscsi"" makes sense in the first patch, but with this refactoring, I think it's better we just don't use it.",technical
,
It's a matter of personal preference.  I prefer to store the inverse relation in a separate variable rather than recomputing it multiple times.  (Maybe OCaml is smart enough to recognize when a value has already been computed and avoid computing it again; I don't know.) In the end this probably doesn't make much difference.,technical
,
Isn't it too early to do this change? Can't we wait until we have a SoC that actually embeds this IP?I'd prefer to have a dw/ subdir where you'd place all dw files. Do we really have to create one module for the core and one per SoC? Can't we have everything in the same .ko?,technical
,
I'm trying to turn it more flexible so the other can reuse the code. Sure. I will change to this:../dwc  -core.h -master.c  -platdrv.cso the user doesn't need to write dw-i3c.. several times. The folder name is the same as for other subsystem (e.g. PCI). What do you think? This will help the introduction of new modules. The design in my mind is to have:-core.h-common.c-master.c-slave.c... I'm not sure if make sense to change core.h to common.h. Thanks for your feedback.,technical
,
"Looking at the separation you've done here, I don't see why you need it. All the resources you request are generic, so why not just adding a new compat in the of match table?Just realized the driver is named dw-i3c-master, while the cadence driver is named i3c-master-cdns.c. I'll send a patch to make that consistent and follow the initial naming scheme: i3c-master-<ipname>.c.",technical
,
"I understand your point. I'm just following what it's done in others Synopsys drivers and what I expect is that in the future we will have the same for the I3C. Some of the current generic functions might be override according with SoC requirements (e.g. i2c-designware, pcie-designware). for now what do you prefer? As I shared with you in previous email, the structure that I have in mind is this one:- core.h (or common.h, any though?)- common.c- master.c- slave.cso for me doesn't make sense to have for instance: i3c-master-dw-slave.c But seeing what is already in the kernel I wasn't coherent and it should be named to i3c-designware-master.cor follow this  This topic rise another one related with the master folder. I understand that now the subsystem doesn't have slave support but the name is limited. Isn't better to have something like controller or busses? What do you have in mind for the slave?",technical
,
"I prefer that we keep the driver as is until we actually need to split things up. If you have several files and they're all placed in a dw/ subdir, then I agree, prefixing everything with i3c-master- is useless, as you'll have to define a custom rule to create the i3c-master-dw.ko object. When there's a single source file, and this source file is directly used to create a .ko, we need this prefix, otherwise we would have dw.ko, and this would basically conflict with any other designware driver that does not have a proper prefix. Actually it's i3c-master-designware.c (or i3c-master-dw.c) if we follow what's been done for the cadence driver. And I agree with Linus on this, except that does not apply to single source file drivers.drivers/i3c/slave/... for slave drivers and drivers/i3c/slave.c for the framework, just like we have drivers/i3c/master/ for master controller drivers and drivers/i3c/master.c.",technical
,
This is already done and will benefit everyone: - for me is better do it now than the secondary master and slave development.  - for the others it will easy the SoC integration avoiding duplicated work and doing things from scratch. I was referring to what was made in other modules and should be applied here too. I have to disagree here. I don't see any place on the kernel with.../master/ and ../slave/ folders and it is very likely that both rules will have some common code. With this structure the user will have the code spread in /master and/slave folders...I would like you consider to change the folder name and the names rules to something like in i2c. Maybe someone else can give his opinion too. ,technical
,
"Sorry, I don't get that one. What would be duplicated? You want to support a new SoC, just add a new entry in the of match table and you're done. When you need to add SoC/integration specific stuff, create a struct and attach a different instance per-compatible so that each SoC can have its own configuration(or even init sequence if needed). That's how we do for pretty much all IPs out there, why should design ware ones be different? This is a subsystem decision. I don't mind changing the naming scheme, though I don't see why yours is better than the one I initially proposed. In any case, what's important here is to keep driver names consistent. I see at least one that uses this model: the USB framework (drivers/usb/gadget/ for device controllers and drivers/usb/host/ for host controllers). Given that I3C is closer to USB than I2C I initially decided to keep this separation. Maybe I'm wrong, but I'd like to understand why you think it's not appropriate. Not sure who you call ""user"" here, but yes,  master controller and slave controller drivers would be placed in different dirs. Why? And more importantly, why is this coming up now? You've been reviewing the framework since the beginning, and never complained about the subdirs/files organization so far. I'm okay changing it, but I want to understand why the proposed separation is not good.",technical
,
"To be more specific, I'd like a real example that shows why the separation is needed.",technical
,
"I finally understand what this separation is all about: supporting both PCI and platform devices. I guess I've been distracted by this sentence: ""This patch will allow SOC integrators to add their code specific to DesignWare I3C IP.""which for me meant each SoC would have its own platform driver.In any case, I think this is a bit premature do this separation, unless you already know about one integrator planning to expose this IP over PCI.",technical
,
"I already share my plan with you. See the structure above. You can say there some features from USB in I3C but cannot compare USBvs I3C since they are in different championships. The aim of I3C is to fill the gaps discovery on I2C over the years but still keeping its simplicity not to go to the complexity of USB. I'm not sure but I think that a controller cannot change between gadget to host in USB in runtime. Even so, this kind of behavior is more likely to have in an I3C bus. Sorry for that and don't take me wrong... maybe I should rise this question early but this only came up now when I started splitting and thinking where to put what is for master for slave, what is common and the thing of putting everything of controller in a folder. Taking the USB as example do you prefer a dwc folder on i3c root? I already tell you my use case and as I said maybe someone can advise :)",technical
,
"My point is, I don't get the relationship between your patch and secondary-master/slave-mode support.I maintain that functionally, I3C is closer to USB than I2C. That does not mean that it's competing with USB performance-wise, it just means  that the SW stack is likely to resemble the USB one (probably a bit simpler, at least at the beginning). Look at the bus discovery mechanism, the notion of DCR (close to the concept of USB class), or the fact that each dev has a unique manufacturer+PID pair (which resemble the product/vendor ID concept) that allows us to easily bind a dev to a driver without requiring a static description.That's called USB OTG. Okay, to be fair, it's not exactly the same, and the mastership handover in I3C is probably more complex than what we have with USB OTG (I'm not a USB expert, so I might be wrong here). Maybe. So you have such a dual-role controller? I mean, Cadence IP has a dummy GPIO mode in its Master IP when is operating in slave mode (secondary master, or main master after it's released the bus), but I'm not sure this was designed for anything else but testing. What I call a slave controller would be something that lets you reply to SDR/DDR transactions or fill a generic regmap that would be exposed to other masters on the bus. This way we could implement generic slave drivers in Linux (the same way we have gadget drivers). Anything else is likely to be to specific to be exposed as a generic framework. Hm, not sure I like this idea either. So I see 2 options:1/ put all controller drivers (both master and slave ones) in a common directory (drivers/i3c/controllers) as you suggest, and prefix them   correctly  place them in separate directories I'm fine either way. I think I understand your concerns now, but only because you started to mention a few things that were not clearly stated before (at least, I didn't understand it this way), like the fact that your controller (and probably others too) supports dual-role, or the fact that you plan to expose your IP through the PCI bus.",technical
,
"Sorry for the delayed response. I think this should be discuss in another thread. Do you agree? Yes, that was what I trying to tell you. For me this might be the best option. I would like to avoid having dual role i3c driver in a master folder. I don't disagree, and for those that have more than one file they should be in a folder, right? What prefix do you have in mind for those files inside a folder? No, it's not. But as you can see to slipt the driver in parts this subject has some relevance. ",technical
,
Ok no problem. We can delay this for PCI and other rules support.,technical
,
It's quite unusual for a backlight device to have a trivial binding. The driver supports fairly extensive parametrization via structlm3530a platform data. It is really the case that none of these properties should ever be set via DT?,technical
,
Similar to my reply to the DT bindings: I would have expected there to be code to handle DT properties here. ,technical
,
"I initially assumed that we would let user space configure these values once the system has booted, but you are right that these should be available in device tree. The driver has two different LED banks that can be configured independently. How do you feel about having a single property in device tree populate the initial values for both banks? I propose that we could use the property default-brightness-level for leda init brtand ledb init brt in struct lm3630a platform data. The max-brightness property can populate leda max brt and ledb max brt. I need to look at other bindings this weekend to see if there are any standard properties that I can use for leda ctrl/ledb ctrl, pwm ctrl,and pwm period. ",technical
,
I agree and I'm not going to use a trivial binding for v2. See below for some questions that I have from my last email.,technical
,
"This is a standard 8-bit white LED driver.  It looks like Brian is just adding DT support to load the driver. I would expect that the bindings need to be updated to be able to register one string or another using the led-sources property.  There are a couple of examples in the kernel and a couple of them in patch form. This driver and binding need to be updated to the latest spec, as you pointed out with child nodes. And Jacek has some new proposed bindings for the LED class so we may want to adopt those standards here as well.  This is what I am waiting on for agreement so I can update my patch set.",technical
,
"This member is only written to, but never used. This function is only used once. Maybe drop the function and put the if to the caller. This error message looks wrong, several others, too. return PTR ERR(clk)?sun8i pwm is shared for all 8 PWMs, right? So if you assign mux-1 here for the second mux, how does this influence the first PWM?mux-0 might already be enabled, it is then never disabled. This looks wrong. If val is > 1 there shouldn't be a reason to abort? I'd degrade this to dev dbg. Noting the underlying formula for the calculation and the bit width for the related register fields above would be good. Why ""<< 0""?sun8i pwm config writes the registers that are relevant for period and duty cycle. When do these values take effect? If it's already here, switching the polarity below might introduce a glitch. Please document how the hardware behaves when being disabled. (Does itdrive a 0? Does it drive a 1 when inverted? Or is the output high-z?) This looks strange to me. While syntactically equivalent it is more usual to write this as	if (val & PWM ACT STA)When the PWM should be enabled, you also set the CLK GATING bit. Should this better be checked for in .get state, too?.data doesn't need to be specified. This prevents a match by driver-name, right? Other than that match is only used to assign pwm->data below to NULL. You might want to handle pwm->clk == ERR PTR(-EPROBE DEFER) without falling back to mux-1 and without printing an error. This is equivalent to	if (ret)because &pwm->chip.npwm is only modified if of property read u32returns 0 and the variable holds a 0 before.dev dbg? If you do this earlier (typically after the allocation succeeded) you can simplify the last few lines to:	ret = pwmchip add(&pwm->chip);	if (ret < 0)		dev err(&pdev->dev, ...);	return ret; This is at least unusual (and maybe broken). Please call pwmchip removebefore clk disable unprepare. I think the space in the alias must be dropped. Giving that the driver doesn't bind by driver-name I suggest to drop this completely.",technical
,
"HI! That should be a separate patch.(also, your patch series don't seem to have the threading properly configured, you might want to fix that.) sun8i here (and in the rest of the driver) is too vague. There's already plenty of SoCs part of the sun8i family that are supported by the other driver. sun8i-r40 would be a better fit (and there's no need to mention all the rebranding that allwinner has done with the R40, just use R40).This is pretty much reimplementing the clock framework. I guess you'd be better off just modeling this clock as a clock registered in the framework. It will take care by itself of the combination of muxing and rate, and making sure the parent clocks are properly enabled when needed. Do you really need that field if you leave it NULL?",technical
,
"Hello, Given that the documentation is publically available, I suggest to add a link to it in a comment here. I think this is the case after taking a look into the reference manual. There are two 16 bit fields in the PWM PERIOD REG. One specifies the number of clock ticks defining the period (""PWM ENTIRE CYCLE"") and the other the duty cycle (""PWM ACT CYCLE""). So if you go from duty cycle=5 + period=10 + POLARITY NORMAL toduty cycle=3 + period=10 + POLARITY INVERTED this might generate: Also there is a PWM PERIOD RDY bit field that probably has to be consulted before writing to the PWM PERIOD REG register. It's not entirely clear to me if the PWM ACT STA bit that is used for inversion is shadowed until the next period, too. That's what I assumed above. If it's not the wave might look as follows: Where * marks the point where the inversion starts to take effect.",technical
,
"Hello, To clarify my question: after the first pwm is used and enabled (maybe using mux-0) changing sun8i pwm->clk for the second pwm is broken because then when the firstpwm is disabled the wrong clock is stopped.",technical
,
"Hi, No, I meant for your mails, sorry. Each patch should be sent in reply to your cover letter, and they are all sent as separate mails, which makes it hard to track. I'm not sure how you're sending your patches, but using git send-email this would be using --no-chain-reply-to --thread if I remember well. You don't need to move it anywhere, you can declare a clock in adriver, without being in drivers/clk. We're doing that in the DRM or the RTC drivers for example.",technical
,
"You should never try to get resources at this point. You should have requested them already at ->probe() time. Otherwise, how are you going to handle failures here? So this isn't really how atomic is supposed to work. The whole point of the single callback is to allow the driver to apply the changes in an atomic way, which means that either everything is applied or nothing is applied. That's not what you do here. In the above you can end up with an enabled clock but the settings not being applied. Similarly sun8i pwm config() can abort in a number of places, which would leave you with a half-configured PWM channel. Instead, what you should be doing is precompute everything and check that the configuration can be applied before touching any registers or enabling clocks. Once you've validate the new state, you need to write everything and there should be no more risk of failure.",technical
,
"Something wrong here, i will fix it, thanks :) Got it, it is useful for me !",technical
,
"So, now it's enabled to be added via regular ndo. I have similar change in mind, but was going to send it after mcast/ucast, and - enabling same vlans patch...2 things stopped me to add this: 1) Moving it to be enabled via regular call is Ok, but in dual mac mode it causes overlaps, at least while vid deletion. So decided to wait till same vlans series is applied. 2) Wanted implement somehow similar handling for single port boards in one patch, not only for dual mac mode. This part was not clear and not verified completely...So, if it's needed  now, maybe better at this moment only remove untag field? and remove vlan0 later, once other vlan changes applied. ",technical
,
"TI driver documentation mentions this restriction ""While adding VLAN id to the eth interfaces, same VLAN id should not be added in both interfaces which will lead to VLAN forwarding and act as switch"" This patch affects only dual mac mode and in this mode adding vid0 by default is definitely make no sense in any case.",technical
,
"It's not accurate now. This sw bug ""acting like a switch"" was fixed indirectly in LKML . And at least for upstream version, not TISDK, desc should be updated, but better do this when it fixed completely and merged with TISDK. I know about this ""written"" restriction (for tiSDK, and it's not TRM after all ...), it can be avoided and it's partly avoided now ...Also, for notice, while you add any of the vlans to any of the ports, vlan0 is added to both of them.....restricted it or not. Thanks to last changes in the driver it's not ""acting like a switch"" The patch in question enables this in ndo, not me.: Adding vlanid 400 to vlan filter I just propose to extend it later, when it's correct to do. But if no harm (basically no harm, only if someone decides to add vlan0 to both ports and then delete on one of them), at least you should take this into account. The above proposition is only to your change, only for dual-mac.",technical
,
"Thank you for your review. Seems not everything works as expected with this patch, so ignore it please.",technical
,
"I'd like to clarify point about supporting same VLANs in dual mac mode, to avoid future misunderstanding, overall: it's *not* supported as adding same VLAN to both net devices will cause unknown unicast packets leaking between interfaces and it can't be avoided - hw limitation. Regarding vid0 - current default configuration of CPSW considersvid0/priority tagged packets as - untagged and assigns pvid to any such ingress packet inside switch. Hence, P0 (Linux host) egress port never modifies packet contents - this behavior is not visible to Linux",technical
,
"Simple test shows no issues with ucast leaking. But for current buggy ucast vlan implementation it's not possible to verify, not sure but probably leaking in your case caused by hidden toggling of interface to promisc while added ucast to vlans or other reason or so. Anyway I just decided to check specifically ucasts (macst as you know are not normal now). For verification you need to apply ucast fix (including vlans) first: This is generic fix (not sure it will be approved, need try RFC) but implement the same as local fix for vlan ucasts: Any of those are correct. I've used generic one. Applied the following scheme, Observe silence on PC wireshark. Thus, no see issues with this. PS: I'm sure in plget tool, you can use your own. I can't verify everything with vlan0 at this moment (not time), just shared my thoughts adding a notice it has same possible overlap issues(or part of them) after this patch as regular vlans have.",technical
,
I'm using packeth to generate udp packets (vlan) src=PC dst=unknown if there is record in ALE table which looks like this then above udp packet will be forwarded to BBB,technical
,
"Agree, seems no normal way to avoid ucast leak.",technical
,
"One of the ways could be removing end ports as memembers, leaving only and allow tagged packets to be received by ports being non members of a vlan. So that only unknown vlans are dropped..",technical
,
"Nice work Rafael. Minor nits below.. minimum observation latency greater What about a short section for the ladder governor as well ? containing Maybe I missed, but I couldn't find any text that says what state 0, 1, ... Nmean. Like which is the deepest idle state and which one is the shallowest. opening constraints",technical
,
"Thanks for the typo fixes.  The spellchecker I have here evidently doesn't work.[cut]There is a paragraph on that above. But this part is missing, good catch!",technical
,
I have this in my .vimrc and I am shown these spelling mistakes somewhat forcefully :) set spell,technical
,
"Interestingly enough, it appears to work when I turn the automatic spell checking, which I don't do for code as a rule, because it distracts me.  I will need to do that for docs going forward it seems, though.BTW, I didn't respond to the remark about the ladder governor.  I have no plans to describe it at this time and that can be done at any time later easily enough if anyone wants to do it.",technical
,
"That would have made this doc complete somewhat. But anyway, that's fine with me.",technical
,
Same as my previous comments.,technical
,
"Hi all, After merging the clk tree, today's linux-next build (armmulti v7 defconfig) failed like this. I have used the clk tree from next-20181129 for today",technical
,
"Did you mean to lose the doubling of ""rate"" above?",technical
,
"Adding the current maintainers on CC.				",technical
,
"Indeed, and it's actually *worse* to read, as 0/1 stands out more and is more compact than false/true...The only reasonable case where bool is recommended is when functions are returning it, to make sure there's no mishap returning something else. But for a plain .c variable? Nope. Ack. ",technical
,
"Personally, I would prefer that assignments involving boolean variables use true or false.  It seems more readable.  Potentially better for tools as well.  But if the community really prefers 0 and 1, then the test can be deleted. ",technical
,
How about it it were suggested only in files that already use true and false somewhere?,technical
,
"Now thinking further about this, I actually still need to validate that the L12 EPT for this gfn actually contains the apic access address. To ensure that I only fixup the fault when the L1 hypervisor sets up both VMCS L12 APIC ACCESS and L12 EPT to contain the same address.Will fix and send v2",technical
,
"The ioctl(BC FREE BUFFER) frees the buffer memory associated with a transaction that has completed processing in user space. If the buffer contains an FDA object (file-descriptor array), then it closes all of the fds passed in the transaction using ksys close(). In the case with the issue, the fd associated with the binder driver has been passed in the array. Since the fdget() optimization didn't increment the reference, this makes us vulnerable to the UAF described above since the rules for fdget() are being violated (ksys close()). This change did prevent the final close during the handling of BC FREE BUFFER, but as you point out, may still result in the final close being processed prematurely after the new fput() (no observed negative side-effects right now, but agreed this could be an issue).I'll rework it according to your suggestion. I had hoped to do this in a way that doesn't require adding calls to non-exported functions since we are trying to clean up binder (I hear you snickering) to be a better citizen and not rely on internal functions that drivers shouldn't be using. I presume there are no plans to export task work add()...There are indeed many things about the binder interface we'd do differently if we had the chance to start over...",technical
,
Thanks for the detailed responses. I'll rework it for v3.,technical
,
"Er...  Your variant critically depends upon binder being non-modular; if it*was* built as a module, you could	* lose the timeslice just after your fput()	* have another process hit the final fput() *and* close the struct file	* now that module refcount is not pinned by anything, get rmmod remove your module	* have the process in binder ioctl() regain the timeslice and find the code under it gone. That's one of the reasons why such kludges are brittle as hell - normally you are guaranteed that once fdget() has succeeded, the final fput() won't happen until fdput().  With everything that guarantees in terms of code/data not going away under you.  This patch relies upon the lack of accesses to anything sensitive after that fput() added into binder ioctl().  Which is actually true, but only because the driver is not modular...At least this variant (task work add()-based) doesn't depend on anything subtle - the lack of exports is the only problem there (IOW, it would've worked in a module if not for that).",technical
,
" ,That should be s/high/above I suppose. Other than that this seems really useful :-) ",technical
,
"Right, thanks for spotting this. :-) Thanks!",technical
,
"s/metrics/metrics am probably pointing out something that has been already debated, apologies if so. exit latency is the *worst* case exit latency for idle states that involve multiple CPUs, we can't say for certain it is the latency that was actually experienced by the idle state exit. It can be microseconds (eg CPU resume) vs milliseconds (eg groups of cpus resume).I think the current approach (which may only understimate the ""below"" by subtracting the worst case value) is reasonable but I pointed this out since I do not know how these stats will be used.",technical
,
"Right, thanks! Right. This is on purpose. I want to count the cases when the selected state has been off for certain.",technical
,
"...[snip]...Same here. Same here.My graphs now include the ""above"" and ""below"" metrics. In particular see Idle State 1 ""above"" (was too deep) graphs in the links below. However, performance is up and power about the same, so O.K. I cherry picked a couple of the mmtests that Giovanni was doing: Test kernels:""stock"" kernel 4.20-rc5 + a couple of rjw patches. Specifically:	 (note: slow upload speed from my server) The above results tables are also here: I wanted to also do the tbench on loopback test, but have not been able to get it working on my system yet. I'll supply more test results at a later date.... ",technical
,
"Thanks a lot for the comprehensive results, much appreciated as always! This basically confirms my own observations, so my overall conclusion is that what we have here is as good as it can get without changing the approach entirely or adding complications that would be difficult to justify in general. And so that's what I'm going to do.",technical
,
"I like that you're getting rid of the extra task let, but the other part of this is properly refilling your rx ring.  The way you have this coded, you always blindly just receive the incoming frame, even if your refill operation fails. If you get a long enough period in which you are memory constrained, you will wind up with an empty rx ring, which isn't good.  With this patch, if your ring becomes empty, then you will stop receiving frames (no buffers to put them in), which in turn will prevent further attempts to refill the ring.  The result is effectively a hang in traffic reception that is only solveable by a NIC reset. Common practice is to, for each skb that you intend to receive: 1) Allocate a replacement buffer/skb2a) If allocation succeeds, receive the buffer currently on the ring, and replace it with the buffer from (1) 2b) If allocation fails, record a frame drop, mark the existing buffer as clean, and move on This process ensures that the ring never has any gaps in it, preventing the above hang condition. ",technical
,
It should be Note that The check is not strictly needed in this artificial example because we never read/write any data there. But I agree that we should add the check to promote the the right programming patterns.,technical
,
The same comments apply here as for PATCH 1/2.,technical
,
thanks for catching this ! will send a V2.,technical
,
"This is unusual and the example below lists a clock phandle (which is the common thing), so I guess the description is just wrong. What is the unit? I'd drop ""approx."", that the driver might not be able to exactly hit the specified period is (IMHO) obvious and doesn't need to be mentioned in the property name.",technical
,
"If there is a publically available reference manual, please add a link to it here.@Thierry: You see, this driver is cheating in the same way that I suggested to implement for imx. You must not use / to divide an u64 (unless you're on a 64 bit arch).Also if real period is for example 10 ms and the consumer requests duty=12 ms + period=100 ms, the hardware is configured for duty=1.2 ms +period=10 ms, right? You should also check polarity (and fail if it's !=PWM POLARITY INVERSED?).If state->duty cycle == state->period, we end up with frac = 0xffff.Does that mean the chip cannot output 100%? Is this the expected behaviour of .apply to update *state? (I think it's a good idea, but I think it misses official blessing.) How does a period start with this PWM hardware. The expected behaviour would be to start with low level for duty cycle and then high for the rest of the period (given that the polarity is always inversed). Is this what the hardware actually does? If the duty cycle changes, is the currently running period completed before the new setting gets active? If yes, .apply is supposed to block until the new setting is active. A single space before the = please. What happens with the output if you don't set the BIT PWM EN ALWAYS bit? I suggest commenting this assignment with something like: ""As scale <=15 the shift operation cannot overflow."" You must use div64 ul for dividing an unsigned long long variable. Can it happen that the result is too big to be hold by read period (which is an unsigned int only)?Maybe add a dev dbg with the new real period here.Please don't emit an error message if PTR ERR(pwm->clk) is-EPROBE DEFER. You're supposed to call clk get rate only after you enabled the clk. In probe you setup the clk notifier before calling pwmchip add. So it's a good habit to do it the other way round in .remove. You're not using the irq that according to the dt binding is required?!",technical
,
This should reference the common doc Paul has written and not re-explain the versioning scheme again. Needs a unit suffix as defined in property-units.txt,technical
,
Ok sure. Will be done. Thanks for the comments!,technical
,
"You are right, I will correct the description. The unit is nanoseconds. Will add the unit suffix to the property name. Thanks for the comments!",technical
,
"Ok will add the link to the reference manual. Will use div u64(). Right. Will add the check for polarity. No, it does not mean that. The chip can output 100% Ok, will update the *state by calling get state() from .apply Yes, Correct. No, it is not the case. Sure. If BIT PWM EN ALWAYS is set, the PWM counter increments continuously. If not set, PWM counter will be disabled. There won't be PWM output unless BIT PWM EN ONCE is set. In that case it will generate single PWM cycle and stop. Ok. Will add that comment and also use div64 ul for division. Regarding the result, I don't think so it will be big enough to overflow read period. Sure, will add it. Will add an ""if"" check. Will fix this. Will change the sequence. Yes, currently there is no use. Thanks for the comments!",technical
,
"(adding some more people, please remember to run get maintainer.plto get the full list in the future)",technical
,
"Are you talking about the precondition?Because apei resources fini()  happens under the same condition check and if arch apei filter addr was false, it should not become true, all of a sudden. Or? Or does that function ptr get set in the meantime on your machine? I.e., this hackery: being called in pci mmcfg early init()...? Hmmm.",technical
,
"please take a look at the stacktrace. For some reason, and only at that specific hardware, the condition is false, there but later the indicated error exit is taken whose message you can see immediately before the stack trace. So this should documents the one observed case where the NULL deref is actually happening. Of course, it would be possible to develop another solution, but this one appears the simplest and safest to me (minimum changes to the logic). I have tested the patch on that specific hardware: I have verified that the patch does not trigger the NULL deref anymore. Of course, on any other hardware we have tested, the bug did not trigger at all. If you don't have that specific hardware, you probably cannot easily trigger / verify the problem. If you need access to the specific hardware, talk to me in a private conversation.",technical
,
"Yes, but if you say ""for some reason"",  then we still don't know what the root cause is. So before we do any fixing, let's find out what the problem is first. Can you pls run this debugging hunk on top of -rc6, on the box and send me full dmesg? Privately is fine too. Thx",technical
,
"Ah, I overlooked that commit e56c92565dfe2 is already providing a different solution to the same problem in newer kernels  only , as a side  effect (not clear to me from the description, but clear from reading the code). But this patch is not present at the 4.4.166 kernel where I found the problem and fixed it internally in a different way. The 4.4.166 code looks like this, without the if-statement you are mentioning, unconditionally trying to free the uninitialized variable under certain circumstances: So another alternative would be backporting to the 4.4 LTSseries. Also fine for me.",technical
,
"Damn, I missed the fact that this is not the upstream kernel: CPU: 0 PID: 1 UID: 0 Comm: swapper/0 Not tainted 4.4.0-ui18344.004-uiabi1-infong-amd64 #1That looks like the right fix. A note for the next time: do not send a fix for a stable kernel which is not upstream: From Documentation/process/stable-kernel-rules.rst:"" - It or an equivalent fix must already exist in Linus' tree (upstream).""The stable kernels track upstream so if a stable kernel has a problem, the first thing one needs to do is to check whether this has been fixed upstream and if so, to backport it. This is the case most of the time. In the very seldom cases where a separate fix is needed, it needs to be handled by asking Greg what to do. :-) Adding stable@ folks to CC to set me straight if I'm missing something.",technical
,
"Nope, you are correct, thanks!",technical
,
"There is no JESD85-B51.  I presume you mean JESD84-B51, but I can't find any reference to DMA in 6.6.39.  All the host controller relevant material seems to be in Annex B.  Can you clarify this reference?",technical
,
Please just fix the test to use the global object created for this purpose instead of an unnecessary on-stack instance. Thanks.,technical
,
"Avoid that the following+AD4 +AD4 is reported with object debugging enabled while running the selftest+AD4 +AD4 from, Please just fix the test to use the global object created for this purpose+AD4 instead of an unnecessary on-stack instance. I will do that. Thanks for the feedback.",technical
,
Do you mean ATF (ARM Trusted Firmware) instead?,technical
,
"TF-A is the name of the day for what was formerly known as ATF...However I don't think that it's correct to just don't cache the clock settings. Normally the secure world firmware should not change any clock settings at runtime, or it would run into all kinds of conflicts with the clock driver. So there are probably some well known points in time like a suspend or resume event when the firmware might change clock settings, so we could instead use those to trigger an explicit invalidate of the clock caches with much lower overhead. ",technical
,
"There is bus-freq feature on imx8m which is to scale ddr clock, this is done in ARM Trusted Firmware, for some setpoints, the DDR PLL clock rate must be changed directly in TF-A, but its child clock like dram core is unaware in Linux kernel, so the clock rate will mismatch with hardware, since ddr related clocks will NOT used by any module in Linux kernel, so it will NOT introduce any conflict. Regarding about the over head, yes, the change in common composite clock register has too many over head for other clocks, what if I ONLY have dram core clock to pass the flag to register the composite clock?",technical
,
"I don't think there is anything implementing the bus frequency scaling in main line, right? IMHO marking clocks under TF-A control explicitly as nocache would be much more acceptable than doing it for every composite clock. This seems okay for a short term solution. Still I think that whatever is causing the bus frequency scale to change should have a way to explicitly invalidate the clock cache for the affected clocks eventually. ",technical
,
"Yes, mainline has no bus-freq scaling so far, but internally we use same composite clock driver as mainline, and bus-freq clock rate issue/bug reported during our internal test, that is why I create this patch to easy our next kernel upgrade. It is because the DDR PLL/clocks can only be changed with strict DDR freq change flow, and it is done in TF-A, Linux kernel can NOT touch it in runtime, so we have to mark the child clock of DDR PLL to be uncached, in V2 patch, I will just add the flag for the DDR PLL child clocks to be a shorten solution, should be only very few ones, hope it is acceptable, thanks. ",technical
,
I fully understand why you are doing the frequency change in TF-A and I agree with the reasoning to do so. I also think that using uncached for the few clocks under TF-A control is fine for now. But if/when the bus frequency scaling is actually implemented for upstream I think the flow should look something like that: 1. Bus freq scaling driver determines that a change is necessary 2. Scaling driver calls into TF-A to do the change 3. TF-A reconfigures clock rates 4. Scaling driver calls into clock driver to signal that a clock change might have happened 5. Clock driver invalidates and recalculates cached values for the affected clocks ,technical
,
"Quoting him, Does any clk consuming driver of the downstream clks that are branched off of the bus clk managed by firmware care about the frequency? Or do they just want the clk to be on. If they don't care then it's possible to break the parent dependency and just not care to tell them what the bus frequency is anymore. I don't know how you would implement #4 above, besides by having the busfreq scaling driver use clk set rate() to tell the bus clk that it wants a new rate and then having that clk implementation do #2. That way the rate propagation works without having to notify clk code somehow.",technical
,
"In our case, the original clock relationship is as below, when DDR freq needs to be scaled, below things are proceeded: Clock tree Linux kernel do SMC call into TF-A;2. in TF-A, we have to scale the IMX8MQ DRAM PLL2 to dedicated frequency along with DDR freq scale flow;3. After TF-A done the DDR freq scale and return back to Linux, all the downstream of IMX8MQ DRAM PLL2 clock will  have mismatch clock rate between clock tree and HW settings since they are cached;4. Maybe we can call clk set rate in Linux for IMX8MQ DRAM PLL2 again (although the HW settings are already expected)  to update the downstream clocks, looks like can fix it, but will the call be skipped by clock framework when clock driver  re-calculate the PLL rate based on HW setting, as HW setting already equal the rate wants to be set, and clk propagate rate change  will be skipped too or it will automatically update all child clocks' rate? Is it correct? if all child clocks' rate can be updated by clk propagate rate change(), then we no need to add any clock flag ,just make sure after TF-A finish the DDR freq scale, make sure calling the clk set rate() for this, then everything should be correct? ",technical
,
"You may want to check linux/next. As far as I can see, the two patches are there already.",technical
,
Wow !!Thank you !! ,technical
,
This makes it impossible to write a wrapper that turns this mode on for unmodified programs. Do you have a real use case where this behavior is a problem?,technical
,
"You can always force disable SSB. In that case, all the child processes will have SSBD on. Yes, we have an enterprise application partner that found that their application slow down up to 10-20% depending on how their application was set up. With the slow setup, the application was spawned by Java processes causing the SSBD bit to stay on when the application was running. ",technical
,
"Okay that sounds reasonable, given the below. Thanks.-",technical
,
Ping! Any comments of objections?,technical
,
"Lot's of MAY's here. Aside of that this fundamentally changes the behaviour. I'm not really a fan of doing that. If there are good reasons to have a non-inherited variant, then we rather introduce that instead of changing the existing semantics without a way for existing user space to notice. Thanks",technical
,
"I understand your point. How about adding a "",noexec"" auxiliary option to the spec store bypass disable command line to activate this new behavior without changing the default. Will that be acceptable?",technical
,
I'd rather have an explicit PR SPEC DISABLE NOEXEC argument for the PRCTL so you can decide at the application level what kind of behaviour you want. ,technical
,
Thanks for the advice. Will work on a v2 to be sent out later this week.,technical
,
"*groans*  Another one where the MODULE LICENSE is different. Michael, Analog copyright, so if you want to express a view on the intent that would be great. My feeling would be that the MODULE LICENSE is the wrong one given it's easier to get that wrong than to add an 'or later' to the text. On these I generally want an ack from the copyright holder anyway just to be sure everything is in order.",technical
,
"Folks, I'm about to vanish for a truly needed break until Jan 7th. Time to lookback to an interesting year. Almost exactly a year ago, all hell broke loose and quite some people were forced to cancel their Christmas and New Year vacation and instead of spending quality time with family and friends they tried to bring the bits and pieces for the Meltdown and Spectre mitigations into shape. While the Meltdown part (KPTI) was in a halfways good shape - at least in mainline - the Spectre mitigations did not make it into mainline on time and caused havoc in distros. The broken microcode updates and other unpleasant issues did not help the situation either. And no, the 6 days extra if the embargo wouldn't have ended early would not have made any difference. It's a wonder that it held up until Jan. 3rd at all. The reasons for this disaster have been pretty much covered in various ways, so no point to go back to that again. Though it's worth to mention that some of the mitigations took quite some time to materialize and the development was not at all driven by those who are responsible for the problem in the first place. Primary examples are KPTI support for 32bit and STIBP which took more than 9 months to get into the mainline. KPTI for 32bit was ignored completely and STIPB only got attention due to performance regressions, though the response was causing more work than help. The next round of speculation-related issues including the scary L1TFhardware bug was a way more ""pleasant"" experience to work on. While forobvious reasons the mitigation development happened behind closed doors in a smaller group of people, we were at least able to collaborate in a way which is somehow close to what we are used to. There were surely a few rough edges with respect to bringing in particular developers and information flow, but both Intel and we as a community have learned how to deal with that and improved a lot. As a consequence, we are going to have a well documented and formalized process for this in the foreseeable future. There are also efforts on th eway to have non-public testing infrastructure available for future events of this kind. No need to speculate whether this makes sense. I'm not overly optimistic that we have seen all of that by now and my gut feeling tells me that we are going to be haunted by that kind of issues for a very long time. For the very unlikely case that I'm proven wrong, then I'm surely not going to shed a tear about the time spent on writing the documentation and getting things prepared. At this point I want to say BIG THANKS to everybody involved for all the great work which was done under not so enjoyable circumstances. Both the required secrecy and the set in stone timelines are pretty different from our normal workflow. At the same time I want to take the opportunity and apologize for any outburst I had. I know that I went overboard occasionally and it's nothing I'm proud of. Looking back, I have to say that all of this certainly had consequences outside of that restricted setting. The coordinated release dates forced quite some people to put a break on other tasks which were piling up nevertheless. The review backlog was from time to time tremendous and I'm sure that we dropped stuff on the way and that we still have things to catch up with on all ends. Though a lot of this pressure and fallout is home-grown and could have been avoided at least to some extent. The underlying reasons are not specific to the mitigation development, the circumstances just emphasized them and made them more observable for everyone - involved or not. 1) Lack of code quality    This is a problem which I observe increasing over many years.    The feature driven duct tape engineering mode is progressing    massively. Proper root cause analysis has become the exception not the    rule.    In our normal kernel development it's just annoying and eats up review    capacity unnecessarily, but in the face of a timeline or real bugs it's    worse. Aside of wasting time for review rounds, at some point other    people have to just drop everything else and get it fixed.    Even if some people don't want to admit it, the increasing complexity    of the hardware technology and as a consequence the increasing    complexity of the kernel code base makes it mandatory to put    correctness and maintainability first and not to fall for the    featuritis and performance chants which are driving this    industry. We've learned painfully what that causes in the last year. 2) Lack of review response    Not addressing review feedback is not a new problem, but again under    time pressure or in the face of real bugs it becomes a real pain and    causes extra work for others and maintainers in particular. 3) Outright refusal    I've seen particularly in this year quite some people who responded to    review feedback with outright and outspoken refusal. The points they    refuse to address are not some esoteric whims of particular    maintainers, no it's refusal to accept that there are documented    process and patch submission rules which apply for everyone.    Again, not a big problem if it's related to features. If it's related    to actual bugs or the timelined mitigation development then it causes    extra burden for others. In other words, if we are exposed to more half-baked patches, sloppy addressing of review feedback or in the worst case refusal to collaborate and then on top getting complaints about maintainers and reviewers being bottlenecks, then this will become a real problem in the not so distant future. Companies have to understand, that the kernel community cannot provide all-inclusive educational programs for their engineers. It's about time, that the companies catch the obvious wreckage before it leaves the house and make sure that feedback is addressed properly and in all points. I'm neither expecting perfect patches nor is there a guarantee that even well thought out and well written code will go into the tree undisputed. Though reviewing and discussing something which is well done is way less time consuming and frustrating than dealing with the above. I know that some people will come forth immediately and educate me once more on maintainer models and the need to bring new maintainers in fast. I'm all for more maintainers, but it's hard to find the right people. All good maintainers - and I've brought quite a few of them into that role myself - had proven themselves in their contributor role before taking that up. Rest assured that I constantly look out for these people and try to get them on board. Picking them out is based on their technical skills but even more so on their mindset. Unfortunately quite some of them don't want to step into that role because they are well aware of the responsibility and the burden which comes with it. I respect that decision and I definitely can understand it. I was more than once on the verge of throwing in the towel during the last year. I'm not opposed to try new things, quite the contrary. But something which worked out for a particular subsystem cannot be applied blindly to everything else in the hope that it works out. That needs a lot more thought and I'm not at all buying that tooling is a crucial part of the solution. Last but not least, I'm not sure whether more maintainers can solve the pain points which bugger me most. I rather think we'd need lots of nursemaids and teachers to address that. Sorry for the lengthy and maybe unpleasant read, but keeping the frustration which built up over the year to myself would just cause me gastric ulcer and a bad mood over Christmas. So I decided to vent and share it with all of you even at the risk that I'm barking up the wrong tree. That said, I'm going to vanish into vacation until Jan. 7th and I'm not going to read any (LKML) mails until then. As I predict from experience that my (filtered) inbox will be a untameable beast by then, don't expect me to actually go through it mail by mail. If your mail will unfortunately end up in the 'lkml/done' folder without being read, I'm sure you'll notice and find a way to resend it.I'm nevertheless looking positively forward to the new challenges of 2019 and I wish you all a Merry Christmas, a Happy New Year and a refreshing break! I wish especially for those who suffered a year ago, that they can enjoy quality time with their families and friends! ",technical
,
"I totally agree on this point by having been hit by the same problem on another project (haproxy). It turns out that everyone are interested in features, reliability and performance. But these ones cannot come without maintainability, and in practice only these 3 former ones can improve overtime. Maintainability only gets worse and is never ever addressed ""later"" by incremental code updates. Now I tend to be a bastard on this point and to demand properly documented patches, properly named functions/variables and everything that helps other people quickly figure why the code works or doesn't work, knowing that performance/features/reliability area easily addressed afterwards by many other contributors when the code is maintainable. Take your well deserved vacation with your family, ignore e-mails and don't read the news, it will only make you relax better, and you'll come back fully recharged. ",technical
,
"Ping? Jonathan, can you pick this up? 						",technical
,
"Thanks a lot for the follow-up to our earlier discussion here! Are we actually worried about concurrent writers here? I thought the only problem was a race between writer and reader, which would mean that we could solve it using only a seqcount t which is cheaper to update than a seqlock t.   ",technical
,
"I considered using just the seqcount t. But, I think we do care about concurrent writers here. A couple of scenarios I can think of:1. When you have 2 concurrent recvmsg() calls on a socket, and they both try to update sk tstamp.2. If a socket has don't have one of the SO TIMESTAMP/NS options set and you have a first recvmsg and a concurrent ioctl call on thesocket.These are corner cases and if we don't care aout these then we can use just the sequence counters. I have missed out tstamp update in the sunrcpc code. If everyone is ok with this approach, I will add it in when I post an update",technical
,
"Since, regardless of whether this is the final approach we will take, it seems that sunrpc needs to be added to this patch. So I'm definitely waiting for a new version. Thanks.",technical
,
"Please come up with something that has zero added costs for 64bit kernels .Most of us do not really care about 32bit kernels anymore, so we do not want to slow down 64bits kernels for such things. Look at include/linux/u64 stats sync.h for initial thoughts. Thanks.",technical
,
"This is similar to what I did here. But, I can add a few ifdef's to make this code a noop on 64 bit arches. I will include this in my next update. I'm assuming there is no contention on whether writers need exclusive access and hence requiring a lock here.Let me know otherwise.",technical
,
"HI, What makes you think that not matching on this compatible is an error? Have you looked at the rest of the driver?",technical
,
"Btrfs have better error message infrastructures (e.g. distinguish different filesystems). Please use btrfs error() or btrfs warn() instead. Despite that, I think the patch looks good.",technical
,
"Were you able to reproduce?  If so, did you use the syzkaller output or something else? ",technical
,
I reproduced on vm using syzkaller utils and verified the fix by syzbot. ,technical
,
"I think this might be a better fix;WRITE PENDING can be set without con->mutex held from socket callbacks.This is the reason we use atomic bit ops here, so testing WRITE PENDING under the lock didn't make sense to me.At the same time, KEEPALIVE PENDING could have been a non-atomic flag. I spent some time trying to make sense of conditioning queue con() call on the previous value of KEEPALIVE PENDING and couldn't see any, so I'm setting it with con flag set(), making ceph con keepalive() symmetric  with ceph con send().",technical
,
"Yes, it looks clear and makes sense to have an atomic operation in if statement but it still triggers warning. KEEPALIVE PENDING should be set afterclear standby() because con fault() can be called right before acquiring the lock here which sets the flag in standby state. I tested the change with syzbot and confirmed there was no warning.",technical
,
"Right, it still triggers one of the warnings.  I was too focused on WRITE PENDING and missed that in plain sight.  I'll update the patch. Thanks for testing!                Ilya",technical
,
"Really?  Why not?  What keeps you from ""knowing"" this?  Can't the developer of the chip tell you? Shouldn't ""Unknown"" really be the same thing as ""Vulnerable""?  A user should treat it the same way, ""Unknown"" makes it feel like ""maybe I can just ignore this and hope I really am safe"", which is not a good idea at all.",technical
,
"Do some of the ""Unknown"" cases arise  from the vulnerability detection code being compiled out of the kernel? I wonder whether at least the detection support should be mandatory. sysfs is not very useful as a standard vulnerability reporting interface unless we make best efforts to always populate it with real information. Also, does ""Unknown"" convey anything beyond what is indicated by the sysfs entry being omitted altogether? ",technical
,
"There tends to be a few cases, possibly incomplete white/black lists, firmware that isn't responding correctly, or the user didn't build in the code to check the mitigation (possibly because its an embedded system and they know its not vulnerable?).I would hope that it is an exceptional case. I tend to agree its not clear what to do with ""unknown"". OTOH, I think there is a hesitation to declare something vulnerable when it isn't. Meltdown for example, is fairly rare given that it currently only affects a few arm parts, so declaring someone vulnerable when they likely aren't is going to be just as difficult to explain.",technical
,
"Hi, yes, I'm not sure about this one. I tend to think the ""unknown"" case encourages users that really want an answer to dig deeper and call their hardware/os/whoever to get an answer. I would tend to think that if the entry is missing it would tend to encourage the behavior that Greg KH mentions where the user assumes ""hey the system doesn't have a sysfs entry for VUNLERABILITY, that probably means that its not possible on the architecture"".",technical
,
"Then fix the lists :) If the firmware doesn't respond, that would imply it is vulnerable :) And if the code isn't built in, again, it's vulnerable. Then have the default be vulnerable, don't give people false hope. If you know it is rare, then you know how to properly detect it so ""unknown"" is not needed, correct? Again, ""unknown"" is not going to help anyone out here, please don't do it. ",technical
,
"i applied your patch series on linux-next-20190103. On my Raspberry Pi 3B+ (defconfig) I'm getting this from sysfs:l1tf:Not affected meltdown: Not affected spec store bypass:Unknown spectre v1:Mitigation:   user pointer sanitizationspectre v2: Unknown AFAIK it has 4 Cortex-A53 cores (no PSCI firmware), so shouldn't be affected. How can this be fixed?",technical
,
"HI, So, for spec store bypass, as you noted your getting hit by the lack of psci/smccc to report the ssb state, and this patch is just reflecting that. In the case of spectrev2 it may be correct to blame this patch set because its displaying ""unknown"" since your core isn't in the blacklist, and your core isn't new enough to have the csv2 bit indicating its not vulnerable. In this case if we do away with the unknown state, we should probably depend entirely on the black list and simply display ""Not affected"" if the core isn't listed. (meaning we may report cores not affected when they are missing from the blacklist). For ssb, the correct answer is probably fix the firmware, but given the situation, its likely this kind of machine is going to force an additional MIDR list to report the state correctly. Maybe Will or someone can chime in here? For spectrev2, wait for another version of this patch.",technical
,
"Marc Z is already working on this iirc, since we need it to fix the message printed to dmesg about the mitigation status anyway.",technical
,
"Thinking about it, ""unknown"" is actually the common case. Kernels that predate the sysfs vulnerabilities interface effectively report this for all vulnerabilities by omitting the sysfs entries entirely. Current kernels also don't know anything about future vulnerabilities that may be added in sysfs later on (but which may nevertheless be discovered subsequently to affect current hardware). So, can we simply omit the sysfs entries for which we can't provide a good answer? IMHO the kernel should make best efforts to provide answers for every vulnerability that it knows about, so the checks should not be Kconfig-dependent without a good reason. There will be cases where whitelists/blacklists are the only source of answers, and we are ultimately reliant on vendors to provide that information.  Upstream Linux is likely to lag, there's not much we can do about that.",technical
,
"As you say, we already do this for older systems. But don't add new logic to explicitly not create the files just because we ""can not figure it out"".  For those systems, I would default to ""vulnerable"" as I think that's what we do today, right? ",technical
,
"Nope: currently the vulnerabilities directory doesn't even exist for arm64 because we don't select GENERIC CPU VULNERABILITIES.There are also a few other things to consider here:  1. The action to take as an end-user is slightly different in the case     that you know for sure that your system is vulnerable, as opposed to     the case that you don't know whether your system is vulnerable or not.     The former needs a firmware update; the second needs a statement about     the CPU, which could result in a simple whitelist update in Linux.  2. There's an unfortunate political angle to this. Whilst the Arm website     [1] provides information for all of the Arm-designed CPUs (i.e.     Cortex-A*), it doesn't comment on partner implementations. I'm not at     all keen to be seen as branding them all as vulnerable in the Linux     kernel, as this is likely to cause more problems than it solves.     If we had complete whitelist information available in public, that     would be ideal, but it's not the case.  3. The architecture has added some ID registers to determine if a CPU     is affected by Spectre and Meltdown, so a whitelist only needs to     cover existing CPUs. So I agree with Dave that continuing to omit the files when we don't know whether or not the system is affected is the right thing to do.",technical
,
Already send a new version My reference is SCSI Block Commands,technical
,
"That is the GROUP NUMBER field. Also found in READ(16) at the same location within its cdb. The proposed code deserves at least an explanatory comment. Since it is relatively recent, perhaps the above should only be done iff:    - the REPORT SUPPORTED OPERATION CODES (RSOC) command is supported, and    - in the RSOC entry for WRITE(16), the CDB USAGE DATA field (a bit mask)      indicates the GROUP NUMBER field is supported That check can be done once, at disk attachment time where there is already code to fetch RSOC.Is there a bi read hint ? If not then the bi write hint should also be applied to READ(16). Makes that variable naming look pretty silly though. ",technical
,
"SBC-5 says that support for the grouping function is indicated by the GROUP SUP bit in the Extended Inquiry VPD page (86h).  I'm not sure how many devices actually support that page though.  Probably most don't. What devices actually DO support the grouping and do something with it? We'd probably need a blacklist flag to turn this off and/or some code in the error path to discontinue setting the field if the device returns INVALID FIELD IN CDB or something, like we do for disabling discard commands if they don't actually work in sd done().",technical
,
"Several devices support it, albeit for various different purposes. It's one of these wonderful features whose interpretation was left outside the scope of the spec for a long time. So even though we absolutely and positively need to make setting GROUPNUMBER conditional on GROUP SUP being reported, we also need additional information from the storage about how the field should be interpreted. The official way to report hinting is for the device to implement the IOAdvice Hints Grouping mode page. I wrote some code to support that but no vendors that I know of ended up actually shipping an implementation. A few implemented my older I/O class proposal but didn't ship that either despite really convincing performance results. If Randall has access to a device which implements hinting, I'd love to know more.",technical
,
I am working on Android phone. The idea is to enable write hint for Turbo write UFS feature.Turbo write feature in UFS 3.x is under discussion in JEDEC JC-64.This patch is the under-lying framework for supporting this feature.,technical
,
"OK, but we can't blindly go setting GROUP NUMBER to a non-zero value. That'll break a massive amount of devices which will fail READ/WRITE commands with INVALID FIELD IN CDB. So aside from requiring the device to report GROUP SUP=1, we'll need some sort of indication that this device supports the UFS Turbo Write feature. If you are engaged with JEDEC on this, please tell them we'll need a VPD page, a mode page, or something similar to use as trigger to entertain enabling this feature.",technical
,
"As far as I know, there are more than one Turbo Write feature proposals in JEDEC, which are currently under discussion. For now, not all proposals are using CDB bytes for enabling TurboWrite. So, maybe making this change in SCSI is still too early.",technical
,
"Given that keycodes are linux-specific, I think the property should be linux,keycodes. Also, I am not sure we need separate rotary-encoder, relative-keys property as we can infer that we want to generate keys from presence of linux, keycodes property. Rob, any comments?",technical
,
"Yes, I had similar thoughts.",technical
,
"Hello, i used this rotary-encoder patch in my embedded project and found two errors. I am sorry, I know that E-Mail style is not good. I have no time right now, but I'll be back in two weeks. Someone, maybe Mr. Han, could submit a new version of the patch. If not, I'll try to do it on my return. (it could take some time, since I am new to patchwork) ",technical
,
"[ resending to Rob... ]Given that keycodes are linux-specific, I think the property should be linux, keycodes. Also, I am not sure we need separate rotary-encoder, relative-keys property as we can infer that we want to generate keys from presence of linux, keycodes property. Rob, any comments?",technical
,
Applied.,technical
,
I wonder if you can take this.,technical
,
Any comment on this patch ?,technical
,
"please take a look at this series, which implements a completely generic set of dma map ops for IOMMU drivers.  This is done by taking the existing arm64 code, moving it to drivers/iommu and then massaging it so that it can also work for architectures with DMA remapping.  This should help future ports to support IOMMUs more easily, and also allow to remove various custom IOMMU dma map ops implementations, like Tom was planning to for the AMD one. A git tree is also available at:    Gitweb:    ",technical
,
Any chance to get a review on this one?---end quoted text---,technical
,
"I've been pondering this for a while now, and I still can't really come up with a case where arch dma prep coherent() would need to behave differently from arch sync dma for device(..., DMA BIDIRECTIONAL). I wonder if we could just save ourselves this little bit of complexity by using that instead...",technical
,
"I think the   KERNEL   and asm/errno.h slip-ups are things I cargo-culted from the arch code as a fresh-faced noob yet to learn the finer details, so ack for those parts. The forward-declarations, though, were a deliberate effort to minimise header dependencies and compilation bloat for includers who absolutely wouldn't care, and specifically to try to avoid setting transitive include expectations since they always seem to end up breaking someone's config somewhere down the line. Admittedly this little backwater is hardly comparable to the likes of the sched.h business, but I'm still somewhat on the fence about that change :/",technical
,
"It also defeats the whole purpose of   iommu dma alloc pages(), so I'm not really buying the simplification angle - you've *seen* that code, right? ;)If you want simple, get rid of the pages array entirely. However, as I've touched on previously, it's all there for a reason, because making the individual iommu map() calls as large as possible gives significant performance/power benefits in many cases which I'm not too keen to regress. In fact I still have the spark of an idea to sort the filled pages array for optimal physical layout, I've just never had the free time to play with it. FWIW, since iommu map sg() was new and promising at the time, using sg alloc table from pages() actually *was* the simplification over copying arch/arm's   iommu create mapping() logic.",technical
,
Agreed - I'd definitely ack a version of this change which didn't depend on patch #3 ;),technical
,
"A lot of architectures do really weird stuff in the dma sync routines.So my plan would be to consolidate a lot more logic in there first, and then maybe as a next step we could look into usingarch sync dma for device eventually.",technical
,
As far as I can tell almost all users of this to be enabled anyway..,technical
,
How does it defeat the purpose of   iommu dma alloc pages?,technical
,
"If there's an actual bug fix here, can we make that before all of the other code movement? If it's at all related to other reports of weird mmap behaviour it might warrant backporting, and either way I'm finding it needlessly tough to follow what's going on in this patch :( ",technical
,
"Yes there is, namely assembling large buffers without the need for massive CMA areas and compaction overhead under memory fragmentation. That has always been a distinct concern from the DMA DIRECT REMAP cases; they've just been able to share a fair few code paths. As far as I'm concerned that splits things the wrong way. Logically, iommu dma alloc() should always have done its own vmap() instead of just returning the bare pages array, but that was tricky to resolve with the design of having the caller handle everything to do with coherency (forcing the caller to unpick that mapping just to remap it yet again in the noncoherent case didn't seem sensible). ",technical
,
"Other than dma-iommu.c itself, none of them *require* it - only arch/arm64 selects it (the one from MTK IOMMU is just bogus), and a lot of the drivers also build for at least one other architecture (and/orarm64 with !IOMMU API). Either way, I have no vehement objection to the change, I just don't see any positive value in it.",technical
,
"Because if iommu map() only gets called at PAGE SIZE granularity, then the IOMMU PTEs will be created at PAGE SIZE (or smaller) granularity, so any effort to get higher-order allocations matching larger IOMMU block sizes is wasted, and we may as well have just done this: Really, it's a shame we have to split huge pages for the CPU remap, since in the common case the CPU MMU will have a matching block size, but IIRC there was something in vmap() or thereabouts that explicitly chokes on them. ",technical
,
I've moved the idef back down below the includes.,technical
,
"True.  I've dropped this patch. That just needs a volunteer to fix the implementation, as there is no fundamental reason not to remap large pages.",technical
,
The bug fix is to handle non-vmalloc pages.  I'll see if I can do a smaller and more bandaid-y fix first.,technical
,
"Well, I guess I need to reword this - there is no  requirement  to remap.  And x86 has been happy to not remap so far and I see absolutely no reason to force anyone to remap. I don't parse this.  In the old code base before this series iommu dma alloc is a relatively low-level helper allocating and mapping pages.  And that one should have done the remapping, and in fact does so since (""dma-iommu: refactor page array remap helpers"").  It just happens that the function is now called iommu dma alloc remap.The new iommu dma alloc is the high level entry point that handles every possible case of different allocations, including those where we do not have a virtual mapping.",technical
,
"Should the timeout be set depending on the max transfer size? 10s seems an age if the max transfer size is 4KB. In other words, we should this only be applied for T194+? Furthermore, in tegra i2c xfer msg() we know the len of the message and so maybe it would be better to dynamically set the timeout depending on length?",technical
,
"Yes, that's the ideal way to compute timeout based on msg len and bus rate. To do this I had to update TEGRA I2C TIMEOUT macro to take arg and there are 3 different patches for tegra i2c under review and all of those will effect as the patch changes use TEGRA I2C TIMEOUT.So, Should I hold on to this change for now till those patches are merged?",technical
,
"If you have a number of patches with interdependencies, it's best to send them out as a whole series. So you'd typically apply them in order to a single branch, then use:	 git format-patch first^..last where first is the SHA1 of the first commit you want to send, and last is the  SHA1 of the last patch. The carret (^) means the parent commit of the specified one and is needed because git format-patch doesn't include the start of the sequence. If the commits are at the top of your branch you can use something like this:	 git format-patch -3 which will generate a series for the last three patches in the branch(more specifically starting from HEAD).If you send them as a series, it's immediately obvious in what order they should be applied and generally makes it easier for people to review and test.I think in this case you can probably just have the other two patches first in the series, then apply the timeout patch on top. That way you can resolve the conflicts between patches 1 and 2, and patch 3 before sending out.",technical
,
"This has the effect to ensure that if USER ACCESS is a module then sois cxgb4, otherwise USER ACCESS can be enabled or disabled Jason",technical
,
"It seems weird to have NOFAIL and then have an error unwind path, what is the deal here?",technical
,
"The other queue allocations in qp.c don't use   GFP NOFAIL.  So either leave it and remove the error check as per this patch, or remove the NOFAIL and leave the check. I suggest you remove the   GFP NOFAIL.",technical
,
"The other queue allocations in qp.c don't use   GFP NOFAIL.  So either leave it and remove the error check as per this patch, or remove the NOFAIL and leave the check. I suggest you remove the   GFP NOFAIL, since I have a recollection that using it was frowned upon.  In this case, if there is no memory available it is reasonable to return that error to the user creating the srq...",technical
,
As per steve's remarks I revised this to the below and applied it to for-next,technical
,
Thanks Jason!,technical
,
thanks for taking care of this - I simply did not have enough context to decide if there would be some special reason for this allocation to need   GFP NOFAIL - keeping its use to a minimum though is the best solution. thx! ,technical
,
"Just to clarify to the new Cc'ed list, I'm waiting on one of the Chromium guys to review before I put my mucky paws over it.",technical
,
"I don't think we need this to be merged ASAP.I feel that most of the todos are done though, so I'll drop the RFCtag and resend a v4 (which also contains some bug fixes found when testing).",technical
,
Boqun had previously pointed this out; you need to WRITE ONCE() node->head too.,technical
,
The patch looks good to me. You can add: Reviewed-by: Jan Kara ,technical
,
"Hum, do we have any users for this API? And wouldn't they also need to control how many lists are allocated then?							",technical
,
"This patch is supposed to be used by the epoll patch from Davidlohr. Ashe has retracted the patch, I can drop this patch also. The number of lists scale with the number of CPU cores in the system whether it is used one way or the others.",technical
,
"Right, I will get that into the next version of the patch.",technical
,
I vote for doing this in the original version. How about the following? Instead of the current O(N) implementation; at the cost of adding an atomic counter. We also need to add a heads pointer to the node structure such that we can unaccount a thread doing list del(). The lock has somehow changed. Retry again if it is--2.13.6,technical
,
"The counter will then become the single contention point for all concurrent updates to the dlock-list. So it will have a big impact on performance. On the other hand, instead of being a counter of # of items, we can make that a counter of # of non-empty lists. So its value will only be changed when a list go from empty to non-empty and vice versa. That will greatly reduce the number of updates to that counter. I don't want to add a new data item into dlock list node as there can be thousands or even of them in the system. Instead, I prefer increasing the size of dlock list head which only have a limited number of them and they have unused space because they are cache line aligned.",technical
,
"Both are good points. Thanks. Instead of the current O(N) implementation, at the cost of adding an atomic counter, we can convert the call to an atomic read(). The counter only serves for accounting empty to non-empty transitions, and vice versa; therefore only modified twice for each of the lists, during the lifetime of the dlock -- thus 2*nr dlock lists. In addition, to be able to unaccount a list del(), we add a dlist pointer to each head, thus minimizing the overall memory footprint.",technical
,
"We are tracking number of non-empty lists here. So I think we need a better name and maybe some documentation of what it is.That is racy. You will need to remember that you have opportunistically incremented the count. After acquiring the spinlock, the code will have to decrement it if the list is no longer empty. You will also have to increment it after lock if the list is empty now, but not previously. Do you need that while the code will do a spin unlock soon? ",technical
,
This patch looks good to me. ,technical
,
Looks good to me. You can add.,technical
,
"Just a general kernel programming question here - I thought the whole point of atomics is that they are, well, atomic across all CPUs so there is no need for a memory barrier?  If there is a need for a memory barrier for each atomic access (assuming it isn't accessed under another lock, which would make the use of atomic types pointless, IMHO) then I'd think there is a lot of code in the kernel that isn't doing this properly. What am I missing here? I don't see how this helps if the operations are executed like:	then the same problem would exist, unless those functions/macros are somehow bound to the atomic operations themselves?  In that case, what makes the use in other parts of the code safe without them?",technical
,
"Atomic update and memory barrier are 2 different things. Atomic update means other CPUs see either the value before or after the update. They won't see anything in between. For a counter, it means we won't miss any counts. However, not all atomic operations give an ordering guarantee. The atomic read() and atomic inc() are examples that do not provide memory ordering guarantee. See Documentation/memory-barriers.txt for more information about it.A CPU can perform atomic operations 1 & 2 in program order, but other CPUs may see operation 2 first before operation 1. Here memory barrier can be used to guarantee that other CPUs see the memory updates in certain order. Hope this help. ",technical
,
"There's an omission here which I think Andreas may have been referring to:   atomic inc/dec operations *are* strongly ordered with respect to each other, so if two CPUs are simultaneously executing atomic inc, the order in which they execute isn't guaranteed, but it is guaranteed that the losing atomic inc will not begin until the winning one is completed, so after both are done the value will have +2.   So although atomic read and atomic inc have no ordering guarantee at all (the point of the barrier above), if you're looking at the return values of atomic inc/dec you don't need a barrier because regardless of which order the CPUs go in, they'll see distinct values (we use this for reference counting). ",technical
,
"Or worse:  in which case dlock lists empty() misses a increment of used lists.That said, this may be fine for the usage of dlock list. But I think the comments are confusing or wrong. The reason is you can not use barriers to order two memory ops on different CPUs, and usually we add comments like:this. could you try to improve the comments by finding both memory ops preceding and following the barriers? Maybe you will find those barriers are not necessary in the end.",technical
,
"So I think that case is OK as CPU1 legitimately reads a 0, the problem would be if we miss the inc it because we are doing spin lock().That is true.Ok so for the dlock list add() the first barrier is so that the atomic inc()is not done inside the critical region, after the head->lock is taken. The other barriers that follow I put such that the respective atomic opis done before the list add(), however thinking about it I don't think they are really needed. Regarding dlock list empty(), the smp mb  before atomic() is mainly for pairing reasons; but at this point I don't see a respective counterpart for the above diagram so I'm unclear. ",technical
,
Note that this is broken is not valid onatomic read().,technical
,
Are you planning on sending a v9 with the discussed changes? Drop last two patches- Fix tearing (WRITE/READ ONCE())- Reduce barrier usage for dlock lists empty() -- which I'll be sending  you shortly.,technical
,
,
"Yes, I am planning to do so when I have some free time as I am working on a high-priority task at the moment. ",technical
,
"What's happened to this patchset? Any plans to repost a more recent version?FYI, I just ran a workload that hit 60% CPU usage on sb inode listlock contention - a multithreaded bulkstat scan of an XFS filesystem with millions of inodes on SSDs. last time I ran this (about 18months ago now!) I saw rates of about 600,000 inodes/s being scanned from userspace. The run I did earlier today made 300,000 inodes/s on the same 16p machine and was completely CPU bound...",technical
,
"I was planning to repost the patchset with the latest change last November and then Meltdown/Spectre happened. I was drafted into backporting fixes to RHEL6. Hopefully, I can finish up the work in early March and work on my upstream patches again.",technical
,
"fixes -> fixes Note sure, maybe you didn't mean to add 'one' here ? Why not just say that that firmware expect values in Q16 ? Looking toward testing it, but I had the bad luck of using an USB storage rootfs, and apparently USB no longer works on 5.0rc+, if you have a baseline tree to suggest, I'll take it. Thanks for this patch.",technical
,
"yes, thanks for the suggestion. Try  release branch at [1].",technical
,
"Noise, I am wrong...",technical
,
"This name does not match up with the ""From:"" line :(Please fix up and resend.",technical
,
Do you have an oops report or test case for this? ,technical
,
"Here is the test module code. Insmod it, we can get the oops. ",technical
,
"Here is the test module code. Insmod it, we can get the oops.",technical
,
"You do not need explicitly unregister input device if it is managed (allocated with devm). This is however is wrong, as you can't shutdown hardware before disapling/freeing IRQ, etc. Given that there are no users ofgp2a platform data in kernel I'd recommend creating a preparatory patch removing platform data support from the driver.",technical
,
"No, light sensor is not an input device, keep it in IIO please.",technical
,
Do you know what it is for?,technical
,
"Thanks for your review of the patches. Considering that the light sensor part should be in IIO, should the entire driver be rewritten as an IIO driver?  There's already the driver for gp2ap020a00f there which is presumably the gp2ap002a00f's successor and does the same functions. It's the control of the main power supply to the chip. ",technical
,
"I'd be fine with that. In this case it should be a power supply (regulator), not gpio. ",technical
,
Not sure why it never made it. I created it for use with a wl1271 which wan't properly reset in case of a fault. Also combined with imx28.,technical
,
Isn't there a GPIO that needs to be toggled and separate clk for the WIFI chip that needs to be enabled as well? You don't need this here as this is already part of the generic struct mmc host (via the struct mmc supply). Ditto. You should use mmc regulator set ocr() instead. Ditto. This isn't needed. The state is already controlled by the mmc core. You should use mmc regulator get supply() instead.,technical
,
Coding style is off here. But really I don't think these inlines are needed here. Put them in qemu or something.,technical
,
"I also wonder why do we want to put this code in the legacy section and use the legacy virtio net hdr as opposed to the new. coding style says:	Descendants are always substantially shorter than the parent and	are placed substantially to the right. placing a line to the left of ( doesn't count as substantially to the right :) Maybe start a new line at this. Lack of documentation is also a problem. Okay but this assumes specific usage. E.g. someone might want an offset and not a pointer. Or have a struct instance on stack. Given all above issues (and also header version issues described above) I'm inclined to say macros are better. But please in any case also add documentation same as we have for fields.",technical
,
Nit: does not seem to be required,technical
,
"Good catch. Notch one more line saved in the incremental diffstat fromv8. I'll wait for Michal's thumbs up on the rest before re-spinning,or perhaps Andrew can drop this line on applying?",technical
,
"The last part is not true with this version anymore, right? I find mm shuffle ctl a bit confusing because the mode of operation is either AUTO (enabled when the HW is present) or FORCE ENABLE when explicitly enabled by the command line. Nothing earth shattering though. Other than that, I haven't spotted any fundamental issues. The feature is a hack but I do agree that it might be useful for the specific HW it is going to be used for. I still think that shuffling only top orders has close to zero security benefits because it is not that hard to control the memory fragmentation. ",technical
,
I have asked in v7 but didn't get any response. Do we really ned perfree area random pool? Why a global one is not sufficient?,technical
,
"Ah, yes, sorry, overlooked that feedback. A global one is sufficient. Will rework.",technical
,
"True, and given that page alloc init late() is waiting for it complete the impact is no different from v8 to v9. I'll drop that sentence from the changelog. Yeah, it's named from the perspective of the kernel internal usage which is flipped from the user facing interaction. ENABLE is called from the command line handler and in a follow-on patch the parser of the platform-firmware table indicating the presence of a cache. FORCE DISABLE is only called from the command line handler. I'll add a comment to this effect. Much appreciated.",technical
,
"This is unfortunate from a testing and coverage point of view.  Atleast initially it is desirable that all testers run this feature. Also, it's unfortunate that enabling the feature requires a reboot. What happens if we do away with the boot-time (and maybe hot plug-time) randomization and permit the feature to be switched on/off at runtime? Can we get a Documentation update for the new kernel parameter? Does shuffle.h need to be available to the whole kernel or can we put it in mm/?Can this be   meminit? Reflowing the comment to use all 80 cols would save a line :) Second sentence is hard to parse. Reflow the comment...Reflow.",technical
,
A static inline would be nicer. Well that's nice and simple.,technical
,
"Currently there's the 'shuffle' at memory online time and a random front-back freeing of max order pages to the free lists at runtime. The random front-back freeing behavior would be trivial to toggle at runtime, however testing showed that the entropy it injects is only enough to preserve the randomization of the initial 'shuffle', but not enough entropy to improve cache utilization on its own. The shuffling could be done dynamically at runtime, but it only shuffles free memory, the effectiveness is diminished if the workload has already taken pages off the free list. It's also diminished if the free lists are polluted with sub MAX ORDER pages. The number of caveats that need to be documented makes me skeptical that runtime triggered shuffling would be reliable. That said, I see your point about experimentation and validation. What about allowing it to be settable as a sysfs parameter for memory-blocks that are being hot-added? That way we know the shuffle will be effective and the administrator can validate shuffling with a hot-unplug/replug?Yes.The wider kernel just needs page alloc shuffle() so that platform-firmware parsing code that detects a memory-side-cache can enable the shuffle. The rest can be constrained to an mm/ local header. Yes. Will do. Earlier versions only arranged to shuffle over non-hole ranges, but the SHUFFLE RETRY works around that now. I'll update the comment. yup. ok.",technical
,
"Hello, Gentle reminder for this new driver review",technical
,
"Nvmem provider driver itself looks fine for me, but I am unable to take his as 5.1 material, as I normally take nvmem patches which are reviewed and ready before rc5.dt bindings patch needs an ack from DT maintainers.",technical
,
Thanks for the feedback. I hope Rob cant take a look at it. ,technical
,
"Several distinct types here. Does s/w need to know the difference rather than just one generic-ish compatible? Access size restrictions maybe? Ability to unlock and program? If not, then why even make this stm32 specific?",technical
,
"The reading part is represented here as ""st,stm32-romem"" compatible, to simply handle read only access. I agree this could be a generic-ish. BUT the specifics are regarding the ability to unlock/lock and program. Access size can vary from one part to another (e.g. reference manual sates: OTP area is divided into 16 OTP data blocks of32 bytes.  OTP area is divided into 16 OTP data blocks of 64bytes.) In STM32MP15, both the read & write access through the BSEC are  specific, represented by dedicated compatible. Do you wish I update the compatible to something like? Thanks for reviewing, ",technical
,
"Yes, I think given the above that makes sense. We can always map specific bindings to generic drivers, but not the reverse.",technical
,
"DT Maintainers, Gentlemen reminder for new DT bindings review.",technical
,
Thanks but I submitted a different patch:,technical
,
"I tried your patch, but when running checkpatch.pl on I still get. Feels that these two patches fix different issues.",technical
,
"Hello everyone, If it's cleared with ptep clear flush notify, change pte still won't work. The above text needs updating with""ptep clear flush"". set pte at notify is all about having ptep clear flush only before it or it's the same as having a range invalidate preceding it. With regard to the code, it relies on the ptep clear flush preceding the set pte at notify that will make sure if the secondary MMU mapping randomly disappears between ptep clear flush and set pte at notify, gup fast will wait and block on the PT lock until afterset pte at notify is completed before trying to re-establish a secondary MMU mapping. So then we've only to worry about what happens because we left the secondary MMU mapping potentially intact despite we flushed the primary MMU mapping with ptep clear flush (as opposed to ptep clear flush notify which would teardown the secondary MMU mapping too). In you wording above at least the ""with a different pfn"" is superfluous. I think it's ok if the protection changes from read-only to read-write and the pfn remains the same. Like when we takeover a page because it's not shared anymore (fork child quit). It's also ok to change pfn if the mapping is read-only and remains read-only, this is what KSM does in replace page.The read-write to read-only case must not change pfn to avoid losing coherency from the secondary MMU point of view. This isn't so much about change pte itself, but the fact that the page-copy generally happens well before the pte mangling starts. This case never presents itself in the code because KSM is first write protecting the page and only later merging it, regardless of change pte or not. The important thing is that the secondary MMU must be updated first (unlike the invalidates) to be sure the secondary MMU already points to the new page when the pfn changes and the protection changes from read-only to read-write (COW fault). The primary MMU cannot read/write to the page anyway while we update the secondary MMU because we did ptep clear flush() before calling set pte at notify(). So this ordering of this ensures whenever the CPU can access the memory, the access is synchronous with the secondary MMUs because they've all been updated already. If (in set pte at notify) we were to call change pte() afterset pte at() what would happen is that the CPU could write to the page through a TLB fill without page fault while the secondary MMUs still read the old memory in the old read-only page. ",technical
,
"Oops, the above two statements were incorrect because ptep clear flush notify doesn't interfere with change pte and will only invalidate secondary MMU mappings on those secondary MMUs that shares the same page tables with the primary MMU and that in turn won't ever implement a change pte method.",technical
,
"This seems racy by design in the way it copies the page, if the vma mapping isn't read only to begin with (in which case it'd be ok to change the pfn with change pte too, it'd be a from read-only to read-only change which is ok). If the code copies a writable page there's no much issue if coherency is lost by other means too. Said that this isn't a worthwhile optimization for uprobes so because of the lack of explicit read-only enforcement, I agree it's simpler to skip change pte above. It's orthogonal, but in this function it can be optimized, otherwise there's no point to retain the  notify.",technical
,
"This is only allocated in the stack, so saving RAM by mixing bitfields with enum in the same 4 bytes to save 4 bytes isn't of maximum priority. A possibly cleaner way to save those 4 bytes without mixing enum with bitfields by hand, is to add a ""unsigned short flags"" which will make ""event/flags/blockable"" fit in the same 8 bytes (bool only needs 1byte) as before the patch (the first bitfield can start from 0 then). Yet another way is to drop blockable and convert it to a flag in ""unsigned int flags"".",technical
,
"So all the above is moot since as you pointed out in the other emailptep clear flush notify does not invalidate kvm secondary mmu hence.Yes i thought this was obvious i will reword and probably just do a list of every case that is fine.Yeah, between do you have any good workload for me to test this ? I was thinking of running few same VM and having KSM work on them. Is there some way to trigger KVM to fork ? As the other case is breaking COW after fork. ",technical
,
Background we are discussing   replace page() in it and whether this can be call on page that can be written too through its virtual address mapping. I am not sure the race exist but i am not familiar with the uprobe code so maybe the page is already write protected and thus the copy is fine and in fact that is likely the case but there is not check for that. Maybe there should be a check ? Maybe someone familiar with this code can chime in. We need to keep the  notify for IOMMU otherwise it would break IOMMU. IOMMU can walk the page table at any time and thus we need to first clear the table then notify the IOMMU to flush TLB on all the devices that might have a TLB entry. Only then can we set the new pte. But yes the mmu notifier invalidate range end can be optimized to only end. I will do a separate patch for this. As it is orthogonal as you pointed out :) ],technical
,
"KVM can fork on guest pci-hotplug events or network init to run hostscripts and re-init the signals before doing the exec, but it won't move the needle because all guest memory registered in the MMUnotifier is set as MADV DONTFORK... so fork() is a noop unless qemu is also modified not to call MADV DONTFORK. Calling if (!fork()) exit(0) from a timer at regular intervals during qemu runtime after turning off MADV DONTFORK in qemu would allow to exercise fork against the KVM MMU Notifier methods. The optimized change pte code in copy-on-write code is the same post-fork or post-KSM merge and fork() itself doesn't use change ptewhile KSM does, so with regard to change pte it should already provide a good test coverage to test with only KSM without fork(). It'll cover the read-write -> read-only transition with same PFN(write protect page), the read-only to read-only changing PFN(replace page) as well as the read-only -> read-write transition changing PFN (wp page copy) all three optimized with change pte. Fork would not leverage change pte for the first two cases.",technical
,
"So i run 2 exact same VMs side by side (copy of same COW image) and built the same kernel tree inside each (that is the only important workload that exist ;)) but the change pte did not have any impact: before  mean  Above is time taken by make inside each VM for all yes config. npages is the number of page shared reported on the host at the end of the build. There is no change before and after the patchset to restore changepte. I tried removing the change pte callback all together to see if that did have any effect (without above) and it did not have any effect either. Should we still restore change pte() ? It does not hurt, but it does not seems to help in anyway. Maybe you have a better benchmark i could run ? ",technical
,
"Did you set /sys/kernel/mm/ksm/sleep millisecs to 0?It would also help to remove the checksum check from mm/ksm.c:-	if (rmap item->old checksum != checksum) {-		rmap item->old checksum = checksum;-		return;-	}One way or another, /sys/kernel/mm/ksm/pages shared and/or pages sharing need to change significantly to be sure we're exercising the COW/merging code that uses change pte. KSM is smart enough to merge only not frequently changing pages, and with the default KSMcode this probably works too well for a kernel build. We could also try a micro benchmark based onlap/testcases/kernel/mem/ksm/ksm02.c that already should trigger a merge flood and a COW flood during its internal processing. ",technical
,
No but i have increase the pages to scan to 10000 and during the kernel build i see the number of page that are shared increase steadily so itis definitely merging thing. Will try with that. Will try that. ,technical
,
"Would it also make sense to track how many pages are really affected by change pte (say, in kvm set pte rmapp, count available SPTEs that are correctly rebuilt)?  I'm thinking even if many pages are merged by KSM it's still possible that these pages are not actively shadowed by KVM MMU, meanwhile change pte should only affect actively shadowed SPTEs.  In other words, IMHO we might not be able to observe obvious performance differences if the pages we are accessing are not merged by KSM.  In our case (building the kernel), IIUC the mostly possible shared pages are system image pages, however when building the kernel I'm thinking whether these pages will be frequently accesses, and whether this could lead to similar performance numbers.",technical
,
"I checked that, if no KVM is running KSM never merge anything (after bumping KSM page to scan to 10000 and sleep to 0). It starts merging once i start KVM. Then i wait a bit for KSM to stabilize (i.e. to merge the stock KVM pages). It is only once KSM count is somewhat stable that i run the test and check that KSM count goes up significantly while test is running. KSM will definitely go through the change pte path for KVM so i am definitely testing the change pte path. I have been running the micro benchmark and on that i do see a perf improvement i will report shortly once i am done gathering enough data. ",technical
,
"So using that and the checksum test removed there is an improvement, roughly 7%~8% on time spends in the kernel. I am guessing for kernel build this get lost in the noise and that KSM changes do not have that much of an impact. So i will reposting the mmu notifier changes shortly in hope to get them in 5.1 and I will post the KVM part separately shortly there after. If there is any more testing you wish me to do let me know.",technical
,
"What determines when you want to use polling mode? I'm not sure DTis the best way to control this unless it's really a property of the h/w. Driver behavior is really outside the scope of the DT. u-boot would use polling even if an interrupt is specified, for example.",technical
,
What's the data type and size? What are valid values?,technical
,
"It's tied to the particular revision of the I2C controller, i.e., the iProc NIC i2c controller does not have interrupt line wired. In this case, the behavior is determined by the DT compatible string of the iProc I2C device. I thought that it makes sense to now move the 'interrupts' property to be under ""Optional"" than ""Required"" which is basically what this change is.",technical
,
It's an unsigned u32 mask value. An example of a valid value is for example. Do you want any of these added to the paragraph above?,technical
,
Bindings should define constraints.,technical
,
"Okay, please put this detail into the commit msg. ",technical
,
Okay will do! Thanks.,technical
,
Will do! Thanks.,technical
,
"OK, cool. Prefer slash for TPM specific commits. I can merge this after those extra logs are removed.",technical
,
"HI, I guess we don't need to initialize it anymore with the check you add? ",technical
,
I agree with the other reviewer that since you check 'ret' the initialization of 'val' is no longer needed.,technical
,
Could you please help to review and provide your comments to this patch series when you have time? ,technical
,
"SoC specific compatibles are preferred. Version numbers can be used but should follow some documented scheme and be meaningful. What we don't want is just Linux developers making up numbering. Unless there's DT resources for each child node, you don't need these. Just make #phy-cells 1 in the parent.",technical
,
"Thanks for review, please see my comments below inline. Both versions are different phy controllers and also have separate register offsets. I will provide more meaningful compatible IDs and their documentation in next patchset. With the use of phy-cell, PHY argument is available with xlate function , But controller specific assignments required to be done while probe. So I will take your first option given in previous comment. ",technical
,
"Not sure if we support backwards compatibility like this? My issue with this change is that by doing this, application will have no clue if the new bits were ignored or not and it may think that an event is enabled while it is not. A workaround would be to do a getsockopt and check the size that was returned. But then, it might as well use the right struct here in the first place. I'm seeing current implementation as an implicitly versioned argument: it will always accept setsockopt calls with an old struct (v4.11 orv4.12), but if the user tries to use v3 on a v1-only system, it will be rejected. Pretty much like using a newer setsockopt on an old system.",technical
,
"With the current implementation, given sources that say are supposed to run on a 4.9 kernel (no use of any newer field added in 4.11 or 4.12), we can't rebuild the exact same sources on a 4.19 kernel and still run them on 4.9 without messing with structures re-definition. I understand your point, but this still looks like a sort of uapi breakage to me. I also had another way to work-around this in mind, by copying optlenbytes and checking that any additional field (not included in the ""current"" kernel structure definition) is not set, returning EINVAL in such case to keep a similar to current behavior. The issue with this is that I didn't find a suitable (i.e. not totally arbitrary such as ""twice the existing structure size"") upper limit to optlen.",technical
,
"I'm not sure I like this.  If you have a userspace application built against more recent uapi headers than the kernel you are actually running on, then by definition you won't have this check in place, and you'll get EINVAL returns anyway.  If you just backport this patch to an older kernel, you'll not get the EINVAL return, but you will get silent failures on event subscriptions that your application thinks exists, but the kernel doesn't recognize. This would make sense if you had a way to communicate back to user space the unrecognized options, but since we don't (currently) have that, I would rather see the EINVAL returned than just have things not work. ",technical
,
"Maybe what we want(ed) here then is explicit versioning, to have the 3 definitions available. Then the application is able to use, say structsctp event subscribe, and be happy with it, while there is structsctp event subscribe v2 and struct sctp event subscribe v3 there too. But it's too late for that now because that would break applications already using the new fields in sctp event subscribe. Not disagreeing. I really just don't know how supported that is. Willing to know so I can pay more attention to this on future changes. Btw, is this the only occurrence? Seems interesting. Why would it need that upper limit to optlen?S ay struct v1 had 4 bytes, v3 now had 12. The user supplies 12 bytes to the kernel that only knows about 4 bytes. It can check that (12-4) bytes in the end, make sure no bit is on and use only the first 4. The fact that it was 12 or 200 shouldn't matter, should it? As long as the (200-4) bytes are 0'ed, only the first 4 will be used and it should be ok, otherwise EINVAL. No need to know how big the current current actually is because it wouldn't be validating that here: just that it can safely use the first 4 bytes.",technical
,
"What given sources say that?  I understand it might be expected, but this is a common concern with setsockopt method on many protocols, it just so happens that sctp extends them more than other protocols. Right, put another way, we support backward compatibility with older userspace applications, but not newer one.  I.e. if you build an application against the 4.9 SCTP API, it should work with the 4.19 UAPI, but not vice versa, which it seems is like what you are trying to do here.There is no real upper limit to the size of the structure in this case, and IIRC this isn't the only sockopt structure that can be extended for SCTP in this way. I really don't see a sane way to allow newer userspaces to be compatible withholder kernels here.  If we were to do it I would suggest moving the responsibility for that feature into lksctp-tools, versioning that library such that corollary symbols are versioned to translate the application view of the socket options structs to the size and format that the running kernel understands.  Note that I'm not really advocating for that, as it seems like a fast moving target, but if we were to do it I think that would be the most sane way to handle it.",technical
,
"Was looking for that. Thanks. Speaking of that, recent lksctp-tools got some defines to help knowing which features the available kernel headers have as it now probes if specific struct members are available or not. Though yeah, it also wouldn't help in this case, just mentioning it.",technical
,
"Yeah, I'm not supportive of codifying that knowledge in the kernel.  If we were to support bi-directional versioning, I would encode it into lksctp-tools rather than the kernel. No, I think there are a few others (maybe paddr params?) I think the thought was to differentiate between someone passing a legit larger structure from a newer UAPI, from someone just passing in a massive inappropriately sized buffer (even if the return on both is the same). I'm less than excited about making the kernel check an unbounded user spacebuffer, that's seems like a potential DOS attack from an unprivileged user tome.  I'm also still hung up on the notion that, despite how we do this, this patch is going into the latest kernel, so it will only work on a kernel that already understands the most recent set of subscriptions.  It would work if we, again someday in the future extended this struct, someone built against that newer UAPI, and then tried to run it on a kernel that had this patch.FWIW, there is an existing implied method to determine the available subscription events. sctp getsockopt events does clamp the size of the output buffer, and returns that information in the optlen field via put user.  An application that was build against UAPIs from 4.19 could pass in the 4.19sctp event subscribe struct to sctp getsockopt events, and read the output length, which would inform the application of the events that the kernel is capable of reporting, and limit itself to only using those events.  Its not a perfect solution, but its direct, understandable and portable.",technical
,
"Right. Can't really say, this is one I witnessed, I haven't really looked for others. The upper limit concern is more regarding the call to copy from userwith an unrestricted/unchecked value. I am not sure of how much of a risk/how exploitable this could be, that's why I cautiously wanted to limit it in the first place just in case.",technical
,
"Copy from user should be safe to copy an arbitrary amount, the only restriction is that optlen can't exceed the size of the buffer receiving the data in the kernel.  From that standpoint your patch is safe.  However,  that exposes the problem of checking any tail data on the userspace buffer.  That is to say, if you want to ensure that any extra data that gets sent from userspace isn't 'set', you would have to copy that extra data in consumable chunks and check them individually, and that screams DOS to me (i.e. imagine a user passing in a 4GB buffer, and having to wait for the kernel to copy each X sized chunk, looking for non-zero values).",technical
,
"And I was just reminded about huge pages. But still, my point of finding a compromise still stands.",technical
,
"Probably not, but I'm not going to pick a magic number to gate what's ok and what's not for sockopt validation. We really don't have to, I refer you to my previous not referencing the fact that the getsockopt variant of this call will return the expected length of this option for the running kernel, allowing userspace to know explicitly what the buffer size should be, and by extension, what options are supported ",technical
,
"It is probably better to break the recompilation of the few programs that use the new fields (and have them not work on old kernels) than to stop recompilations of old programs stop working on old kernels or have requested new options silently ignored. There are all sorts of reasons why programs get built on systems that are newer than the ones they need to run on. I'm currently planning to get around the glibc 'mercy()' fubar so I can retire some very old build systems before their disks die. Fortunately our sctp code is in the kernel - so has to be compiled with the correct headers. Agreed, these structures should never be changed.",technical
,
"I got confused here, not sure what you mean. Seems there is one ""stop"" word too many. You can virtualize those. That's not really a good reason for building with newer kernel and running on old systems, as virtually any old system can be virtualized.  ",technical
,
More confusing than I intended...With the current kernel and headers a 'new program' (one that needs the new options) will fail to run on an old kernel - which is good. However a recompilation of an 'old program' (that doesn't use the new options) will also fail to run on an old kernel - which is bad. Changing the kernel to ignore extra events flags breaks the 'new 'program. Versioning the structure now (even though it should have been done earlier) won't change the behaviour of existing binaries. However a recompilation of an 'old' program would use the 'old 'structure and work on old kernels. Attempts to recompile a 'new' program will fail - until the structure  name (or some #define to enable the extra fields) is changed. Breaking compilations is much better than unexpected run-time behaviour.,technical
,
"What about reusing the same socket option, but defining a new struct? Say, MYSOCKOPT supports struct mysockopt, struct mysockopt2, structmysockopt3...That way we have a clear definition of the user's intent. I'm afraid clearing out may not be enough, though seems it's the best we can do so far. If the struct is allocated but not fully initialized via a memset, but by setting its fields one by one, the remaining newfields will be left uninitialized. One use case here is: a given distro is using kernel X and app Foo is built against it. Then upgrades to X+1, Foo is patched to fix an issue and is rebuilt against X+1. The user upgrades Foo package but for whatever reason, doesn't upgrade kernel or reboot the system. Here,Foo doesn't work anymore until the new kernel is also running.",technical
,
"Need to clarify the ""clearing out"", I think it was meant differently. It was more about on how to ensure that the 16-bytes long of the v3 supplied to a v1-only kernel is compatible with the 12-bytes long v1. The kernel would have to check the trailing 4 bytes after v1-size and make sure they are all zeroed in order for the old kernel to accept it as a v1. But, as I said above, there are situations that this will not be enough. We have issues on read path too. ",technical
,
"That's possible, but I think that's pretty equivalent to what daves saying, in that he wants us to identify all the sizes of this struct and the git history and act on them accordingly.  Having internal versions of the struct seems like a fine way to get there, but I think we need to consider how we got to this situations before we go down the implementation path. I'm not sure this even makes sense.  Currently (as I understood it), the issue we are facing is the one in which an application is built against a newer kernel and run on an older one, the implication there being that the application will pass in a buffer that is larger than what the kernel expects.  In that situation, clearing isn't needed, all that's needed (I think), is a memcmp of the space between the sizeof(kernel struct version), and sizeof(userspace struct version) to see if any bits are non-zero.  If they are, we error out, otherwise, we ignore the space and move forward as though that overage doesn't exist. Mind you, I'm not (yet) advocating for that approach, just trying to clarify what's needed. Yes, that's the use case that we're trying to address. FWIW, before we decide on a course of action, I think I need to point out that, over the last 10 years, we've extended this structure 6 times, in the following commits. The first two I believe were modifications during a period when sctp was actually getting integrated to the kernel, but the last 4 were definitely done during more recent development periods and won't in without any commentary about the impact to UAPI compatibility.  The check for optlen > sizeof(structsctp event subscribe) was made back in 2008, and while not spelled out, seems pretty clearly directed at enforcing compatibility with older applications, not compatibility with newer applications running on older kernels. I really worry about situations in which we need to support applications expecting features that the running kernel doesn't have.  In this particular situation it seems like a fixable thing, but I could envision situations in which we just can't do it, and I don't want to set that expectation when we can't consistently meet it.So, if the consensus is that we need to support applications built on newer kernels, but run on older kernels (and I'd like to get verbal consensus on that), then we need to identify a method to fix this.  I'm still hesitant to do anything that involves us accepting any size buffer over the kernel expected size, as that puts us in a position to have to read large amounts of user data(i.e. possible DOS), and just picking an arbitrary large number to limit the buffer size seems wrong.  What if, on receipt of a structure from a newer kernel (implying a size larger than what the kernel expects), we clamp optlen to the kernel size, and put user it back to the application?  i.e. we don't check any data above and beyond what the the kernel knows about, but we use the optlen as an indicator to user space that not all the data was processed?  That allows the kernel to ignore the overage safely, and while its not in the socket api extension RFC, its not violating anything, and is something we can document in the sctp(7) man page as a linux only behavior. Thoughts?",technical
,
"I was more referring to future stuff, but yes. I find it a bit easier to handle than having to switch the sockopt too and so far I couldn't find drawbacks to it.That is, when using a new sockopt, we could accept a buffer larger than the needed, but I'm not considering that as a valid point anymore. Putting this compatibility aside for a moment, that pretty much means the user doesn't know what it wants and so we also don't. That's exactly what I tried to mean. :-)Ok. Yes from my side. We can't do that on setsockopt calls, as optlen is R/O there. Returning > 0 is not specified on setsockopt(2). I also need to dig deeper on this, but in general what if we draw a line based on the current implementation:- Current struct is X bytes long- Patch current and older kernels to accept up to X bytes, as long as  the trailing bytes are zeroed. Otherwise, EINVAL.  X may be a magic number for old kernel, but this way we avoid unbounded buffers and the limit is not random.- On further changes, create a new, explicitly versioned struct.  Older kernels will EINVAL if this new struct is used, which is  expected.  Newer kernels will then have to cope with the different  sizes/structs accordingly.  ",technical
,
"The API shouldn't change like this at all. Is this from the RFC or elsewhere??If the structure changes the socket option name and value should also change. IMHO large chunks of the sctp rfc are just horrid. In particular all the places where is states that API functions are implemented using setsockopt() - that should be an implementation detail. Also ISTR that some of the structures are defined to have holes in them...	",technical
,
"I would think so. That commit is from 2005, pretty close to initial SCTP RFCs. That's what is at the core of this thread.  ",technical
,
Hello!    field's.,technical
,
"Oops, missed that. Will fix in v2.",technical
,
"You probably can have a   init buffer somewhere in ppc code, append data to it, step by step, and call dump stack set arch desc() all the time. But no real objections; dump stack add arch desc() can do.",technical
,
This shows me that this can be called at a time when more than one CPU is active. What happens if we have two CPUs calling dump stack add arch desc() at the same time? Can't that corrupt the dump stack arch desc str?,technical
,
"Can overwrite part of it, I guess (but it seems that Michael is OK with this). The string is still NULL terminated. The worst case scenario I can think of is not the one when two CPUs call dump stack add arch desc(), but when CPUA calls dump stack add arch desc() to append some data and at the same time CPUB calls dump stack set arch desc() and simply overwrites dump stack arch desc str. Not sure if this is critical (or possible).",technical
,
"The comment doesn't say  why  we need to order these stores: IOW, what will or can go wrong without this order?  This isn't clear to me. Another good practice when adding constructs is to indicate the matching construct/synch. mechanism.  ",technical
,
"Yes, one barrier without a counter-part is suspicious. If the parallel access is really needed then we could define the current length as atomic t and use:	+ atomic cmpxchg() to reserve the space for the string	+ %*s to limit the printed length In the worst case, we would print an incomplete string. See below for a sample code.BTW: There are very few users of dump stack set arch desc(). I would use dump stack add arch desc() everywhere to keep it simple and have a reasonable semantic. This is what I mean (only compile tested)",technical
,
"As is this silence..., what happened to this patch? did you submit a new version? Seems worth exploring, IMO; but I'd like to first hear  clear about the  intended semantics (before digging into alternatives)...,  who first raised the question about ""parallel accesses""  ",technical
,
"No, I'm just busy, it's the merge window next week :) I thought the comment was pretty clear, if the stores are observed out of order we might print the uninitialized tail. And the barrier on the read side would need to be in printk somewhere, which is obviously unpleasant. It is not my intention to support concurrent updates of the string. The idea is you setup the string early in boot. The concern with a concurrent reader is simply that the string is dumped in the panic path, and you never really know when you're going to panic. Even if you only write to the string before doing SMP bring up you might still have another CPU go rogue and panic before then. But I probably should have just not added the barrier, it's over paranoid and will almost certainly never matter in practice. ",technical
,
"Got it. Indeed. Understood, thanks for the clarification. OH, well, I can only echo you: if you don't care about the stores being observed  out of order, you could simply remove the barrier; if you do care, then you need ""more paranoid"" on the readers side.  ;-)  ",technical
,
"Hmm, the barrier might be fine and actually useful. The purpose is to make sure that the later '\0' is written before the existing one is replaced by ' '.The reader does not need the barrier as long as it reads the string sequentially. I would expect that it is always the case. But who knows with all the speculation-related CPU bugs around. In each case, any race could never crash the kernel. The dump stack arch desc str is zeroed out of box and the very last '\0' is never rewritten.",technical
,
"Please don't make this a kconfig option. According to you description this is a HW feature, so please add the maxnumber of messe objects to ""struct c can driver data"" and adjust the drvdata accordingly. You probably have to pass it via ""struct c can priv"" so that it's available in the c can.",technical
,
This still lacks a prototype. ,technical
,
Aside of that this doesn't apply to tip or Linus tree....,technical
,
Sigh. This is absolutely the wrong place. The weak function is declared and used in fs/proc/... So the prototype wants to be in a header which is included from there independent of x86…,technical
,
Can the prototype be in the architecture header if they want to call the function? Like the following? Meminfo is used in this as well.,technical
,
"Actually both way exist in the current kernel, the reason I chose to put the prototype into architecture header file is that I found some architectures rename the function name by a micro definition while others use prototype. See below; This looks more flexible than it in the common header file.  Anyway, putting the prototype into the common header file like include/linux/proc fs.his also acceptable to me if you persist, please just let me know, :)",technical
,
Basic C programming course: The prototype must be available before the declaration of the global function.fs/proc/array.c: Oh well....,technical
,
"Is this because patch 1/3 applied alone? If the whole patch set are applied,the prototype is included, which is at the beginning of array.c file, so it is available before the declaration. ",technical
,
"Okay, will cook a new version to put it into the common header. ",technical
,
1) Each patch has to be correct stand alone 2) This file is compiled for every architecture the kernel supports and how   many of them are including this?   There is a world outside x86 and it's rather large.,technical
,
"Got it, thanks!",technical
,
"I'm not exactly sure how Linux switch driver works, but from DT perspective I think we should rather have *hardware* described instead of a common Linux case. If I'm right, we should rather have all 3 switch ports described (5, 7,8) and have Linux just use the one it needs.",technical
,
"Yes, let's do that.",technical
,
Which I'm making it a priority to go and review and apply (if there's no issues) right now ;-),technical
,
"Thank you for your great work! I like this very much! One point I would like to comment is to add a kind of entry number tag, so that user distinguish the error message, e.g. What would you think?",technical
,
"OK, can I take this over? I would like to try to use this framework. Yes, that is much better, especially for ftrace test.",technical
,
Thanks![snip]I think that makes sense and would be simple to add - will do in the next version.,technical
,
"Sure, be my guest. ;-)",technical
,
What's the range of valid values? What's the range of valid values? Should be 2 entries here since there are 2 channels? Or the description above is wrong.,technical
,
"Hello, Maybe use this? broken indention. ",technical
,
"I'm wondering a bit about this: In this case, the caller that doesn't provide a struct device *, PWM provider isn't responsible for that. So I just hope this wouldn't be miss-leading ? Oops, I'll fix it.",technical
,
"IMHO it's more the wording that might make the message misleading. If you use this ""No consumer device specified to create a device link to\n""); that's completely fine in my eyes. ",technical
,
"Thanks for the suggestion, I'll update this as well in v5.",technical
,
Looks good to me.,technical
,
"I case of   entry-> total time is zero, why do you show '100' instead of '0'(zero)?I think that it might make the some confusion for user. If it show the '100' in case of ""  entry->total time is zero"", it cannot distinguish between the real 100% utilization and ""total time is zero"".",technical
,
"Good point, I will change it.",technical
,
This driver is quite old and we are not actively using/testing it. Are you wiring that up with qemu? Maybe it should be labeled differently in MAINTAINERS file. Anyway whatever fix is fine for me. Acked-by: Michal Simek,technical
,
"Not specifically; it may be wired up as uninitialized device. Presumably that is why it reports version 0, which causes the driver to bail out. It is instantiated by the device tree file (I think). ",technical
,
"Hi, This crash is now quite persistent in mainline. The fix didn't make it. Should I stop testing virtex-ml507 with qemu ? ",technical
,
"I've applied the fix now. But given Michal's comments, should we kill the driver for 5.2?",technical
,
"If the driver is no longer used or maintained, removing it would indeed make sense.",technical
,
I have no problem with it. ,technical
,
"oof, great bug report by the way!  Thanks for the fix. Note that ""sparse"" designated initialization zero initializes unnamed members: This transform you've done is safe because hexium was zero initialized via kzalloc, and struct hexium contains a struct i2c adapter (as opposed to  a pointer to a struct i2c adapter).  The same is true for both translation units you've touched.",technical
,
"Thanks for the fix! In general, for -Wstack-frame-larger-than=warnings, is it possible that these sets of stack frames are already too large if entered?  Sure, in lining was a little aggressive, causing more stack space use than maybe otherwise necessary at runtime, but isn't it also possible that ""no in lining"" a stack frame can still be a problem should the stack frame be entered?  Doesn't the kernel have away of estimating the stack depth for any given frame?  I guess I was always curious if the best fix for these kind of warnings was to non-stack allocate (kmalloc) certain locally allocated structs, or no-inline the function.  Surely there's cases where no-in lining is safe, but I was curious if it's still maybe dangerous to enter the problematic child most stack frame?",technical
,
"What I think is happening here is that llvm fails to combine the stack allocations for the inclined functions in certain conditions, while gcc can reuse it here. We had similar issues in gcca few years ago, and they got fixed there, but I have not looked at this one in more detail. My guess is that it's related to the bug I mentioned in patch 3. ",technical
,
"Please add it for fwht as well. It makes no sense to have it for this but not the fwht function. Got to say this is all very magic...I think it would be good to perhaps have a comment at the start of the source that explains why no inline for stack is added to selected functions. Patches 1 & 3 are fine, BTW. ",technical
,
"Your SoB is missing for patch 1. And besides my ack, I'd still like some feedback from the actual driver maintainers.",technical
,
Thanks for the feedback Wolfram. I will wait for input from the driver maintainers .,technical
,
Any comments?,technical
,
this is turned on by default for 32-bit. So all you have to do is remove this line.,technical
,
"Thanks a lot for clarifications here! I suspected that it was the case, just needed a confirmation here. Is it possible we would have this config option enabled? If you would agree, then I prepare a separate patch to have it removed. Thanks a lot!",technical
,
"Yes, that's what I was alluding to. ",technical
,
"Nice! I appreciate open-source hardware! Please send the patch via git send-email, it's a bit broken: Applying this, trailing whitespace.error. It's unusual to have uppercase letters here.Is ctu already registered in Documentation?Is this needed or not? Is this needed or not?",technical
,
"I had actually applied this to pci/msi with the intent of merging it for v5.1, but by coincidence I noticed [1], where Jon was basically solving another piece of the same problem, this time in nvme-pci.AFAICT, the consensus there was that it would be better to find some sort of platform solution instead of dealing with it in individual drivers.  The PCI core isn't really a driver, but I think the same argument applies to it: if we had a better way to recover from readl()errors, that way would work equally well in nvme-pci and the PCI core. It sounds like the problem has two parts: the PCI core part and the individual driver part.  Solving only the first (eg, with this patch)isn't enough by itself, and solving the second via some platform solution would also solve the first.  If that's the case, I don't think it's worth applying this one, but please correct me if I'm wrong. ",technical
,
"I think that patches with the pattern ""if (disconnected) don't do IO""are fundamentally broken and we should look for alternatives in all cases. They are fundamentally broken because they are racy: if it's an actual sudden disconnect in the middle of IO, there's no guarantee that we'll even be notified in time. They are fundamentally broken because they add new magic special cases that very few people will ever test, and the people who do test them tend to do so with old irrelevant kernels. Finally, they are fundamentally broken because they always end upbeing just special cases. One or two special case accesses in adriver, or perhaps all accesses of a particular type in just  one special driver. Yes, yes, I realize that people want error reporting, and that hot-removal can cause various error conditions (perhaps just parity errors for the IO, but also perhaps other random errors caused by firmware perhaps doing special HW setup). But the ""you get a fatal interrupt, so avoid the IO"" kind of model is completely broken, and needs to just be fixed differently. See above why it's so completely broken. So if the hw is set up to send some kind of synchronous interrupt or machine check that cannot sanely be handled (perhaps because it will just repeat forever), we should try to just disable said thing. PCIe allows for just polling for errors on the bridges, afaik. It's been years since I looked at it, and maybe I'm wrong. And I bet there are various ""platform-specific value add"" registers etc that may need tweaking outside of any standard spec for PCIe error reporting. But let's do that in a platform driver, to set up the platform to not do the silly ""I'm just going to die if I see an error"" thing. It's way better to have a model where you poll each bridge once a minute (or one an hour) and let people know ""guys, your hardware reports errors"", than make random crappy changes to random drivers because the hardware was set up to die on said errors. And if some MIS person wants the ""hardware will die"" setting, then they can damn well have that, and then it's not out problem, but it also means that we don't start changing random drivers for that insane setting. It's dead, Jim, and it was the users choice. Notice how in neither case does it make sense to try to do some ""if(disconnected) dont do io()"" model for the drivers.                   ",technical
,
"I disagree with the idea of doing something you know can cause an error to propagate. That being said, in this particular case we have come to rely a little too much on the if (disconnected) model. You mentioned in the other thread that fixing the GHES driver will pay much higher dividends. I'm working on reviving a couple of changes to do just that. Some industry folk were very concerned about the ""don't panic()"" approach, and I want to make sure I fairly present their arguments in the cover letter. I'm hoping one day we'll have the ability to use page tables to prevent the situations that we're trying to fix today in less than ideal ways. Although there are other places in msi.c that use if (disconnected), I'm okay with dropping this change -- provided we come up with an equivalent fix. But even if FFS doesn't crash, do we really want to lose hundreds of milliseconds to SMM --on all cores-- when all it takes is a couple of cycles to check a flag? ",technical
,
"My main gripe with the if (disconnected) model is that it's only really good for inactive devices. If a device is being used then odds are the driver will do an MMIO before the pci core has had a chance to mark the device as broken so you crash anyway. What's the idea there? Scan the io remap space for mappings over the device BARs and swap them with a normal memory page? Using pci dev is disconnected() to opportunistically avoid waiting for MMIO timeouts is fair enough IMO, even if it's a bit ugly. It would help your case if you did some measurements to show the improvement and look for other cases it might help. It might also be a good idea to document when it is appropriate to use pci is dev disconnected() so we aren't stuck having the same argument again and again, but that's probably a job for Bjorn though. ",technical
,
Are all versions of the MDIO controller capable of doing C45?    ,technical
,
Now driver support c22 and c45 PHY. Are you suggesting to add check for C45 PHY using is c45 in phydev ? ,technical
,
"You are unconditionally supporting C45. Are there versions of the hardware which don't actually support C45? You have this endless loop: If there is hardware which does not support C45, will this loop forever?",technical
,
"You need a timeout here, and anywhere you wait for the hardware to complete. Try to make use of readx poll timeout() variants.	  Andrew",technical
,
"There is controller which don't support C45. I will add check for that using is c45. Yes, this bit is supposed to be set. I will add timeout here. ",technical
,
"Yes, I will add timeout here. ",technical
,
"You have 3 patches in your series, you need to resend all of them even if there is only one to which you are making changes, this is not documented in netdev-FAQ.rst though, let's update that.",technical
,
"Thanks for quick reply. I am still working on other patches, so I think I will send all of them in another single mail chain.",technical
,
"subject line should be this. And the commit message before should look like: ""Modify MDIO read/write functions to support communication with C45 PHY in Cadence ethernet controller driver. ""You changed tabs to spaces here: please conform to preceding driver's style and other lines of this .h file. Otherwise it looks correct to me on the macb point-of-view. Please correct the little things noted before, re-send independently as a v3 and we also make sure that things are good on the phy side.",technical
,
"Something i asked last time, but I'm not sure i got an answer.  Do all generations of the MDIO controller support C45? If the older versions only support C22, we need to be sure it does the right thing when asked to do a C45 transfer.      ",technical
,
"Ok, I will change subject in v3 of this patch. Ok. I will change spaces by tab. Thank you for comments. I will make above mentioned changes and re-send this patch independently as a v3. ",technical
,
There are some older versions of controller which doesn't support C45. I will make sure that driver return error for C45 transfer requests when it is not supported by controller.,technical
,
"I like your name :) If you post a v2 for any reason, please update the subject like to this. Add support for sdm845 PCIe controller That way it matches the existing conventions and it looks nice.  If there's no need for a v2, Lorenzo will likely fix that up for you.",technical
,
"Thanks for the patch! What means Q2A? It'd be nice to describe it. Is TBU related to SMMU or to something else? please drop the blank line. you should use usleep range, please fix it. check for errors, please.",technical
,
"I'll see what I can do. Yes, the ARM SMMU is split in a centralized controller (TCU) and translation blocks (TBU) for each hardware peripheral. So this is th eclock for the translation block sitting between the two PCIe controllers and the system NOC.The clock is described here, rather than in the SMMU node in the upstream way of representing client-related resources - although we don't use the device link to toggle it in the implementation below. Sure thing, and I'll throw in some defines for the two 2s, per your feedback on the pending patch, will print an error indicating which of the multiple clocks that failed to turn on, so I don't think adding another- more generic - error line will add value. Looking again shows that I missed that regulator bulk enable() does the same thing, so I will align the two by removing above error print.10ms looks arbitrary as well, I'll review this. Of course. Thanks for your review!",technical
,
"Here you are reporting a bug. (More on this below.)Here you are introducing a new behavior. (Also discussed below.)Although the two are similar, they are really 2 different things. Wouldn't the fix to the bug be to move the ""skip"" target here?skip:My only objection to this is that the ""messages dropped"" only comes if anon-supressed message comes. So information about dropped information may never get printed unless some task prints something non-supressed.Imagine a situation where I am expecting a message to come, but don't see it because it was dropped. But if no more non-supressed messages come, I see neither the expected message nor the dropped message.",technical
,
"[..]I think this is exactly the problem (and thus the patch) we discussed some3 years ago. I had a number of rather strangely looking serial logs, which clearly had lost messages but no ""%llu printk messages dropped"" markers. SoI added `static bool lost messages' to console unlock(), set it when printing loop would discover lost messages, then print ""%llu printk messages dropped""attached to whatever msg was next in the logbuf, regardless of msg->level.IOW, if lost messages was set then suppress message printing(msg->level)was not even invoked. Yes, that would sometimes print several ""debugging noise"" messages, but the main part was that I would have ""%llu printk messages dropped"" markers in the logs; which was much more important to me.P.S. I'm very sorry, I'm overloaded with work at the moment; will start     looking at pending patches in a day or two, or three, or four...	",technical
,
"I guess you are referring to this [0] thread. I would agree with the proposed solution from 2016. My experience is that the dropped messages are very important. Yes, printing them could lead to the loss of even more messages. But still, it is important information that needs to get out.",technical
,
"Yes. No, I am replacing random behavior with a predictable one to fix the bug. The above paragraph explains why the fix looks like it looks. Maybe I should have written something like: A solution would be to print the warning regardless the log level. But it might cause loosing more important messages because of delay caused by the warnings. A better solution is to count all dropped messages until there is a non-suppressed one. Then we could print the summary together with the message. No, the entire loop skiping suppressed messages is done under the logbuf lock. No old messages can be lost inside this loop. Good point! There is a simple fix for this. We could print the warning also when all messages are proceed and we are about to leave the for-cycle.",technical
,
"Right. Yes, printing out messages does take time. But I think it's easier to start losing messages due to pre-emption under console sem than due to call console drivers() latencies. I'd agree. A summary ""you lost %d messages somewhere between current and previous messages"" is surely better than what we have now, but is still a bit less informative than ""you lost %d messages just now"".	",technical
,
Yes! That is the piece that was missing!,technical
,
"I'd prefer to have lost-messages reporting be less of a summary and more of an ""error"". I think the sooner we report it the better. I don't think that the time we need to print lost-messages on the consoles is significant enough to worry about it; if call console drivers() becomes such a problem that any extra char we print causes message-drop then we can disable printk time or/and printk caller id prefix printout (can save us more CPU cycles).",technical
,
Any comments on this set? ,technical
,
I've tested this patch and it gets rid of a slew of these message.,technical
,
Thanks but please do not top-post when replying to mails on public mailing lists,technical
,
Please don't do silly things like that. I fixed it up now.,technical
,
Thanks. Sorry I missed that.-,technical
,
Already fixed,technical
,
"Awesome. :)I was looking for that link, but I didn't find it.  That's why I sent this patch. Happy to see it is fixed now. ",technical
,
"Although this fixes the warning, i suspect there i something wrong with the original patch adding mv88e6390x port set cmode(). It should also be used without CONFIG NET DSA LEGACY.     ",technical
,
I checked the commit by Florian. Do you mean that CONFIG NET DSA LEGACY shall be removed completely? :-),technical
,
"No, i suspect it is being called from the wrong place, or needs to be called from a second location.[Goes and looks at the code] Yes, it should also be called in the probe. I would call it just after the call to mv88e6xxx detect(), so that it is the same as in mv88e6xxx drv probe(). There are two ways DSA drivers can be probed. The legacy way, which is optional, and is slowly getting removed, and the current way. Heiner is new to DSA and probably missed that, and only handled the legacy probe method. I also missed checking when i reviewed to patch :-(   ",technical
,
"Right, I missed that, will submit a fix. I just saw that the Kconfig entry comment for NET DSA LEGACY says:"" This feature is scheduled for removal in 4.17."" Was forgotten to remove it or did somebody scream loud enough ""But I depend on it"" ?",technical
,
"The intent was to remove it by that kernel version but the 88e6060 driver still depends on it, and there appears to be some active users that Andrew worked with.",technical
,
"I see, thanks. And migrating this driver to the new DSA framework version isn't possible or not worth the effort?",technical
,
Andrew is working on it actually. We could have 88E6060 select NET DSA LEGACY and drop legacy probing from mv88e6xxx as an in between solution.,technical
,
"Ok, I didn't check it carefully. Hmm, I am not familiar with it,  please feel free to fix it",technical
,
All files being opened by the kernel should be calling one of these helper routines.  Has that changed? ,technical
,
"prepare binprm() uses kernel read() and has done since at least 2014.  The binfmt drivers also use kernel read(). Since kernel read file() is used by a bunch of things that aren't exec, even if we switch exec to it, it should probably still go in fs/read write.c since it seems generic.",technical
,
Oh this commit moved kernel read() to fs/read write.c without moving the helpers. Definitely makes sense to move the helpers.  Please include a reference to the commit in this patch. r,technical
,
"These look good to me. Perhaps they can go via -mm, maybe with Eric and/or Peter's ack/review? I'd really like to get these done so we can get closer to finishing the refcount t conversions…",technical
,
"Some of these patches probably fix issues, like making PASID work correctly on VFs.  But that needs to be made explicit in the commit logs. The current commit logs read more like ""make XYZ follow the spec"", and that's not really what we need to know.  The commit log needs to tell us why we need the change, not just what the spec says. For example, maybe VFs can't use PASID because Linux incorrectly tries to use the PASID capability on the VF when it should be using the capability on the PF.  The commit log should say that explicitly and also say what the current behavior is, e.g., does it cause IOMMU faults, does it cause data corruption, does some DMA mapping interface called by the VF driver fail when it shouldn't, etc?",technical
,
Is there something need do for this patch? Pls let me know. I saw the patchwork status labelled to  'Not Applicable',technical
,
"I am afraid of this change. Besides the rate there might be other reasons to choose one mux input over another, consider for example low power audio playback where we need one specific mux setting because it provides a clock which runs at low power mode. On the IPU on i.MX5/6 there are clocks being used as pixel clocks derived from different muxes. I don't think you want to pick an input clock just because it happens to deliver the best clock rate at that point in time, but really is shared with some other clock that changes its rate in the next moment. I have no concrete examples for things that break with this change, but I would be more confident if we change the behaviour explicitly only for the muxes that we have reviewed to cope with this change. ",technical
,
Fair enough. We could replace all the imx clk mux with imx clk mux noreparent and after that we can independently switch the clocks that are safe (to switch) to imx clk mux (which would not have the noreparent flag set).The main idea is to simplify the clock control from drivers point of view. The end goal here would be to only have noreparent flag set for the clocks that need specific mux control.,technical
,
"Ok with me.Which drivers do you have in mind? I hardly ever missed reparenting on rate changes, so where is this feature useful?",technical
,
"Reparenting is very useful for Audio IPs (SAI, SPDIF, PDM, FSL DSP).We often need clock rates that are multiples of 8000 or 11025 which can be obtain from different PLLs. But in order for this to work we need to have the correct parent. In our current tree we do the re parenting manually inside drivers. For example: This is far from optimal. Abel's patch will be of a great help for Audio. ",technical
,
"Take for instance this one (.config attached); it has both and it compiles: Which, afaict is correct given the asm, but is absolute nonsense given the original C. If you follow that code path, it appears to do the memops without STAC, and then complains it does CLAC. Which is of course complete crap. Maybe I've been staring at this too long and am (again) missing the obvious :/",technical
,
"No, this is a hypercall with parameters passed through from user land(Xen tools). So setting AC=1 is not related to hysterical reasons, but to indicate that unprivileged buffers are okay here. So please change the commit message to something like: Some Xen hypercalls allow parameter buffers in user land, so they need to set AC=1. Avoid the warning for those cases. Mind Cc-ing the maintainers of the code you are touching in future patches? With the commit message adjusted: Reviewed-by: Juergen Gross",technical
,
"*yuck* that's gross...And I understood from Andrew yesterday (but perhaps I misunderstood) that the whole privcmd thing was a bit of a hack. But sure, I'll change the message. Sure; my bad, I forgot to ask get maintainers about this.",technical
,
"That was an unwise design decision in the past, yes. Changing it will require a lot of effort. Thanks. :-)",technical
,
"Bye-bye STAC, jumped straight over. We do not take this branch and fall-through. Fault, take exception:  ",technical
,
"And that is the error, I think. We should've taken it and went to:  return -EFAULT; because: is an unconditional code sequence, but there's no way objtool can do that without becoming a full blown interpreter :/",technical
,
"How about just turning off SMAP checking for the really odd cases? At some point it's not worth worrying about excessive debug infrastructure, I think. Just give up and say ok.. Or just make preempt schedule() check AC and not schedule. It already does that for IF.                   ",technical
,
"It's certainly one of the safer functions to call with AC set, but it sounds wrong anyway. It's not like it's likely to leak kernel data(most memset's are with 0, and even the non-zero ones I can't imagine are sensitive - more like poison values etc).What's the call site that made you go ""just add   memset() to the list""?       ",technical
,
"This ""fixes"" it, and also seems to help -Os make much code:",technical
,
much *smaller* code,technical
,
"Yeah, considering that this   trace if() macro from hell is doing an 'if()' on the result of that inner thing, it makes sense to *not*  use that ""looks simpler and shorter"" array sequence, but depend on the compiler then noticing that the conditionals are the same and joining the two branches together. The compiler has to do much more work to either generate the actual dynamic array thing, or notice that the  index  of the array matches the  conditional  on the branch, and undo that thing. But that macro really is the macro from hell regardless. Do people really use CONFIG PROFILE ALL BRANCHES?                ",technical
,
if (cond)?  Or is    r used elsewhere?,technical
,
Agreed... it seems fishy at least.,technical
,
"Yeah, agreed.  Now it doesn't have to do the funky xor thing to convert the conditional to an int.IIRC, Steven runs it once a year or so…",technical
,
      r is also the return value.  And it's needed because cond should only be evaluated once.,technical
,
So put a true; and false; inside the if.,technical
,
"That's I don't know, that mostly just gets in the way I think. Also; it seems to me that something PT, or maybe even simply, this would get you similar or sufficient information.",technical
,
Excellent; let me put the kids to bed and then I'll have a poke.,technical
,
"Yeah, I'm not really seeing a lot of upside to PROFILE ALL BRANCHES. Particularly since it doesn't actually profile all branches at all. It only basically profiles ""if ()"" statements, which obviously misses loops etc, but then also  does  hit things where people turned loops into, which happens in (for example) low-level locking code etc that often has a fast-case ""first try ""thing followed by a slow-case ""ok, let's loop for it"" thing. So I think PROFILE ALL BRANCHES tends to have very random coverage. I'd love to get rid of it, because it seems so random.    ",technical
,
Is that possible to do in a C macro?  Doesn't seem to work for me…,technical
,
"Here's a proper patch. Subject: [PATCH] tracing: Improve ""if"" macro code generation With CONFIG PROFILE ALL BRANCHES, the ""if"" macro converts the conditional to an array index.  This can cause GCC to create horrible code.  When there are nested ifs, the generated code uses register values to encode branching decisions. Make it easier for GCC to optimize by keeping the conditional as a conditional rather than converting it to an integer.  This shrinks the generated code quite a bit, and also makes the code sane enough for objtool to understand. ",technical
,
"The meat of that macro could easily be done as a helper inline function. But as mentioned, I think a better option would be to remove it entirely, if at all possible. The best part about that config option is the comment, and while cute I don't think that's really worth saving it for...               ",technical
,
"Ugh. I think I almost just agree with your decision to just let that memset go unchecked. I'm not saying it's right, but it doesn't seem to be a fight worth fighting. Again, maybe we could avoid the static checking entirely for the complex configs, and just make preempt schedule() not do it for AC regions. Because AC vs KASAN in general ends up smelling like ""not a fight worth fighting"" to me. You've done a herculean job, but..                 ",technical
,
Let's move it instead.  I looked at the code and it should be fine.,technical
,
"As Josh said, I run it once a year on two production (real use)machines for 2 to 4 weeks and collect the data to see if there are places that can be optimized better. I currently have one of my engineers looking at the data and may be sending patches soon. It's basically an entry level way to get into kernel development. Note, no patch will be sent just because of the data from the profiling. The task is to look at and understand the code, and see if it can be optimized (with likely/unlikely or flow changes). It's a way to get a better understanding of the kernel in various locations. It is by no means ""profiler said this, lets change it."" All changes must be rational, and make sense. The profiler is only used to help find those places. The data that was run at the end of January can be found here",technical
,
"One think I could do; is add a filter to each function and only allow  memset from the kasan code, and not from anywhere else. Another thing I need to look at is why objtool only found memset orig(from   memset) but not memset erms, which if I read the code right, is a possible alternative there. I know,.. I've been so close to doing that so many times, but it seems like defeat, esp. since I'm so close now :-)",technical
,
   memset kasan()?,technical
,
Something like so; or did I miss something subtle?,technical
,
"Ah.. how about I feed objtool a text file with all these symbol names; and I have Makefile compose file that from fragments. Then only KASAN builds will have memset whitelisted, and any other build will still flag memset abuse. Now I only have to figure out how to make Makefile do something like that :-)",technical
,
"Also, this stuff is pretty well covered by the x86 self tests, mostly because getting it right in the first place was way too subtle for comfort.",technical
,
"These two can be called only with CONFIG KASAN EXTRA=y which was removed very recently, so it should be safe to delete these functions.",technical
,
"Turns out we only look for sibling calls in the original instruction stream, not in any alternatives; which in general seems like a fair enough assumption.",technical
,
Ooh shiny. Clearly my tree still has them; what commit do I need to look for?,technical
,
"And while I'm looking at memset 64.S, why are memset erms and memset orig global functions? At the very least they should be local, and ideally not even functions.",technical
,
"""kasan: remove use after scope bugs detection.""",technical
,
"Instead of adding all those additional moving parts, I would much rather either: a) have kasan call a special whitelisted version of memset (like hpa   suggested); orb) just don't use the objtool --uaccess flag for KASAN builds.",technical
,
"I think the only benefit is that they would show up better on stacktraces, but that could also be solved by just making them local labels inside memset.  Which is what I think they should be. The general rule is that ENDPROC is only used for callable functions, so yeah, I think the current setup isn't ideal, and also prevents objtool from properly doing the AC analysis as you pointed out earlier.",technical
,
"The problem went away. That is, the problematic part of KASAN just got removed in this merge window. Also; I just about had hpa's   memset kasan implemented when I got that email.",technical
,
"Boris wanted to use alternative call 2, just like copy user generic(). Which makes more sense to me still.",technical
,
"This is an unstructured piece of code rather than a callable function, END would probably be more appropriate.  Or maybe it should just be a local label (.Lcopy user handle tail) because I don't think the alignment and ELF symbol size are even needed.",technical
,
Shouldn't these be using SMAP-based alternatives like stac()/clac() do?,technical
,
Also this you could get rid of the comment if there were an ANNOTATE AC SAFE macro which does a similar thing.,technical
,
Can you also say  why ?  I presume it's so the warning messages will be saner.,technical
,
Also you can clarify why we need to track aliases?,technical
,
We should have done this a long time ago.  Thank you for this!,technical
,
ENDPROC makes it STT FUNC and gets us stricter AC tests.,technical
,
"Right; because I'll introduce function attributes later on, so it becomes important to know what function an instruction belongs to. I'll update the Changelog.",technical
,
"Whoops :-) Function attributes (as per that previous patch) and: git grep ""alias ""mm/kasan.",technical
,
How so?  I would have thought the opposite.  Doesn't objtool only follow a jump if its destination is to a non-function?  Otherwise it's considered a sibling call.,technical
,
"Ok, such details would be good in the patch description.",technical
,
I like this approach.  I wonder if we can do something similar to get rid of the nasty fake jumps,technical
,
and we need to know who we're calling because...s/class/call/ I would rather have 2-3 duplicated lines of code than complicating the control flow like this.,technical
,
"Style nit, no need for all those brackets and newlines. A short comment would be good here, something describing why a function might be added to the list. This won't work if the function name changes due to IPA optimizations. I assume these are all global functions so maybe it's fine? These goto's make my head spin.  Again I would much prefer a small amount of code duplication over this. Why is this a problem? Same question here.--",technical
,
"Needs more description -- namely, what and why.",technical
,
Can you elaborate on why (in the patch description)?  Did this actually find any occurrences?,technical
,
"There is; but I'm thinking it might be too short? Those are gone. That is the main kasan report function, and is for, as the comment says: kasan :-) These, are, again as the comment suggests, the out-of-line KASAN ABIcalls.The in-line KASAN ABI Also, can I just say that this bugs the hell out of me? the logger function KCOV ABI implementation UBSAN ABI With one or two exceptions, yep.I didn't think the code was that bad once you see the end result, but sure, I can try something else. 'obvious' violation? If we do the STAC we must also do the CLAC. If we don't do the STAC we must also not do the CLAC. In general, validating NOPs isn't too interesting, so all NOP/INSN binary alternatives could be forced on. I've actually changed this to depend on --uaccess, when set we force on FEATURE SMAP, otherwise we force off.",technical
,
"Nope, didn't find anything. Also, all DF users are in asm so I didn't really expect any. Having it escape would probably result in fairly instant wreckage though. DF=1 results in things like rep mov going  backwards .",technical
,
This actually hard relies on those fake jumps. Or am I missing the point?,technical
,
"I actually just meant a comment above uaccess safe builtin describing what the purpose of the list is and what the expectations are for the listed functions.  i.e. these are functions which are allowed to be called with the AC flag on, and they should not clear it unless they're saving/restoring. Makes sense now, can you add that last sentence to the paragraph? Right, but it doesn't sound like there's any real benefit to adding extra logic. Ok.",technical
,
"Ok, I wonder if we really need to add this then.",technical
,
"I was just wondering out loud if we could somehow keep the same ""fake jump"" functionality, but do it in a cleaner way that doesn't require creating fake instructions.  I'll might give it a try.",technical
,
something like so then?,technical
,
"See patch 8. This really should be folded back in the previous patch, but it is easier to look at without all that mixed in.",technical
,
"Well, Linus asked for it, and it was a fairly trivial add-on :-)",technical
,
Can't you just have those same engineers look at perf data? This seems like a very expensive and convoluted way of getting something.,technical
,
How about we just rename the one annotation we have? I figured it was a waste of LoC to do yet another annotation that does the very same thing.,technical
,
"Normally yes, but we don't do that for .fixup I think. And by setting STT FUNC we enable the 'redundant CLAC' warning, which is ignored for !STT FUNC.",technical
,
"I think ANNOTATE IGNORE ALTERNATIVE would hurt readability too.  How about just see this or, make both ANNOTATE UACCESS SAFE and ANNOTATE NOSPEC ALTERNATIVE defined to the same OBJTOOL IGNORE ALTERNATIVE macro. ",technical
,
"If it doesn't matter much either way, I'd rather err on the side of less code.  Your call though. ",technical
,
"Yeah, that looks a lot nicer to me.  Thanks. ",technical
,
"Commit-ID:  tracing: Improve ""if"" macro code generation With CONFIG PROFILE ALL BRANCHES=y, the ""if"" macro converts the conditional to an array index.  This can cause GCC to create horrible code.  When there are nested ifs, the generated code uses register values to encode branching decisions. Make it easier for GCC to optimize by keeping the conditional as a conditional rather than converting it to an integer.  This shrinks the generated code quite a bit, and also makes the code sane enough for objtool to understand. ",technical
,
"I haven't tried the perf data. How well does it work with running over a 2 weeks to a month period? That's what I do yearly. Here's the results of my last run: I have a cron job that runs nightly that copies the current state, and if the machine reboots, it starts a new file (which is why there's multiple files for mammoth - it rebooted). I've never used it, so I have no idea if it is suitable or not. But is it a real burden? It's been in the kernel for over 10 years with very little issue. Only when we do something drastic does it showup, and it's usually a quick fix to get it working again. I believe Josh even told me that it found a bug in the objtool code, so it does still have benefit staying in the kernel even without people using it for profiling. Note, I'm in the middle of writing a LWN article about learning the kernel from branch profiling and it would be a shame if it disappears before I finish it.",technical
,
"They are,   ASM {ST,CL}AC are the raw instructions ASM {ST,CL}AC are the alternatives.",technical
,
"Hello After thinking about this dt header I am not happy about having it. I am thinking of removing this file completely and doing some calculations in the driverfile for the impedance, averaging time and ramp times. I would still like to get comments on the rest of the code but in v2 the dt bindings header will go away.",technical
,
"Hi! I guess dt people will have some comments here. I'd expect ramp-up-us = <1024> would be more natural. Ok, so we'll have lm3532::backlight. That is not too useful, as it does not tell userland what kaclight it is.main display::backlight ?OTOH this one is not too important as backlight subsystem should handle this. I guess best variant would be inputX::backlight here, but that might be tricky to implement.				",technical
,
Thanks for the review. Actually ramp-up-us/ramp-down-us is more correct this is an error in this dt definition and will be fixed in v2For Droid4 I am not particular to any specific label. And if the series in is ever implemented then the label may change as well. The driver forces the lm3532 string to be part of the label. This was a discussion a while back with Jacek when I submitted other drivers.Yeah because we don't know what input the keyboard would be on. Unless there are APIs to retrieve that info.  I have not looked at the input framework in a while.,technical
,
"Lets just make it ""platform::kbd backlight"". *:kbd backlight is already used by quite a few drivers.",technical
,
Sounds fine to me.  I will update in v3 as I posted v2 with a lot of deltas ,technical
,
"Thank your for the patch. I have some comments below, please take a look. If control bank was validated in lm3532 parse node(), then you wouldn't have to take into account this option here. Besides, you don't need this switch statement, but can easily calculate. Ditto. Similarly here. use following macro. mutex destroy() is missing here. This should go in a separate patch, with DT bindings - see checkpatch.pl complaint.",technical
,
Thanks for the review.  I have pushed v2 with some code changes but those changes that were made seem to be outside your comments. So I will implement these comments in v3.Ack.  Probably don't even need the macro for this. Same as above Ack. Probably don't need a macro since it is only used once.  Ack I have removed this in v2 of the next patchset.,technical
,
"One more thing I forgot to mention before.[...] Let's have ""//"" comments here.",technical
,
Awhile ago...commit checkpatch: allow c99 style // comments    Sanitise the lines that contain c99 comments so that the error doesn't    get emitted.    ,technical
,
"The general form uses a space after the //And much more recently commit is specified for various file types, in Documentation so add an appropriate test for    .[chsS] files because many proposed file additions and patches do not use    the correct style.",technical
,
Thanks JoeI guess I was referring to this SPDX warning WARNING: Missing or malformed SPDX-License-Identifier tag in line 1#1.,technical
,
 Like I said earlier in v2 this header went away. And checkpatch takes issue with // in headers. Unless they have removed that requirement.,technical
,
iput while holding spinlock is not good I don't understand this comment. no matter the 'retry' parameter is true or not/maybe we should  distinguish them.,technical
,
"Yep, that's definitely not a good idea.  And maybe this loop could even race with a lookup currently in progress and a new inode could be added to the list after the cleanup.  I didn't verified this race could really occur, because an easy fix is to simply move this loop into ceph mdsc destroy() where we don't really need the spinlock anymore.<snip> Right, maybe the parameter name and the comment are a bit misleading. Get quota realm may need to do an inode lookup and then retry to get the quota realm.  If this lookup is required, it has to drop the rwsem. However, in ceph quota is same realm we need to lookup 2 quota realms ""atomically"", i.e. with the rwsem held.  If get quota realm needs to drop it, it will do the MDS inode lookup anyway but instead of retrying to get the quota realm it will return -EAGAIN (because 'retry' will be 'false').  This allows for ceph quota is same realm to restart the operation itself, and retry to get both quota realms without get quota realm dropping the rwsem. Does it make sense?  I agree the design isn't great :-/ I tried to describe this behavior in get quota realm comment, but it's probably not good enough. Sure, that makes sense.",technical
,
Can someone review this patch?,technical
,
"Patch looks good to me.Terry, did you have time to review it? ",technical
,
"Actually, the good thing of having a test suite, is that it raises bugs :) You are also missing the computation of the usage in hid scan main().This makes the autoloading of hid-multitouch fail, and thus the test suite failing. ",technical
,
"Thanks for the review! Ok, I'll look into it and send a follow-up.",technical
,
I believe it is Bosch M-CAN Revision 3.2.1.1,technical
,
"in the previous email you said that it uses:  Bosch M-CAN Revision 3.2.1.1 This means this, hence it's using FIFOs: I think this frees the skb twice! Also, the index depends on version! If > 30, the last used index should be selected. What happens to the ""tx skb"" if this functions returns with NETDEV TX BUSY? Will it be freed? You stop the queue immediately, even if fifos are used. Is this by purpose? .",technical
,
It appears so.  I will fix it up.OK I will need to look into that. I am missing the change I put in to free this. It must have been lost when rebasing the patch setI will add it back and fix this up Good catch.  I should let the work queue handle this.  I see that < 3.0 stops the queue immediately but > 3.0 does not. I don't know what version of core the hi or mcp parts are using it looks to be 3.0. I believe that's where I pulled the stop from. I will remove it.,technical
,
I'm sorry you're going to have to break this up into multiple patches. Probably one for each item on your list. ,technical
,
"No problem. In a previous patch I had one for each item, but I thought it could be packed in a single one - and avoid '[PATCH n/m]'. Thanks.",technical
,
I'm not sure all that remaining is worth it to be honest. It adds a lot of noise for no particular reason (and the same goes for renaming the file itself).,technical
,
"Maybe keeping names is okay ""for historical reasons"". In fact I want to keep them.",technical
,
"Thanks for working on this, I was actually close to submitting similar patches for V3 support! I just reviewed the definitions and found a mistakes about the LVDS function (that should be 0x3 instead of 0x2). Otherwise, things look good and match what I had came up with. ",technical
,
"Hi :My two cents about this: kernel development is plagued by the inability to rename and rework things as soon as backward compatibility is involved. I believe that renaming and reworking things is quite a good thing to do when it leads to a situation that is easier to understand and makes more sense. In this case, I don't see any blockers that would prevent us from doing this, so I am strongly in favour of it. I really don't see how increased noise and ""historical reasons"" make up for better clarity. ",technical
,
"It simplifies the git history, for once, which has the side effect of reducing conflicts too. A second one is:  Do you prefer to review patches that have some significant value (like a new feature, a bugfix, a new SoC support, etc) or one that renames files and / or symbols ?",technical
,
"Hi: Note that the V3 has a NMI controller at 1c000d0, that is required for handling the AXP209 interrupts IIRQ. I have no idea whether it's also the case on the V3s/S3/S3L though but it would be good to know. Note that I can totally add support for it when adding support for myV3 device that uses the AXP209 this way. Also, could we get proper compatibles and config options for these new SoCs, since they are distinct from the V3S? ",technical
,
"It's not mentioned on the datasheet. If it's present, please send a patch.BTW all V3 series chip share the same die (sun8iw8).Thanks",technical
,
"Hi ,Indeed, it is not documented but the block is definitely there (and it shows up in All winner's kernel source too). I'll send a patch once these series is merged then! Right, so I think it's safe to assume that the controller is there on all of them then. ",technical
,
,
Seems like a bunch of duplication for just 2 differences in clocks. Can't you keep the definitions the same and just skip registering the clocks not present? Perhaps reword as 'Clocks only on V3',technical
,
"I'd rather not, this can lead to access to registers that might not be there when the CCF will read / write that clock ",technical
,
"I see there is some review comments on this patch set so I am waiting for v2.When we have something Maxime, Rob etc has ACKed, I suggest I merge the pinctrl stuff into an immutable branch in the pinctrl tree so that it can be pulled in to ARM SoC if need be (for DTS files to compile for example).",technical
,
No need to put this in the changelog text at all. Please fix up and resend. ,technical
,
"OK.	",technical
,
Good catch.If we do that we need to set 'err' to an error code here [1] and here[2].  Do you think you could fix this and roll another version of your patch?,technical
,
"Next time please submit two patches, one for the PTR ERR and another for not throwing away the err = -E!INVAL and returning just -EINVAL, I'm doing it this time.",technical
,
Please hold off on that - I've asked for other modifications to be done on this patch.,technical
,
"Do you really think that it is necessary to hold? The fixes are trivial and I already have them split and applied, see them below, I think whatever other changes can be done in further patches, no?",technical
,
"Proceeding with the patches below would created two new bugs, hence asking Yue for modifications.",technical
,
"Sorry for late, then I can send a new patch to fix the two new bugs.",technical
,
"Ok, I'll take your word on it then, dropping the patches.",technical
,
Some stray whitespace in here. I cleaned it up. ,technical
,
"hostfs now mostly works.  Almost all of the common operations are now    implemented, the main exceptions being mknod and executing files from    a hostfs filesystem. Enough archaeology for today. :-) ",technical
,
Nice. How do you find these ancient git commits?,technical
,
"Hi,  Colin is obviously right with that. But my guess is that the error occurred because the pattern (from, to) is broken here. Also Maybe the maintainer can fix that. just my 2 cents",technical
,
"This commit is from the old UML cvs tree. I did a import to git some time ago and pushed it to: For classic pre-git stuff, check:",technical
,
"Hello,[I put Thierry into To: because some remaining questions depend on his views]The situation here is as follows: The actual period length calculates as. Consider a clk rate of 600 MHz, then the driver maps ""requested period ""to ""actual period"" as follows: There is an obvious rounding issue: If 218452 ns are requested, we should end in the scale = 1 case for sure. (Similar issues exist for the other cases.) And then there are cases that are not that clear: What if 218000 ns are requested? Where should the line be drawn? Thierry? And what about long periods? The longest actually supported period length is around 3.5 seconds. What if a consumer requests 18 seconds? Where should the line be drawn when the driver is supposed to return-EINVAL (or -ERANGE)? Thierry? Starting with this write the new period length might be active with the previous duty cycle. Is this worth a comment? I think the window where this can actually happen should be made as small as possible, so it would be great to first calculate both register values and then write them in two consecutive writels.What about rounding here? I'd say use ""round closest"" instead of ""round down"".If the PWM was configured for this, get state still returns .duty cycle =0. Is this acceptable? Space after : please. Also applies to the other error strings. I think it would be more common to call the struct pwm device pointer ""pwm"" and the struct pwm sifive ddata pointer ""ddata"".""dev"" is usually a pointer to a struct device. The other functions need the same adaption of course. Here is another rounding question. Given that the period length can only be modified by factors of two there are cases where the real period is off by a factor of at least 1.4 which has an effect on the duty cycle. Consider again an input clk rate of 600 MHz. We either have to go for real period = 109226 ns (as is currently implemented) or with 218452 ns. Which one should be chosen I already asked above. Here the question is (probably depending on the former question) how should the actual duty cycle be calculated? This goto is a noop and so can be dropped. Is it a problem when the notifier is called before pwmchip add was called? Out of interest: Is a real problem addressed here? I.e.: Doe sthe input clock actually change in practise? Also note that pwm sifive clock notifier only adapts the period but not the duty cycle (any more).Given that a clk rate change affects the output, I wonder if the change should be declined if the pwm is running. If the bootloader setup a display with a backlight driven by a PWM it would be ideal to not modify the already running hardware here. Here is another consumer API function call. I think you're leaking a clk enable here. The probe function does one unconditionally that is never undone. ",technical
,
"The comment for this scenario has already been mentioned under the limitation on top of this driver. Anyway, I will try to implement your suggestion of consecutive writes Sure Ok. Will change that Sure Ok, will drop this initialization. Will avoid using it.Will use this instead",technical
,
"Yeah, the comment at the top is for general information about the shortcomings. The comment here would be to say: The problem occurs *here*. ",technical
,
Are you sure this can happen? I don't think this is possible if client->dev.of node is non-NULL. The 'if (client->dev.of node)' above should see to that.,technical
,
"We would not even enter this path without matching compatible, so I think a check here is not really necessary. ",technical
,
"I'd suggest using the username ""fsgqa2"" so that other tests that want to use a second username can do so using a more logically name username.       	 ",technical
,
"This isn't a ""test names with digits"" test, but ""test names beginning with digits"" test.  Changing the username to not begin with digits invalidates the entire purpose of the test which is to ensure that xfs quota can differentiate between UIDs and names beginning with numbers....So from that perspective, NAK. IF there are distros not allowing usernames to start with digits, then this test needs a  requires check and to  not run on those systems.",technical
,
"These should not run automatically if you don't have an external log device configured. Every test should either work with an external logdev or explicitly not run them, so I'm not sure what you're trying to achieve here....This change also looks wrong because This test requires a valid test. this is the external logdev test, and it is a duplicate uuid mount test that has nothing to do with external log devices. And, FWIW, we already have a ""log"" group to indicate tests that exercise the log, and that mostly includes all the tests that use external logs. It would be better to tag all the tests that exercise the log with ""log"" rather than create some new group that doesn't really provide any added benefit",technical
,
NAK. this should be automatically detected by the tests and they not run when support is not available. ,technical
This already  not runs on new systems. we do not want to be adding one-off group descriptors to avoid tests like this - the tests themselves already detect whether they should run or not.xfs/096 2s ... [not run] Requires older mkfs without strict input checks: the last supported version of xfsprogs is 4.5.Why are you trying to add groups to define tests that not runcorrectly on systems they aren't supported on?,technical
,
"The way I'm splitting up tests is one first run with a basic xfs section on a configuration file, with no external log, which pretty much runs all tests but excludes all which require external or funky configurations. A secondary pass then goes through these extra groups and then runs tests only for the previously excluded groups but with their own respective section. So for instance in this case I have. Automatic detection if the requirements are met is fine, but this doesn't let me easily use say. I see...So for my case would one better goal be to just run check without the external one and one with the external log? ",technical
,
"Which seems to me like a misguided attempt to optimise test runtimes. i.e. this doesn't provide test coverage of external log behaviour in all the cases that need to be tested. Data integrity code paths are affected by having an external log. IO ordering changes with external logs, which can expose update/crash recovery problems. external logs can expose data IO race conditions that are masked by interleaved log IO. etc, etc, etc. You can't just run an internal log test then add couple of extra external log tests and say ""external logs work fine"". You can do that if we ignore the fact that a large number of tests need to be run on both internal and external log devices to cover the differences in behaviour between them. See above. Your test coverage assumptions are wrong, so what you are trying to do really doesn't tell you whether external logs work correctly or not. It's worse that not testing external logs at all,  because it gives the false impression that they have been exhaustively tested and work just fine when that really isn't the case. ",technical
,
"This looks fine to me. The only system I have by hand that have both but not any is openSUSE Tumbleweed. Without this patch, dbtest was not built on openSUSE, and was built successfully with this patch applied. And dbtest is still built on RHEL6/7 and Fedora. BTW, I'll queue patch 3 and this patch for next fstests release, while other patches seem not necessary, I agreed with Dave that groups are not for excluding tests, the required tools and environments should be detected by tests and  not run if not met. (The README change looks fine, but it doesn't apply due to the ""fsgqa-381"" change, so I drop it too for now.) ",technical
,
"Makes sense, thanks.  ",technical
,
"Indeed, openSUSE and SLE releases. Yeap. Feel free to modify the commit log accordingly then. Curious, what packages does Fedora/ RHEL6/7 use for the requirement here? We just have one: I think patch 2 is fine too. Yeah makes sense now. I think we should also document when adding a group makes sense as well. Feel free to modify it, its not a big deal.  ",technical
,
"gdbm-devel too, but it has this pointing, so there's no such problem and dbtest is building normally OK, I'll modify on commit, thanks! ",technical
,
"Hi guys -This change breaks on older releases like SLES 11 where both define datum, so we get build failures.  The failure is new, but not because it used to pass and now doesn't.  It's apparently never built on SLES releases since we ship it and then we not run the test that uses.  Now that we're looking for gdbm.h and find it, we attempt to build src/dbtest and fail. This fix isn't the right solution.  The problem is that we have a couple layers of old cruft that needs to be cleaned out.1) As Luis notes, nothing sets HAVE GDBM H.  The thing is that there is no version of gdbm.h that exports the NDBM interface.  Further, looking at the git history, nothing has ever set HAVE GDBM H.  It was dead code when it was committed initially as best I can tell.2) openSUSE Tumbleweed doesn't need <gdbm.h> at all.  It needs <ndbm.h>and this fix works because Luis added it to the one sed to check for <ndbm.h> but it was a check for IRIX and the caller was removed ages ago.  It wouldn't matter if it were called anyway since libndbm is an IRIX library.  Linux, IIRC, has never shipped one. I'll post a few patches following this to clean it up and get it working on SLES11.",technical
,
"What do you mean by 1st and 2nd level? Pretty much. (3) is true in the sense that memory is first allocated from hostmem resource (which is non-dom0 RAM). Not anymore, as far as that particular commit is concerned, but that's because of this commit (Move and shrink AMD 64-bit window to avoid conflict) which was introduced after balloon patch. IIRC there were some issues with it unrelated to balloon. The concern is that in principle nothing prevents someone else to do exact same thing commit did, which is grab something from right above end of RAM as the kernel sees it. And that can be done at any point.",technical
,
"Ah, OK. Doesn't additional memory resource()->insert resource(iomem resource) place the RAM at 1st level? And if not, can we make it so? Since this seems to have broken existing feature this would be an option. But before going that route I'd like to see if we can fix the patch. I have been unable to reproduce your problem. Can you describe what you did? I am not sure I agree that this is plainly wrong. If not for BIOS issues that 03a551734cf mentions I think what the original implementation offa564ad963 did was perfectly reasonable. Which is why I would prefer to keep keep the hostmem resource *if possible*.",technical
,
"That'd mean splitting ""Unusable memory"" resource. Since it's allocated from bootmem it has proven to be quite difficult but there are seem to be special functions available particularly for memory resource management operations that I've not yet experimented with. So the answer is probably - maybe yes but not straightforward. It doesn't happen on all configurations as sometimes the memory is successfully hot plugged to a hole depending on the size of Dom0 memory. But we reproduced it quite reliably with small Dom0 sizes like 752MB. XenServer is using this feature to hot plug additional memory for grantable operations so we started a VM and observed a stable hang. Exactly, those *are* BIOS issues and are not supposed to be workarounded by the OS. And as the next commit showed even the workaround didn't quite helped with it. I agree that having hotmem as a precaution is fine but only if there is a non-cringy way to keep things working with it which I'm not sure does exist.",technical
,
"We have most of the interfaces in the resource framework to do what we want. I put together a semi-working prototype but the tricky part is resource locking --- we need to remove a chunk from hostmem (which will cause hostmem to be resized and possibly split), and insert this chunk to iomem's top level as System RAM, all while holding resource lock.I haven't been able to come up with an acceptable interface for that. Given that we are actually broken I guess I am OK with reverting the patch, but please make sure this works on AMD boxes (I think family 15his what needs to be tested).",technical
,
"After their last commit I don't see how this can be broken:1) They only claim addresses starting from this*unconditionally* which means if there is some memory behind this range on the host (regardless if it's Dom0 or native Linux) they'll break their own systems. 2) So, theoretically, to trigger the original issue we'd need to have a system with RAM higher than it and that shouldn't be assigned to Dom0 but that contradicts (1).Igor",technical
,
"This commit breaks Xen balloon memory hot plug for us in Dom0 with""hoplug unpopulated"" set to 1. The issue is that the common kernel memory onlining procedures require ""System RAM"" resource to be 1-stlevel. That means by inserting it under ""Unusable memory"" as the commit above does (intentionally or not) we make it 2-nd level and break mem or yonlining.There are multiple ways to fix it depending on what was the intention of original commit and what exactly it tried to workaround. It seems it does several things at once:1) Marks non-Dom0 host memory ""Unusable memory"" in resource tree.2) Keeps track of all the areas safe for hot plug in Dom03) Changes allocation algorithms itself in balloon driver to use those areas Are all the things above necessary to cover the issue in fa564ad96366(""x86/PCI: Enable a 64bit BAR on AMD Family 15h (Models 00-1f, 30-3f,60-7f)"")? Can we remove ""Unusable memory"" resources as soon as we finished booting? Is removing on-demand is preferable over ""shoot them all"" in that case? Does it even make sense to remove the 1-st level only restriction in kernel/ resource.c ? ",technical
,
"Forget it 0-day bot, and assorted crickets :) With stock knob settings, that's too late to switch from llc -> l2 affinity for sync wakeups, and completely demolished tbench top end on huge socket NUMA box with lots of bandwidth. Lovely for desktop, somewhere below gawd-awful for big box performance.	",technical
,
crap. google helpfully disabled this account while I was posting this set. I'll repost can call it v3,technical
,
Please change it to read the hardware directly and not use this.,technical
,
"Thanks for the review the change. Here intention is to know the software status of the RCG instead of HW status and we have intentionally not defined the 'is enabled'ops for clk rcg2 shared ops. This clk rcg2 shared ops are only applicable for the RCGs with shared branches across different subsystems. Reason for using the same is mentioned below. When RCG gets enabled by other subsystem (outside the Application processor subsystem):   In this case when RCG gets enabled by branch clock managed by   other subsystem (outside the Application processor subsystem)   and if we check HW status of RCG in clk rcg2 shared set rate()   instead of checking its software status then it will give the status as ENABLED without overlying software knowing its status   and during source switch, update configuration will get fail as   new parent will be in disabled state.   In above scenario, clock framework will not enable the new   parent before configuration update as enable and prepare counts   are zero for RCG clock and clk set rate() will follow below path.   clk rcg2 shared set rate()     clk set parent before()-->New parent will be disabled as prepare count = 0   clk change rate()   clk set rate()So solution of this problem is as follows and same is explained in the commit text of. If software status of the RCG is disabled(enable/prepare counts are0)    then just cache or store the rate in current freq variable and if    software status is enabled then follow the normal update procedure.2. Set the rate and switch to new source only inclk rcg2 shared enable()    i.e. during RCG enable sequence. This will make sure that required    parents are already in enable state before configuration update and    RCG switch will happen successfully every time. In past, We have encountered similar RCG update configuration failure issues for some display RCGs, where there are two branch clocks, one is controlled by application processor subsystem and another one controlled by other subsystem. So to handle such cases, we need clk rcg2 shared ops.",technical
,
"Hello, Ideally you would mention the commit description since the id is not yet usptream.  I found it here (its 1 in this series):  .Ideally we could move <asm-generic/io.h> include down to the bottom of the file and not have to do the defines like like this, it seems clumsy to me.  In'cris', 'nios2' and other architectures I can see they have the generic include at the bottom of the file and not need for #define's.I tried that but I get a lot of errors.  Does your patch to asm-generic/io.h cause build issues for those architectures as well?",technical
,
I got this email from kbuild test robot. I personally tried arm64/x86before I sent the generic asm io.h patch.I tried openrisc/sparc before I sent these v5 patches.,technical
,
"unit address without reg is not valid. Drop the ""@0"".All the memory mapped peripherals should be under at least one simple-bus node. ..",technical
,
,
I'd like to modify it like this in the next version patch.        ,technical
,
"HI, If necessary to handle these, symlink might help here i believe. Upon trying to understand memory-barriers.txt, i felt that it might be better to have it in PDF/HTML format, thus attempted to convert it to rst. And i see it not being welcomed, hence shelving the conversion.",technical
,
"Yes, I understand that some of us have a (reasonable) doubt about the reST markup.  It is not perfect in any matter, e.g. I don't like the ``monospace`` markup.  But this is my home opinion.    My hope is, that those of us who have a doubt give reST    a chance ... it is a compromise, not as bad as you might    think first ... your cooperation and your criticism is    needed and welcome. Please let me invite you / read on: There are other plain-text markups e.g. AsciiDoc or Markdown. The reST markup and the Sphinx-builder is a compromise from a evaluation in 2016 (see linux-doc ML subject ""muddying the waters"" [1]). Jani wrote an article about the evaluation and it results [2]. And there are other articles documenting all the various aspects.- A report from the documentation maintainer [3]- Kernel documentation with Sphinx, part 2: how it works [4]- Kernel documentation update [5]To summarize it with my words: The old DocBook-based toolchain was hard to maintain and of course who want's to write XML? A consistent plain-text markup for articles in /Documentation/* and source-code comments (kernel-doc)was needed. The markup: IMO reST wins that race, because it has a extendable markup specification while others plain-text markups like Markdown have various (HTML) builders with various markup dialects[7] (which is more a mess than a definition).The builder: IMO Sphinx-Doc wins that race, since it is (well) maintained, widely used and has a interface for extensions.I.e. the one extension we wrote: 'kernel-doc' to parse kernel-doc comments from source code and include them in the articles. Perspective: Sphinx-Doc also offers solutions we might use in the future (e.g. building man-pages). Not to end in a mess, extensions should be implemented cautiously and deliberately (be patient). But that should not fool you; yes we have known problems with our toolchain and it is not yet ;) perfect in any matter (e.g. the highlighting in kernel-doc comments or the PDF generation or the sphinx-doc versions shipped with various distributions or ..) Anyway, today we have more than before: The reST learning curve is(compared to DocBook) not hard for newbees and our toolchain is flexible for all the requirements which might come up in the future. IMO the actual challenge is the content and the organization of the doc-tree and for this That's exactly what I mean: give reST a chance :)",technical
,
"Hi, Okay, the outcome is exactly as was feared. Abandoning the patch, let this be > ",technical
,
"Hello, I don't think we want to fully guarantee the current behavior.  On the scheduler side, I don't think it's likely to change but blkio side*might* change.  Can you please collect the root behavior in a separate section and clearly note that the behaviors are subject to change? ",technical
,
"Hello, Will do. ",technical
,
Seems like kallsyms would be one to absolutely scan... it shouldn't cause hangs either.,technical
,
Haven't we fixed kallsyms now? Do you mean that we should be checking to see if the scanned kernel has been patched to include the kallsysms fixes in 4.14? If so perhaps we should add functionality to just check the first line for an address and warn if one is found. No real reason to include ever address in kallsyms in the output. Script doesn't hang but it times out with the default timer (10 seconds).,technical
,
"(versions of patches 1,2 and 4 have been queued by Catalin)(Nit 'ACPI / APEI:' is the normal subject prefix for ghes.c, this helps the maintainers know which patches they need to pay attention to when you are touching multiple trees) This reads as if this patch is handling SError RAS notifications generated by a CPU with the RAS extensions. These are about CPU->Software notifications. APEIand GHES are a firmware first mechanism which is Software->Software. Reading the v8.2 documents won't help anyone with the APEI/GHES code. Please describe this from the ACPI view, ""ACPI 6.x adds support for NOTIFY SEIas a GHES notification mechanism... "",  its up to the arch code to spot a v8.2RAS Error based on the cpu caps. There are problems with doing this: How do SEA and SEI interact? As far as I can see they can both interrupt each other, which isn't something the single in nmi() path in APEI can handle. I thinks we should fix this first.[..] SEA gets away with a lot of things because its synchronous. SEI isn't. Xie XiuQi pointed to the memory failure queue() code. We can use this directly from SEA, but not SEI. (what happens if an SError arrives while we are queueing memory failure work from an IRQ). The one that scares me is the trace-point reporting stuff. What happens if an SError arrives while we are enabling a trace point? (these are static-keys right?)  I don't think we can just plumb SEI in like this and be done with it.  (I'm looking at teasing out the estatus cache code from being x86:NMI only.  This way we solve the same 'cant do this from NMI context' with the same  code'.)I will post what I've got for this estatus-cache thing as an RFC, its not ready to be considered yet.external modules? You mean called by the arch code when it gets this NOTIFY SEI?",technical
,
"If your patch can be consider that, this patch can based on your patchset. thanks. yes, called by kernel ARCH code, such as below, I remember I have discussed with you. ",technical
,
"sorry fix a typo. Yes, I know you are dong that. Your serial's patch will consider all above things, right? If your patch can be consider that, this patch can based on your patchset. thanks.",technical
,
"After this patch user-space can trigger an SError in the guest. If it wants to migrate the guest, how does the pending SError get migrated? I think we need to fix migration first. Andrew Jones suggested using KVM GET/SET VCPU EVENTS:Given KVM uses kvm inject vabt() on v8.0 hardware too, we should cover systems without the v8.2 RAS Extensions with the same API. I think this means a bit to read/write whether SError is pending, and another to indicate the ESR should be set/read.CPUs without the v8.2 RAS Extensions can reject pending-SError that had an ESR.user-space can then use the 'for migration' calls to make a 'new' SError pending. Now that the CPU feature bits are queued, I think this can be split up into two separate series for v4.16-rc1, one to tackle NOTIFY SEI and the associated plumbing. The second for the KVM 'make SError pending' API. Does nothing in the patch that adds the support? This is a bit odd.(oh, its hiding in patch 6...)",technical
,
"Thanks a lot for your review and comments. For the CPUs without the v8.2 RAS Extensions, its ESR is always 0, we only can inject a SError with ESR 0 to guest, cannot set its ESR. About how about to use the KVM GET/SET VCPU EVENTS, I will check the code, and consider your suggestion at the same time. The IOCTL KVM GET/SET VCPU EVENTS has been used by X86. Ok, thanks for your suggestion, will split it. To make this patch simple and small, I add it in patch 6.",technical
,
"thanks for the review. yeah, I have seen that. I pick your modification of setting an impdef ESR for Virtual-SError, so add your name, I change it to 'CC'will follow that. Ok, I will adjust that. thanks, I will directly call pend guest serror() in this function. Thanks, I will call it",technical
,
" It's always implementation-defined. On Juno it seems to be always-0, but other systems may behave differently. (Juno may generate another ESR value when I'm not watching it...) Just because we can't control the ESR doesn't mean injecting an SError isn't something user-space may want to do. If we tackle migration of pending-SError first, I think that will give us the API to create a new pending SError with/without an ESR as appropriate.(Not my suggestion, It was Andrew Jones idea.)We would be re-using the struct to have values with slightly different meanings. But for migration the upshot is the same, call KVM GET VCPU EVENTS on one system, and pass the struct to KVM SET VCPU EVENTS on the new system. If we're lucky Qemu may be able to do this in shared x86/arm64 code. But that made the functionality of this patch: A new API to return -EINVAL from the kernel. Swapping the patches round would have avoided this. Regardless, I think this will fold out in a rebase.",technical
,
"Assuming I got it right, yes. It currently makes the race Xie XiuQi spotted worse, which I want to fix too. (details on the cover letter) I'd like to pick these patches onto the end of that series, but first I want to know what NOTIFY SEI means for any OS. The ACPI spec doesn't say, and because its asynchronous, route-able and mask-able, there are many more corners than NOTFIY SEA.This thing is a notification using an emulated SError exception. (emulated because physical-SError must be routed to EL3 for firmware-first, and virtual-SError belongs to EL2). Does your firmware emulate SError exactly as the TakeException() pseudo code in the Arm-Arm? Is the emulated SError routed following the routing rules for HCR EL2.{AMO, TGE}?What does your firmware do when it wants to emulate SError but its masked?(e.g.1: The physical-SError interrupted EL2 and the SPSR shows EL2 had PSTATE. A set. e.g.2: The physical-SError interrupted EL2 but HCR EL2 indicates the emulated SError should go to EL1. This effectively masks SError.) Answers to these let us determine whether a bug is in the firmware or the kernel. If firmware is expecting the OS to do something special, I'd like to know about it from the beginning! Sure. The phrase 'external modules' usually means the '.ko' files that live in/lib/modules, nothing outside the kernel tree should be doing this stuff. ",technical
,
"Thank you for your time to reply me. For the armv8.0 cpu without RAS Extensions, it does not have vsesr el2, so when guest take a virtual SError, its ESR is 0, can not control the virtual SError's syndrome value, it does not have such registers to control that. Does Juno not have RAS Extension? if yes, I think we can only inject an SError, but can not change its ESR value, because it does not have vsesr el2. yes, we may need to support user-space injects an SError even through CPU does not have RAS Extension. sure, we should. Last week, I checked the KVM GET/SET VCPU EVENTS IOCTL, it should meet our migration requirements Got it. Thanks for the reminder, I know your meaning. In the x86, the kvm vcpu events includes exception/interrupt/nmi/smi. For the RAS, we only involves the exception, so Qemu handling logic is different. Anyway, I will try to share code for the two platform in Qemu. yes, I will, thanks for your kind suggestion.",technical
,
", it is.Yes, it is. Currently we does not consider much about the mask status(SPSR).I remember that you ever suggested firmware should reboot if the mask status is set(SPSR), right? I ever suggest our firmware team to evaluate that, but there is no response. I CC ""liu jun""who is our UEFI firmware Architect, if you have firmware requirements, you can raise again. I know your meaning, thanks for raising it again. I will rename 'external modules' to other name. Thanks.",technical
,
"My point was its more nuanced than this: the ARM- ARM'sTake Virtual SErrorException() pseudo-code lets virtual-SError have an implementation defined syndrome. I've never seen Juno generate anything other than '0', but it might do something different on a Thursday. The point? We can't know what a CPU without the RAS extensions puts in there. Why Does this matter? When migrating a pending SError we have to know the difference between 'use this 64bit value', and 'the CPU will generate it'. If I make an SError pending with ESR=0 on a CPU with VSESR, I can't migrated to a system that generates an impdef SError-ESR, because I can't know it will be 0.It's two types of v8.0 CPU, no RAS extensions. I agree, this means we need to be able to tell the difference between 'pending 'and 'pending with this ESR'. Great! ",technical
,
"Thanks for the mail. I checked it, you are right, the virtual SError's syndrome value can be 0 or implementation defined value, not always 0,which is decided by the ""exception.syndrome<24>"".thanks for the clarification. Yes, But it will have a issue, For the target system, before taking the SError, no one can know whether its syndrome value is IMPLEMENTATION DEFINED or architecturally defined. when the virtual SError is taken, the ESR ELx.IDS will be updated, then we can know whether the ESR value is impdef or architecturally defined. It seems migration is only allowed only when target system and source system all support RAS extension, because we do not know whether its syndrome is IMPLEMENTATION DEFINED or architecturally defined.",technical
,
"For a virtual-SError, the hypervisor knows what it generated. (do I haveVSESR EL2? What did I put in there?).True, the guest can't know anything about a pending virtual SError until it takes it. Why is this a problem? I don't think Qemu allows migration between hosts with differing guest-ID registers. But we shouldn't depend on this, and we may want to hide the v8.2 RAS features from the guest's ID register, but still use them from the host. The way I imagined it working was we would pack the following information into that events.The problem I was trying to describe is because there is no value of serror esrwe can use to mean 'Ignore this, I'm a v8.0 CPU'. VSESR EL2 is a 64bit register, any bits we abuse may get a meaning we want to use in the future. When it comes to migration, v8.{0,1} systems can only GET/SET events where serror has esr == false, they can't use the serror esr. On v8.2 systems we should require serror has esr to be true.If we need to support migration from v8.{0,1} to v8.2, we can make up an impdefserror esr.We will need to decide what KVM does when SET is called but an SError was already pending. 2.5.3 ""Multiple SError interrupts"" of [0] has something to say. ",technical
,
" sorry for my late response due to chines new year. so when migration from v8.{0,1} to v8.2, QEMU should make up an impdefserror esr for the v8.2 target. Can you give me some suggestion how to set that register in the QEMU?I do not familiar with the QEMU migration. Thanks very much. how about KVM set again to the same VCPU? thanks!",technical
,
" and yet ..... this is a problem. If you ignore SPSR EL3 you may deliver an SError to EL1 when the exception interrupted EL2. Even if you setup the EL1 register correctly, EL1 can't eret toEL2. This should never happen, SError is effectively masked if you are running at an EL higher than the one its routed to. More obviously: if the exception came from the EL that SError should be routed to, but PSTATE. A was set, you can't deliver SError. Masking SError is the only way the OS has to indicate it can't take an exception right now. VBAR EL1 may be 'wrong' if we're doing some power-management, the registers may contain live values that the OS would lose if you deliver another exception over the top. If you deliver an emulated-SError as the OS eret's, your new ELR will point at the eret instruction and the CPU will spin on this instruction forever. You have to honour the masking and routing rules for SError, otherwise no OS can run safely with this firmware. Yes, this is my suggestion of what to do if you can't deliver an SError: store the RAS error in the BERT and 'reboot'.(UEFI? I didn't think there was any of that at EL3, but I'm not familiar with all the 'PI' bits).The requirement is your emulated-SError from EL3 looks exactly like a physical-SError as if EL3 wasn't implemented. Your CPU has to handle cases where it can't deliver an SError, your emulation has to do the same. This is not something any OS can work around. ",technical
,
"Happy new year, Ah! This is where it came from. Sorry, this was just to illustrate the information/sizes we wanted to transfer.... I didn't mean it literally. I should have said ""64 bits of ESR, so that we can transfer anything that is added to VSESR EL2 in the future, a flag somewhere to indicate an serror is pending, and another flag to indicate the ESR has a value we should use"".",technical
,
"Thanks for this mail and sorry for my late response. James, I  summarized the masking and routing rules for SError to confirm with you for the firmware first solution,1. If this is set, which means the SError should route to EL2, When system happens SError and trap to EL3,   If EL3 find them.A are both set, and find this SError come from EL2, it will not deliver an SError: store the RAS error in the BERT and 'reboot'; but if it find that this SError come from EL1 or EL0, it also need to deliver an SError, right?2. If the HCR EL2.{AMO,TGE} is not set, which means the SError should route to EL1, When system happens SError and trap to EL3, If EL3 findHCR EL2.{AMO,TGE} and SPSR EL3.A are both not set, and find this SError come from EL1, it will not deliver an SError: store the RAS error in the BERT and 'reboot'; but if it find that this SError come from EL0, it also need to deliver an SError, right?",technical
,
"You also said ""Currently we does not consider much about the mask status(SPSR)."" If one or the other of these bits is set. Yes. If neither of these bits is set:(I'm reading this as all three of these bits are clear) No, this means SError is routed to EL1, this exception interrupted EL1 and the A bit was clear, so EL1 can take an SError. The two cases here are: AMO==0,TGE==0 means SError should be routed to EL1. If SPSR EL3 says the exception interrupted EL1 and the A bit was set, you need to do the BERT trick. If SPSR EL3 says the exception interrupted EL2, you need to do the BERT trick regardless of the A bit, as SError is implicitly masked by running at a higher exception level than it was routed to.(this is re-iterating the two-cases above:)'not be routed to' is one of two things: EL1 is fine, regardless of SPSR EL3.A the emulatedSError can be delivered to EL2, as EL2 can't mask SError when executing at a lower EL.Route-to-EL1+interrupted-EL2 is the problem. SError is implicitly masked by running at a higher EL. Regardless of SPSR EL3.A, the emulated SError can not be delivered. KVM does this on the way out of a guest, if an SError occurs during this time the CPU will wait until execution returns to EL1 before delivering the SError.Your firmware has to do the same.Table D1-15 in ""D1.14.2 Asynchronous exception masking"" has a table with all the combinations. The ARM-ARM is what we need to match with this behaviour. I thought interrupted-EL0 could always be delivered: but re-reading the ARM-ARM's ""D1.14.2 Asynchronous exception masking"", if asynchronous exceptions are routed to EL1 then EL0&EL1 are treated the same. So if SError is routed to EL1, the exception interrupted EL0, and SPSR EL3.A was set, you still can't deliver the emulated-SError you have to do the BERT-trick. Linux doesn't do this today, but another OS might (e.g. UEFI), and we might do this in the future. This is really tricky for firmware to get right. Another alternative would be to put the CPER records in a Polled buffer, unless something needs doing right now, in which case a BERT-reboot is probably best. ",technical
,
"Thanks for this mail. Yes, we currently do not consider much it. After clarification with you, we want to modify the EL3 firmware to follow this rule. sorry, it is a typo issue.it should be HCR EL2.AMO and HCR EL2.TGE are both clear, but SPSR EL3.A is set. Agree. ""BERT trick"" is storing the RAS error in the BERT and 'reboot, right? Agree. ""can not be delivered"" means storing the RAS error in the BERT and 'reboot, right? In the Table D1-15 in ""D1.14.2 Asynchronous exception masking"", for the case, it is ""C""""C""means SError is not taken regardless of the value of the Process state interrupt mask. for this case, whether it will be unsafe if  BIOS directly reboot? For this case, whether it will be unsafe if  BIOS directly reboot? For example, for some test purpose, EL0 set PSTATE. A, just right happen SError, then BIOS will reboot system. I am afraid that system will become unsafe because BIOS will reboot system. In summary:[1]:Route-to-EL1 + interrupted-EL1, if SPSR EL3.A is set, EL3 firmware can't deliver the emulated-SError, store the RAS error in the BERT and 'reboot.Route-to-EL2 + interrupted-EL2, if SPSR EL3.A is set, EL3 firmware can't deliver the emulated-SError, store the RAS error in the BERT and 'reboot. I agree above two cases, but maybe we need to ensure that only in EL2 SError handler and EL1 SError exception handler the PSTATE.A is set, for other places, the PSTATE.A is not set. Then BIOS can know this is nested-SError when find the SPSR EL3.A is set, can we ensure that in the Linux kernel code and KVM code?[2]:Route-to-EL2 + interrupted-EL1, regardless of SPSR EL3.A the emulated SError can be delivered to EL2.Route-to-EL2 + interrupted-EL0, regardless of SPSR EL3.A the emulated SError can be delivered to EL2.I agree above two cases.[3]:Route-to-EL1+interrupted-EL0, if SPSR EL3.A is set, EL3 firmware can't deliver the emulated-SError, store the RAS error in the BERT and 'rebootRoute-to-EL1+interrupted-EL2, EL3 firmware store the RAS error in the BERT and 'reboot regardless of SPSR EL3.A.For above two cases, I am worried system will become unsafe because BIOS will reboot system.",technical
,
The patch should be split into two: one is for dt-bindings part and the other is for driver part. Where dt-binding part should require additionally to send to Rob should include 2018 ? how about use devm request irq to simplify error path?,technical
,
"Thanks for review, Sure. Will do it in next version.Will fix in next version Will change in next version",technical
,
"Ok, makes sense. Still one minor thing left",technical
,
Thanks for adding the comment.,technical
,
"Sorry, also...I think you'll find you don't need to set MII SPEED here, since MII SPEED selects between 10 and 100, GMII SPEED always takes precedence selecting 1000, and 2500 is done by the comphy increasing the clocks by 2.5x",technical
,
The mvpp2 driver uses phy ethtool get link ksettings() to report the link speed to Ethtool. So it's reporting the speed set by the PHYdriver.So it'll be something to ensure when adding PHYs supporting the mode. We'll have the opportunity to see this when adding the last mcbin interface. Thanks!,technical
,
"Comments always welcomed :)I just had a look at the datasheet, and as you say it seems GMII SPEED takes over MII SPEED. I'll see if there is a corner case here or if selecting MII SPEED doesn't make sense, and update accordingly. Thanks!",technical
,
"I just checked, this can be removed for this mode. I'll update the patch. Thanks! ",technical
,
 Could you please have a look of this thanks ,technical
,
"That was an extensive changelog, thanks for the details and for working on this!",technical
,
"I've tested this on two very similar systems. On the M3-based system, everything seems to work fine. On the H3-based system, the serial console (the /dev/ttySC0 device, not kernel serial output) is dead after resume from s2ram, with and without no console suspend.With no console suspend, I see this. 1 input overrun(s) after typing on the serial console, so it looks like an interrupt problem. This issue seems to be caused by patch [1/2]. But I have no idea what's really happening, and why the two systems behave differently. Oh well, have a nice weekend! ",technical
,
"Good. Well, that's not dramatic. Let's make a deal that we'll fix this on top of [1/2].Which driver is this BTW?  sh-sci?  That one doesn't even support runtime PM, confusingly enough. Thanks, you too!",technical
,
"What do you mean by ""managed by runtime PM""? I will do that, no worries.OK, I'll send a patch on top of this series. ",technical
,
"Could be a firmware issue, too.While the kernel images are identical, the ARM trusted firmware configs aren't (same version, though).I'll do some more investigation...;-) Yes, sh-sci. It does make pm runtime *() calls. And of course there's uart ops.pm, which is driven from serial core...",technical
,
"OK, thanks! It also would be good to know the topology of the device hierarchy and how that maps to the domains on the failing system (and which UARTclocks are operated by genpd). Hmm.  I overlooked that part. This is sort of unusual, because the driver doesn't provide any runtime PM callbacks, but still it does provided system suspend ones.It looks like the idea is to never put it into runtime suspend if any ports are enabled and always put it into runtime suspend otherwise. Which one is the case in your testing?  Is the port disabled or enabled during system-wide suspend? What does this point to for that particular device?",technical
,
"The topology is the same on both systems. The UART's module clock is operated by genpd, on both systems. It's enabled on both systems, as a getty is running.sci pm(), on both systems. See, there's no difference in topology on both systems, so I'll have to look a bit deeper first...",technical
,
"I did miss a small difference in topology: in pm/linux-next, H3 has DMA enabled for SCIF2, while M3 hasn't (yet).With DMA enabled on M3, it fails in the same way. As it no longer calls this is no longer called, and the DMAC's registers are no longer reinitialized after system resume, breaking the serial port. ",technical
,
In drivers I would try to replace the below line in case that may be too early to suspend the dma device (which is rather common for dma devices) then try this,technical
,
"[cut]Yes, that probably is the least intrusive thing that can be done to address the issue. Good suggestion, and I would go straight for it anyway. can you try if this works, please? ",technical
,
" Works. Both using them, But given this is a DMA engine driver, I'd settle for the latter. And I did verify doing so doesn't break the system without the patch in subject.Thanks! Will send a patch…",technical
,
Thank you!,technical
,
"Thanks.I just sent a v3 that changes the VERMAGIC only, based on Greg's earlier feedback.It has the drawbacks that it:- refuses loading instead of warns- doesn't stop refusing when the feature is runtime disabled But it's much simpler, just a few lines of ifdef. We can either go with the v3, or rework this one into a v4?",technical
,
I think simple is good at this point,technical
,
V3 is fine. Not loading is the right thing to do :) ,technical
,
"You are setting ssi->synchronous in the AC'97 mode here, the old code didn't do that (see the next patch hunk below). Since in the previous patch you have replaced cpu dai drv.symmetric rateswith ssi->synchronous this will likely break asymmetric rate support in the AC'97 mode, since the driver will use STCCR for programming of both playback and capture. The next patch in this series (17) also looks affected by this change. You can see it here that the old code didn't set ssi->synchronous in theAC'97 mode.",technical
,
Will modify this part. Thanks,technical
,
I neglected the comments in the middle. Sorry. Will add it back then.,technical
,
"The above totally does not parse (no pun intended).Are you trying to say: ""User space can pass in a C null character '\0' along with its input. The function trace get user() will try to process it as a normal character, and that will fail to parse.The above should be something like: ""Have the parser stop on '\0' and cease any further parsing. Only process the characters up to the null '\0' character and do not process it.""",technical
,
"Thanks for your polish, let me update commit msg with your words.--Thanks",technical
,
"RESEND: fix typo in email address. HI, A few weeks ago, I have sent an RFC about adding bias support for GPIOs [1].It was motivated by the fact that I wanted to enable the pinmuxing strict mode for my pin controller which can muxed a pin as a peripheral or as a GPIO. Enabling the strict mode prevents several devices to be probed because requesting a GPIO fails. The pin request function complains about the ownership of the GPIO which is different from the mux ownership. I have to remove my pinctrl node to avoid this conflict but I need it to configure mypins and to set a pull-up bias for my GPIOs. My first idea was to add new flags in addition to GPIO ACTIVE HIGH and others. Obviously, it was not the way to go since many new flags may be added: strength, debounce, etc. Then I proposed a very ""quick and dirty"" patch to give the picture of what I have in mind but I had no feedback. It was probably too dirty. The idea was to add a cell to the gpios property with a phandle on a pinctrl node which contains only the pinconf, no pinmux. The configuration is applied later when requesting the GPIO. The main issue is that enabling the strict mode will break old DTBs. I was going to submit patches for this but, after using the sysfs which still show me a bad ownership, I decided that it should be fixed. So I did these patches. Unfortunately, there are several ways to lead to gpiod request(). It does the trick only for the gpiod get family. The issue is still present with legacy gpio request and fwnode get named gpiod. It seems that more and more drivers are converted to use GPIO descriptors so there is some hope. The advantage of this solution is to not break old DTBs. As I am not aware of all usage of the gpiolib, I tried to implement it in the safest way.",technical
,
"thanks for your patches! I was confused I think, because the issue of ownership and adding bias support were conflated. I think I discussed properly the ideas I have for pin control properties vs the GPIOlib API/ABI in my response to patch 1.So that is a different thing from bias support. Okay I think the right solution is to fix the ownership issue, and setup bias using pin control/config but use the line through gpiolib for now. Yeah we need to work around that. Yep :) fwnode get named gpiod() must really be fixed too. You probably want to have things like LEDs and GPIO keys working even if your pin controller is strict. I don't care so much about the old functions, I guess you just have to make sure that the drivers for *your* pin controller all use descriptors so that you can enable strict mode on *your* pin controller, right? Restrict your task to this, I'd say. Yeah I'm doing this when I have time. There is plenty of work...Help appreciated. ",technical
,
"I think we need to think over what is a good way to share ownership of a pin. Russell pointed me to a similar problem incidentally and I briefly looked into it: there are cases when several devices may need to hold the same pin. Can't we just look up the associated gpio chip from the GPIO range, and in case the pin is connected between the pin controller and the GPIO chip, then we allow the gpiochip to also take a reference?I.e. in that case you just allow gpio owner to proceed and take the pin just like with a non-strict controller.",technical
,
"It's the probably the way to go, it was Maxime's proposal and Andy seems to agree this solution.",technical
,
"I'm looking into SPI and regulators for the next kernel cycle, so those will hopefully get fixed. ",technical
,
"If pin request() is called with gpio range not NULL, it means that the requests comes from a GPIO chip and the pin controller handles this pin. In this case, I would say the pin is connected between the pincontroller and the GPIO chip. Is my assumption right? I am not sure it will fit all the cases:- case 1: device A requests the pin (pinctrl-default state) and mux it  as a GPIO. Later, it requests the pin as a GPIO (gpiolib). This 'weird'  situation happens because some strict pin controllers were not declared  as strict and/or pinconf is needed.- case 2: device A requests the pin (pinctrl-default state). Device B  requests the pin as a GPIO (gpiolib).In case 1, pin request must not return an error. In case 2, pin request must return an error even if the pin is connected between the pincontroller and the GPIO chip. ",technical
,
How do you find my proposal about introducing ownership level (not requested yet; exclusive; shared)? Confirm with caveat that this is a fix for subset of cases. I think it doesn't cover cases when you have UART + UART + GPIO (I posted early a use case example).But at least it doesn't move things in a wrong direction. For these cases looks OK to me.,technical
,
"Yes but I don't see how I can fix my issue with these levels. In my case, I need an exclusive ownership at device level not at pin level. In reality, it is at pin level but I am in this situation because my pincontroller was introduced as non strict and also because I need to set the configuration of the pin which is going to be used as a GPIO.If the ownership is exclusive, pinmuxing coming from pinctrl-default will be accepted but the GPIO request will fail even if it comes from the same device.If the ownership is shared then, pinmuxing coming from pinctrl-default will be accepted but a GPIO request from another device will be accepted too. Both situations are incorrect in my case. Let me know if I have not well understood your proposal. My concern is to get out of this situation without breaking current DTs. ",technical
,
"The problem here is to declare a right consumer of the resource. My understanding that consumer at the end is device or device(s):none: resource is free to acquire exclusive: certain device has access to the resource (pin) shared: several devices may access to the resource In both cases couple of caveats:- power management has a special access level to the resource on behalf of the owner(s)- it can have some flags, like 'locked', which means no more owners can be changed / added, but still possible to free resource by all owners to go to state 'none' Yes, since the ownership design is based on subsystem rather consumer device. See above, hope it clarifies a bit.",technical
,
"Yes I get it but I still don't see how I can use your approach to solve my issue. We have a situation for several pin controllers. If I can't know who is requesting the GPIO, I have no idea about how to solve this issue. Bypassing the strict mode, as suggested, if the pin controller is also a gpio controller may lead, IMO, to wrong behaviors. Do I have to try to find a way to fix this situation? Maybe, it will be easier to progress on the muxing and configuration topic and to introduce a DT property to enable the strict mode or whatever modes you want once everything is ready and DTs fixed. I'd prefer to fix the current situation then to improve muxing and configuration stuff because it will take time. ",technical
,
Maybe you can apply a similar idea to kvm nested on kvm. ,technical
,
"Yes we can. Basically, that would mean directly accessing 'structvmcs12' from L1 hypervisor.",technical
,
"Haven't looked into the details, but we have to watch out for other VCPUs trying to modify that vmcs12.Basically because other VCPUs could try to modify values in vmcs12 while we are currently building vmcs02. Nasty races could result in us copying stuff (probably unchecked) into vmcs02 and therefore running something that was not intended. If this is not possible with the current design, perfect :)",technical
,
"Yes, the vmcs12 would have to be copied from memory to internal hypervisor data before prepare vmcs02. I'm curious how well the ""clean"" flags overlap with the choice of fields for which we allow shadow VMCS vmread/vmwrite. ",technical
,
"I don't think we share VMCS among vCPUs, do we?",technical
,
"VMCS is just memory, so who knows what a malicious L1 guest will do. But for vmread/vmwrite we can go through hypervisor memory, for enlightened VMCS we cannot. ",technical
,
"True; not sure if Hyper-V actually copies the data to some internal storage, probably it does. TLFS explicitly forbids making the same enlightened VMCS active on several vCPUs simultaneously but again, this is just memory…",technical
,
"FWIF, on nested s390x we pin the guest provided SIE control block (""SCB""- s390x VMCS12). As this is just guest memory, another VCPU can write to that memory. When building our shadow SCB (""VMCS02""), we directly access the pinned block, but we basically only copy values and mask them for the critical parts (execution controls).However, as I realize, the compiler might fetch values several times, so we better add READ ONCE() to these places. Will look into that.",technical
,
"You don't even need to make them active, you can just scribble on it simultaneously with a VMRESUME. ",technical
,
"Nice!IIUC, eVMCS replaces VMCS when enabled, hence doing it for all VMs would be simplest -- we wouldn't need to setup VMCS nor reconfigure Hyper-V on the fly.  (I'm thinking we could have a union in loaded vmcs for actually used type of VMCS.)Static keys seem like a good choice. I'd go for a separate mapping from Intel VMCS into its MS eVMCS and dirty bit, something like vmcs field to offset table.Thanks.",technical
,
This wants to be split into x86 and core changes. Ideally you make the core changes before the previous patch and add the empty inline into Linux/processor.h....,technical
,
"Good point, will fix. I agree with you that this behavior fits better a ""global"" definition than a ""shared"" one, especially given that it does not target a specific shared memory mapping. The main issue I have is due to the pre-existing MEMBARRIER CMD SHARED introduced in Linux 4.3. That one should also have been called ""MEMBARRIER CMD GLOBAL"" based on the current line of thoughts. Do you envision a way to transition forward to a new ""MEMBARRIER CMD GLOBAL"" for the currently existing MEMBARRIER CMD SHARED ?Perhaps with a duplicated enum entry ?enum membarrier cmd ",technical
,
"That should work. Though I doubt that you ever can get rid of CMD SHARED, but at least the code is clearer that way.",technical
,
"Good point, done. The first commit introducing the new command now also introduces the generic stuff moved from the x86 patches.",technical
,
Scratch this: it's cleaner if I add a separate generic patch to introduce just the empty inline into linux/processor.h and the ARCH HAS SYNC CORE BEFORE USERMODE in init/Kconfig. ,technical
,
"FWIW, SLCG stands for ""second level clock gating"".",technical
,
boutside protection'? In general I'd rather have the tracepoints when actually submitting the request; with this tracepoint we might be getting a trace which doesn't really indicate if the command was submitted at all. ,technical
,
Pleas keep the trace header under drivers/nvme/host/,technical
,
yes but if we really want to be 100% certain we need to take the tracepoints in either in blk mq dispatch rq list() and trace the return value of the respective->queue rq() or in the PCI/FC/RDMA drivers.,technical
,
"The timer is supposed to be triggered by carrier detect interrupt. After remove the line noise, the carrier detect interrupt is never triggered again, because the carrier is always ok and it only trigger the timer once, Since the protocol was terminated and no new interrupts happen, the link will never be back. So the case here is that the line noise is good and just good to make the carrier detect still good  but the protocol fail, the timer will be never triggered again. Of course, if you increase the noise and make even the carrier detect fail, then remove the noise, the link will be up, Because the carrier down and up again and then trigger the timer to restart. The timer is supposed to restart the protocol again, that's how this whole thing is designed to work. I think you are making changes to the symptom rather than the true cause of the problems you are seeing. Sorry, I will not apply this until the exact issue is better understood. Thank you.",technical
,
"Ok, I submit it  again. In drivers/net/wan/hdlc ppp.c, some noise on physical line can cause the carrier detect still ok, but the protocol will fail. So if carrier detect ok, don't turn off protocol negotiation This patch is against the kernel version Linux 4.15-rc8From: Please resubmit it and I'll think about it again, thank you.",technical
,
"How   is your thinking about this patch? [PATCH] netdev: carrier detect ok, don't turn off negotiation Sometimes when physical lines have a just good noise to make the protocol handshaking fail, but the carrier detect still good. Then after remove of the noise, nobody will trigger this protocol to be start again to cause the link to never come back. The fix is when the carrier is still on, not terminate the protocol handshaking.  Ok, I submit it   again. In drivers/net/wan/hdlc ppp.c, some noise on physical line can cause the carrier detect still ok, but the protocol will fail. So if carrier detect ok, don't turn off protocol negotiation This patch is against the kernel version Linux 4.15-rc8  Please resubmit it and I'll think about it again, thank you.",technical
,
"Please resubmit it and I'll think about it again, thank you.",technical
,
Well this introduces significant overhead for large sized allocation. Does this not matter because the areas are small? Would it not be better to use compound page allocations here? page head (whatever) gets you the head page where you can store all sorts of information about the chunk of memory.,technical
,
"IIUC, he means PageHead(), which is also hard to grep for, since it is a constructed name, via Page",technical
,
,
"Thank you, I'll try to provide a meaningful reply soon, but I'll be AFK during most of next 2 weeks, so it might be delayed :-(",technical
,
Ok its compound head(). See also the use in the SLAB and SLUB allocator. If you save the size in the head page struct then you could do that pretty fast.compund pages are higher order pages that are handled as a single page by the VM. See ,technical
,
"Ok, now I get what you mean. But it doesn't seem to fit the intended use case, for other reasons(maybe the same, from 2 different POV):- compound pages are aggregates of regular pages, in numbers that are powers of 2, while the amount of pages to allocate is not known upfront. One *could* give a hint to pmalloc about how many pages to allocate every time there is a need to grow the pool.  Iow it would be the size of a chunk. But I'm afraid the granularity would still be pretty low, so maybe it would be 2-4 times less.- the property of the compound page will affect the property of all the pages in the compound, so when one is write protected, it can generate a lot of wasted memory, if there is too much slack (because of the order) With vmalloc, I can allocate any number of pages, minimizing the waste. Finally, there was a discussion about optimization:  The patch I sent does indeed take advantage of the new information, not just for pmalloc use. I have not measured if/where/what there is gain, but it does look like the extra info can be exploited also elsewhere.",technical
,
I thought the intend here is to create a pool where the whole pool becomes RO?,technical
,
"LOCAL variable names should be short, and to the point.  If you have some random integer loop counter, it should probably be called ``i` `Calling it ``loop counter`` is non-productive, if there is no chance of it being mis-understood.  Similarly, ``tmp`` can be just about any type of variable that is used to hold a temporary value.",technical
,
"Yes, but why would I force the number of pages in the pool to be a power of 2, when it can be any number? If a need, say, 17 pages, I would have to allocate 32.But it can be worse than that. Since the size of the overall allocated memory is not known upfront, Iwold have a problem to decide how many pages to allocate, every time there is need to grow the pool. Or push the problem to the user of the API, who might be equally unaware. Notice that there is already a function (prealloc) available to the user of the API, if the size is known upfront. So I do not really see how using compound pages would make memory utilization better or even not worse.",technical
,
"ok, will do, thanks for the pointer!",technical
,
I've done this as the first line of my new documentation files:.. SPDX-License-Identifier: CC-BY-SA-4.0I think this is the CC license that's closest in spirit to the GPL without the unintended consequences of the GPL when used on documentation.  The GFDL seems to be out of favour these days.,technical
,
"I think that's a great license.  I still fear that it is not suitable for kernel documentation, though, especially when we produce documents that include significant text from the (GPL-licensed) kernel source.  The result is almost certainly not distributable, and I don't think that's a good thing.  The GPL is not perfect for documentation, but I don't think that we have a better alternative for in-kernel docs.jon",technical
,
"That's a reasonable concern.  I've read other reasonable concerns about the unintended effects of using the GPL to produce a printed book (e.g. can you print it in a proprietary font, do you have to provide an electronic version of the text, and so on).  I fear these wise words still ring true:  But the real problem is that we as a community lack a copyleft license  that works well for both code and text. About the only thing that even  comes close to working is putting the documentation under the GPL as  well, but the GPL is a poor fit for text. Nonetheless, it may be the  best we have in cases where GPL-licensed code is to be incorporated  into documentation. I dare suggest another possibility: that we create a further exception to the license that the kernel is distributed under.  Something along these lines: Documentation [1] extracted from files marked as GPL [2] may be distributed under the terms of the CC-BY-SA-4.0 license.[1] This includes text explicitly marked for extraction using the kernel-doctool.  It may include short example code sequences.  It does not include code that would normally be expected to be compiled.[We'd want to run it by a lawyer, of course, to have them check for unintended consequences.",technical
,
May be you can merge above with the previous entry which already has it. Otherwise looks good,technical
,
I already sent a fix for this,technical
,
Okay. Will add comment.,technical
,
"I am removing checks from core. Export and import were optional in beginning of crypto framework, but as time goes on they become mandatory.",technical
,
"Seems like if the driver doesn't implement those, the core can easily detect that and perform the necessary action. Moving the checks out of core seems like the wrong thing to do, rather you should enhance the checks in core if they're insufficient in my opinion.",technical
,
"I removed all checks. No checks in driver and no checks in crypto framework. If you would like any check, I think the place to add them is in ahash alg registration, in function ahash prepare alg add something like",technical
,
All applied.  Thanks.-,technical
,
"The bug can only be in driver which will not implement those two functions ,but we already had all drivers with those due to patches 1..4 All other drivers do have them. Additionally, with crypto we want minimize code and run as fast as possible. Moving checks out of core will impose on driver author need for implement those functions, or declare them empty, but in case of empty ones crypto will not work properly with such driver.",technical
,
"Page tables are protected by their locks.  VMAs may change while migration is active on them, but does that need locking against? I have not been keeping up with Michal's recent migration changes, but migrate pages() never used to need mmap sem held (despite being called with an mmap sem held from some of its call sites), and it would be a backward step to require that now. There is not even an mm argument to migrate pages(), so which mm->mmap sem do you think would be required for it?  There may be particular cases in which it is required (when the new page function involves the old page's vma - is that so below?), but in general not. ",technical
,
"This doesn't make much sense to me, to be honest. We are holding mmap sem for  read  so we allow parallel updates like page faults or unmaps. Therefore we are isolating pages prior to the migration. The sole purpose of the mmap sem in add page for migration is to protect from vma going away  while  need it to get the proper page. Moving the lock up is just wrong because it allows caller to hold the lock for way too long if a lot of pages is migrated. Not only that, it is even incorrect because we are doing get user() (aka page fault) and while read lock recursion is OK, we might block and deadlock when there is a writer pending. I haven't checked the current implementation of semaphores but I believe we do not allow recursive locking",technical
,
mmap sem is held during migrate pages() in current implementation.,technical
,
You mean in the original code? I strongly suspect this was to not take it for each page.,technical
,
"Right. The original code gathers 169 pages, whose information (struct page to node, 24bytes) fits into a 4KB page, then migrates them at a time. So mmap sem is not held for long in the original code, because of this design. I think the question is whether we need to hold mmap sem for migrate pages(). Hugh also agrees it is not necessary on a separate email. But it is held in the original code.",technical
,
"[...]I would be really surprised if we really needed the lock. If we do, however, then we really need a very good explanation why. The code used to do so is not a valid reason.",technical
,
Compiled and booted on my test system. No dmesg regressions. Thanks,technical
,
"There are 13 patches in this series, all will be posted as a response  to this one.   If anyone has any issues with these being applied, please let me know. Responses should be made by Wed. Anything received after that time might be too late.    The whole patch series can be found in one patch at  or in the git tree and branch at:              git: linux-3.18.y  and the diffstat can be found below.  No regressions noticed on the tree requires reverting commit  to avoid conflicting with the patch titled &quot;usb: gadget: f fs: Process all descriptors during, has no merge problems. Thanks for the update.",technical
,
xtensa patch is now dropped,technical
,
"I wanted, I got. This is upstream already (66f793099a63) so this commit message possibly wants tweaking slightly",technical
,
"Right you are, I should have re-read these Changelogs before posting ;-)",technical
,
"I couldn't make it work there, but it could be my makefile foo isn't strong enough. The ordering of arch/*/Makefile vs scripts/Makefile.build is forever confusing me.",technical
,
Can't you do that with the existing check in arch/x86/Makefile? cf.   which is going nowhere until gets fixed but still I'd like to have the check in *one* place.,technical
,
 any objection to this patch? ,technical
,
"No objections with my ext4 hat on. It should be noted though that this is a partial backport because it only fixes ext4, while Al's original upstream fix addressed a much larger set of file systems.  In the Android kernel the f2fs fix had been backported separately.  But for the upstream kernel, it *might*be the case that we should try backporting the original commit so that in case there is some other general purpose distribution decides (a)to base their system on 4.4, and (b) support a 32-bit kernel, they get the more general bug fixes which applies for btrfs, isofs, ocfs2, nfs,etc.I haven't been paying attention to what LTS kernels general purpose distro's are using, so I don't know how important this would be.  And if there are companies like Cloudflare which are using upstream LTS kernel, it seems unlikely they would want to use a 32-bit kernel, so.... shrug.  Greg, I'll let you decide if you want to backport the full commit or not.(We had a similar discussion on the AOSP kernel, and came to the conclusion that we only needed to make the patch support ext4.  No one was going to test the other file systems besides ext4 and f2fs, anyway.  But the calculus might be different might be different for the general upstream LTS kernel.)			",technical
,
"Well, the main point of backporting this change is to fix symlink decryption on32-bit systems.  So, it would be needed on both ext4 and f2fs.  Jin, it might be a good idea to fix f2fs in this patch at well, since unlike the AOSP kernels, the LTS kernels do not have the latest f2fs backported to them. I don't think backporting this change for other filesystems is particularly important, since if I understand correctly, the reasons that Al made the change originally were:- to allow following symlinks in RCU mode, but that's not implemented in old  kernels- to prevent a process from using up all kmaps and deadlocking the system, which  I'm not sure is a real problem (someone would need to try to put together a  reproducer), but if so it would probably just be a local device of service. Also if we actually backported the full commit there are follow-on fixes such ase8ecde25f5e that would be needed as well but might be missed.",technical
,
"Sure, uploaded. PTAL.jin",technical
,
"Yup... and *that's* only a problem on 32-bit systems.  And aside from Android, it's unclear to me how much we need to support 32-bit systems on upstream LTS kernels.  I suppose there might be Raspberry PI's which are 32-bits and which might want to use btrfs.  Personally I'm not sure we should care all that much, but others who care more about LTS kernels and 32-bit systems might have a different opinion. ",technical
,
"HI, You have on extra line here The leading 0 will generate a DT warning Since you have that extra line above, you can just drop this new line here. Thanks! ",technical
,
This should be the whole size of the memory region. ,technical
,
OK. (Although on the datasheet digital part and analog part is listed as one region.),technical
,
"Queued, thanks.",technical
,
I am not arguing about the kvm change but do we actually want to warn for 0 sized allocations? This just doesn't make much sense to me. In other words don't we want this? ,technical
,
"There have been quite a few reports of this from syzkaller and generally we've fixed them.  It does seem like a recipe for NULL-pointer dereferences when the size is user-controlled (as in this case).But here I'm actually not sure that the ""allocation failure: 0 bytes ""can happen, since we have a check above for ""if (routing.nr)"", and there is a check also so that the maximum allocation here is a meagre 128 KiB. So I'm wondering if this patch is obsolete actually after commit.",technical
,
Are you sure that you got the right vmalloc?,technical
,
"Nice catch!  But well, it's the only one in the whole file. :) That seems very much like an old patch then.  I'm unqueuing it. ",technical
,
We do return NULL for that case regardless the above. The patch just doesn't warn. Or do you think it is helpful to warn?,technical
,
"It certainly helps bringing potential issues in the spotlight (through fuzzing, mostly). ",technical
,
Fair enough.,technical
,
"It's not a catch at all, the fact that I saw this warning with an older kernel for KVM SET GSI ROUTING doesn't mean that I can't patch it with an upstream kernel.  Would you prefer I remove the stack trace completely?",technical
,
"FWIW, your stack trace did not complain about a too big allocation, it complained about 0 allocation. this case should be prevented. The only question is does your patch makes sense nevertheless as we gracefully handle the ENOMEM case? So a reproducer on a newer kernel would be good. Maybe use the ""vmalloc"" kernel parameter to force this.",technical
,
"The upstream kernel doesn't warn.  It checks ""if (routing.nr)"" before calling vmalloc. Paolo",technical
,
It will warn of the vmalloc space is really exhausted. But then I really ask myself if we really want to suppress this warning. This should be a big ALERT to the host admin.,technical
,
Especially since the biggest allocation it can do is 128 KiB...Paolo,technical
,
"Shouldn't this be the other way around?  Everyone is used to 7 now, so you're changing the default back to 6.  I would think that it should be 7 by default, and platforms like Brahma-B53 should force it to 6.",technical
,
"That is debatable, is there a good publicly available table of what the typical L1 cache line size is on ARMv8 platforms?",technical
,
"I don't have that, but I was under the impression that we moved from 6to 7 because more and more ARMv8 platforms have 128-byte caches, so that is the ""new normal""",technical
,
"That does not seem to be the data that I am collecting from ARM's website and some quick googling: The following cores appear to have a 64bytes L1D cache line size: A55,A73 (fixed), A35, A32, A53, A57 (fixed), A72 (fixed) even the Falkor seems to be that way according to [1].APM Mustang also seems to be 64b L1D according to [2]And then we seem to covering what the ARM64 mainline kernel knows about non-ARM implementations: ThunderX and ThunderX2 (formerly BroadcomVulcan). There is possibly the Qualcomm Kryo is different, but wikipedia seems to suggest it is a derivative of existing Cortex-A CPUs which have a 64b cache line size. Let's see what Catalin and Will think about what the default should be.",technical
,
This approach has been raised before ([1] as an example but you can probably find other threads) and NAK'ed. I really don't want this macro to be configurable as we aim for a single kernel Image. My proposal was to move it back to 6 and this to 128 as this is the largest known CWG. The networking code is wrong in assuming SKB DATA ALIGN only needs SMP CACHE BYTES for DMA alignment but we can add some safety checks (i.e. WARN ON) in the arch dma ops code if the device is non-coherent. I'll send a patch to the list (hopefully later today). ,technical
,
"With a server hat on...There are many ARMv8 server platforms that do 64b today, but future designs are likely to head toward 128b (for a variety of reasons). Many of the earlier designs were 64b because that's what certain other arches were using in their server cores. I doubt Vulcan will remain a unique and special case for very long. On the CCIX side of things, I've been trying to push people to go with 128b lines in future designs too. ",technical
,
"Mmm, I'm afraid we can't do this.   sched setscheduler might be called from interrupt contex by normalize rt tasks().",technical
,
Maybe conditionally grabbing it if pi is true could do? I guess we don't care much about domains when sysrq.,technical
,
Ops.. just got this. :/,technical
,
Arrghhh... Back to the drawing board.,technical
,
Eh.. even though the warning simply happens because unlocking of CPU set lock is missing,technical
,
I was then actually thinking that try locking might do.. not sure however if failing with -EBUSY in the contended case is feasible (and about the general ugliness of the solution :/),technical
,
"Or, as suggested by Peter in IRC, the following (which still would require conditional locking for the sysrq case) Avoid rq from going away on us: ",technical
,
"Just for the record, while this may work for media, it won't work for all subsystems.  One will quickly get a complaint that the big patch needs to go into multiple trees. ",technical
,
For the record: this only applies to drivers/media. We discussed what do to with series like this during our media summit last Friday and this was the conclusion of that. Obviously I can't talk about other subsystems.,technical
,
I have got an other impression. I continued with the presentation of suggestions from my selection of change possibilities. It seems that there are very different expectations for the preferred patch granularity. Can it happen again that you are going to use a development tool like  (as a maintainer) for the desired recombination of possible update steps?,technical
,
I find it safer in this way  while I was browsing through the landscape of Linux software components. It is just the case that there are so many remaining open issues. Thanks for a bit of change acceptance. Will any chances evolve to integrate 146 patches in any other combination? Can we achieve an agreement on the shown change patterns? Is a consensus possible for involved update candidates? I find that I did this already. I got an other software development opinion. Do we need to try any additional communication tools out? I hope that various change possibilities (from my selection) will become useful for more Linux users. How will the clarification evolve further? ,technical
,
"Do both I2C and OF have stubs so that a driver will build when they are both disabled?  I.e., only COMPILE TEST is enabled? ",technical
,
"Please no.16 temperature channels ...Excessive [and inconsistent] ( ) Please make this	if (ret)		return ret;	...	return 0; return followed by break doesn't make sense.	&buf -> buf Unnecessary ( )... but this array only has 8+1 elements. This seems inconsistent. How about using some defines for array sizes ?Also, why initialize those arrays ? You are overwriting them below. You could just use a static size array instead. I assume it is guaranteed that there is only exactly one instance of this device in the system. Have you tried what happens if you declare two instances anyway ? The result must be interesting,  with all those static variables. The matching gsc fan ch has only 5 entries. You declare local 'dev' variables all over the place, except here, where it would actually be used multiple times. Please either declare one here as well, or drop all the others. It must be interesting to see what happens if no 'label' property is provided. Have you tried ? Also, no validation of 'reg' and 'type' ?Are you sure ?This leaves the string unterminated if it is too long. Have you tested what happens in this situation ? Consider using strlcpy instead. Also please use sizeof() instead of '32'. I would suggest to abort with EINVAL if this happens. Otherwise the last entry is overwritten, which doesn't make much sense. Also, this accepts up to ARRAY SIZE() entries, leaving no termination. So a reg value of 0xXXyy is auto-converted to 0xYY ? It is going to be interesting to see what happens if there are more than5 such entries. All other types are silently ignored ? I would suggest to move above code into a separate function. The error would be ENOMEM. Is it necessary to report that again ?",technical
,
"""To compile this driver as a module..."" Let's keep the same // comment block fir the copyright notice as well. An one-line describing what this driver is would be appreciated too. Please no. Why is this needed? To clear irq? What happens if several events happen at the same time? Do we lose one of them? Could we provide the mapping in DTS instead of hard-coding them?		input sync(); Why not use threaded interrupt? Not needed - it is set by devm input allocate device().I'd say mapping should be done by MFD piece. You can add interrupts as resources and fetch them here.",technical
,
" Maybe it is in these patches, and i missed it....How do these emulated devices work? Does the controller respond to different addresses for these different emulated devices? Or is it anI2c bus mux?    ",technical
,
"You didn't miss it - I probably need to explain it better. The 'emulated devices' do respond on different slave addresses (which match one of or the only the slave addresses those parts support). For example the device-tree for the GW54xx has the following which are all from the GSC which is the only thing on i2c1; One issue I'm trying to figure out the best way to deal with is the fact that the GSC can 'NAK' transactions occasionally which is why I override the regmap read/write functions and provide retries. This resolves the issue for the mfd core driver and sub-module drivers but doesn't resolve the issue with these 'emulated devices' which have their own stand-alone drivers. I'm not sure how to best deal with that yet. I tried to add retires to the i2c adapter but that wasn't accepted upstream because it was too generic and I was told I need to work around it in device-drivers. I'm guessing I need to write my own sub-module drivers that will largely duplicate what's in the stand-alone drivers (ds1672, at24, pca9553x) .",technical
,
There appears to be a few spaces vs tabs issues in this file.      ,technical
,
Thanks. I'll run through checkpatch prior to v2.,technical
,
"That was just to point out that you use smaller arrays later on.That is what you do, isn't it ? So they will both map to 0xff. Or, in other word, the code happily accepts invalid values and converts them into something else. Much preferred.",technical
,
"Please note that there is nothing wrong in the generated code, just that it confuses objtool. Clang has simply omitted the statement where NULL is returned since the pointer was always dereferenced post in lining. Note that GCC will also remove the NULL pointers if it knows that the pointer is dereferenced. Here is an example. ",technical
,
... but returning NULL would be far more sane than falling through to the next function. This is why we use -fno-delete-null-pointer-checks. ,technical
,
"Or, as the case may be, oopsing at the point of failure. ",technical
,
"Thanks all for your input, we'll try to get-fno-delete-null-pointer-checks or a similar flag to be added to clang.",technical
,
"Nope, clang doesn't currently have such a flag. IIRC this patch was needed to work around the lack of the flag.  But when building the kernel with Clang,    the compiler assumes &pos->member cannot be NULL if the member's offset    is greater than 0 (which would be equivalent to the object being    non-contiguous in memory).  Therefore the loop condition is always true,    and the loops become infinite.    To work around this, introduce the member address is nonnull() macro,    which casts object pointer to uintptr t, thus letting the member pointer    to be NULL.    Other than that I am not aware of any known issues.",technical
,
I think you have gotten lucky :) ,technical
,
Looks good.  Thanks! ,technical
,
He just hasn't gotten to it yet.,technical
,
Are there any opinions? I'd like to know how this patch is going.,technical
,
He does not take drivers/staging/media/* patches :),technical
,
"I don't know the state in 3.16, but in 3.12, I had to fix the 32bit entry on 64bit in arch/x86/ia32/ia32entry.S (ia32 sysenter target &others) too. Thanks",technical
,
"Build result. The failures are due to newly added tests; the init process crashes.v3.16 passes those tests, so the problem was introduced later. I'll run a bisect later to see if I can find the culprit. If not, I'll drop the new tests from this kernel version.",technical
,
Turns out I did the bisect k already. Attached. It does suggest that there may be a real problem. bad signals sent from emulation too,technical
,
"Thank you, yes I need to fix them in 3.16 too.  I also failed to use retpolines there. The first rule of tautology club is the first rule of tautology club.",technical
,
"worked out that it depends on commit (""MIPS: Normalise code flow in the CpU exception handler"") which I mistakenly omitted.  I'll include that in the next update (3.16.57).",technical
,
"As I watched this email send, I noticed the ""3/3"" in the Subject. ;) I see the support now. :P This question still stands, though.",technical
,
"wrote: From the same patch: if this happens, in other words, if serial ports are disabled, but we still want to parse the APCI SPCR TABLE, which ""defaults y if X86"". Perhaps that logic should be changed (no need to parse ACPI SPCR table if we are going to disable serial anyway)?",technical
,
"But won't this break? ""static const bool ..."" but the code tries to set a value but I'd expect the compiler to still yet about it? I think you could drop the .h #ifdef and use it.",technical
,
"if er... yeah.oooohhh... nice.  I like it, will change.",technical
,
Please take a look at drivers/staging/ipx/TODO,technical
,
"II address space is always mapped, and address conversion done through this.Indeed. This should print the physical addresses, using pdata->base instead of base",technical
,
Does this use driver model? I cannot see it. ,technical
,
I would replace debug by dev err() here.,technical
,
"As you get access to the struct udevice, prefer dev err() here. Ditto ",technical
,
dev err() Is it useful to print this each time the backlight is enabled ? Ditto dev err() ditto debug() ?,technical
,
 is it useful to print this each time ? or is it for debug purpose ? In this case use debug() dev err() ditto ,technical
,
I'll send a patch for xtables too to reject bogus names coming from userspace (syzbot reports WARN() ).,technical
,
"Hmm, patch is probably good idea, but now it means that userspace can trigger WARN()s, and can hide objects from root by naming them '.' and'..'... which is not good. If you know where this happens, it would be nice to fix them in addition to this patch.",technical
,
"Patch rejects creation of such entries. And they should be harmless as VFS lookup won't find them, only readdir would. It not clear how they could be useful.",technical
,
"Yeah, as I said, that's half of problem. If I can name my object ""."" and it will be hidden from root, that sounds like a security hole to be prevented. So if you know  which  subsystem allow creating files and directories in /proc with names directly controlled by userspace, please let us know, we want to fix that.				",technical
,
"is neater, but the whole function should be thus converted and I'll let you decide on that  ",technical
,
"Oh, I hate this style of WARN. For one thing it overlaps with comma operator.",technical
,
"This is slightly dodgy. You are assuming this, which may change depending on configuration or future changes. Could you add a BUILD BUG ON() here to ensure that this remains the case? Where is plt ent ever used? Please move the if () check into prealloc fixed(), and only keep the loop below Please use a normal for loop here and iterate upward starting at 0",technical
,
"Above is exactly the place it's used. I need to cache it because after the module load is finished the ELF header is freed, pointer is not valid any more. With the above modification it's possible to call the function during the whole lifetime of the module .I'll prepare v5 based on your other comments.",technical
,
"Right, ok. That's a problem. This means that you are relying on get module plt() being called at least once at module load time, which is not guaranteed.Instead, you should set this member (and perhaps the entire prealloc routine) in a module finalize() implementation.",technical
,
"I think it would be much better to use the module finalize() hook for this, given that it is only called once already, and all the data you need is still available.Note that ARM already has such a function, so you'll need to add a call there;or something like that. The CONFIG FTRACE dependency can be kept local to module-plts.c",technical
,
Do you consider this a legal C code if without module-plts.o the function would not exist at all? That's too much relying on optimizer I think…,technical
,
"Yes, we rely on that in many different places in the kernel.",technical
,
"However, this approach still allows the C compiler to see the code inside the block, and check it for correctness (syntax, types, symbol references, etc).  Thus, you still have to use an #ifdef if the code inside the block references symbols that will not exist if the condition is not met. ""But we can of course ignore it",technical
,
"No problem, but some kind of (*) block would still be required, because get module plt() has to work after module finalize() as well as *before* it.So before module finalize() we would have to dereference it conditionally.",technical
,
"""will not exist"" is ambiguous here. It is rather common to declare symbols, but only define them conditionally, and use IS ENABLED() to refer to them. As the documentation says, this gets rid of #ifdefs, making the code always visible to the compiler which is a good thing.",technical
,
"now when I have a new implementation via module finalize(), I must admit it's not possible to do it sanely this way.module finalize() can only add entries at the end of PLT, which means, they will be different from the entries module loader/relocator has created before, which means, FTRACE will not be able to replace these entries with NOPs.As I don't want to do O(N) search on every dynamic ftrace operation, seems this is not an option. Either v4 has to be accepted, or I cannot propose a solution for upstream  combination.",technical
,
I'm trying to parse this but I'm not really sure. All I know is and that is really a 4-byte unsigned quantity so anything less is an arbitrary limitation.,technical
,
"There is no limit on CPU equivalence table length in this patch series like it was in the previous version. The maximum possible value returned by comes from the maximum value of this 'size'variable (that is UINT MAX) plus the header length This won't fit in 'int' type, hence this patch. That's because this functions tells its caller how much bytes to skip from the beginning of a microcode container file to the first patch section contained in it.",technical
,
"OK, will do then.",technical
,
"Sure, it leaves the function to deal with the equiv table length only and the caller then adds the header length. Which is actually cleaner",technical
,
"This can be done if this function is modified to return only the CPU equivalence table length (without the container header length), leaving its single caller the job of adding the container header length to skip to the fist patch section. Otherwise we introduce a equivalence table length limit of UINT MAX - CONTAINER HDR SZ, as anything more will overflow an unsigned int variable on a 64-bit kernel (on 32-bit this will be caught by the equivalence table truncation check).",technical
,
So this one missed writel calls in,technical
,
"thank you very much, I will resend my patch.BR..",technical
,
"Is this commit ready to hit tip/sched/core? I'm looking for an immutable branch that I can use as a basis for the ""dax vs dma vs truncate"" fix series. Thanks in advance.",technical
,
will do. thanks for the feedback,technical
,
You can update the writel call in fm10k tx map as well. Of the drivers updated in drivers/net/ethernet/intel/* it looks like this is the only one that still requires any additional changes. Thanks.,technical
,
"I don't think this bring anything. However, if you want to fix something you should jump below on error to disable the clock instead of returning 'ret' directly.",technical
,
"I'm still not sure I understand the full extent of the originally-reported error (it's still likely a SPI transport issue?), but I believe this patch is good anyway: I wonder if we should tone down the BUG ON()'s in drivers/mfd/cros ec*and drivers/platform/chrome/* too. That's basically a no-no these days, as all of these type of things should be able to gracefully propagate errors, no matter how ""unlikely"" it should be to see a crazy protocol version number or a bad message length.",technical
,
"Jon tracked down the root cause of the originally-reported error, but we should still land this patch, as it fixes error signaling that was previously broken.",technical
,
Can you also queue this one as a fix for v4.15?,technical
,
"Applied, thanks.",technical
,
"Dear all, Cc'ing some more chromium folks. This patch is a bit old and is already applied but I would like to discuss an issue that Tomeu found playing with kernelci and a VeyronJaq attached to our lab. Seems that after this patch, the veyron jaq spits out lots of spitranfer error messages. See full log here. The issue is random, not always happens, but when happens makes cros-ec unusable. Reproduce the issue is easier if CONFIG SMP is not set. What happens is that the master starts the transmission and the cros-ec returns an spi error event. The difference between after and before the patch is that now the cros-ec does not recover, whereas without this patch after some tries it succeeds (note that before the patch the affected code returned -EAGAIN whereas now returns -EREMOTEIO)I think that reverting this patch is NOT the solution, but I don't have enough knowledge yet to understand why the cros-ec fails. I need to look at the cros-ec firmware to understand better what is happening, but meanwhile, thoughts? clues? An important note is that I did not reproduce the issue on a Veyron Minnie, even with CONFIG SMP disabled.",technical
,
"Hello Enric Would it be possible for you to run ""ectool version"" on both your machines? Based on the code the EC is running we might have some hints on what the differences are. You can find both ectool and the code the ec runs on Though I would use ectool from the master branch. One thing I suspect is different is that veyron minnie regularly polls an accelerometer, depending on the userspace you're running it's possible it unwedged itself with a few accelerometer requests.",technical
,
"I think that accessing to the ec console should give the same result, right? In such case here is: We're running the RW firmware and I just discovered that our jaq is a mighty (but I guess it's the same?)",technical
,
"Yep, even better. Looks like your mighty is running the RO firmware, whereas your minnie runs RW. Is it possible you have the 0x200 bit in the gbb flags set on mighty? That would prevent the RO->RW transition, and give you an older firmware.6727e1d..242f6bd is quite the change. I see some spi changes too, though i believe it's mostly at power state transitions (suspend/resume). Other changes include battery settings (yeah.. you should really avoid running that RO if you can avoid it) and a ton of accelerometer stuff for minnie. If it's not the gbb flags, and we can't figure it out why you're stuck in RO, you can also use ""sysjump RW"" to force the RW copy on mighty. See if there's any behavior changes in what you care about. They're essentially the same, but they're running slightly different firmware. In practice the only difference is that mighty's firmware reads an extra gpio for the battery presence. Feel free to diff the board/{jaq,mighty} ec folders for yourself for more details/assurances. All in all I'm not sure that the version differences are enough to explain the spi errors you see in the kernel. My bet is back to the accelerometer stuff: Are you running chrome os ui on this device (is there a chrome os-chrome process constantly polling the accelerometer, so asking the cros-ecdriver for transfers)? Another thing to make sure accelerometer is disabled is to run""accelerate 0 0"" on the minnie EC. If none of that accelerometer stuff is enabled, minnie should essentially act like a mighty/jaq.",technical
,
Reviewed-by: Gilad Ben-Yossef ,technical
,
"This one had various issues.  I've fixed most of what I saw and staged in linux-next (purely for build test coverage purposes).  I may drop this patch if others disagree with it (or my sg deallocation in the error path question isn't answered).I've staged the changes here (and in linux-next via 'for-next') switched all the new GFP KERNEL uses to GFP NOIO.  The fact that you're doing allocations at all (per IO) is bad enough.  Using GFP KERNEL is a serious liability (risk of deadlock if dm-verity were to be used for something like.. swap.. weird setup but possible).But the gfp flags aside, the need for additional memory and the expectation of scalable async parallel IO is potentially at odds with changes like this (that I just staged, and had to rebase your 2 patches on top of) So I'm particularly interested to hear from google folks to understand if they are OK with your proposed verity async crypto API use. ",technical
,
"Out of my curiosity, since I thought whether or not this should use GFP NOIO during my review but than answered to myself ""Nah, dm-verity is read only, can't swap to that"" - how does one use a read only DM-Verity to host swap partition/file? :-)If by ""scalable async parallel IO"" you mean crypto HW than for what it's worth, my experience is that makers of devices with is less powerful CPUs are the ones that tend to add them, so they stands to benefit  the most of this change. Gilad",technical
,
"Okay, I definitely would like to see dm-verity better support hardware crypto accelerators, but these patches were painful to read. There are lots of smaller bugs, but the high-level problem which you need to address first is that on every bio you are always allocating all the extra memory to hold a hash request and scatter list for every data block.  This will not only hurt performance when the hashing is done in software (I'm skeptical that your performance numbers are representative of that case), but it will also fall apart under memory pressure.  We are trying to get low-end Android devices to start using dm-verity, and such devices often have only 1 GB or even only 512MB of RAM, so memory allocations are at increased risk of failing.  In fact I'm pretty sure you didn't do any proper stress testing of these patches, since the first thing they do for every bio is try to allocate a physically contiguous array that is nearly as long as the full bio data itself (n blocks *sizeof(struct dm verity req data) = n blocks * 3264, at least on a 64-bit platform, mostly due to the 'struct dm verity fec io'), so potentially up to about 1 MB; that's going to fail a lot even on systems with gigabytes of RAM...(You also need to verify that your new code is compatible with the forward error correction feature, with the ""ignore zero blocks"" option, and with the new""check at most once"" option.  From my reading of the code, all of those seemed broken; the dm verity fec io structures, for example, weren't even being initialized...)I think you need to take a close look at how dm-crypt handles async crypto implementations, since it seems to do it properly without hurting the common case where the crypto happens synchronously.  What it does, is it reserves space in the per-bio data for a single cipher request.  Then, *only* if the cipher implementation actually processes the request asynchronously (as indicated by-EINPROGRESS being returned) is a new cipher request allocated dynamically, using a mempool (not kmalloc, which is prone to fail).  Note that unlike your patches it also properly handles the case where the hardware crypto queue is full, as indicated by the cipher implementation returning -EBUSY; in that case,dm-crypt waits to start another request until there is space in the queue. I think it would be possible to adapt dm-crypt's solution to dm-verity.",technical
,
"Mike and others, did anyone even try to run verity setup tests? We have verity-compat-test in our test suite, is has even basic FEC tests included. We just added userspace verification of FEC RS codes to compare if kernel behaves the same. I tried to apply three last dm-verity patches from your tree to Linus mainline. It does even pass the *first* line of the test script and blocks the kernel forever...(Running on 32bit Intel VM.)*NACK* to the last two dm-verity patches.(The ""validate hashes once"" is ok, despite I really do not like this approach...)And comments from Eric are very valid as well, I think all this need to be fixed before it can go to mainline.",technical
,
"Thanks for the detailed feedback, I'll have a look at how  dm-crypt avoid dynamic allocation per-bio, and also do forward error correction tests. Subject: Re: [dm-devel] [PATCH 2/2] md: dm-verity: allow parallel processing of bio blocks I definitely would like to see dm-verity better support hardware crypto accelerators, but these patches were painful to read. There are lots of smaller bugs, but the high-level problem which you need to address first is that on every bio you are always allocating all the extra memory to hold a hash request and scatter list for every data block.  This will not only hurt performance when the hashing is done in software (I'm skeptical that your performance numbers are representative of that case), but it will also fall apart under memory pressure.  We are trying to get low-end Android devices to start using dm-verity, and such devices often have only 1 GB or even only 512 MB of RAM, so memory allocations are at increased risk of failing.  In fact I'm pretty sure you didn't do any proper stress testing of these patches, since the first thing they do for every bio is try to allocate a physically contiguous array that is nearly as long as the full bio data itself (n blocks * sizeof(struct dm verity req data) = n blocks * 3264, at least on a 64-bit platform, mostly due to the 'struct dm verity fec io'), so potentially up to about 1 MB; that's going to fail a lot even on systems with gigabytes of RAM...(You also need to verify that your new code is compatible with the forward error correction feature, with the ""ignore zero blocks"" option, and with the new ""check at most once"" option.  From my reading of the code, all of those seemed broken; the dm verity fec io structures, for example, weren't even being initialized...)I think you need to take a close look at how dm-crypt handles async crypto implementations, since it seems to do it properly without hurting the common case where the crypto happens synchronously.  What it does, is it reserves space in the per-bio data for a single cipher request.  Then, *only* if the cipher implementation actually processes the request asynchronously (as indicated by -EINPROGRESS being returned) is a new cipher request allocated dynamically, using a mempool (not kmalloc, which is prone to fail).  Note that unlike your patches it also properly handles the case where the hardware crypto queue is full, as indicated by the cipher implementation returning -EBUSY; in that case, dm-crypt waits to start another request until there is space in the queue. I think it would be possible to adapt dm-crypt's solution to dm-verity.",technical
,
"I need to rewrite these patches according to issues you and Eric Biggers mentioned. Please drop this v1 patch. Subject: Re: [PATCH 2/2] md: dm-verity: allow parallel processing of bio blocks This one had various issues.  I've fixed most of what I saw and staged in linux-next (purely for build test coverage purposes).  I may drop this patch if others disagree with it (or my sg deallocation in the error path question isn't answered).I've staged the changes here (and in linux-next via 'for-next') I switched all the new GFP KERNEL uses to GFP NOIO.  The fact that you're doing allocations at all (per IO) is bad enough.  Using GFP KERNEL is a serious liability (risk of deadlock if dm-verity were to be used for something like.. swap.. weird setup but possible).But the gfp flags aside, the need for additional memory and the expectation of scalable async parallel IO is potentially at odds with changes like this (that I just staged, and had to rebase your 2 patches on top of): So I'm particularly interested to hear from google folks to understand if they are OK with your proposed verity async crypto API use.",technical
,
"I will run verity setup test on next version of these patches and contact you about verity-compat-test test suits. Subject: Re: [dm-devel] [PATCH 2/2] md: dm-verity: allow parallel processing of bio blocks Mike and others, did anyone even try to run verity setup tests? We have verity-compat-test in our test suite, is has even basic FEC tests included. We just added userspace verification of FEC RS codes to compare if kernel behaves the same. I tried to apply three last dm-verity patches from your tree to Linus mainline. It does even pass the *first* line of the test script and blocks the kernel forever...(Running on 32bit Intel VM.)*NACK* to the last two dm-verity patches.(The ""validate hashes once"" is ok, despite I really do not like this approach...) And comments from Eric are very valid as well, I think all this need to be fixed before it can go to mainline.",technical
,
"I have a question regarding scatter list memory: I noticed that all blocks in dm verity end up using two buffers: one for data and other for salt. I'm using function similar to verity for io block to iterate and find the number of buffers, in my case data dev block bits =12, to do=4096, thus the do while will iterate only once. I assume that since it's there there are cases it'll iterate more. I'm trying to figure out which cases will require more than one buffer of data per block? In dm crypt there is limitation of static 4 scatter list elements per in/out (see struct dm crypt request).Is there an upper bound regarding number of buffers per block in dm-verity? I need this for the implementation of  mempool per scatter list buffers.",technical
,
Peter points out that this isn't sufficient...  Let me try again.,technical
,
"That's a hefty cc list. I can't see Rob Herring though, and he's usually the person who you need to convince to get your bindings accepted. I recommend using it to build your CC list, and then add others you think are relevant. I'm not sure what the guidelines are for generic bindings, so I'll defer to Rob for this patch. ",technical
,
"We try to capitalise ASPEED. Are you sure that this is driven by clkin? Most peripherals on the A speed are attached to the apb, so should reference that clock. Can you explain why you need both the parent clock and this frequency to be specified? Perhaps msg-timing-period? Or just msg-timing?",technical
,
The patches to the device trees get merged by the ASPEED maintainer(me). Once you have the bindings reviewed you can send the patches tome and the linux-aspeed list (I've got a pending patch to maintainers that will ensure get maintainers.pl does the right thing as far as email addresses go). I'd suggest dropping it from your series and re-sending once the bindings and driver are reviewed. ,technical
,
"Thanks a lot for sharing your time. Please see my inline answers. No it isn't. Will drop the line. Okay. I'll use bool instead of int. The two above functions are slightly different but uses the same PECI command which provides both Tthrottle and Tcontrol values in pkg configarray so it updates the values to reduce duplicate PECI transactions. Probably, combining these two functions into get ttrottle and tcontrol()would look better. I'll rewrite it.Are you pointing out this code? Then I'll rewrite it as a function. If not, please point out the duplication. Core temperature group will be registered only when it detects at least one core checked by check resolved cores(), so find core index() can be called only when priv->core mask has a non-zero value. The 'nothing is found' case will not happen.cputemp read string() is mapped to read string member of hwmon opsstruct, so hwmon subsystem passes the channel parameter based on the registered channel order. Should I modify hwmon subsystem code? As explained above, find core index() returns a correct index always. This is an attribute of DTS margin temperature which reflects thermal margin to Tcontrol of the CPU package. If it shows '0' means it reached to Tcontrol, the first level of thermal warning. If the CPU keeps getting hot then this DTS margin shows a negative value until it reaches to Tjmax. When the temperature reaches to Tjmax at last then it shows the lower critical value which lcrit indicates as the second level of thermal warning. Both Tjmax and Tcontrol have positive values and Tjmax is greater than Tcontrol always. As explained above, lcrit of DTS margin should show a negative value means the margin goes down across '0'. On the other hand,crit hyst of Die temperature should show absolute hysteresis value between Tcontrol and Tjmax.This driver provides multiple channels and each channel has its own supplement attributes. As you mentioned, Die temperature channel and Core temperature channel have their individual crit attributes and they reflect the same value, Tjmax. It is not reporting several times but reporting the same value. Each function is called from cputemp read() which is mapped to read function pointer of hwmon ops struct. Since each channel has different set of attributes so the cputemp read() calls an individual channel handler after checking the channel type. Of course, we can handle all attributes of all channels in a single function but the way also needs channel type checking code on each attribute. Sure, will use defines instead. For proper operation of this driver, PECI CMD GET TEMP and PECI CMD RD PKG CFG have to be supported by a client CPU.PECI CMD GET TEMP is provided as a default command but PECI CMD RD PKG CFG depends on PECI minor revision of a CPU package so this checking is needed. Got it. I'll remove the error message and will add a proper handling code into PECI core. This driver can't support core temperature monitoring if a CPU doesn't support PECI CMD RD PCI CFG LOCAL command. In that case, it skips core temperature group creation and supports only basic temperature monitoring of Die, DTS margin and etc. I'll add this description as a comment. Should I split out hwmon documents and dt bindings too? No. Will drop the line. It is temperature monitoring specific function and it touches module specific variables. Do you really think that this non-generic function should be moved to PECI core? I'm sure it isn't. Sure, I'll rewrite it. Okay. In case of check cpu id(), it could be used as a generic PECIfunction. I'll move it into PECI core. I'll rewrite it to.Should I use -EPERM? Any suggestion? Client address range validation will be done inpeci check addr validity() in PECI core before probing a device driver. I'll remove the error message and will add a proper handling code into PECI core on each error type. Yes, it would be safer. Will fix it.",technical
,
"There is lots of other duplication. That doesn't guarantee a match. If what you are saying is correct there should always be a well defined match of channel -> idx, and the search should be unnecessary. Huh ? Changing;requires a hwmon core change ? Really ?The hwmon ABI reports chip values, not constants. Even though some drivers do it, reporting a constant is always wrong. The hwmon ABI requires reporting of absolute temperatures in milli-degrees C.Your statements make it very clear that this driver does not report absolute temperatures. This is not acceptable. Then maybe fold the functions accordingly ?I do not question the check. I question the error message and error return value. Why is it an  error  if the CPU does not support the functionality, and why does it have to be reported in the kernel log ?  The message says ""Failed to ..."". It does not say ""This CPU does not support ..."".Why does this message display the device name twice ? Actually, that is wrong. You refer to address-of. Bit operations do have lower precedence that comparisons. I stand corrected. Is it an  error  if the CPU does not support this functionality ?",technical
,
"Thanks for sharing your time. Please see my answers inline. Yes, it took a hidden review process between v2 and v3. I know it's an unusual process but it was requested. Hopefully, change logs in cover letter could roughly provide the details. Thanks for your comments. Agreed. I'll change the description. Okay then, better change it now than later. Will change all defines. It doesn't use all but better keep for bug fix or improvement use, I think. Yes, that would be better. I'll rewrite it.Yes, it could be simplified like you pointed out. Will change it.Got it. I'll replace it with print hex dump debug() after removing the define. Intention was that make it run just amount up to the rx len but it's not efficient. I'll rewrite it like you suggested. No specific reason. regmap makes some overhead as you mentioned but it also provides some advantages on access simplification, endianness handling and register dump at run time. I'd not insist using of regmap if you prefer using of raw readl and writel. Do you? You are right. I'll keep this checking only in  probe() function and remove all redundant error checking codes on memory mapped IO. This code makes changes on the status ack variable to write back ack bit on each interrupt. Unlike other HW module, PECI uses the 24MHz external clock as its clock source. Should it use clk-aspeed.c in this case? Agreed. I'll make it print out the message only when this.I'll test it again and will remove it if it is not necessary. You are right. I'll remove the flag.",technical
,
"Got it. Will capitalize all A speed words. According to the datasheet, PECI controller module is attached to apb but its clock source is the 24MHz external clock. Based on this setting, driver code makes clock divisor value to set operation clock of PECI controller which is adjustable. Will use msg-timing instead.",technical
,
Do you mean that bindings and driver of ASPEED peci adapter driver including documents?,technical
,
"Sorry but can you point out the duplication? There could be some disabled cores in the resolved core mask bit sequence also it should remove indexing gap in channel numbering so itis the reason why this search function is needed. Well defined match of channel -> idx would not be always satisfied. Sorry for my misunderstanding. You are right. I'll change the parameter passing of find core index() from 'channel' to 'channel -DEFAULT CHANNEL NUMS'.Okay. I'll remove the 'DTS margin' temperature. All others are reporting absolute temperatures. I'll use a single function for 'Die temperature' and 'Core temperature' that have the same attributes set. It would simplify this code a bit. Got it. I'll change that to dev dbg. Got it. Will correct the message. For an example, dev name(hwmon dev) shows 'hwmon5' and priv->name shows'peci-cputemp0'.Actually, it returns from this probe() function without making any hwmon info creation so I intended to handle this case as an error. Am I wrong?",technical
,
"write a python script to do a semantic comparison. Are you saying that each call to the function, with the same parameters, can return a different result ?And dev dbg() shows another device name. So you'll have something like this. If the functionality or HW supported by the driver isn't available, it is customary to return -ENODEV and no error message. Otherwise the kernel log would drown in ""not supported"" error messages. I don't see where it would add any value to handle this driver differently. Invalid argument EPERM.Operation not permitted You'll have to work hard to convince me that any of those makes sense, and that ENODEV	No such device doesn't. More specifically, if EINVAL makes sense, the caller did something wrong, meaning there is a problem in the infrastructure which should get fixed. The same is true for EPERM.",technical
,
"Okay. I'll try to simplify this code again. No, the result will be consistent. After reading the priv->core mask once in check resolved cores(), the value will not be changed. I'm saying about this case, for example if core number 2 is unresolved in total 4 cores, then the idx order will be '0, 1, 3' but channel order will be '5, 6, 7' without making any indexing gap. Practically it shows like where 0-30:00 is assigned by peci core. Now I fully understood what you pointed out. Thanks for the detailed explanation. I'll change the error return value to -ENODEV and will use dev dbg for the message printing. Thanks!",technical
,
"And you yet you claim that this is not well defined ? Or are you concerned about the amount of memory consumed by providing an array for the mapping ? Note that an indexing gap is acceptable and, in many cases, preferred. And what message would you see for cpu1 ?",technical
,
"""dt-bindings: ..."" for the subject prefix please. This should be all one document. No need for these 2 here. Some details on the addressing for PECI would be good. This part of the example is not relevant. Just start with the adapter node. I don't understand what this has to do with PECI? ""simple-bus"" already has a defined meaning. Bindings are for h/w, not client drivers. How are PECI devices defined? 8 devices should be enough for anyone...Where is PECI OFFSET MAX defined? Not a valid node name. is what it probably should be. Fora new bus you can define unit-address format you like, but it must be based on the contents of reg. However, it doesn't look like you should create anything special here.",technical
,
"This is the frequency of the bus or used to derive it? It would be better to specify the bus frequency instead and have the driver calculate its internal freq. And then use ""bus-frequency"" instead/ /-/All these either need vendor prefixes or should be standard properties for PECI adapters. I think probably the latter case. If so, the first2 should probably be in units of clocks (not 4 clocks). And they should then be documented in the common PECI binding doc.No need to show this part in examples.",technical
,
"""dt-bindings: hwmon: ..."" for the subject. Again, where is PECI OFFSET MAX defined? It can't depend on something in the kernel. Unit-address is wrong. It is a different bus from cputemp? Otherwise, you have conflicting addresses. If that's the case, probably should make it clear by showing different host adapters for each example.",technical
,
"Thanks for sharing your time. Please see my answers inline. Sure, I'll change the subject. Okay. I'll combine them into one document. Will drop these 2.It is for the PECI client address. Will add details. Will remove that part. Thanks! Maybe I'm wrong but I intended to show this node is an umbrella node of a PECI bus subsystem. What should I use then? Got it. I'll correct the description. PECI client device is Intel CPU which is connected through a PECI bus.PECI OFFSET MAX is defined in include/linux/peci.h based on the maximum CPU numbers of the current IA generation. I'll remove the unnecessary details. A setting out of range would be handled accordingly in kernel. Got it. I'll fix these node name like function@30 and function@31.Thanks a lot for your comments!",technical
,
"I agree with you. Actually, it is being used for operation frequency setting of PECI controller module in SoC so it's different from the meaning of ""bus-frequency"". I'll change it to ""operation-frequency"". Will fix it.So far I've checked that these are ASPEED PECI controller specific properties so it should be listed in here. Got it. Will drop the part.",technical
,
"I'll change the subject. I'll remove the unnecessary description. Will fix it using the reg value. It could be the same bus with cputemp. Also, client address sharing is possible by PECI core if the functionality is different. I mean, cputemp and dimmtemp targeting the same client is possible case like this.peci-cputemp@30peci-dimmtemp@30",technical
,
"Oh, I got your point. Probably, I should change these separate settings into one like peci-client@30;Then cputemp and dimmtemp drivers could refer the same compatible string. Will rewrite it.",technical
,
"No, now you've gone from a standard property name to something custom. Why do you need to set the frequency in DT if it is not related to the interface frequency? Rob",technical
,
"Just a drive-by nit:FWIW, <linux/bitfield.h> already provides functionality like this, so it might be worth taking a look at FIELD {GET,PREP}() to save all these local definitions.",technical
,
"Actually, the interface frequency is affected by the operation frequency but there is no description of its relationship in datasheet. I'll check again about the detail to ASPEED chip vendor and will use 'bus-frequency' if available.",technical
,
"Yes, that looks better. Thanks a lot for your pointing it out.",technical
,
"I've checked it again and realized that it should use function based node name like:peci-cputemp@30peci-dimmtemp@30If it use the same string like 'peci-client@30', the drivers cannot be selectively enabled. The client address sharing way is well handled in PECI core and this way would be better for the future implementations of other PECI functional drivers such as crash dump driver and so on. So I'm going change the unit-address only.",technical
,
"I investigated it more deeply. Basically, by the spec, PECI bus speed cannot be set as a fixed speed. A PECI bus can have a wide speed range from 2Kbps to 2Mbps which is dynamically set by a handshaking sequence between an originator and clients called 'timing negotiation' in spec. This timing negotiation behavior happens on every single transaction so the bus speed also can vary on every transactions. So I'm thinking a custom property name for it, 'peci-clk-frequency' if it is acceptable.",technical
,
"Okay, seems bus-frequency is not appropriate here. So use 'clock-frequency' (note the '-' not ' ' as that is the standard property).",technical
,
"2 nodes at the same address is wrong (and soon dtc will warn you on this). You have 2 potential options. The first is you need additional address information in the DT if these are in fact 2 independent devices. This could be something like a function number to use something from PCI addressing. From what I found on PECI, it doesn't seem to have anything like that. The 2nd option is you have a single DT node which registers multiple hwmon devices. DT nodes and driver don't have to be 1-1. Don't design your DT nodes from how you want to partition drivers in some OS.",technical
,
Thanks! I'll use 'clock-frequency' for it.,technical
,
"Please correct me if I'm wrong but I'm still thinking that it is possible. Also, I did compile it but dtc doesn't make a warning. Let me show an another use case which is similar to this case:]This is device tree setting for LPC interface and its child nodes.LPC interface can be used as a multi-functional interface such as snoop 80, KCS, SIO and so on. In this use case are sharing their address range from their individual driver modules and they can be registered quite well through both static dt or dynamic dtoverlay. PECI is also a multi-functional interface which is similar to the above case, I think.",technical
,
"I did say *soon*. It's in dtc repo, but not the kernel copy yet. This case too is poor design and should be fixed as well. Simply put, you can have 2 devices on a bus at the same address without some sort of mux or arbitration device in the middle. If you have a device/block with multiple functions provided to the OS, then it is the OS's problem to arbitrate access. It is not a DT problem because OS's can vary in how they handle that both from OS to OS and over time.",technical
,
"If I change it to a single DT node which registers 2 hwmon devices using the 2nd option above, then I still have 2 devices on a bus at the same address. Does it also make a problem to the OS then?",technical
,
"Additionally, I need to explain that there is one and only bus host(adapter) and multiple clients on a PECI bus, and PECI spec doesn't allow multiple originators so only the host device can originate message. In this implementation, all message transactions on a bus from client driver modules and user space will be serialized well in the PECI core bus driver so bus occupation and traffic arbitration will be managed well in the PECI core bus driver even in case of a bus has 2client drivers at the same address. I'm sure that this implementation doesn't make that kind of problem to OS.",technical
,
"Thanks a lot for your review. I think, it should contain actual device resource release code which is being done, or a coupling logic should be added between them.As you suggested, I'll check it again after reading documentation and understanding core.c code more deeply.",technical
,
"Does it make sense one driver per patch? Are we talking about x86 CPU IDs here? If so, why x86 corresponding headers, including intel-family.h are not used?",technical
,
"All comments you got for patch 6 are applicable here. And perhaps in the rest of the series. The rule of thumb: when you get even single comment in a certain place,re-check  entire  series for the same / similar patterns!",technical
,
"Thanks a lot for your review. Please check my inline answers. Yes, I'll separate it into two patches. Yes, that would make more sense. I'll include the intel-family.h and will use these defines instead.",technical
,
Thanks for your advice. I'll keep that in mind.,technical
,
Thanks a lot for letting me know that. I'll do as you suggested.,technical
,
"If the indexing gap is acceptable, the index search function isn't needed anymore. I'll fix all relating code to make that use direct mapping of channel -> idx then. Thanks! It shows this.",technical
,
"Hello, so I finally got to this :) I know some people (Matthew Wilcox?) wanted to do something like KSM for file pages - not all virtualization schemes use overlay fs and e.g. if you use reflinks (essentially shared on-disk extents among files) for your container setup, you could save significant amounts of memory with the ability to share pages in page cache among files that are reflinked.It is interesting that you can get rid of page->mapping uses in most places. For page reclaim (vmscan) you'll still need a way to get from a page to an address space so that you can reclaim the page so you can hardly get rid of page->mapping completely but you're right that with such limited use that transition could be more complex / expensive. What I wonder though is what is the cost of this (in the terms of code size and speed) - propagating the mapping down the stack costs something... Also in terms of maintainability, code readability suffers a bit. This could be helped though. In some cases it seems we just use the mapping because it was easily available but could get away without it. In other case (e.g. lot of fs/buffer.c) we could make bh -> mapping transition easy by storing the mapping in the struct buffer head - possibly it could replace b bdev pointer as we could get to that from the mapping with a bit of magic and pointer chasing and accessing b bdev is not very performance critical. OTOH such optimizations make a rather complex patches from mostly mechanical replacement so I can see why you didn't go that route. Overall I think you'd need to make a good benchmarking comparison showing how much this helps some real workloads (your motivation) and also how other loads on lower end machines are affected.So I'm interested in this write protection mechanism but I didn't find much about it in the series. How does it work? I can see KSM write protects pages in page tables so that works for userspace mappings but what about in-kernel users modifying pages - e.g. pages in page cache carrying filesystem metadata do get modified a lot like this. AFAIK you're right. Auch, the fact that we could share a page as data storage for several inode+offset combinations that are not sharing underlying storage just looks viciously twisted ;) But is it really that useful to warrant complications? In particular I'm afraid that filesystems expect consistency between their internal state (attached to page->private) and page state(e.g. page->flags) and when there are multiple internal states attached to the same page this could go easily wrong..",technical
,
"Yes i believe they are still use case where KSM with file back page make senses, i am just not familiar enough with those workload to know how big of a deal this is.Idea for vmscan is that you either have regular mapping pointer store inpage->mapping or you have a pointer to special struct which has a function pointer to a reclaim/walker function (rmap walk ksm)I haven't checked that, i will, i was not so concern because in the vast majority of places there is already struct address space on the stackframe (i.e. local variable in function being call) so moving it to function argument shouldn't impact that. However as i expect this will be merge over multiple kernel release cycle and the intermediary step will see an increase in stack size. The code size should only grow marginally i expect. I will provide numbers with my next posting after LSF/MM.I am willing to do the buffer head change, i remember considering it but I don't remember why not doing it (i failed to take note of that).Do you have any specific benchmark you would like to see ? My list was:  For workload i care this will be CUDA workload. We are still working on the OpenCL open source stack but i don't expect we will have something that can shows the same performance improvement with OpenCL as soon as with CUDA.So i only care about page which are mmaped into a process address space. At first i only want to intercept CPU write access through mmap of file but i also intend to extend write syscall to also ""fault"" on the write protection i.e. it will call a callback to unprotect the page allowing the write protector to take proper action while write syscall is happening. I am afraid truly generic write protection for metadata pages is bit out of scope of what i am doing. However the mechanism i am proposing can be extended for that too. Issue is that all place that want to write to those page need to be converted to something where write happens between write begin and write end section (mmap and CPU pte does give this implicitly through page fault, so does write syscall). Basically there is a need to make sure that write and write protection can be ordered against one another without complex locking. So at first i want to limit to write protect (not KSM) thus page->flags will stay consistent (i.e. page is only ever associated with a single mapping). For KSM yes the page->flags can be problematic, however here we can assume that page is clean (and up to date) and not under writeback. So problematic flags for KSM. Idea again would be to PageFlagsWithMapping(page, mapping) so that for non KSM write protected page you test the usual page->flags and for write protected page you find the flag value using mapping as lookup index. Usually those flag are seldomly changed/accessed. Again the overhead (ignoring code size) would only be for page which are KSM. So maybe KSM will not make sense because perf overhead it has with page->flags access (i don't think so but i haven't tested this).Thank you for taking time to read over all this.",technical
,
"Imagine container farms where they deploy the base os image via cp --reflink.This would be a huge win for btrfs/xfs but we've all been too terrified of the memory manager to try it. :)For those following at home, we had a track at LSFMM2017 (and hallwaybofs about this in previous years): starts to look at this big series, having sent his own yesterday. :)",technical
,
"So e.g. mmtests have a *lot* of different tests so it's probably not realistic for you to run them all. I'd look at bonnie++ (file & dir tests),dbench, reaim - these are crappy IO benchmarks because they mostly fit into page cache but for your purposes this is exactly what you want to see differences in CPU overhead :).I understand metadata pages are not interesting for your use case. However from mm point of view these are page cache pages as any other. So maybe my question should have been: How do we make sure this mechanism will not be used for pages for which it cannot work? Yeah, sure, page->flags could be dealt with in a similar way but at this point I don't think it's worth it. And without page->flags I don't think abstracting page->private makes much sense - or am I missing something why you need page->private depend on the mapping? So what I wanted to suggests that we leave page as is currently and just concentrate on it.",technical
,
"Oh that one is easy, the API take vma + addr or rather mm struct + addr(i.e. like KSM today kind of). I will change wording in v1 to almost generic write protection :) or process' page write protection (but this would not work for special pfn/vma so not generic their either).Well i wanted to go up to KSM or at least as close as possible to KSM for file back page. But i can focus on page->mapping first, do write protection with that and also do the per page wait queue for page lock. Which i believe are both nice features. This will also make the patchset smaller and easier to review (less scary).KSM can be done on top of that latter and i will be happy to help. I have a bunch of coccinelle patches for page->private, page->index and i can do some for page->flags.",technical
,
"Your approach seems useful if there are lots of locked pages sharing the same wait queue. That said, in the original workload from our customer with the long wait queue problem, there was a single super hot page getting migrated, and itis being accessed by all threads which caused the big log jam while they wait for the migration to get completed. With your approach, we will still likely end up with a long queue in that workload even if we have per page wait queue.",technical
,
"Ok so i re-read the thread, i was writing this cover letter from memory and i had bad recollection of your issue, so sorry. First, do you have a way to reproduce the issue ? Something easy would be nice :)So what i am proposing for per page wait queue would only marginally help you (it might not even be measurable in your workload). It would certainly make the code smaller and easier to understand i believe. Now that i have look back at your issue i think there is 2 things we should do. First keep migration page map read only, this would at least avoid CPU read fault. In trace you captured i wasn't able to ascertain if this were read or write fault. Second idea i have is about NUMA, every time we NUMA migrate a page we could attach a temporary struct to the page (using page->mapping). So if we scan that page again we can inspect information about previous migration and see if we are not over migrating that page (i.e. bouncing it all over). If so we can mark the page (maybe with a page flag if we can find one) to protect it from further migration. That temporary struct would be remove after a while, i.e. autonuma would preallocate a bunch of those and keep an LRU of them and recycle the oldest when it needs a new one to migrate another page.LSF/MM slots:Michal can i get 2 slots to talk about this ? MM only discussion, one to talk about doing migration with page map read only but write protected while migration is happening. The other one to talk about attaching auto NUMA tracking struct to page.",technical
,
"Unfortunately it is a customer workload that they guard closely and wouldn't let us look at the source code.  We have to profile and backtrace its behavior. Mel made a quick attempt to reproduce the behavior with a hot page migration, but he wasn't quite able to duplicate the pathologic behavior. In certain cases if we have lots of pages sharing a page wait queue, your solution would help, and we wouldn't be wasting time checking waiters not waiting on the page that's being unlocked.  Though I don't have a specific workload that has such behavior. The goal to migrate a hot page with care, or avoid bouncing it around frequently makes sense.  If it is a hot page shared by many threads running on different NUMA nodes, and moving it will only mildly improve NUMA locality, we should avoid the migration.",technical
,
"Sorry about that, I actually had three people review my code internally, then I managed to send out an old version. 100% guilty of submitting code when I needed sleep. As for the change, that was in response to a request from Andrew to make the update function less racy. Should I resend a correct v2 now that the thread exists? ",technical
,
"Let's just discuss open questions for now. Specifics of the code are the least interesting at this stage. If you want some help with the code review, you can put it somewhere in the git tree and send a reference for those who are interested.",technical
,
"Ok, I will go back through the thread and make sure all questions and concerns have been addressed.",technical
,
"ok How about include/asm-generic/early pfn.h ?And could I use it in this case? Currently, arm/arm64 have memblock enable by default. When other arches implement their, they can include this file?",technical
,
"Thanks, thus the binary search in next step can be discarded?",technical
,
I don't know all the circumstances in which this is called.  Maybe a linear search with memo is more appropriate than a binary search.,technical
,
"That's been brought up before, and the reasoning appears to be something along the lines of...Academics and published wisdom is that on cached architectures, binary searches are bad because it doesn't operate efficiently due to the overhead from having to load cache lines.  Consequently, there seems to be a knee-jerk reaction that ""all binary searches are bad, we must eliminate them. ""What is failed to be grasped here, though, is that it is typical that the number of entries in this array tend to be small, so the entire array takes up one or two cache lines, maybe a maximum of four lines depending on your cache line length and number of entries. This means that the binary search expense is reduced, and is lower than a linear search for the majority of cases. What is key here as far as performance is concerned is whether the general usage of pfn valid() by the kernel is optimal.  We should not optimise only for the boot case, which means evaluating the effect of these changes with  real  workloads, not just ""does my machine boot a milliseconds faster"".--RMK's Patch system: broadband for 0.8mile line in suburbia: sync at 8.8Mbps down 630kbps up According to speedtest.net: 8.21Mbps down 510kbps up",technical
,
"This is actually a good point.a) This does not make sense. At least in general case.b) It is not the case here. Here it's really mostly called with sequentially incremented pfns, AFAICT.In this case it hits mostly the last result or eventually the sequentially next one.IIUC, this is only used during early boot (and memory hot plug) and it does not influence regular runtime. Whether the general usage of pfn valid() by the kernel is optimal is another good question, but that's totally unrelated to this series, IMHO. On the other hand I also wonder if this all really is worth the negligible boot time speedup.",technical
,
"Thanks for your comments, Russell IIUC, are you opposed to entirely removing the binary search instead of my previous patch set? hmm.. But pfn is linearly increased during the booting time. This assumption is not correct in real workload for pfn valid out of booting time. So in my patchset, I defined another pfn valid region for booting time only. I didn't have many arm/arm64 boxes to verified. What I can do is guaranteeing the improvement in my armv8a (QUALCOMM centriq 2400). Sorry about it.",technical
,
"arm96 common'?!  No.  Just no. The right way to share common code is to create a header file (or use an existing one), either in asm-generic or linux, with a #ifdef CONFIG fooblock and then 'select foo' in the arm Kconfig files.  That allows this common code to be shared, maybe with powerpc or x86 or ... in the future.",technical
,
"Sure, but I bet if we are >end pfn, we're almost certainly going to the start pfn of the next block, so why not test that as well?",technical
,
Please pull from  next to receive the latest Thermal Management updates ,technical
,
"Just ""make allmodconfig"" and the warning is about a uninitialized variable. Line 304 if my shell history is to be believed.                ",technical
,
"These couple of warnings were introduced by:  There should be no functional changes caused by this patch.     After digging into, there is no obvious fix. It returns effectively an uninitialized value and the callers are assuming the value is always correct, so it is also not possible to simply return an error.",technical
,
"Hello,Yeah, this has also passed my local compilation error. Somehow my gcc4.9 is not catching it. Using an older gcc (gcc4.6) does catch it.Anyways, given that the conversion functions are written to cover for unexpected cal type, the right way of fixing this is to rewrite the conversion functions to allow for returning error codes and adjusting the callers as expected. please consider the following fix:",technical
,
"I think there are two problems here1. Actually, this error has been raised by 0-day earlier. Don't know why it still goes into thermal-soc tree.2. After pulled the thermal-soc changes, I also asked 0-day to run build test, but I didn't get any warning report (email attached), to look at this issue.",technical
,
"As it is late in this merge window, I'd prefer to1. drop all the thermal-soc material in the first pull request which I will send out soon.2. you can prepare another pull request containing the thermal-socmaterials except the exynos fixes3. exynos fixes with the problem solved can be queued for -rc2 or later.",technical
,
"HI, Since this condition cannot happen (the driver makes sure of this during probe) I would prefer much simpler fix from Arnd: I've already ACKed it two weeks ago). Ditto",technical
,
"I'm not sure these are correct fixes. The change 480b5bfc16e1 tells: ""There should be no functional changes caused by this patch. ""but the fix above returns 0 as a default value instead of '50' or '25'for the 5440 and that impacts the threshold etc ...IMO, the correct fix would be to define a default value '50', override it at init time to '25' if it is a 5440. And then the variable 'temp 'and 'temp code' get this value in the default case.-- ",technical
,
It is okay to return 0 because this code-path (the default one) will be never hit by the driver (probe makes sure of it) - the default case is here is just to silence compilation errors.,technical
"Sent you this I see there is still some discussion around the topic of how to fix this. So, once we get to a point of agreement, I will send the remaining with exynos fixes.",technical
,
What should I do now to help resolve the issue?[ There has been no action from you on Arnd's fix for over two weeks and  also you have not commented on it now…,technical
,
"Actually the switch statement was fine until the cleanup. Regarding the latest comment, this can be fixed properly by 'return' (or whatever you want which does not get around of gcc warnings).",technical
,
I don't see how it was fine before as the driver has never used the default case (always used).Could you please explain this more? Do you mean that you want the patch with switch statement removal? Is incremental fix OK or do you want something else?,technical
,
this is much better fix. ,technical
,
"I'm not saying the code path was fine but from the compiler point of view, it was. By removing the defaulting temp value there is a code path gcc sees the temp variable as not initialized. Your cleanups are relevant.",technical
,
"He has already posted it, I hope the fix is fine with you. Also sorry for the delay with handling issue - I was on holiday last two days and for some reason I was under (wrong) impression that the previous fix has been in thermal tree (so I was quite surprised today reading this mail thread).",technical
,
"should have been. has already posted it, I hope the fix is fine with you.(& sorry for the typo)",technical
,
Please resend your series with the patches without the warnings.,technical
,
Thanks for your reply :)I agree that usleep range() here will not much affect the real execution of this driver. But I think usleep range() can more opportunity for other threads to use the CPU core to schedule during waiting. That is why I detect mdelay() that can be replaced with msleep() orusleep range().,technical
,
"James is right, You have added all usleep range() during system boot-uptime. During boot-up system will run as single threaded. Where this change will not make much sense. System first priority is match the exact timing on each and every boot-up.",technical
,
"What an understatement :-) Can't we instead improve the error message and turn this into apr debug2? Isn't it a reasonable scenario that the user expects this topology information to be present and then ends up without it? Perhaps something like:	",technical
,
"Fine with me, I will provide a version 2....",technical
,
"Will do in my next email. Apologies. This must be Thunderbird, I've been trying to coax into doing otherwise. Will send the corrected patch shortly.",technical
,
"Best is to set up and use git send-email. Anyway, you can see the result of the corruption at; it appears that patchwork doesn't understand your patch either. Guenter",technical
,
"I'm not sure how the input is freed for managed devices. Either you don't have to destroy it here, or you also need to destroy it in a release() function. No need to unregister managed input devices.",technical
,
"I checked again, we do not need to destroy it manually. Will remove in next version. You are correct. Will remove these in next version. Thanks for your comments.-- ",technical
,
So are you saying that multi-slot support is a no go in general or it is only applicable to DW MMC (I really doubt that's a case)?BTW there're other controllers that seem to support multi-slot like Atmel etc.,technical
,
"In general.Yeah, none of those are working - it just bad attempts to try to make*something* work. ",technical
,
Previous multi slot implementation was removed as nobody used it and nobody tested it. There are lots of mistakes in previous implementation which are not related to request serialization like lack of slot switch / lack of adding slot id to CIU commands / ets...So obviously it was never tested and never used at real multi slot hardware.In current implementation data transfers and commands to different hosts (slots) are serialized internally in the dw mmc driver. We have request queue and when .request() is called we add new request to the queue. We take new request from the queue only if the previous one has already finished. So although hosts (slots) have separate locks (mmc claimrelease host())the requests to different slots are serialized by driver. Isn't that enough? I'm not very familiar with SD/SDIO/(e)MMC specs so my assumptions might be wrong in that case please correct me .Nevertheless we had to deal somehow with existing hardware which has multislot dw mmc controller and both slots are used...This patch at least shouldn't break anything for current users (which use it in single slot mode) Moreover we tested this dual-slot implementation and don't catch any problems (probably yet) except bus performance decrease in dual-slot mode (which is quite expected).,technical
,
"That isn't sufficient. The core expects all calls to *any* of the hostops to be serialized for one host. It does so to conform to the specs.For example it may call: ->set ios()->request()->set ios()->request()->request() Sorry, but no. Well, that kind of explains your simplistic approach. I would suggest you to study the specs and the behavior of the mmccore a bit more carefully, that should give you a better understanding of the problems. Honestly, I don't think efforts of implementing this is worth it! Even if we would be able to solve the problem from an mmc subsystem point of view, we would still have the I/O scheduling problem to address. To solve that, we would need to be able to configure upper block layer code to run one scheduling instance over multiple block devices",technical
,
A bit remark for better understanding: All card settings change are serialized too. These settings are applied after slot switch before execution of new request for this slot. So situations like calling any host 0 ops while another host (host 1) is active are handled by current code. This is example of simultaneous ops calls for both slots ,technical
,
This doesn't work as it would mean violation of the specs in some scenarios. Particular during the card initialization and card power off. Ditto. Etc…,technical
,
"HI, Sorry for late. :(Well, I will read the other comments. And reply soon. ",technical
,
"Usually perf would help, but even a simple printk() should suffice to see what's going on there :)We know that there are some cases where the codec / controller communication stalls on the recent Coffee Lake or such platforms. But quite not sure how it happens. Moving the stuff into async just moves something ugly, and it's no fix, per se, if such a long delay itself is unexpected",technical
,
"Indeed.  But even from this result, you can have a rough idea. As you can see, the most of time was spent before ""1"" point, which is the very beginning of azx probe().  That is, the slowness is not in HD-audio driver probe itself.  Rather it's likely because of parallel probing with other multiple devices.",technical
,
"I agree. But that also makes it clear, that the probe can be done in async task, doesn't it?",technical
,
"Yes, but it's no fix, either.  The probe callback itself doesn't take any long time, but the problem is the stage before that.  By declaring the async probe, you can hide it, but it doesn't mean that the whole issue is solved by that.",technical
,
"Thanks, I 'll mark this series as rejected at patchwork.linuxtv.org.Please feel free to resubmit any patch if they represent a real threat, adding a corresponding description about the threat scenario at the body of the e-mail. Anytime.",technical
,
Yeah. I got it. Much appreciated. :),technical
,
"The intent of that comment is to be provocative, in the sense that people would argue against and point flaws (if any) on my rationale. As I explained when reviewing this patch, I don't care much if an automatic tool is saying that there's a vulnerability at the code, as it could be a false positive. So, what I want at the patch description is a threat analysis explaining how an algorithm is exploited. With regards to Spectre, I never tried to write an exploit myself, nor had to study it in detail in order to mitigate it. So, what I know about it is what I read on a few places. From the places where I read, the boundaries for an array exploit are limited to L1 cache, but, as I said before, I can be wrong on that. It will be great to hear to Peter's comment on that, as he knows a lot more than me about it.",technical
,
"TL;DR: read the papers [1] & [2]I suspect you didn't get the gist of Spectre V1 [1], let me explain: Suppose userspace provides f->index > ARRAY SIZE(format), and we predict the branch to -EINVAL to not be taken. Then the CPU  WILL  load (out of bounds) format[f->index] int of->pixel format and continue onwards to use this bogus value, all the way until it figures out the branch was mis-predicted. Once it figures out the mispredict, it will throw away the state and start over at the condition site. So far, so basic. The thing is, is will not (and cannot) throw away all state. Suppose our speculation continues into v4l fill fmtdesc() and that switch there is compiled as another array lookup, it will then feed our f->pixel format(which contains random kernel memory) into that array to find the requested descr pointer. Now, imagine userspace having flushed cache on the descr pointer array, having trained the branch predictor to mis-predict the branch (see branchscope paper [2]) and doing that out-of-bounds ioctl().It can then speculative do the out-of-bounds array access, followed by the desc array load, then figure out it was wrong and redo. Then usespace probes which part of the descr[] array is now in cache and from that it can infer the initial out-of-bound value. So while format[] is static and bound, it can read random kernel memory up to format+4g, including your crypto keys. As far as V1 goes, this is actually a fairly solid exploit candidate. No false positive about it. Now kernel policy is to kill any and all speculation on user controlled array indexing such that we don't have to go look for subsequent side channels (the above cache side channel is the one described in the Spectre paper and by far the easiest, but there are other possible side channels) and we simply don't want to worry about it. So even from that pov, the proposed patch is good.",technical
,
"Just had a better look at v4l fill fmtdesc() and actually read the comment. The code cannot be compiled as a array because it is big and sparse. But the log(n) condition tree is a prime candidate for the branchscope side-channel, which would be able to reconstruct a significant number of bits of the original value. A denser tree gives more bits etc.",technical
,
"Just to let you know, I was running smatch during the weekend and the tool is still reporting all these Spectre media warnings (and a lot more)",technical
,
"Yep. I get the same warning multiple times. BTW, Mauro, you sent a patch to fix an spectre v1 issue in this file yesterday, but it seems there is another instance of the same issue some lines above",technical
,
It seems that something is broken... getting this error/warning,technical
,
"Never mind. Found it using grep. I'm running this:	",technical
,
"How? Here, I just pull from your git tree and do a ""make"". At most, make clean; make. That makes more sense to me, as the same pattern is used by almost all VIDIOC ENUM foo ioctls.",technical
,
"Interesting, I've rebuild the db twice and now I get a total of 75 Spectre warnings in drivers/media",technical
,
"After rebuilding the db (once), these are all the Spectre media warnings I get warn: potential spectre issue I just want to double check if you are getting the same output. In case you are getting the same, then what Mauro commented about these issues: being resolved by commit seems to be correct.",technical
,
"Yeah, I was thinking that is would be harder to clean this up on smatch. I proposed a patch to the ML that simplifies the logic, making easier for both humans and Smatch to better understand how  the arrays are indexed. ",technical
,
"It's hard to silence this because Smatch stores the current user controlled range list, not what it was initially.  I wrote all this code to detect bounds checking errors, so there wasn't any need to save the range list before the bounds check.  Since ""op"" is a u32, I can't even go by the type of the index....Oh...  Huh.  This is a bug in smatch.  That line looks like:	Smatch see the ntohs() and marks everything inside it as untrusted network data.  I'll fix this. ",technical
,
You'd need to rebuild the db (possibly twice but definitely once),technical
,
"Yeah, that's the same I'm getting from media upstream. This one seems a false positive, as the index var is u8 and the array has 256 elements, as the userspace input from 'op' is initialized with:	u8 v;	u32 op;	if (!kstrtou8(token, 0, &v))		op = this one seems a real issue to me. Sent a patch for  failed to see what's wrong here, or if this is exploited. Here, I'm at this commit: commit  db: make call implies rows unique Plus the diff below (that won't affect Spectre errors)",technical
,
"Thanks, Mauro. These are all the Spectre media issues I see smatch is reporting in I pulled the latest changes from the smatch repository and compiled it. I'm running smatch now. Is this the latest version? I wonder if there is anything I might be missing.",technical
,
Possibly...  There was an ancient bug in Smatch's function pointer handling.  I just pushed a fix for it now so the warning is there on linux-next.,technical
,
"There was no direct fix for it, but maybe this patch has something to do with the smatch error report cleanup: commit with stub functions    This change removes  and adds stub functions where    needed using the DEFINE V4L STUB FUNC macro. This fixes indirect call    mismatches with Control-Flow Integrity, caused by calling standard    ioctls using a function pointer that doesn't match the function type.  ",technical
,
I'm curious about how you finally resolved to handle these issues.I noticed Smatch is no longer reporting them.,technical
,
"Well, the issues will be there everywhere on all media drivers. I marked your patches because I need to study it carefully, after Peter's explanations. My plan is to do it next week. Still not sure if the approach you took is the best one or not. As I said, one possibility is to change the way v4l2-core handles VIDIOC ENUM foo ioctls, but that would be make harder to -stable backports.I need a weekend to sleep on it. ",technical
,
"Sadly no; the whole crux is the array bound check itself. You could maybe pass around the array size to the core code and then do something like: in generic code, and have all the drivers use f->index as usual, but even that would be quite a bit of code churn I guess.",technical
,
I saw your comment on LWN.  You argue on LWN that since the format array is static the CPU won't speculatively read past the L1 cache? I don't know if that's true.  It should be easy enough to filter out the reads into static arrays.  Peter do you know the answer here?,technical
,
"I noticed you changed the status of this series from rejected to new. Also, there are other similar issues in media/pci/I can write proper patches for all of them if you agree those are not False Positives",technical
,
"These files are generated from other files, so any future update to them will bring back the spelling mistakes, so, as Andi pointed out  previously, we better fix the spellings in the original files, maintained by Intel.",technical
,
Thanks. I forwarded to the right people.-,technical
,
"Looks like part of the problem was introduced in that patch, part well predated it (BDU).As you way, this needs to be a bit different to take into account the change to regmap.  We'll need to have that upstream before we look at a back port.  One element inline surprises me and needs further explanation. Why drop the enable setting?  Seems that we want to do this 'as well', if the device was previous enabled.",technical
,
"I have sent a patch based on iio-for-4.17b [1], Lorenzo and I are still discussing our findings. It's not just the CTRL1 reg, but also the AV CONF(0x10) rightish loses it's contents coming out of suspend. Yes, will cover this in v2.",technical
,
Greg doesn't accept patches without a commit message.  Just say which tool warned for example.,technical
,
"Is your divider using the CLK DIVIDER ROUND CLOSEST flag? When divider get val() is called, the incoming rate should already be rounded to something that is what the actual frequency would be so the rounding here hopefully doesn't matter. Maybe something is lost in translation though, because we do round rate() to figure out some divider based on a requested rate, and then we return that rate we calculated that we can achieve to the framework. In turn, that rate comes back into this divider get val()function to get the divider out of the rate again. It's sort of annoying how circular this is. Perhaps your example can show this call sequence and the associated math gymnastics that go on and then I'll be convinced that we need to update this part of the code.",technical
,
"gen tunnel() may not ever return a value larger than is a u8 and therefore can never take on a value outside of the range of 0 to 255.I'm not applying this patch, sorry.",technical
,
"I only read the code and find this possible data race. It is not found in real driver execution. I am not sure of it, so I use ""may"" and ""possible"" here.",technical
,
"Guys, any comments? That is a kinda useful feature, in worst case only some of memory could get corrupted instead of trashing the whole memory. In my experience with T20/30, the interrupt handling latency is low and blocking happens immediately after the first page fault.",technical
,
"One potential issue is with host1x clients where userspace processes can submit jobs with invalid memory accesses (addresses not mapped to IOMMU). If when such a failure happens, we disable the DMA for the whole host1x client, unrelated userspace processes may see failures even though there is no problem with their jobs.",technical
,
"Good point, I'll take a look at partial resurrection of patch [0]. Anyway I think it's still better to fail even the unrelated jobs, rather than to have corrupted memory.",technical
,
"Tsukada-san,I am not familiar with memcg so can't comment about whether the patchset is the right way to solve the problem outlined in the cover letter but had a couple of comments about this patch. Please move this before Patch 1/7. This is to prevent wrong accounting of pages to memcg for size != PMD SIZE. Instead of replacing calls to hpage nr pages(), is it possible to modify it to do the calculation? ",technical
,
I just noticed that the default state is off so the change isn't enabled until the sysfs node is exposed in the next patch. Please ignore this comment. One below still applies.,technical
,
"Thank you for review my code and please just call me Tsukada.I think it is possible to modify the inside of itself rather than replacing the call to hpage nr pages().Inferring from the processing that hpage nr pages() desires, I thought that the definition of hpage nr pages() could be moved outside the CONFIG TRANSPARENT HUGEPAGE. It seems that THP and HugeTLBfs can be handled correctly because compound order() is judged by seeing whether itis PageHead or not. Also, I would like to use compound order() inside hpage nr pages(), but since huge mm.h is included before mm.h where compound order() is defined, move hpage nr pages to mm.h. Instead of patch 3/7, are the following patches implementing what you intended?",technical
,
"I was staring at memcg code to better understand your changes and had the below thought. Instead of tying the surplus huge page charging control per-hstate,could the control be made per-memcg? This can be done by introducing a per-memory controller file in sysfs(memory.charge surplus hugepages?) that indicates whether surplus hugepages are to be charged to the controller and forms part of the total limit. IIUC, the limit already accounts for page and swap cache pages. This would allow the control to be enabled per-cgroup and also keep the userspace control interface in one place. As said earlier, I'm not familiar with memcg so the above might not be a feasible but think it'll lead to a more coherent user interface. Hopefully, more knowledgeable folks on the thread can chime in.",technical
,
That looks a lot better. Thanks for giving it a go.,technical
,
"Thank you for good advise. As you mentioned, it is better to be able to control by per-memcg. After organizing my thoughts, I will develop the next version patch-set that can solve issues and challenge again.",technical
,
" Thank you for your feedback. That makes sense, surplus hugepages are charged to both memcg and hugetlb cgroup, which may be contrary to cgroup design philosophy. Based on the above advice, I have considered the following improvements, what do you think about? The 'charge surplus hugepages' of v2 patch-set was an option to switch ""whether to charge memcg in addition to hugetlb cgroup"", but it will be abolished. Instead, change to ""switch only to memcg instead of hugetlb cgroup"" option. This is called 'surplus charge to memcg'.The surplus charge to memcg option is created in per hugetlb cgroup. If it is false(default), charge destination cgroup of various page types is the same as the current kernel version. If it become true, hugetlbc group stops accounting for surplus hugepages, and memcg starts accounting instead. A table showing which cgroups are charged: I stared at the commit log of mm/hugetlb cgroup.c, but it did not seem to have specially considered of surplus hugepages. Later, I will send a mail to hugetlb cgroup's committer to ask about surplus hugepages charge specifications.",technical
,
"I went back and looked at surplus huge page allocation.  Previously, I made a statement that the hugetlb controller accounts for surplus huge pages. Turns out that may not be 100% correct. Thanks to Michal, all surplus huge page allocation is performed via the alloc surplus huge page() routine.  This will ultimately call into the buddy allocator without any cgroup charges.  Calls to alloc surplus huge page are made from:- alloc huge page() when allocating a huge page to a mapping/file.  In this  case, appropriate calls to the hugetlb controller are in place.  So, any  limits are enforced here.- gather surplus pages() when allocating and setting aside 'reserved' huge  pages. No accounting is performed here.  Do note that in this case the  allocated huge pages are not assigned to the mapping/file.  Even though  'reserved', they are deposited into the global pool and also counted as  'free'.  When these reserved pages are ultimately used to populate a  file/mapping, the code path goes through alloc huge page() where appropriate  calls to the hugetlb controller are in place. So, the bottom line is that surplus huge pages are not accounted for when they are allocated as 'reserves'.  It is not until these reserves are actually used that accounting limits are checked.  This 'seems' to align with general allocation of huge pages within the pool.  No accounting is done until they are actually allocated to a mapping/file.",technical
,
"Thank you for your time. I do not know if it is really a strong use case, but I will explain my motive in detail. English is not my native language, so please pardon my poor English. I am one of the developers for software that managing the resource used from user job at HPC-Cluster with Linux. The resource is memory mainly. The HPC-Cluster may be shared by multiple people and used. Therefore, the memory used by each user must be strictly controlled, otherwise the user's job will runaway, not only will it hamper the other users, it will crash the entire system in OOM. Some users of HPC are very nervous about performance. Jobs are executed while synchronizing with MPI communication using multiple compute nodes. Since CPU wait time will occur when synchronizing, they want to minimize the variation in execution time at each node to reduce waiting times as much as possible. We call this variation a noise.THP does not guarantee to use the Huge Page, but may use the normal page. This mechanism is one cause of variation (noise).The users who know this mechanism will be hesitant to use THP. However, the users also know the benefits of the Huge Page's TLB hit rate performance, and the Huge Page seems to be attractive. It seems natural that these users are interested in HugeTLBfs, I do not know at all whether it is the right approach or not. At the very least, our HPC system is pursuing high versatility and we have to consider whether we can provide it if users want to use HugeTLBfs. In order to use HugeTLBfs we need to create a persistent pool, but in our use case sharing nodes, it would be impossible to create, delete or resize the pool. One of the answers I have reached is to use HugeTLBfs by overcommitting without creating a pool (this is the surplus hugepage).Surplus hugepages is hugetlb page, but I think at least that consuming buddy pool is a decisive difference from hugetlb page of persistent pool. If nr overcommit hugepages is assumed to be infinite, allocating pages for surplus hugepages from buddy pool is all unlimited even if being limited by memcg. In extreme cases, overcommitment will allow users to exhaust the entire memory of the system. Of course, this can be prevented by the hugetlb cgroup, but even if we set the limit for memcg and hugetlb cgroup respectively, as I asked in the first mail(set limit to 10GB), the control will not work. I thought I could charge surplus hugepages to memcg, but maybe I did not have enough knowledge about memcg. I would like to reply to another mail for details. Actually, I am opposed to the 64KB page, but the proposal to change the page size is expected to be dismissed as a problem.",technical
,
"Because they have already allocated from the buddy allocator so the end result is very same. But this is simply not correct. Surplus pages are fluid. If you increase the hugetlb size they will become regular persistent hugetlb pages. Not really. Memcg accounts primarily for reclaimable memory. We do account for some non-reclaimable slabs but the life time should be at least bound to a process life time. Otherwise the memcg oom killer behavior is not guaranteed to unclutter the situation. Hugetlb pages are simply persistent. Well, to be completely honest tmpfs pages have a similar problem but lacking the swap space for them is kind a configuration bug.Ohh, it is very much interested. The primary goal of memcg is to enforce the limit. How are you going to do that in an absence of the reclaimable memory? And quite a lot of it because hugetlb pages usually consume a lot of memory. It does change when ou change the hugetlb pool size, migrate pages between per-numa pools (have a look at adjust pool surplus).",technical
,
"Sure, asking for guarantee makes hugetlb pages attractive. But nothing is really for free, especially any resource  guarantee , and you have to pay an additional configuration price usually. Why? I can see this would be quite a PITA but not really impossible. Not really, you can specify how much you can overcommit hugetlb pages",technical
,
"I really can not understand what's wrong with this. That page is obviously released before being added to the persistent pool, and at that time it is uncharged from memcg to which the task belongs(This assumes my patch-set).After that, the same page obtained from the pool is not surplus hugepage so it will not be charged to memcg again. Absolutely you are saying the right thing, but, for example, can mlock(2)edpages be swapped out by reclaim?(What is the difference between mlock(2)edpages and hugetlb page?) Simply kill any of the tasks belonging to that memcg. Maybe, no one wants reclaim at the time of account of with surplus hugepages.[...]As I looked at, what kind of fatal problem is caused by charging surplus hugepages to memcg by just manipulating counter of statistical information?",technical
,
"I do not see anything like that. adjust pool surplus is simply and accounting thing. At least the last time I've checked. Maybe your patchset handles that? No mlocked pages cannot be reclaimed and that is why we restrict them to a relatively small amount . But that will not release the hugetlb memory, does it? Fatal? Not sure. It simply tries to add an alien memory to the memcg concept so I would presume an unexpected behavior (e.g. not being able to reclaim memcg or, over reclaim, trashing etc.).",technical
,
"Yes. If do not support multiple size hugetlb pages such as x86, because number of pages between THP and hugetlb is same, the failure rate of obtaining a compound page is same, as you said. I think that what you say is absolutely right. I understand the superiority of THP, but there are scenes where k hugepaged occupies cpu due to page fragmentation. Instead of overcommit, setup a persistent pool once, I think that hugetlb can be superior, such as memory allocation performance exceeding THP. I will try to find a good way to use hugetlb page. I sincerely thank you for your help.",technical
,
"Note.  You do not want to use THP because ""THP does not guarantee"". Using hugetlbfs overcommit also does not provide a guarantee.  Without doing much research, I would say the failure rate for obtaining a hugepage via THP and hugetlbfs overcommit is about the same.  The most difficult issue in both cases will be obtaining a ""huge page"" number of pages from the buddy allocator.I really do not think hugetlbfs overcommit will provide any benefit over THP for your use case.  Also, new user space code is required to ""fall back"" to normal pages in the case of hugetlbfs page allocation failure.  This is not needed in the THP case.",technical
,
"This is inconsistent with the ext2 and xfs implementations ...I'm worried this is too much complexity to push down to the filesystems. When should errors get reported through the return value; when should they be reported through the errseq t?Can we hide all of this?  Maybe ext4 could do: (we'd need to make calling errseq set(x, 0) be a no-op instead of an error) and then the caller is the one who takes care of calling errseq check and advance() so we don't have to pass 'since' into each filesystem.",technical
,
"Thanks, I wasn't sure about xfs. I'll drop this hunk.FWIW, I think pushing this call down into the sync fs routines is still probably the right thing to do, regardless of the state of the later patches.I patterned the name after the call mmap (and now-defunct call fsync) helpers. I'll rename it and change it to be non-inclined.",technical
,
"An earlier patch that pushed this down into the sync fs routines. We call this today for all filesystems, and I wasn't sure about xfs.Christoph already pointed out that it's not needed so it's removed from my current branch.Ok, sounds good. I'll fix that too.FWIW, we'll actually want to advance the cursor even if xfs log force returns an error to ensure that we don't end up reporting errors twice, but that's simple enough to do.",technical
,
"XFS never uses the block device mapping for anything, so this is not needed. The proper name for this would be vfs sync fs.  And I don't think it warrants an inline.",technical
,
"fs/udf/unicode.c also uses char2uni and uni2char but evades your SMPLpatch. So you'll need to fix that up manually. Also note that I have some changes to that area queued in my tree.					",technical
,
"Ah, OK, now I see that you've handled that in patch 3. Sorry for the noise.",technical
,
"Thank you for the patch! Yet something to improve:[auto build test ERROR on linus/master][also build test ERROR on v4.17-rc7][cannot apply to next-20180530][if your patch is applied to the wrong git tree, please drop us a note to help improve the system]",technical
,
Hmm.  Don't we also need to cover suspend-to-idle?,technical
,
"Well, if you agree with the approach then I would look into it.",technical
,
"As long as the SYSTEM SUSPEND system state is defined unambiguously, I don't have a problem with doing this.",technical
,
Should be fixed in my for-next branch,technical
,
"The help text above should be indented (as below), with one tab + 2 spaces.",technical
,
"   Sure, I will send v2 patch with the corrected indentation.",technical
,
"Thanks, I've picked this up.",technical
,
"  builtin expect returns always long, see the GCC documentation (it used to return int in very old gcc versions such as 2.96).I think this is a bug in the macro   branch check  . The variable       r should be long, but it is int. This bug may cause misbehavior of other kernel parts (i.e. truncation of long value to int), so it should be fixed in   branch check   - not in dm-write cache.",technical
,
"Nice catch. I'm curious to what that bug was. Anyway, I can pull this in my tree and test it.",technical
,
Eric points out this wont initialize the rest of the dest if src if less than size.,technical
,
"Hi,This should be the very first line.And this is redundant with the SPDX header. These two pinctrl property aren't needed. Each step should increase the perceived brightness by roughly 1/Nth, N being the number of steps. Usually PWM backlights don't work like that. You can drop these two pinctrl properties as well And all these nodes. Thanks! ",technical
,
"The X11 license notice states explicitly that the notice has to be included in the file.  Wouldn't removing it be a violation of the license?FYI, this was copied from another .dts file.  All of the other brightness-levels settings in sun{4,5,7}i .dts files follow similar patterns",technical
,
"Well, the top bit that I quoted above says that the licenses refer to only that one file in particular and not the project as a whole.  Then the X11 license states that the notice can't be removed from 'this software and associated documentation files (the ""Software"")' which would seem to refer to the single file.  Therefore, removing the notice from the single file and replacing it with an SPDX header would seem to violate the license.It's a fine point but it makes me nervous.  I originally based my .dtson sun4i-a10-inet1.dts.  I've CC'd the original copyright holder, Hansde Goede.  Hans, are you willing to give permission for the license notice to be replaced with just an SPDX header indicating the dual licensing? While we're at it, there are a number of other files with the same license text.  Hans, are you prepared to give permission for your other license notices to be replaced with SPDX headers? ",technical
,
"HI, Yes that is fine by me and you've my permission to switch to using just the SPDX header.FWIW I do not believe the ""can't be removed from 'this software and associated documentation files (the ""Software"")'"" language applies to the software as a whole and not individual files. Yes you may make the same change to all files with my copyright. ",technical
,
"Excellent, thanks! :-)",technical
,
This looks a lot like phy write mmd().  ,technical
,
"thanks for the hint. But actually I cannot confirm - or I don't see it yet. Without having tested, just from the code, the struct phy driver instance for PHY ID KSZ8061 in micrel.c does not have a .write mmd function assigned, thus phy write mmd should evaluate to its else-clause (see below) and not to mdiobus write (as in phy write).Also the function which I have added uses the same principle as already existing HW-specific functions in micrel.c for similar reasons (kszphy extended write and ksz9031 extended write).They use phy write all over the place in that file and never phy write mmd - for whatever reason they had. Thus I thought it would be a good idea ...",technical
,
"thanks for your feedback. I was on holiday, thus just delayed, not forgotten...Sorry for top-posting - odd company default mail setup. I checked again phy write mmd(), you are right !Patch with changed implementation will follow. PHY link failure after cable connect Please don't top post. And wrap your lines at around 75 characters Look closely at the two implementations. Look at what mmd phy indirect() does. I  think  these are identical. So don't add your own helper, please use the core code.     ",technical
,
"is more of a glue layer, and this is more device specific. While I like adding it here for more common code, it should probably reside in the ntb hw *.c files to enforce the hw specific code all reside in that layer.  So, this probably needs to be replaced with a patch which adds the setting of the mask to the switchtec driver.",technical
,
"This is a very long way of saying ""no clients are checking the error codes,  so removing them"". :)I think the history and references to follow-on patches are not necessary in the commit message and belong more in a 0/X.This is more of a feature than a bug fix.  Can you break this (and the pingpong and perf changes caused by this) off into a separate series, as I'll want to apply this to the ntb-next and not bugfixes branch? ",technical
,
"I disagree strongly. You can tell it's not device specific seeing we have 4 devices that need the exact same code. In fact, there is nothing device specific in those lines of code as the device specific part comes when a driver sets the PCI parent device's DMA mask. These lines just initialize the dma mask for the new NTB device with its parent's mask. This is just sensible given that nothing now works if it is not done and trusting driver writers to get it right is not a good idea seeing we already screwed it up once. Furthermore, it violates DRY (do not repeat yourself).If there is something driver specific that must be done (although I can't actually imagine what this would be) the drivers are free to change the mask after calling ntb register but getting the common case setup in common code is just good design.",technical
,
"Good day,  .Thanks for the patchset you submitted. My hopefully useful comments are under the corresponding patches. Regards,",technical
,
"This is weird. Neither me nor the folks' who tested the script saw this warning. I tried it on my laptop with bash and on a target device with busy box-shell. The warning never occurred. I even tried a simple command like: It might be that your bash is more modern than mine. Anyway if this patch solves the problem you see, that's great. Thanks for it.",technical
,
"As a part of the multi-port NTB API the port-index interface was freshly introduced. The main idea was to somehow address local/peer domains within one NTB device, since from now there can be more than one peer domain to send message to or to set MWs up with. For this we invented the two-spaces interface which mapped in general non-linear ports space to the locally linear ports indexes space, and vise-versa. That mapping was implemented by new.Even though it perfectly fitted the IDT NTB functions, the Intel/AMD devices didn't have explicit ports numbering. Instead we decided to assign the numbers by using the topology type. So the Primary and B2B US sides got port, Secondary and B2B DS sides got port NTB PORT SEC DSD. In order to make it being default for all pure two-ports devices like Intel/AMD the new methods ntb default port number() and were developed and utilized in the API functions (see ntb.h header file).So to speak the main purpose of the default methods is to assign some unique port number to the NTB devices based on the topology at current implementation. Please note, that it is essential for the NTB API to have each port uniquely enumerated within one device. This is the way the multi-port NTB API has been designed in the first place. That was the reason we altered the Intel/AMD and IDT drivers about two years ago. Based on this I redeveloped the ntb tool/ntb perf/ntb pingpong drivers. Needless to say that I was sure all the NTB devices followed the API convention regarding the port numbers. Since the Switchtec driver doesn't provide the explicit port-index API callbacks, the NTB API internals uses the default methods, which as you can see don't know anything about SWITCH and CROSSLINK topologies. That's why the methods return -EINVAL so the test drivers don't work properly. Concerning the fix of the discovered issues and fixes introduced by this patchset. I'd suggest to add the ports-index callbacks to the Switch tecdriver, which identify local and peer ports. After this the current version of all the test drivers shall perfectly work. As far as I can see the PFX family switches documentation operates with the definitions like Ports/Partitions (similar to the IDT switches) as well as the switchtec management driver. It might be a clue to the switch functionality, which can be used to find something similar to the ports numbering. ",technical
,
"Thanks for the patch. It was the original version of the ping-pong driver, I was going to submit. But I've decided to develop it a bit different. And here is why. My goal was to create the multi-port version of the ping-pong test. The idea of the new driver was to implement the cyclic port-to-portping-pong algorithm. Simply speaking each port selects two partner-ports, one partner would be used as the source of pings and another one would be target of pongs sent to with the defined delay. Since IDT got a global Doorbell register, which is shared between all the ports, I had to assign an unique doorbell bit to each port. I created a simple algorithm, which linearised in general non-linear port numbers. Then I used the globally unique port index to select the corresponding doorbell bit. pp init flds() methods implements the corresponding algorithm, while performs the next port selection to convey the pong to. Regarding the patch. The idea of using the port number instead of linearised unique index should also work for Intel/AMD/IDT drivers. But the ports-space linearization algorithm was created for the case if the real port numbers would exceed the available Doorbell bits. I thought this might be the case of multi-ports version of the switchtec driver. Needless to say, that if Switchtec driver had the ports-index API implementation, this patch wouldn't be needed.",technical
,
Good catch. Thanks. IDT got a lot of MWs especially if LookUpTables are enabled. That's why I didn't find the effect of this error.,technical
,
"Please, see the comment to the patch 3/8. I explained everything there including the fact, that the Intel/AMD drivers do have unique port numbers assigned.",technical
,
Well that will work for the simple switchtec case. The crosslink topology CAN NOT produce port numbers like you ask. It is perfectly symmetric and the two hosts cannot reliably figure out which is port 0 and which is port 1. So these patches support this case.,technical
,
"Hmm, this behavior is the feature of the driver and isn't a bug or race to be fixed. ntb perf driver returns -ENOLINK until the link is actually established, when the memory windows are properly initialized so the test can be performed. What do you think of leaving the algorithm as is, but instead to develop the polling scheme in the ntb test.sh script and break the script execution if the link isn't established after sometime? At least we won't need to wait forever in case if the peer hanged up or crashed while the NTB link negotiation algorithm was in-progress.",technical
,
"Good catch. Thanks for the patch. I discovered this problem myself a few days before you sent this patchset. So was going to submit the fix, but you were faster. I also tested this script in the looped-back setup. It is the case when two NTB-device ports are available at the same RootComplex. So the NTB can be configured from the single executional context. In this case the REMOTE HOST is left empty, so the colon is left prepended to the corresponding paths and causes multiple errors including the one fixed by this patch. In order to fix it, we need to discard the colon for remote-less case, for instance, by the next patch so on for REMOTE PP and REMOTE PERF. It is necessary for NTB devices, which ports are looped-back to the same Root-Port. Would you be amenable if you resent this patch together with the fix I suggested?",technical
,
"Well, the switchtec driver splits its 64 doorbells in two sets, one for each port right now. That will likely have to change when we go to a multi-port implementation. In that case we will have 64 doorbells and a maximum 48 ports. So I don't think we have to concern ourselves with more ports than doorbells. As I've said, it's impossible to write for the crosslink topology so the clients must support that case.",technical
,
"I think polling is really ugly and doesn't really address solve the issue of waiting forever. It's pretty easy to interrupt out of the wait and provides a much better clue to what's going on than an error. If we want to be more explicit, it would be pretty easy to start a timer in the bash script and use SIGALRM to exit if the link doesn't come up after 30 seconds or something.",technical
,
See my comments as to why this is impossible.  ,technical
,
Thanks. It would be good to get Acked-bys or Reviewed-bys on the patches you approved. ,technical
,
SLAB PANIC was added to not worry about error handling.,technical
,
Commit msg missing Preferred name if there only one compatible is the full compatible string plus '.txt'.-ohms and -micro-ohms are the documented units. Please use '-ohms'unless there is a compelling reason not to. Light-sensor is the documented node name for ALSs.,technical
,
The grammar is now incorrect :(Shouldn't we just have a SPDX line at the top here and this whole boilerplate text be removed? ,technical
,
"Sure, will get right to it. I am actually newbie to all this. I watched your tutorial on YouTube when you were at FOSDEM, Huge help, thank you for that.",technical
,
Are you sure you are not changing the logic here? I think it'd be nicer to refactor the code instead. Something like:,technical
"Actually this binding already exists for mediatek timers, it is useless to add a new one. I note the binding. However the existing driver does only use <&system clk> AFAICT, I'm questioning if <&rtc clk> is really needed. So, I suggest you sort out and fixup the rtc clk thing (drop it) and then just add your new platform in the list in this binding.",
"Actually this binding already exists for mediatek timers, it is useless to add a new one. I note the binding in Documentation. However the existing driver does only use   AFAICT, I'm questioning if s really needed. So, I suggest you sort out and fixup the rtc clk thing (drop it) and then just add your new platform in the list in this binding.",technical
,
Recent platforms have the arch arm timer and it will be always selected. What is the benefit of adding this timer ?,technical
,
"To save power as much as possible, our platform enables""arch timer c3stop"" in arch arm timer, and thus another always-on timer is required for tick-broadcasting. System Timer is introduced for above purpose. Thanks.",technical
,
Obviously ,technical
,
Please merge mtk systimer.c and mtk timer.c into a single file:timer-mediatek.c. Change the name in MakefilePatch 2: Change function prefix name to  gpt Patch 2.1 : Move the gpt's init code to timer-ofPatch 3: Add code for syst in timer-mediatek.c A couple of comments below. Wouldn't make sense to clear the interrupt after disabling the timer ?Why do you need IRQF TRIGGER HIGH ?Why IRQF PERCPU ?No it is consistent with the DT binding.,technical
,
OK! We'll fix it and merge two timers into single document file in v3.Thanks.,technical
,
"Thanks for suggestion. Will do above all in v3.The comment may mislead readers. The first step, we do both things in the same time,1. Clear interrupt status.2. Disable interrupt engine in timer hardware, so the interrupt cannot come repeatedly. After that, we shall be safe enough to do followings. Both flags are wrong and will be removed. We will sort out bindings of these two timers in v3.Thanks.",technical
,
"Thanks for the patch, but Andrew Jones has also posted a patch[1] which I had a look but was not sure what is the best approach to fix it yet. I will think about it and respond to that.",technical
,
"I'll send a v1 yet today. The RFC version was actually OK, as the concern with ACPI nodes not being in the expected order wasn't actually a problem. The thread-id or core-id would only be reset to zero when a yet to be remapped core-id (and all its peers) was found when iterating the PEs. Since all peers were handled at the same time, the counter reset was correct, even when the ACPI nodes were out-of-order. The code didn't make that very obvious, though, and there was some room for other cleanups, so I've reworked it. Once I run it through a couple more rounds of testing I'll repost. FYI, I'm able to easily test a variety of configs using a KVM guest and this QEMU branch[*]. To use threads it's necessary to revert the last QEMU patch and to hack KVM to set MPIDR.MT for the VCPUs. ",technical
,
OK sure. I liked the approach in Shunyong's patch. I was thinking if we can avoid the list and dynamic allocation on each addition and make it more simpler.,technical
,
"This one reads simpler, but yes I agree we should try to avoid the dynamic allocation. OTOH, I think that dropping the dynamic allocation leads to an algorithm that picks a value and replaces all the matches. Which of course is Andrew's patch, although I did have to read it a couple times to get a grasp how it works. I'm guessing that is due to the fact that he seems to have optimized 3 double loops into a single loop with two individual nested loops. AKA its probably more efficient than the naive implementation, but readability seems to have suffered a bit in the initial version he posted. I'm not sure the optimization is worth it, but I'm guessing there is a middle ground which makes it more readable. Finally, thanks for putting the effort into this...",technical
,
Completely agree. RFC from Andrew is not so readable and easy to understand.,technical
,
"Middle ground coming up. At the expense of a triple-nested loop (which will never be N^3 iterations due to conditions at the start of each loop),we can avoid dynamic allocations and list iterations and still gain readability.",technical
,
"Hi, All I have a new approach. As we've already got the offset of the node with physical package bit set, which is the parent of the cpu we are querying. We can iterate from the beginning of PPTT to count the nodes with physical package bit set till we reach the offset we've got. Then, the count value is the package id. This avoid list and dynamic allocation. And PPTT provides length for each node, iteration should be easy. I think this may implemented in pptt.c.I am writing this mail on my phone. Maybe I should try to write a patch to test in office tomorrow if you think it's feasible.",technical
,
"I was thinking of simple solution like add the offset to sorted array and assign the index to that. In this way if ACPI PROCESSOR ID VALID flag is set at the package level too and they start and increase linearly from 0, we are matching them(requires 1 line change I posted in the other thread)--Regards, Sudeep IMPORTANT NOTICE: The contents of this email and any attachments are confidential and may also be privileged. If you are not the intended recipient, please notify the sender immediately and do not disclose the contents to any other person, use it for any purpose, or store or copy the information in any medium. Thank you.",technical
,
I'm assuming this is no longer needed now that we have queued the series from Sudeep?,technical
,
"The series relating to topology/numa that you have queued helps us tore-introduces numa mask check that was reverted partial in v4.18 and is not related to the issue reported or addressed by this patch. However, there's no proper solution to the issue reported in this thread and the one from Andrew Jones[1]. The only solution is to rely on ACPI firmware for that instead of trying to fix in OS and we have already merged the patch to fix that in acpi/pptt.c. In short, all the know issues are addressed so far and nothing else needs to be queued for now.",technical
,
Same problem here :(,technical
,
"I wasn't expecting you to put them into your tree - the general concept/implementation is still too immature for that, let alone the commit messages :) However, I'll address this before sending another spin of the patches. Sorry for the noise and thanks for the quick response .",technical
,
Will fix.,technical
,
,
"It got a bit perverse from attempting to separate the devicetree handling from everything else. I'll fix the issues you've pointed out and rework the sysfs/kobj stuff. However, the patch (and the series) was intended as a straw man. I should have put more effort in to avoid some of the distraction (sorry), but I was also hoping to get feedback on the general approach (so devicetree design and how to expose the bits to userspace in a useful manner).Regarding the class, yeah, it's probably not the right choice and I'm not going to double down on it, but it collates the fields in an easy to discover location. Not doing something like this means a lot of grubbing around in /sys/device. Is there a better approach? Thanks for the feedback so far.",technical
,
No changelog :(,technical
,
"Yes, the cpu locks should be irq safe too; however, as irq is always disabled in that function, save/restore is redundant, no? We at least used to do this in the kernel - manipulating irq safe locks with spin lock/unlock() if the irq state is known, whether enabled or disabled, and ISTR lockdep being smart enough to track actual irq state to determine irq safety.  Am I misremembering or is this different on RT kernels?",technical
,
"On not RT enabled kernels. On RT enabled kernels spin lock irq.*() is turned into a sleeping spinlock which do not disable interrupts. as I pointed out above only the raw spin lock t really disables interrupts on -RT. That is the difference between those two. No, this is correct. So on !RT kernels the spin lock irq() disables interrupts and the raw spin lock() has the interrupts already disabled, everything is good. On RT kernels the spin lock irq() does not disable interrupts and the raw spin lock() acquires the lock with enabled interrupts and lockdep complains properly. Lockdep sees the hardirq path via: {IN-HARDIRQ-W} state was registered at",technical
,
"I feel weary about applying a patch which isn't needed in mainline, especially without annotations or at least comments.  I suppose it may not be too common but this can't be the only place which needs this and using irqsave/restore spuriously in all those sites doesn't sound like a good solution.  Is there any other way of handling this? ",technical
,
"This looks much better than the previous version. Yes, I think we need device links between the users of the power domains and the PGC devices. Since this is one of the use-cases for which device links were conceived I would guess that implementing this should be fairly straight-forward. If possible the regulator should also be disabled in the system suspend path, as this might reduce power consumption some more. But then this might have issues with suspend ordering of the regulator. As I don't want to send you down the rabbit hole too much with adding ordering to this stuff, I won't object to this going in as-is. If this is in fact due to a ordering limitation I would like to see this documented in a code comment.",technical
,
"I think anything is useful, thanks. The truth is that nobody is left that seems to really understand this code and syzkaller has shown it is full of various bugs. If there is someone out there that would like to tackle it, let me know. There might be a possibility to support such work. ",technical
,
"You should use prefix ""nl80211: "" in the title",technical
,
"Thanks for cleaning this up! Correct. Yeah, I think this is fine.",technical
,
"Thanks for your review! I have one question. The package information is contained in the warning/error message. Is it better to keep this? If so, I will send v2to move the information to the help, like this.",technical
,
"As you can see, we have a long list of SoCs which are poorly supported. I'm not very keen to just add another SoC which supports booting into a ramdisk using the serial console. Do you have a roadmap adding mainline support for this SoC?",technical
,
"Yes, that's a valid concern.mt6755 and mt6795 are in a similar state, the latter after three years. I'm all for supporting new SoCs, but this feels looks a box-ticking exercise (""hey, look, our SoC is supported in mainline"") which doesn't help anyone. My Ack still stands, but I'd definitely like to see some more complete support before this patch goes in. ",technical
,
"Yes, we do arrange more resources to do upstream task for mt6765,clk/pinctrl drivers are almost ready to submit. systimer is under reviewing (v9). other drivers including. We have dedicated owners to handle them and will cowork tightly with members to make sure things happen in the following weeks. For previous chips, we did have no enough support after shell. It is due to fast pace of smartphone SoC and other resource issues. We also know that is no excuse so that we already confirmed owners and their schedules for this.If there is any suggestion, please let us know. Thanks.",technical
,
"Ok, so let's wait until pinctrl driver is submitted. I'd prefer if you could add the clk driver to this series. This way we can get rid of the dummy clocks in the device tree. I know that smartphone SoC is a fast paced business. Never the less I'm convinced that the basic building blocks won't change much from one version to another. And that mainline support for the previous version of your SoC will help you to get your new drivers faster upstream. For me the best example is the mt7622 which got to a reasonable upstream support quite fast, thanks to a good foundation of mt7623 in mainline. I'd love to see that happen on the smartphone SoCs as well. Not to mention that upstream support will help you internally when you have to rebase your BSP code-base to a new kernel version. That said I think it is good news that you have already defined owner for the different devices and hope to see submissions for them in the near future :)As a suggestion I would say that upstream submission takes time and effort and it will help your engineers if they can allocate some time to do so. But that's most probably a management decision and all engineers know that management bases it's decision on some hard-to-understandable abbreviations like EBITDA etc. ;)",technical
,
"Got it, I will submit this series with clk support in v5. and pinctrl after that. Thanks for your suggestions. We will try to catch up on this mission :-)",technical
,
"In fact, it's not just trying to avoid confusing users. Kexec loading and kexec file loading are just do the same thing in essence. Just we need do kernel image verification on uefi system, have to port kexec loading code to kernel. Kexec has been a formal feature in our distro, and customers owning those kind of very large machine can make use of this feature to speed up the reboot process. On uefi machine, the kexec file loading will search place to put kernel under 4G from top to down. As we know, the1st 4G space is DMA32 ZONE, dma, pci mmcfg, bios etc all try to consume it. It may have possibility to not be able to find a usable space for kernel/initrd. From the top down of the whole memory space, we don't have this worry. And at the first post, I just posted below with AKASHI's version. Later you suggested to use list head to link child sibling of resource, see what the code change looks like. Then I posted v2 Rob Herring mentioned that other components which has this tree struct have planned to do the same thing, replacing the singly linked list with list head to link resource child sibling. Just quote Rob's words as below. I think this could be another reason. From Rob The DT struct device node also has the same tree structure withparent, child, sibling pointers and converting to list head had been on the to do list for a while. ACPI also has some tree walking functions. Perhaps there should be a common tree struct and helpers defined either on top of list head or a new struct if that saves some size.Kexec was invented for kernel developer to speed up their kernel rebooting. Now high end sever admin, kernel developer and QE are also keen to use it to reboot large box for faster feature testing, bug debugging. Kernel dev could know this well, about kernel loading position, admin or QE might not be aware of it very well. Understood. The list head replacing patch truly invokes too many code changes, it's risky. I am willing to try any idea from reviewers, won't per suit they have to be accepted finally. If don't have a try, we don't know what it looks like, and what impact it may have. I am fine to take AKASHI's simple version of walk system ram res rev() to lower risk, even though it could be a little bit low efficient",technical
,
Please let's get all this into the changelogs? The larger patch produces a better result.  We can handle it ;),technical
,
I do not have the full context here but let me note that you should be careful when doing top-down reservation because you can easily get into hot pluggable memory and break the hot remove use case. We even warn when this is done. See memblock find in range node,technical
,
"Sorry for late reply because of some urgent customer hot plug issues.I am rewriting all change logs, and cover letter. Then found I was wrong about the 2nd reason. The current kexec file load calls kexec locate mem hole() to go through all system RAM region, if one region is larger than the size of kernel or initrd, it will search a position in that region from top to down. Since kexec will jump to 2nd kernel and don't need to care the 1st kernel's data, we can always find a usable space to load kexec kernel/initrd under 4G.So the only reason for this patch is keeping consistent with kexec load and avoid confusion. And since x86 5-level paging mode has been added, we have another issue for top-down searching in the whole system RAM. That is we support dynamic 4-level to 5-level changing. Namely a kernel compiled with 5-level support, we can add 'no5lvl' to force 4-level. Then jumping from a 5-level kernel to 4-level kernel, e.g. we load kernel at the top of system RAM in 5-level paging mode which might be bigger than 64TB, then try to jump to 4-level kernel with the upper limit of 64TB. For this case, we need add limit for kexec kernel loading if in 5-level kernel. All this mess makes me hesitate to choose a delegate method. Maybe I should drop this patchset. For this issue, if we stop changing the kexec top down searching code, I am not sure if we should post this replacing with list head patches separately.",technical
,
"Kexec read kernel/initrd file into buffer, just search usable positions for them to do the later copying. You can see below struct kexec segment,for the old kexec load, kernel/initrd are read into user space buffer, the stores the user space buffer address, stores the position where kernel/initrd will be put. In kernel, it calls kimage load normal segment() to copy user space buffer to intermediate pages which are allocated with flag GFP KERNEL. These intermediate pages are recorded as entries, later when user execute ""kexec -e"" to trigger kexec jumping, it will do the final copying from the intermediate pages to the real destination pages which @mem pointed. Because we can't touch the existed data in 1st kernel when do kexec kernel loading. With my understanding, GFP KERNEL will make those intermediate pages be allocated inside immovable area, it won't impact hot plugging. But the@mem we searched in the whole system RAM might be lost along with hot plug. Hence we need do kexec kernel again when hot plug event is detected.",technical
,
"I am not sure I am following. If @mem is placed at movable node then the memory hot remove simply won't work, because we are seeing reserved pages and do not know what to do about them. They are not migrateable. Allocating intermediate pages from other nodes doesn't really help. The memblock code warns exactly for that reason.",technical
,
"OK, I forgot the 2nd kernel which kexec jump into. It won't impact hot remove in 1st kernel, it does impact the kernel which kexec jump into if kernel is at top of system RAM and the top RAM is in movable node.",technical
,
It will affect the 1st kernel (which does the memblock allocation top-down) as well. For reasons mentioned above.,technical
,
"And btw. in the ideal world, we would restrict the memblock allocation top-down from the non-movable nodes. But I do not think we have that information ready at the time when the reservation is done.",technical
,
"Oh, you could mix kexec loading up with kdump kernel loading. For kdump kernel, we need reserve memory region during bootup with memblock allocator. For kexec loading, we just operate after system up, and do not need to reserve any memory region. About memory used to load them, it's quite different way.",technical
,
I didn't know about that. I thought both use the same underlying reservation mechanism. My bad and sorry for the noise.,technical
,
Not at all. It's truly confusing. I often need take time to recall those details.,technical
,
"The patch looks correct to me. In fact I have a patch [1], which does the same thing and switches to using per-cpu variable for the paths. I think it is safe to use cpu online mask outside the get/put online cpus(). Using present mask may fail as we could fail to create a path for a CPU that is not online. Please could you check if [1] fixes the problem for you ?",technical
,
"Not only did you beat me to the punch but I think using a per-cpu variable is cleaner, so let's go with yours.",technical
,
"This will still deadlock if ->runtime suspend commences before the hot plug event and the hot plug event occurs before polling has been disabled by ->runtime suspend.The correct fix is to call pm runtime get sync() *conditionally* in the atomic commit which enables the display, using the same conditional as, i.e. if.Now I realize I sent you down the wrong path when I suggested to introduce a DRM helper here.  My apologies, I didn't fully appreciate what is going awry here! Anything that happens in nouveau's poll worker never needs to acquire a runtime PM ref because polling is only enabled while runtime active, and ->runtime suspend waits for an ongoing poll to finish. Thinking a bit more about this, our mistake is to acquire runtime PMrefs too far down in the call stack.  As a fix that can be backported to stable, adding if conditionals seems fine to me, but the long term fix is to push acquisition of refs further up in the call stack.E.g., if the user forces connector probing via sysfs, a runtime PM ref should be acquired in status store() in drm sysfs.c before invoking connector->funcs->fill modes().  That way, if the user forces connector probing while the GPU is powering down, rpm resume() will correctly wait for rpm suspend() to finish before resuming the card.  So if we architect it like this, we're actually using the functionality provided by the PM core in the way that it's supposed to be used. The problem is that adding pm runtime get sync() to status store() conflicts with the desire to have a library of generic DRM functions: Some GPUs may be able to probe connectors without resuming to runtime active state, others don't use runtime PM at all.  One solution that comes to mind is a driver features flag which tells the DRM core whether to acquire a runtime PM ref in various places. In your original patches 4 and 5, what exactly was the call stack which led to i2c being accessed while runtime suspended?  Was it sysfs access?  If so, acquisition of the runtime PM ref needs to likewise happen in that sysfs entry point, rather than deep down in the call stack upon accessing the i2c bus. ",technical
,
"Hm, a runtime PM ref is already acquired in nouveau connector detect().I'm wondering why that's not sufficient? ",technical
,
"As an additional note, I realized this might seem wrong but it isn't pm runtime put sync() calls down to nouveau's runtime idle callback, which does this: So, it doesn't actually synchronously suspend the GPU, it just starts up the auto suspend thread Just wanted to make sure there wasn't any confusion :)",technical
,
"Why do we need all this? i915 has full rpm support, including i2c and dpaux correctly working, and everything else too. Why is nouveau special? Also, there's a metric pile of other drivers using the existing helpers an infrastructure, with full rpm support, all seemingly being happy too. Given all that it smells a bit like nouveau has it's rpm design backwards, which would indicate that these new helpers should be nouveau-specific wrappers and not generic. If that's not the case, then I'd like to first understand why nouveau needs them and why no one else seems to need these.",technical
,
"Because there's a difference between DPMST connectors and encoders vs. the rest of the device's encoders. Every DP MST topology will take up a single ""physical"" DP connector on the device that will be marked as disconnected. This connector also owns the ""mstm"" (MST manager, referred to as the drm dp mst topology mgr in DRM), which through the callbacks nouveau provides is responsible for creating the fake DP MST ports and encoders. All of these fake ports will have DPMST encoders as opposed to the physical DP ports, which will have TMDS encoders. Hence-mstms are only on physical connectors with TMDS, not fake connectors with DPMST.--",technical
,
"Hoping for some review from Gergory, Ralph or Richard who all seem to use this driver! ",technical
,
"Would it be possible to resend the series adding me in CC?  I would like to comment the patches, but unfortunately it seems that I was in none of the mailing list where the series had been sent. I found the patches on patchwork, and started to have a look on it for example, in the patch ""gpio: mvebu: Add support for multiple PWM lines per GPIO chip"", I wonder why the id is stored as it was not used at all. Having the patch inclined in an email would male the review easier.",technical
,
Same for me :),technical
,
"Sorry, for the noise, but please use my Gmail address. Thanks ! ",technical
,
"Thanks for all your catches here. return unsigned long.(hm, arch/arc/include/asm/bitops.h is a little different.)(and unicore32)I'll send a v2 patch (for hexagon).That will still fix the printk format warning above.",technical
,
"Do you want me to try to write a patch that does that change? Yeah, true. Should I just remove the method name from the pr err once? Or should I also use one of the  rate limited things and print multiple messages? I guess that would conform to normal kernel coding standards a bit better. Thanks for the quick feedback!",technical
,
"    Protect posix clock array access against speculation Documentation/process/submitting-patches.rst:  For these reasons, the ``summary`` must be no more than 70-75  characters, and it must describe both what the patch changes, as well  as why the patch might be necessary.  It is challenging to be both  succinct and descriptive, but that is what a well-written summary  should do. The above subject violates all of these rules. We should? Changelogs are about facts. So having:  The ""array index mask nospec"" code has been updated to allow index  argument to have const-qualified type.  So the stack variable ""idx"" which was introduced in commit 19b558db12f9  (""posix-timers: Protect posix clock array access against speculation"") to  cast the const argument 'id' is not longer required. is factual and precise. Hmm?This SOB chain is wrong. Please read and follow the Documentation.",technical
,
"How does the caller know whether or not a particular attribute has multiple values?  Is that a fundamental attribute of a particular attribute?  (e.g., the documentation for each attribute must state whether or not that attribute returns multiple attributes or not)	       	    	 ",technical
,
Adding fall through isn't wrong but its reasonable to ask why there is a complex hand unrolled loop here in the first place (and doubly so without a comment). The whole switch statement would be much clear expressed as,technical
,
"Yeah, I agree. I can send a patch for that. Thanks for the feedback",technical
,
"Thanks for your patch. Since erofs and gasket are different feature, it is better to separate into two patches. and could you please cc linux-erofs mailing list as well? Yes, there is an extra semicolon in z erofs vle unzip all, it was reported by Julia Lawall several days ago. Actually, there is a patch in linux-erofs mailing list, but it seems that Chao hasn't reviewed it yet. to the original patch if if you don't mind, do you?",technical
,
"Oh, sorry, I missed this one, let me check/review all recent patches again and update them in tree later.",technical
,
It is nothing. :) I will send v2 version of this patch if doesn't mind.,technical
,
 do it.  Thanks.  Then I will just resend the gasket feature separately. ,technical
,
Thanks for your understanding :),technical
,
"Why not just one call with:	The comment applies for the next patch as well.",technical
,
it seems like you missed to fix part of the subject as Jacek suggested. I think it should be,technical
,
"Thanks for the heads-up.""dt-"" prefix is indeed more preferred than ""dt: "" .Dan - I will fix it by myself, no need to resend. ",technical
,
"This is quite long way to describe a bitmask, no? Could we make it so that control-bank-cfg is not needed? Can we compute control-bank-cfg from this? Does the example show correct config? AFAICT all controls go to bank A according to control-bank-cfg, yet led@1 describes bank B...		",technical
,
"HI! That's rather long and verbose way to describe a bitmap, right? Is rbtree good idea? You don't have that many registers.....No error checking required here? This checks if we have just one bank, I see it. Should it also check the led actually uses the correct bank? Is not the fwnode handle put() done twice for non-error case? The if is not needed here. Misleading, this does nothing with regulators.							",technical
,
"HI! Can we forget about the LED strings, and just expose the sinks as Linux LED devices?			",technical
,
2 sinks 3 LED strings.  How do you know which LED string is which and what bank it belongs to when setting the brightness.  Each Bank has a separate register for brightness control. ,technical
,
"Yes, and LED strings are statically assigned to banks, right? So why not simply forget about LED strings for sake of hw abstractions, and work just with banks?	",technical
,
How would you set the control bank register for the correct LED string configuration? ,technical
,
JacekI could change the name to led-sources.  But this part does not really follow the 1 output to a1 LED string topology.,technical
,
"Have property at each LED saying which  HVLEDs it controls?			",technical
,
"Hi! Yes. No, I don't want that. I'd like 2 child nodes, each specifying which HVLEDs it controls. Let me edit the original proposal.	",technical
,
"What I'd like to see: Remove control-bank-cfg. Add required child propertyset of outputs this child controls.						",technical
,
"led-sources was designed for describing the topology where one LED can be connected to more then one output, see bindings of max77693-led.Here the topology is a bit different - more than one LED (string) can be connected to a single bank, but this is accomplished inside the chip. Logically LEDs configured that way can be treated as a single LED(string) connected to two outputs, and what follows they should be described by a single DT child node.led-sources will fit very well for this purpose. You could do the following mapping. Then, in the child DT nodes you would use these identifiers to describe the topology: Following node would describe strings connected to the outputsHVLED1 and HVLED2 controlled by bank. I agree with Pavel, but I propose to use already documented common DT LED property.",technical
,
"Jacek and PavelI agree to use the led-sources but I still believe this approach may be confusing to other sw devas and will lead to configuration issues by users. This implementation requires the sw dev to know which strings are controlled by which bank. And this method may produce a misconfiguration like something below where HVLED2 is declared in both bank A and bank Bled. The driver will need to be intelligent and declare a miss configuration on the above. Not saying this cannot be done but I am not sure why we want to add all of these extra LoC and intelligence in the kernel driver. The driver cannot make assumptions on the intention.  And the device tree documentation will need to pretty much need a lengthy explanation on how to configure the child nodes. The implementation I suggested removes that ambiguity.  It is a simple integer that is written to the device as part of the device configuration, which the config is a setting for the device. The child nodes denote which bank the exposed LED node will control.  Removing any need for the sw developers new or old to know the specific device configurations. ",technical
,
Yes I know that the driver can check the string but if the same string is declared by another child then the driver must exit with -EINVAL.  Again a lot of code for little pay off. I believe we should keep drivers as simple as possible. I will add the changes. Unfortunately in either case this high level of documentation will need to be done. I believe both solutions will raise questions and concerns. There does not seem to be a good way to describe this device. Both solutions are wrought with issues and concerns. But like I said I will re-write the code with the above suggestion.  ,technical
,
"HI! Yes. But please do it that way, it is still better than being different from all the other drivers.",technical
,
 Thanks for the review It will be removed with v3 ack Ack It will be removed with v3.,technical
,
Isn't that what I have already using the reg property? Then we would have to aggregate the configuration and make a determination in the driver. But that does not follow the LED child node ideology. Each output of the LED driver should have a child node. In this case the outputs are the sinks (inputs) and there are only 2 sinks so having 3 LED child nodes would be confusing and there are required properties for each child like label. Each child node would then need to present 1 LED node to the user space to control the LED string.  Which would be technically incorrect because you would have 2 LED nodes controlling the same control bank sink.,technical
,
"Besides this, please split it into two patches. The RCU change does not belong to ""comment fix"" for sure. Thanks",technical
,
"Thanks for the reminder. Because this change is trivial, I change the subject.",technical
,
"Why ""bridge"" and not ""host"" or even something to stand for ""root complex""? Or maybe it can still be ""host bridge""?",technical
,
"This looks fishy, as bridge->private is not set at this point AFAICS, unless one of the previous patches changes that.",technical
,
"bridge->private what comes after the bridge structure, and it's allocated by pci alloc host bridge() passing the size of the structure we want for this private area. ",technical
,
"I did this for consistency with the naming in drivers/pci/probe.c,which always declares the local variable as 'struct pci host bridge *bridge'.It's easy to change here if you feel strongly about it (I don't).    ",technical
,
I would leave host bridge here.  It would make the patch smaller too I think.,technical
,
"I see, sorry for the noise.",technical
,
"Ok, I've changed my local copy as you suggested now. ",technical
,
"I really like the idea behind this series. I wonder if there is a way to avoid some of that by adding a few more helpers, but even without the helpers that approach looks ok to me. Do you have a git tree somewhere to play around with the changes?",technical
,
"Ok, thanks for taking a first look. One core part that gets duplicated a lot (also in existing drivers) is the chunk that could be handled by this: That would probably help, but we should think carefully about the set of fields that we want pass here, specifically because the idea of splitting the probing into two parts was to avoid having to come up with a new interface every time that list changes due to some rework. For instance, the numa node is something that might get passed here, and if we decide to split out the operations into a separate pci host bridge ops structure, the pointer to that would also be something we'd want to pass this way. I now uploaded it (with fixes incorporated) to ",technical
,
Hm... are you turning direct calls into retpolined indirect calls?,technical
,
He does.  But not anywhere near the fast path.,technical
,
"Sorry for the late response to this. I think I'm generally on-board with this.  I admit I'm a little hesitant about adding 200 lines of code when this is really more ""cleanup"" than new functionality, but I think a lot of that is because this series contains costs (e.g., duplicating code) for everybody but only has the corresponding benefits for a few (ACPI, x86, xenfront).Those cases are much closer to parity in terms of lines added/removed. I saw some minor comments that suggested you had some updates, so I'll watch for an updated posting.",technical
,
"I'm inclined to apply this -- because it will not break anything, and it would at least enable testing by people, who have this hardware. Thoughts?",technical
,
"(Whoops, resending this message. Forgot that Gmail defaulted to HTML...) By all means go ahead and apply the package power patch as well. I've tested it on both Summit (single-die desktop) and Raven (desktop/laptopapu) with good results. The worst case as it stands is that if the multi-die packages (EPYC/TR) don't aggregate the value per package, then package power will be under-reported.(I previously wrote that this would be easier to test with the patch applied, because the package power is reported with the -Dump option, but I'm not sure that's actually the case - the value might still only be collected once per package?) ",technical
,
"Right -- we'd have to move it to a per-core or per-CPU data structure to capture it on a finer granularity than per package. Of course, if you want to see if you get different values on the cpus, you can always dump the raw values from each cpu using rdmsr(1).Unfortunately, I don't have that AMD hardware -- somebody with an interest in it can test it out.",technical
,
"Applied, thanks!",technical
,
"As enable is required and direction is optional, enable should come first. So fix the pwms property instead. (And perhaps make the binding more explicit as to what the order should be.",technical
,
"You are right. The XFS filesystem was created on a small ramfs device and so the disk space was tiny. The micro benchmark that I used exposes some extreme cases that may not be normally observed. Those were part of the perf trace that I marked down: A spurious wakeup shouldn't change the behavior as the waiter should find that it is still being queued (list not empty) and so will sleep again. However, if the waiter is rightfully waken but find that there is not enough log space somehow, it will put itself back to the end of the queue. That is a change in behavior that I think could be an issue. I am just wondering why the code doesn't reserve the actual log space at the time a task is being waken. Instead, it has to try to get it itself after being waken. Also I think the current code allows consecutive waiters to come in and wake up the same set of tasks again and again. That may be where a part of the performance slowdown come from. As I said above, spurious wakeup shouldn't cause problem. However, consecutive waiters will probably wake up more tasks than there is logspace available. In this case, some of the tasks will be put back to the queue. Maybe a counter to record the log space temporarily reserved for the waken-up task can help to prevent over-subscription. The wake q should work correctly as it is used by mutexes and rwsems. We will see a lot of problem reports if that is not the case. That is probably true. I am using a old git (1.8). I should probably upgrade to a newer one. OK It is because the wake q uses a field in the task structure for queuing. So a task can only be in one wake q at a time. Leaving the ticket in the queue may cause the task to be put into multiple wake q causing missed wakeup, perhaps. Yes, you are right. The xlog grant head wake all() need to be modified as well.OK, I need more time to think about some of the questions that you raise.  Thanks for reviewing the patch. ",technical
,
The original is better than the new version.  Please leave it as is. Regards,technical
,
"Oh, I just noticed you are using a ramfs for this benchmark,tl; dr: Once you pass a certain point, ramdisks can be *much* slower than SSDs on journal intensive workloads like AIM7. Hence it would be useful to see if you have the same problems on, say, high performance nvme SSDs.-----Ramdisks have substantially different means log IO completion and wakeup behaviour compared to real storage on real production systems. Basically, ramdisks are synchronous and real storage is a synchronous. That is, on a ramdisk the IO completion is run synchronously in the same task as the IO submission because the IO is just a mercy().Hence a single dispatch thread can only drive an IO queue depth of 1IO - there is no concurrency possible. This serialises large parts of the XFS journal - the journal is really an asynchronous IO engine that gets it's performance from driving deep IO queues and batching commits while IO is in flight. Ramdisks also have very low IO latency, which means there's only a very small window for ""IO in flight"" batching optimisations to be made effectively. It effectively stops such algorithms from working completely. This means the XFS journal behaves very differently on ramdisks when compared to normal storage. The submission batching techniques reduces log IOs by a factor of 10-20 under heavy synchronous transaction loads when there is any noticeable journal IO delay - a few tens of microseconds is enough for it to function effectively, but a ramdisk doesn't even have this delay on journal IO.  The submission batching also has the effect of reducing log space wakeups by the same factor there are less IO completions signalling that space has been made available. Further, when we get async IO completions from real hardware, they get processed in batches by a completion work queue - this leads to there typically only being a single reservation space update from all batched IO completions. This tends to reduce log space wakeups due to log IO completion by a factor of 6-8 as the log can have upto 8 concurrent IOs in flight at a time. And when we throw in the lack of batching, merging and IO completion aggregation of metadata writeback because ramdisks are synchronous and don't queue or merge adjacent IOs, we end up with lots more contention on the AIL lock and much more frequent log space wakeups (i.e. from log tail movement updates). This further exacerbates the problems the log already has with synchronous IO.IOWs, log space wakeups on real storage are likely to be 50-100xlower than on a ramdisk for the same metadata and journal intensive workload, and as such those workloads often run faster on real storage than they do on ramdisks. This can be trivially seen with dbench, a simple IO benchmark that hammers the journal. On a ramdisk, I can only get 2-2.5GB/s throughput from the benchmark before the log bottlenecks at about 20,000 log tiny IOs per second. In comparison, on an old, badly abused Samsung 850EVO SSD, I see 5-6GB/s in 2,000 log IOs per second because of the pipelining and IO batching in the XFS journal async IO engine and the massive reduction in metadata IO due to merging of adjacent IOs in the block layer. i.e. the journal and metadata writeback design allows the filesystem to operate at a much higher synchronous transaction rate than would otherwise be possible by taking advantage of the IO concurrency that storage provides us with. So if you use proper storage hardware (e.g. nvme SSD) and/or an appropriately sized log, does the slow path wakeup contention go away? Can you please test both of these things and report the results so we can properly evaluate the impact of these changes?",technical
,
"Note that all these ramdisk issues you mentioned below will also apply to using the pmem driver on nvdimms, which might be a more realistic version.  Even worse at least for cases where the nvdimms aren't actually powerfail dram of some sort with write through caching and ADR the latency is going to be much higher than the ramdisk as well.",technical
,
"Oh sorry, I made a mistake. There were some problems with my test configuration. I was actually running the test on a regular enterprise-class disk device mount on. It was not an SSD, nor ramdisk. I reran the test on ramdisk, the performance of the patched kernel was 679,880 jobs/min which was a bit more than double the 285,221 score that I got on a regular disk. So the filesystem used wasn't tiny, though it is still not very large. The test was supposed to create 16 ramdisks and distribute the test tasks to the ramdisks. Instead, they were all pounding on the same filesystem worsening the spinlock contention problem.",technical
,
"Yes, I realise that. I am expecting that when it comes to optimising for pmem, we'll actually rewrite the journal to map pmem and mercy() directly rather than go through the buffering and IO layers we currently do so we can minimise write latency and control concurrency ourselves. Hence I'm not really concerned by performance issues with pmem at this point - most of our still users have traditional storage and will for a long time to come...",technical
,
Is it ok to share VTune GUI screenshots I sent you the last time to demonstrate the advantage of AIO trace streaming?,technical
,
"HI, I am not sure it would be representative in perf stat data however that could be visible there as well. I could share screenshots of VTune GUI that demonstrate the advantage of Perf implementing AIO streaming in comparison with the serial streaming. Data loss metrics can be easily understood from that. Is it ok to post it here?",technical
,
"Hi, VTune release manager permitted to share it, well, sorry for bothering.",technical
,
"Which branch is this works based on? I don't see any out label in current code. Please add same test in ovl can decode fh(). Problem: none of the ovl export operations functions override creds.I guess things are working now because nfsd is privileged enough. IOW, the capability check you added doesn't check mounter creds when coming from nfs export ops - I guess that is not what you want although you probably don't enable nfs export.",technical
,
<sigh> I can only truly test this on 4.14 (android's current top of tree) and on Hikey with that. Lack of due diligence for Top of Linux. Ahhhh NFS export/Import blocked on Android devices.,technical
,
"Well, not sure how that review is going to work out. Anyway, this case should not return an error. Returning NULL should be just fine.",technical
,
I have picked this up Dave.,technical
,
"Thanks, Jeffrey.",technical
,
"Are you sure you did this right?  With the clock source set to TSC(which is the only reasonable choice unless KVM has seriously cleaned up its act), with retpolines enabled, I get 24ns for CLOCK MONOTONIC without your patch and 32ns with your patch.  And there is indeed a retpoline in the disassembled output:  You're probably going to have to set -fno-jump-tables or do something clever like adding a whole array of (seconds, nsec) in gtod and indexing that array by the clock id. Meanwhile, I wrote the following trivial patch to add a  vdso clock gettime monotonic export.  It runs in 21ns, and I suspect that the speedup is even a bit bigger when cache-cold because it avoids some branches.  What do you all think?  Florian, do you think glibc would be willing to add some magic to turnclock gettime when CLOCK MONOTONIC is a constant?",technical
,
"What's the goal here?  Turn the indirect call/conditional jump/indirect call sequence into a single indirect call, purely for performance reasons?",technical
,
"Almost.  It's to bypass some of the branches in  vdso clock gettime(), which is supposed to be very fast.  AFAIK most user code that uses clock gettime() passes a constant for the first argument, and we can squeeze out some performance by optimizing that case.  The indirect branches internal to the vDSO are a separate issue and should be solved separately.(It's really too bad that x86 doesn't have a 64-bit call instruction.If it did, then the PLT could get rewritten at dynamic link time to avoid indirect calls entirely, and presumably glibc could use the same technique to call into the vDSO without indirect calls.)",technical
,
See the patch below. It's integrating TAI without slowing down everything and it definitely does not result in indirect calls. On a HSW it slows down clock gettime() by ~0.5ns. On a SKL I get a speedup by ~0.5ns. On a AMD Epyc server it's 1.2ns speedup. So it somehow depends on the uarch and I also observed compiler version dependent variations.,technical
,
"and actually this wants to become u64 unconditionally as we need to provide the full seconds even on 32bit for the upcoming y2038 support. We still have to truncate it for the current 32bit interface, but the core code can be made ready now.",technical
,
"Does this mean glibc can keep using a single vDSO entry point, the one we have today?",technical
,
We have no intention to change that. But we surely could provide separate entry points as an extra to avoid a bunch of conditionals.,technical
,
"Okay, I was wondering because Andy seemed to have proposed just that. We could adjust to that, but the benefit would be long-term because it's an ABI change for glibc, and they tend to take a long time to propagate. But I must say that clock gettime is an odd place to start.  I would have expected any of the type-polymorphic multiplexer interfaces (fcntl,ioctl, ptrace, futex) to be a more natural starting point. 8-)",technical
,
"Well, the starting point of this was to provide clock tai support in thevdso. clock gettime() in the vdso vs. the real syscall is a factor of 10 in speed. clock gettime() is a pretty hot function in some workloads. Andy then noticed that some conditionals could be avoided entirely by using a different entry point and offered one along with a 10% speedup. We don't have to go there, we can. The multiplexer interfaces need much more surgery and talking about futex, we'd need to sit down with quite some people and identify the things they actually care about before just splitting it up and keeping the existing overloaded trainwreck the same.",technical
,
"There's also the issue of how much the speedup matters. For futex, maybe a better interface saves 3ns, but a futex syscall is hundreds of ns. clock gettime() is called at high frequency and can be ~25ns. Saving a few ns is a bigger deal.",technical
,
"My concern is that the userspace system call wrappers currently do not know how many arguments the individual operations take and what types the arguments have (hence the  type-polymorphic nature I mentioned). This could be a problem for on-stack argument passing (where you might read values beyond the end of the stack, and glibc avoids that most of the time by having enough cruft on the stack), and for architectures which pass pointers and integers in different registers (like some m68kABIs do for the return value).",technical
,
"Isn't clock gettime already special because of the vDSO entry point, though?",technical
,
"Somewhat special, yes, but not overly so, and not in the type-polymorphic sense.  We can't give direct access of the vDSO implementation to applications because the kernel does not know about the userspace errno variable.  We do that for time on x86 64, where applications call into the vDSO directly, bypassing glibc completely after binding. I suspect most Linux libcs that know about the vDSO at all have generic vsyscall support, just like they have generic support for plain system calls.",technical
,
"If the vDSO adds special helpers for CLOCK MONOTONIC and CLOCK REALTIME, I think we can reasonably safely promise that they never fail. (seccomp can obviously break that promise if there's no TSC, but I think that seccomp users who do that get to keep both pieces.)",technical
,
"I agree, I thought about the same thing.  We already do not return EFAULT for invalid pointers, for obvious reasons.  And if the clock ID is fixed, the EINVAL error is impossible. That would shave off a few nanoseconds more if the calling convention is identical to what glibc exposes to applications.  If the vDSO is not available or the symbol is missing, we can provide an implementation based on the current clock gettime in glibc.",technical
,
"Now tfm could be removed from the macro arguments, no? ",technical
,
"Are you sure this is a representative sampling? I haven't double checked myself, but we have plenty of drivers for peripherals in drivers/crypto that implement block ciphers, and they would not turnup in tcrypt unless you are running on a platform that provides the hardware in question.",technical
,
"Hrm, excellent point. Looking at this again: The core part of the VLA is using this in the ON STACK macro. I don't find any and the initial reqsize is here: And in my earlier skcipher wrapper analysis, lrw was the largest skcipher wrapper is an extreme outlier, with cvm req ctx at a bit less than half. Making this a 2920 byte fixed array doesn't seem sensible at all (though that's what's already possible to use with existing SKCIPHER REQUEST ON STACK users).What's the right path forward here?-",technical
,
"The skcipher implementations based on crypto IP blocks are typically asynchronous, and I wouldn't be surprised if a fair number of SKCIPHER REQUEST ON STACK() users are limited to synchronous skciphers. So we could formalize this and limit SKCIPHER REQUEST ON STACK() to synchronous skciphers, which implies that the reqsize limit only has to apply synchronous skciphers as well. But before we can do this, we have to identify the remaining occurrences that allow asynchronous skciphers to be used, and replace them with heap allocations.",technical
,
"Looks similar to ahash vs shash. :) Yes, so nearly all crypto alloc skcipher() users explicitly mask away ASYNC. What's left appears to be: I'll cross-reference this with SKCIPHER REQUEST ON STACK...Sounds good; thanks!",technical
,
"According to Herbert, SKCIPHER REQUEST ON STACK() may only be used for invoking synchronous ciphers. In fact, due to the way the crypto API is built, if you try using it with any transformation that uses DMA you would most probably end up trying to DMA to/from the stack which as we all know is not a great idea. Any such occurrences are almost for sure broken already due to the DMA issue I've mentioned.",technical
,
"Ah yes, I found which contains that quote. So that means that Kees can disregard the occurrences that are async only, but it still implies that we cannot limit the reqsize like he proposes unless we take the sync/async nature into account. It also means we should probably BUG() or WARN() in SKCIPHER REQUEST ON STACK() when used with an async algo. I am not convinced of this. The skcipher request struct does not contain any payload buffers, and whether the algo specific ctx struct is used for DMA is completely up to the driver. So I am quite sure there are plenty of async algos that work fine with SKCIPHER REQUEST ON STACK() and vmapped stacks.",technical
,
"Something like this should do the trick: DOC: Symmetric Key Cipher API That way, we will almost certainly oops on a NULL pointer dereference right after, but we at least the stack corruption.",technical
,
"You are right that it is up to the driver but the cost is an extra memory allocation and release*per request* for any per request data that needs to be DMA able beyond the actual plain and cipher text buffers such as the IV, so driver writers have an incentive against doing that :-)",technical
,
A crash is just as bad as a BUG ON.Is this even a real problem? Do we have any users of this construct that is using it on async algorithms?,technical
,
"Perhaps not, but it is not enforced atm. In any case, limiting the reqsize is going to break things, so that needs to occur based on the sync/async nature of the algo. That also means we'll corrupt the stack if we ever end up using SKCIPHER REQUEST ON STACK() with an async algo whose reqsize is greater than the sync reqsize limit, so I do think some additional sanity check is appropriate.",technical
,
"I'd prefer compile-time based checks.  Perhaps we can introduce a wrapper around crypto skcipher, say crypto skcipher sync which could then be used by SKCIPHER REQUEST ON STACK to ensure that only sync algorithms can use this construct.",technical
,
"That would require lots of changes in the callers, including ones that already take care to use sync algos only. How about we do something like the below instead?",technical
,
"Oh, I like this, thanks!",technical
,
"All of these are ASYNC (they're all crt ablkcipher), so IIUC, I can ignore them. None of these use that I can find. crypto init skcipher ops blkcipher() doesn't touch reqsize at all, so the only places I can find it gets changed are with direct callers of crypto skcipher set reqsize(), which, when wrapping a sync blkcipher start with a reqsize == 0. So, the remaining non-ASYNC callers ask for So, following your patch to encrypt/decrypt, I can add reqsize check there. How does this look, on top of your patch",technical
,
"If the lack of named initializer is too ugly, we could do something crazy like",technical
,
Any comments about this patch? ,technical
,
"The description is slightly confusing, but the patch looks correct and the original code is clearly wrong. Thank you for finding this! Basically, if you request 1 channel 3 times, release the first two and then request 4 channels, you'll be stuck, right?",technical
,
"Yes, you are right. But the real case that I reproduced the issue is a little different: I have 2 stp-policy: ""console"": masters ""256 259""  channels ""7 10""""user""   : masters ""256 1024"" channels ""0 127""I understand the policies should not be overlapped, which is caused by some other issues. So if someone uses ""console"" to request a channel (who will get Channel #7) and then another uses ""user"" to request more than 8 channels, it will be stuck. The commit message is what I trying to abstract the above case, sorry for the confusion.",technical
,
"OK, so given that all SKCIPHER REQUEST ON STACK occurrences are updated in this series anyway, perhaps we should add skcipher [ende]crypt onstack() flavors that encapsulate the additional check? Only question is how to enforce at compile time that those are used instead of the ordinary ones when using a stack allocated request. Would you mind using some macro foo here involving it?",technical
,
"I'll continue to investigate alternatives, but I wanted to point out that the struct change actually fills an existing padding byte (so no change in memory usage) and marking this as an unlikely() test means it wouldn't even be measurable due to the branch predictor (so no change in speed). encrypt/decrypt entry is a tiny tiny fraction of the actual work done during encryption/decryption, etc.",technical
,
Something like a completely new type which in reality is just a wrapper around skcipher: These functions would just be trivial inline functions around their crypto skcipher counterparts.,technical
,
The point is the ON STACK request stuff is purely for backwards compatibility and we don't want it to proliferate and pollute the core API.,technical
,
"This means new wrappers for the other helpers too, yes? For example ;For the above, we'd also need",technical
,
"Wait, I think I misunderstood you. Did you mean a new top-level thing(tfm?) not a new request type? That would mean at least replacing skcipher request set tfm() with a wrapper (since the tfm argument is different), but  not encrypt/decrypt like you mention. I could perform a type test in this. API misuse would be caught at build-time (via SKCIPHER REQUEST ON STACK type checking) and any request size problems would be caught at allocation time. Does this sound like what you had in mind?",technical
,
"Hi, Please document both of these kernel parameters in Documentation/admin-guide/kernel-parameters.txt. ",technical
,
"This setup *should* work. It should be possible to set CPU. Scheduled independent of the CPU. Scheduled values of parent and child task groups. Any intermediate regular task group (i.e. CPU. Scheduled==0) will still contribute the group fairness aspects. That said, I see a hang, too. It seems to happen, when there is a cpu. scheduled!=0 group that is not a direct child of the root task group. You seem to have ""/sys/fs/cgroup/cpu/machine"" as an intermediate group.(The case ==0 within !=0 within the root task group works for me.) I'm going to dive into the code. If you're willing, you can try to get rid of the intermediate ""machine"" cgroup in your setup for the moment. This might tell us, whether we're looking at the same issue.",technical
,
"Ah I see, that makes sense, thank you. Yep I will do this now. Note that if I just try to set machine's CPU. Scheduled to 1, with no other changes (not even changing any child cgroup's CPU. Scheduled yet), I get the following trace I'll reboot and move some cgroups around :)",technical
,
"Yep, this does fix the soft lockups for me, thanks! However, if I do a which should co-schedule all the cgroups for emulator and vcpu threads, I see the same warning I mentioned in my other e-mail:",technical
,
This goes away with the change below (which fixes patch 58/60).,technical
,
"Yep, I can confirm this one as well, is now fixed.",technical
,
"I guess, it would be possible to flatten the task group hierarchy, that is usually created when nesting cgroups. That is, enqueue task group SEs always within the root task group. That should take away much of the (runtime-)overhead, no? The calculation of shares would need to be a different kind of complex than it is now. But that might be manageable. CFS bandwidth control would also need to change significantly as we would now have to dequeue/enqueue nested cgroups below a throttled/unthrottled hierarchy. Unless *those* task groups don't participate in this flattening.(And probably lots of other stuff, I didn't think about right now.)",technical
,
"That sounds like it will wreck the runnable weight accounting. Although, if, as you write below, we do away with the hierarchical run queues, that isn't in fact needed anymore I think. Still, even without runnable weight, I suspect we need the 'runnable' state, even for the other accounting. Can't do that, tasks might have individual constraints that are tighter than the CPU set. Also, changing affinities isn't really a hot path, so who cares. I have yet to go over your earlier email; but no. The scheduler is very much per-cpu. And as I mentioned earlier, CFS as is doesn't work right if you share the runqueue between multiple CPUs (and 'fixing' that is non trivial).Yes, Rik was going to look at trying this. Put all the tasks in the rootrq and adjust the vtime calculations. Facebook is seeing significant overhead from cpu-cgroup and has to disable it because of that on at least part of their setup IIUC. That is the hope; indeed. We'll still need to create the hierarchy for accounting purposes, but it can be a smaller/simpler data structure. So the weight computation would be the normalized product of the parent setc.. and since PELT only updates the values on ~1ms scale, we can keep a cache of the product -- that is, we don't have to recompute that product and walk the hierarchy all the time either. Right, so the whole bandwidth thing becomes a pain; the simplest solution is to detect the throttle at task-pick time, dequeue and try again. But that is indeed quite horrible. I'm not quite sure how this will play out. Anyway, if we pull off this flattening feat, then you can no longer use the hierarchy for this co-scheduling stuff. Now, let me go read your earlier email and reply to that (in parts).",technical
,
"You did mention this work first to me in the context of L1TF, so I might have jumped to conclusions here. Also, I have, of course, been looking at (SMT) co-scheduling, specifically in the context of L1TF, myself. I came up with a vastly different approach. Tim - where are we on getting some of that posted? Note; that even though I wrote much of that code, I don't particularly like it either :-)",technical
,
"Specifically for L1TF I hooked into/extended KVM's preempt notifier registration interface, which tells us which tasks are VCPUs and to which VM they belong. But if we want to actually expose this to userspace, we can either do aprctl() or extend struct sched attr.Well, you mentioned it as an alternative to paravirt spinlocks -- I'm saying that co-scheduling cannot do that, you need full featured gang-scheduling for that.",technical
,
"The thing is, if you drop the full width gang scheduling, you instantly require the paravirt spinlock / tlb-invalidate stuff again. Of course, the constraints of L1TF itself requires the explicit scheduling of idle time under a bunch of conditions. I did not read your [7] in much detail (also very bad quality scan that:-/; but I don't get how they leap from 'thrashing' to co-scheduling. Their initial problem, where A generates data that B needs and the 3scenarios: 1) A has to wait for B 2) B has to wait for A 3) the data gets buffered Seems fairly straight forward and is indeed quite common, needing co-scheduling for that, I'm not convinced. We have of course added all sorts of adaptive wait loops in the kernel to deal with just that issue. With co-scheduling you 'ensure' B is running when A is, but that doesn't mean you can actually make more progress, you could just be burning a lot of CPU cycles (which could've been spend doing other work).I'm also not convinced co-scheduling makes  any  sense outside SMT --does one of the many papers you cite make a good case for !SMT co-scheduling? It just doesn't make sense to co-schedule the LLC domain, that's 16+ cores on recent chips.",technical
,
"I don't get the affinity part. If I create two cgroups by giving them only cpu shares (no CPU set) and set their CPU. Scheduled=1, will this ensure co-scheduling of each group on core level for all cores in the system?",technical
,
"Short answer: Yes. But ignoring the affinity part will very likely result in              a poor experience with this patch set. I was referring to the CPU affinity of a task, that you can set via sched setaffinity() from within a program or via taskset from the command line. For each task/thread within a cgroup, you should set the affinity to exactly one CPU. Otherwise -- as the load balancing part is still missing --you might end up with all tasks running on one CPU or some other unfortunate load distribution. Coscheduling itself does not care about the load, so each group will be(co-)scheduled at core level, no matter where the tasks ended up. Regards Jan PS: Below is an example to illustrate the resulting schedules a bit better, and what might happen, if you don't bind the to-be-scheduled tasks to individual CPUs. For example, consider a dual-core system with SMT (i.e. 4 CPUs in total),two task groups A and B, and tasks within them a0, a1, ..  and b0, b1, ..respectively .Let the system topology look like this:        System          (level 2)      /        \  Core 0      Core 1    (level 1)  /    \      /    \CPU0  CPU1  CPU2  CPU3  (level 0)If you set CPU. Scheduled=1 for A and B, each core will be scheduled independently, if there are tasks of A or B on the core. Assuming there are runnable tasks in A and B and some other tasks on a core, you will see a schedule like:  A -> B -> other tasks -> A -> B -> other tasks -> ...(or some permutation there of) happen synchronously across both CPUs of a core -- with no guarantees which tasks within A/within B/within the other tasks will execute simultaneously -- and with no guarantee what will execute on the other two CPUs simultaneously. (The distribution of CPU time between A, B, and other tasks follows the usual CFS weight proportional distribution, just at core level.) If neither CPU of a core has any runnable tasks of a certain group, it won't be part of the schedule (e.g., A -> other -> A -> other).With CPU. Scheduled=2, you lift this schedule to system-level and you would see it happen across all four CPUs synchronously. With CPU. Scheduled=0, you get this schedule at CPU-level as we're all used to with no synchronization between CPUs. (It gets a tad more interesting, when you start mixing groups with CPU. Scheduled=1 and =2.)Here are some schedules, that you might see, with A and B scheduled at core level (and that can be enforced this way (along the horizontal dimension) by setting the affinity of tasks; without setting the affinity, it could be any of them):Tasks equally distributed within A and B. You will never see an A-task sharing a core with a B-task at any point in time (except for the 2 microseconds or so, that the collective context switch takes).",technical
,
"AFAIK, changing the affinity of a cpu set overwrites the individual affinities of tasks within them. Thus, it shouldn't be an issue. This kind of code path gets a little hotter, when a scheduled set gets load-balanced from one core to another. Apart from that, I also think, that normal user-space applications should never have to concern themselves with actual affinities. More often than not, they only want to express a relation to some other task (or sometimes resource), like ""run on the same NUMA node"", ""run on the same core"", so that application design assumptions are fulfilled. That's an interface, that I'd like to see as a cgroup controller at some point. It would also benefit from the ability to move/balance whole run queues.(It might also be a way to just bulk-balance a bunch of tasks in the current code, by exchanging two CFS run queues. But that has probably some additional issues.)No sharing. Just not allocating run queues that won't be used anyway. Assume you express this ""always run on the same core"" or have other reasons to always restrict tasks in a task group to just one core/node/whatever. On an SMT system, you would typically need at most two run queues for a core; the memory foot-print of a task group would no longer increase linearly with system size. It would be possible to (space-)efficiently express nested parallelism use cases without having to resort to managing affinities manually (which restrict the scheduler more than necessary).(And it would be okay for an adjustment of the maximum number of run queues to fail with an -ENOMEM in dire situations, as this adjustment would be an explicit(user-)action.)Yeah. I might be a bit biased towards keeping or at least not fully throwing away the nesting of CFS run queues. ;)However, the only efficient way that I can currently think of, is a hybrid model between the ""full nesting"" that is currently there, and the ""no nesting"" you were describing above. It would flatten all task groups that do not actively contribute some function, which would be all task groups purely for accounting purposes and those for*unthrottled* CFS hierarchies (and those for coscheduling that contain exactly one SE in a runqueue). The nesting would still be kept for *throttled* hierarchies (and the coscheduling stuff). (And if you wouldn't have mentioned a way to get rid of nesting completely, I would have kept a single level of nesting for accounting purposes as well.) This would allow us to lazily dequeue SEs that have run out of bandwidth when we encounter them, and already enqueue them in the nested task group (whose SE is not enqueued at the moment). That way, it's still a O(1) operation to re-enable all tasks, once runtime is available again. And O(1) to throttle a repeat offender.",technical
,
"No, it only shrinks the set. Also nothing stops you calling sched setaffinity() once you're inside the CPU set. The only constraint is that your mask is a subset of the CPU set mask.",technical
,
You forget that SMT4 and SMT8 are fairly common outside of x86.,technical
,
"I meant setting the affinity of the CPU set *after* setting an individual affinity. Like this: pid 4745's current affinity list: 0,1#The individual affinity of  is lost, despite it being a subset.",technical
,
"I do not have a strong bias either way. However, I would like the overhead of the cpu controller to be so low that we can actually use it :)Task priorities in a flat runqueue are relatively straightforward, with vruntime scaling just like done for nice levels, but I have to admit that throttled groups provide a challenge. Dequeueing throttled tasks is pretty straightforward, but requeuing them afterwards when they are no longer throttled could present a real challenge in some situations. I suspect most systems will have a number of runnable tasks no larger than the number of CPUs most of the time. That makes ""re-enable all the tasks"" often equivalent to ""re-enable one task"". Can we handle the re-enabling (or waking up!) of one task almost as fast as we can without the cpu controller?",technical
,
"What are the other use cases, and what kind of performance numbers do you have to show examples of workloads where coscheduling provides a performance benefit?",technical
,
"Do you know/have an idea how the flat approach would further skew the approximations currently done in?With the nested hierarchy the (shared) task group SE is updated whenever something changes. With the flat approach, you'd only be able to update a task SE, when you have to touch the task anyway. Just from thinking briefly about it, it feels like values would be out of date for longer periods of time. We could start by transparently special-casing the ""just one SE in a runqueue"" case, where that single SE is enqueued directly into the next parent, and everything falls back to nesting, the moment a second SE pops up. That might allow reaping the benefits for many cases without hurting other cases. It's also more a gradual conversion of code.",technical
,
"Ok got it. Can we have a more generic interface, like specifying a set of task ids to be co-scheduled with a particular level rather than tying this with cgroups? KVMs may not always run with cgroups and there might be other use cases where we might want co-scheduling that doesn't relate to cgroups.",technical
,
"For further use cases (still an incomplete list) let me redirect you to the unabridged Section B of the original e-mail:   If you want me to, I can go into more detail and make the list from that e-mail more complete. Note, that many coscheduling use cases are not primarily about performance. Sure, there are the resource contention use cases, which are barely about anything else. See, e.g., [1] for a survey with further pointers to the potential performance gains. Realizing those use cases would require either a user space component driving this, or another kernel component performing a function similar to the current auto-grouping with some more complexity depending on the desired level of sophistication. This extra component is out of my scope. But I see a coscheduler like this as an enabler for practical applications of these kind of use cases. If you use coscheduling as part of a solution that closes a side-channel, performance is a secondary aspect, and hopefully we don't lose much of it. Then, there's the large fraction of use cases, where coscheduling is primarily about design flexibility, because it enables different (old and new) application designs, which usually cannot be executed in an efficient manner without coscheduling.  For these use cases performance is important, but there is also a trade-off against development costs of alternative solutions to consider. These are also the use cases where we can do measurements today, i.e., without some yet-to-be-written extra component. For example, with coscheduling it is possible to use active waiting instead of passive waiting/spin-blocking on non-dedicated systems, because lock holder pre-emption is not an issue anymore. It also allows using applications that were developed for dedicated scenarios in non-dedicated settings without loss in performance -- like an (unmodified) operating system within a VM, or HPC code. Another example is cache optimization of parallel algorithms, where you don't have to resort to cache-oblivious algorithms for efficiency, but where you can stay with manually tuned or auto-tuned algorithms, even on non-dedicated systems. (You're even able to do the tuning itself on a system that has other load.)Now, you asked about performance numbers, that *I* have. If a workload has issues with lock-holder pre-emption, I've seen up to 5x to20x improvement with coscheduling. (This includes parallel programs [2] and VMs with unmodified guests without PLE [3].) That is of course highly dependent on the workload. I currently don't have any numbers comparing coscheduling to other solutions used to reduce/avoid lock holder pre-emption, that don't mix in any other aspect like resource contention. These would have to be micro-benchmarked. If you're happy to compare across some more moving variables, then more or less blind coscheduling of parallel applications with some automatic workload-driven (but application-agnostic) width adjustment of scheduled sets yielded an overall performance benefit between roughly 10% to 20% compared to approaches with passive waiting [2]. It was roughly on par with pure space-partitioning approaches (slight minus on performance, slight plus on flexibility/fairness).I never went much into the resource contention use cases myself. Though, I did use coscheduling to extend the concept of ""nice"" to sockets by putting all niced programs into a scheduled task group with appropriately reduced shares.  This way, niced programs don't just get any and all idle CPU capacity -- taking away parts of the energy budget of more important tasks all the time -- which leads to important tasks running at turbo frequencies more often. Depending on the parallelism of niced workload and the parallelism of normal workload, this translates to a performance improvement of the normal workload that corresponds roughly to the increase in frequency (for CPU-bound tasks) [4]. Depending on the processor, that can be anything from just a few percent to about a factor of 2.",technical
,
"Currently: no. At this point the implementation is tightly coupled to the cpu cgroup controller. This *might* change, if the task group optimizations mentioned in other parts of this e-mail thread are done, as I think, that it would decouple the various mechanisms. That said, what if you were able to disable the ""group-based fairness"" aspect of the cpu cgroup controller? Then you would be able to control just the coscheduling aspects on their own. Would that satisfy the use case you have in mind?",technical
,
"Sounds like a co-scheduling system would need the following elements:1) Identify groups of runnable tasks to run together.2) Identify hardware that needs to be co-scheduled   (for L1TF reasons, POWER7/8 restrictions, etc).3) Pack task groups into the system in a way that   allows maximum utilization by co-scheduled tasks.4) Leave some CPU time for regular time sliced tasks.5) In some cases, leave some CPU time idle on purpose. Step 1 would have to be revaluated periodically, as tasks (e.g. VCPUs) wake up and go to sleep. I suspect this may be much better done as its own scheduler class, instead of shoehorned into CFS.I like the idea of having some co-scheduling functionality in Linux, but I absolutely abhor the idea of making CFS even more complicated than it already is. The current code is already incredibly hard to debug or improve. Are you getting much out of CFS with your current code? It appears that your patches are fighting CFS as much as they are leveraging it, but admittedly I only looked at them briefly.",technical
,
"Can't say much about tlb-invalidate, but yes to the spinlock stuff: if there isn't any additional information available, all runnable tasks/vCPUs have to be scheduled to avoid lock holder preemption. With additional information about tasks potentially holding locks o potentially spinning on a lock, it would be possible to coschedule smaller subsets -- no idea if that would be any more efficient though. That is true for some of the resource contention use cases, too. Though, they are much more relaxed wrt. their requirements on the simultaneousness of the context switch. In my personal interpretation, that analogy refers to the case where the waiting time for a lock is shorter than the time for a context switch --but where the context switch was done anyway, ""thrashing"" the CPU. Anyway. I only brought it up, because everyone has a different understanding of what ""coscheduling"" or ""gang scheduling"" actually means. The memorable quotes are from Ousterhout:  ""A task force is scheduled if all of its runnable processes are exe-   cutting simultaneously on different processors. Each of the processes   in that task force is also said to be scheduled.""(where a ""task force"" is a group of closely cooperating tasks), and from Feitelson and Rudolph:  ""[Gang scheduling is defined] as the scheduling of a group of threads   to run on a set of processors at the same time, on a one-to-one   basis.""(with the additional assumption of time slices, collective preemption, and that threads don't relinquish the CPU during their time slice).That makes gang scheduling much more specific, while coscheduling just refers to the fact that some things are executed simultaneously. I don't think, that coscheduling should be applied blindly. Just like the adaptive wait loops you mentioned: in the beginning there was active waiting; it wasn't that great, so passive waiting was invented; turns out, the overhead is too high in some cases, so let's spin adaptively for a moment. We went from uncoordinated scheduling to system-wide coordinated scheduling(which turned out to be not very efficient for many cases). And now we are in the phase to find the right adaptiveness. There is work on enabling coscheduling only on-demand (when a parallel application profits from it)or to be more fuzzy about it (giving the scheduler more freedom); there is work to go away from system-wide coordination to (dynamically) smaller isles (where I see my own work as well). And ""recently"" we also have the resource contention and security use cases leaving their impression on the topic as well. There's the resource contention stuff, much of which targets the last level cache or memory controller bandwidth. So, that is making a case for coscheduling larger parts than SMT. However, I didn't find anything in a short search that would already cover some of the more recent processors with 16+ cores. There's the auto-tuning of parallel algorithms to a certain system architecture. That would also profit from LLC coscheduling (and slightly larger time slices) to run multiple of those in parallel. Again, no idea for recent processors. There's work to coschedule whole clusters, which goes beyond the scope of a single system, but also predates recent systems. (Search for, e.g., ""implicit coscheduling"").So, 16+ cores is unknown territory, AFAIK. But not every recent system has 16+ cores, or will have 16+ cores in the near future. ",technical
,
"Both, Peter and Subhra, seem to prefer an interface different than cgroups to specify what to coschedule. Can you provide some extra motivation for me, why you feel that way?(ignoring the current scalability issues with the cpu group controller)After all, cgroups where designed to create arbitrary groups of tasks and to attach functionality to those groups. If we were to introduce a different interface to control that, we'd need to introduce a whole new group concept, so that you make tasks part of some group while at the same time preventing unauthorized tasks from joining a group. I currently don't see any wins, just a loss in flexibility. ",technical
,
"I found another issue today, while attempting to test (with 61/60applied) separate coscheduling cgroups for vcpus and emulator threads[the default configuration with libvirt]",technical
,
I got an non-truncated log as well:,technical
,
Yes that will suffice the use case. We wish to experiment at some point with co-scheduling of certain workers threads in DB parallel query and see if there is any benefit ,technical
,
"I think cgroups will the get the job done for any use case. But we have, e.g. affinity control via both sched setaffinity and cgroup CPU sets. It will be good to have an alternative way to specify co-scheduling too for those who don't want to use cgroup for some reason. It can be added later on though, only how one will override the other will need to be sorted out.",technical
,
"I got it reproduced. I will send a fix, when I have one. ",technical
,
"In case nobody else brought it up yet, you're going to need a handshake to strengthen protection against L1TF attacks. Otherwise, there's still a small window where an attack can occur during the reschedule. Perhaps one could then cause this to happen artificially by repeatedly have a VM do some kind of pause/mwait type operation that might do a reschedule. ",technical
,
Have you considered using CPU set to specify the set of CPUs inside which you want to coschedule task groups in? Perhaps that would be more flexible and intuitive to control than this CPU. Scheduled value. Unless you require this feature to act always symmetrical through the branches of a given domain tree? Thanks.,technical
,
Do you know how much is the delay? i.e. what is overlap time when a thread of new group starts executing on one HT while there is still thread of another group running on the other HT? ,technical
,
"Yes, I did consider CPU sets. Though, there are two dimensions to it: a) at what fraction of the system tasks shall be scheduled, and b) where these tasks shall execute within the system.cpusets would be the obvious answer to the ""where"". However, in the current form they are too inflexible with too much overhead. Suppose, you want to coschedule two tasks on SMT siblings of a core. You would be able to restrict the tasks to a specific core with a CPU set. But then, it is bound to that core, and the load balancer cannot move the group of two tasks to a different core. Now, it would be possible to ""invent"" relocatable CPU sets to address that issue (""I want affinity restricted to a core, I don't care which""), but then, the current way how CPU set affinity is enforced doesn't scale for making use of it from within the balancer. (The upcoming load balancing portion of the coscheduler currently uses a file similar to CPU. Scheduled to restrict affinity to a load-balancer-controlled subset of the system.)Using CPU sets as the mean to describe which parts of the system are to be scheduled *may* be possible. But if so, it's a long way out. The current implementation uses scheduling domains for this, because (a) most coscheduling use cases require an alignment to the topology, and (b) it integrates really nicely with the load balancer. AFAIK, there is already some interaction between CPU sets and scheduling domains. But it is supposed to be rather static and as soon as you have overlapping CPU sets, you end up with the default scheduling domains. If we were able to make the scheduling domains more dynamic than they are today, we might be able to couple that to CPU sets (or some similar interface to *define* scheduling domains). ",technical
,
"Oh ok, I understand now. Affinity and node-scope mutual exclusion are entirely decoupled, I see. So what is the need for cosched split domains? What kind of corner case won't fit into scheduler domains? Can you perhaps spare that part in this patchset to simplify it somehow? If it happens to be necessary, it can still be added iteratively. Thanks.",technical
,
"Oh boy, so the coscheduler is going to get its own load balancer? At that point, why bother integrating the coscheduler into CFS, instead of making it its own scheduling class? CFS is already complicated enough that it borders on unmaintainable. I would really prefer to have the coscheduler code separate from CFS, unless there is a really compelling reason to do otherwise.  ",technical
,
"I guess he wants to reuse as much as possible from the CFS features and code present or to come (nice, fairness, load balancing, power aware, NUMA aware, etc...). OTOH you're right, the thing has specific enough requirements to consider a new sched policy. And really I would love to see all that code separate from CFS, for the reasons you just outlined. So I cross my fingers on what Jan is going to answer on a new policy.",technical
,
"I wonder if things like nice levels, fairness, and balancing could be broken out into code that could be reused from both CFS and a new co-scheduler scheduling class. A bunch of the cgroup code is already broken out, but maybe some more could be broken out and shared, too? Some bits of functionality come to mind:- track groups of tasks that should be co-scheduled  (eg all the VCPUs of a virtual machine)- track the subsets of those groups that are runnable  (e.g. the currently runnable VCPUs of a virtual machine)- figure out time slots and CPU assignments to efficiently  use CPU time for the co-scheduled tasks  (while leaving some configurable(?) amount of CPU time  available for other tasks)- configuring some lower-level code on each affected CPU  to ""run task A in slot X"", etc This really does not seem like something that could be shoehorned into CFS without making it unmaintainable. Furthermore, it also seems like the thing that you could never really get into a highly efficient state as long as it is weighed down by the rest of CFS.  ",technical
,
"Not ""its own"". The load balancer already aggregates statistics about sched-groups. With the coscheduler as posted, there is now a runqueue per scheduling group. The current ""ad-hoc"" gathering of data per scheduling group is then basically replaced with looking up that data at the corresponding runqueue, where it is kept up-to-date automatically. Exactly. I want a user to be able to ""switch on"" coscheduling for those parts of the workload that profit from it, without affecting the behavior we are all used to. For both: scheduling behavior for tasks that are not scheduled, as well as scheduling behavior for tasks *within* the group of scheduled tasks. Maybe. The primary issue that I have with a new scheduling class, is that they are strictly priority ordered. If there is a runnable task in a higher class, it is executed, no matter the situation in lower classes. ""Coscheduling"" would have to be higher in the class hierarchy than CFS. And then, all kinds of issues appear from starvation of CFS threads and other unfairness, to the necessity of (re-)defining a set of pre-emption rules, nice and other things that are given with CFS.cgroups runqueues CFS run queues and associated rules for pre-emption/time slices/etc. There is no ""slot"" concept, as it does not fit my idea of interactive usage. (As in ""slot X will execute from time T to T+1.) It is purely event-driven right now (eg, ""group X just became runnable, it is considered more important than the currently running group Y; all CPUs (in the affected part of the system) switch to group X"", or ""group X ran long enough, next group"").While some planning ahead seems possible (as demonstrated by the Tableau scheduler that Peter already pointed me at), I currently cannot imagine such an approach working for general purpose workloads. The absence of true pre-emption being my primary concern. I still have this idealistic notion, that there is no ""weighing down"". I see it more as profiting from all the hard work that went into CFS,avoiding the same mistakes, being backwards compatible, etc. If I were to do this ""outside of CFS"", I'd overhaul the scheduling class concept as it exists today. Instead, I'd probably attempt to schedule instantiations of scheduling classes. In its easiest setup, nothing would change: one CFS instance, one RT instance, one DL instance, strictly ordered by priority (on each CPU). The coscheduler as it is posted (and task groups in general), are effectively some form of multiple CFS instances being governed by a CFS instance. This approach would allow, for example, multiple CFS instances that are scheduled with explicit priorities; or some tasks that are scheduled with a custom scheduling class, while the whole group of tasks competes for time with other tasks via CFS rules. I'd still keep the feature of ""coscheduling"" orthogonal to everything else, though. Essentially, I'd just give the user/admin the possibility to choose the set of rules that shall be applied to entities in a runqueue. Your idea of further modularization seems to go in a similar direction, or at least is not incompatible with that. If it helps keeping things maintainable, I'm all for it. For example, some of the (upcoming) load balancing changes are just generalizations, so that the functions don't operate on *the* set of CFS run queues, but just *a* set of CFS runqueues. Similarly in the already posted code, where task picking now starts at*some* top CFS runqueue, instead of *the* top CFS runqueue.",technical
,
The leader doesn't kick the other cpus  immediately  to switch to a different cosched group. So threads from previous cosched group will keep running in other HTs till their sched slice is over (in worst case). This can still keep the window of L1TF vulnerability open?,technical
,
"It does. (Or at least, it should, in case you found evidence that it does not.)Specifically, the logic to not pre-empt the currently running task before some minimum time has passed, is without effect for a collective context switch. No. Per the above, the window due to the collective context switch should not be as long as ""the remaining time slice"" but more towards the IPI delay. During this window, tasks of different coscheduling groups may execute simultaneously. In addition (as mentioned in the quoted text above), there more cases where a task of a scheduled group on one SMT sibling may execute simultaneously with some other code not from the same scheduled group: tasks in scheduling classes higher than CFS, and interrupts -- as both of them operate outside the scope of the coscheduler.",technical
,
"Can you point to where the leader is sending the IPI to other siblings? I did some experiment and delay seems to be sub microsec. I ran 2 threads that are just looping in one cosched group and affinitized to the 2 HTs of a core. And another thread in a different cosched group starts running affinitized to the first HT of the same core. I time stamped just before context switch() in   schedule() for the threads switching from one to another and one to idle. Following is what I get on cpu 1 and 45 that are siblings, cpu 1 is where the other thread preempts. Not sure why the first switch on cpu to idle happened. But then onwards the difference in timestamps is less than a microsec. This is just a crude way to get a sense of the delay, may not be exact.",technical
,
"Did your approach get posted to LKML? I never saw it I don't think, and I don't see it on lore. Could it be posted as an RFC, even if not suitable for upstreaming yet, just for comparison? Thanks!-Nish",technical
,
"I kind of agree with Jan here that this is just going to add yet another task group mechanism, very similar to the existing one, with run queues inside and all. Can you imagine kernel/sched/fair.c now dealing with both groups implementations? What happens when cgroup task groups and cosched sched groups don't match wrt. Their tasks, their priorities, etc...I understand cgroup task group has become infamous. But it may be a better idea in the long run to fix it.",technical
,
"One detail here, is that hierarchical task group a strong requirement for cosched or could you live with it flattened in the end?",technical
,
"Currently, it is a strong requirement. As mentioned at the bottom of  it should be possible to pull the hierarchical aspect out of CFS and implement it one level higher. But that would be a major re-design of everything. I use the hierarchical aspect to a) keep scheduled groups in separate sets of runqeues,so that it is easy to select/balance tasks within a particular group; and b) to implement per-core, per-node, per-system run queues that represent larger fractions of the system, which then fan out into per-CPU run queues (eventually).",technical
,
"Thanks for your patch.  Unfortunately, this entire function is scheduled for deletion, so I won't be applying it. If you're interested in the radix tree, I'd recommend looking at its replacement, the XArray.  The current version is at but I'll be pushing another version out in the next few days.",technical
,
Why? What bug does this fix?,technical
,
"NACK for any bindings that are linux specific. The suspend feature is so platform dependent that I see no need for generic Linux bindings for the same. We have power domains and idle states. If you have platforms that doesn't support some of the states, just disable them in the DT.What makes any of the above linux specific. So once again NACK.",technical
,
suspend to mem and suspend to disk are pretty generic states and i agree implementation is platform dependent so why not have properties that convey if they are supported? Is the disagreement over making the properties being linux specific?,technical
,
"We already have power domains and idle states for that. If you need to restrict few states on some platform for whatever reasons, just disable those states. I don't see the need to add any more bindings for the same. Yes.",technical
,
"Oh do you mean the ""domain-idle-states"" property as mentioned in the Documentation? Yeah that should do and the DOMAIN PWR DN and DOMAIN RET can be SoC specific and then the board can select which ones to use depending on how things are wired for GPIOs, memory, PMIC and so on. Hmm I don't see any users for this binding though? ",technical
,
"Yes, exactly that. All the idle-states are platform specific. DOMAIN RET and DOMAIN PWR DN are just examples used in the bindings. It was added specifically to deal with such SoC idles states or hierarchical CPU power domains states, no users in upstream yet. But IMO it fits what subject is trying to address.",technical
,
OK OK great thanks for confirming that.,technical
,
"......The copypasta above and below is not my favorite, but I suppose it's either this or wrap it all up in a macro that you stamp down 4 times. I'm not sure if that's really any cleaner, so I guess this is fine.CONFIGFS ITEM NAME LEN is only 20. Is there anything preventing the device name passed in here from being longer than that? You'd have a nasty overrun on your hands if not. Maybe snprintf here?",technical
,
Agree. I need to use snprintf here. Will update.,technical
,
Quoting So then why use devm clk get()? Please replace both so that devm clk put() doesn't need to be used..,technical
,
Indeed between the successful devm clk get and the devm clk put we don't exit the function in error so I can use clk get and clk put. ,technical
,
"This kind of coding style fix has very little value for a subsystem which is essentially frozen from changes, and to which the less changes that happen to it the better in order to avoid potential regressions. Therefore I am not applying this patch, sorry.",technical
,
"for reviewing the patch. rtnl needed is not a shared variable, it is part of bonding structure, that is one per bonding driver instance. There can't be two parallel instances of bond miimon inspect for a single   bonding driver instance at any given point of time. and only bond miimon inspect updates it. That ™s why I think there is no need of any synchronization here. Thank you for cautioning us on bool usage. even a u8 can meet our requirement.  we will change it.  but; if time permits can you share more on ""particularly dangerous here, at least on some arches"".",technical
,
"Thank you, we are making the changes and will repost the patch after testing it.",technical
,
"Please review the updated patch, I have changed the rtnl needed to an atomic variable, to avoid any race condition: When link status change needs to be committed and rtnl lock couldn't be taken, avoid redisplay of same link status change message.",technical
,
What happens to pmus that got added later? The rest looks good. Can you post a non RFC version?,technical
,
There is a hunk a bit lower in the patch where in perf pmu register the initial setting is assigned from the global sysctl. Sure! ,technical
,
Which paranoia level would be used for the i915.perf event paranoid setting in such a case? Perhaps also CC  on the next version.,technical
,
" thanks a lot for involving the folks! If you ask me then, IMHO, unprivileged access to CBOX pmu looks unsafe and is now governed by traditional *core* perf event paranoid setting. But *core* paranoid >= 1 (per-process mode) prevents simultaneous perf record sampling and perf stat -I reading from IMC, UPI, PCIe and other uncore counters. This kind of monitoring could make process performance observability thru Perf subsystem more flexible and better tailored for cloud and cluster environments. However it requires fine-tuning control capabilities in order to still keep system as secure as possible. Could i915, IMC, UPI, PCIe pmus be safe enough to be governed by a separate perf event paranoid settings?",technical
,
"There's also been prior discussion on these feature in other contexts(e.g. android exploits resulting from out-of-tree drivers). It would be nice to see those considered. IIRC The conclusion from prior discussions (e.g. [1]) was that we wanted finer granularity of control such that we could limit PMU access to specific users -- e.g. disallow arbitrary android apps from poking *any*PMU, while allowing some more trusted apps/users to uses *some* specific PMUs. e.g. we could add /sys/bus/event source/devices/{PMU}/device, protect this via the usual fs ACLs, and pass the fd to perf event open() somehow. A valid fd would act as a capability, taking precedence over perf event paranoid.",technical
,
That sounds like an orthogonal feature. I don't think the original patchkit would need to be hold up for this. It would be something in addition.BTW can't you already do that with the syscall filter? I assume the Android sandboxes already use that. Just forbid perf event openfor the apps.,technical
,
"I have to say that I disagree -- these controls will have to interact somehow, and the fewer of them we have, the less complexity we'll have to deal with longer-term. Note that this was about providing access to *some* PMUs in some cases.IIUC, if that can be done today via a syscall filter, the same is true of per-pmu paranoid settings.",technical
,
"Just to make it clear. I'm not against separate settings at all. But I'm against adding knobs for every PMU the kernel supports wholesale without analysis and documentation just because we can and somebody wants it. Right now we have a single knob, which is poorly documented and that should be fixed first. But some googling gives you the information that allowing unprivileged access is a security risk. So the security focussed sysadmin will deny access to the PMUs no matter what. Now we add more knobs without documentation what the exposure and risk of each PMU is. The proposed patch set does this wholesale for every PMU supported by the kernel. So the GPU user will ask the sysadmin to allow him access. How can he make an informed decision? If he grants it then the next user comes around and wants it for something else arguing that the other got it for the GPU already. How can he make an informed decision about that one? We provide the knobs, so it's also our responsibility towards our users to give them the information about their usage and scope. And every single PMU knob has a different scope. The documentation of the gazillion of knobs in /proc and /sysfs is not really brilliant, but we should really not continue this bad practice especially not when these knobs have potentially security relevant implications. Yes, I know, writing documentation is work, but it's valuable and is appreciated by our users. To make this doable and not blocked by requiring every PMU to be analysed and documented at once, I suggested to make this opt-in. Do analysis for a given PMU, decide whether it should be exposed at all. If so, document it proper and flip the bit. That way this can be done gradually as the need arises and we can exclude the riskier ones completely. I don't think this is an unreasonable request as it does not require thei915 folks to look at PMUs they are not familiar with and does not get blocked by waiting on every PMU wizard on the planet to have time. Start with something like Documentation/admin-guide/perf-security.rst or whatever name fits better and add a proper documentation for the existing knob. With the infrastructure for fine grained access control add the general explanation for fine grained access control. With each PMU which opts in for the knob, add a section with guidance about scope and risk for this particular one.",technical
,
"You're proposing to completely redesign perf event open. This new file descriptor argument doesn't exist today so it would need to create a new system call with more arguments(and BTW it would be more than the normal 6 argument limit we have, so actually it couldn't even be a standard sycall) Obviously we would need to keep the old system call around for compatibility, so you would need to worry about this interaction in any case! So tying it together doesn't make any sense, because the problem has to be solved separately anyways. The difference is that the Android sandboxes likely already doing this and have all the infrastructure, and it's just another rule. Requiring syscall filters just to use the PMU on xn system that otherwise doesn't need them would be very odd.",technical
,
"And I think it would be a very good redesign. :) I love things that use file descriptors to represent capabilities. Is that true? The first argument is a pointer to a struct that contains its own size, so it can be expanded without an ABI break. I don't see any reason why you couldn't cram more stuff in there.",technical
,
"You're right we could put the fd into the perf event, but the following is still true:-",technical
,
"<blasphemy>Is that true? IIRC if you want to use the perf tools after a kernel update, you have to install a new version of perf anyway, no? I think after I run a kernel update, when I run ""perf top"", it just refuses to start and tells me to go install a newer version. Would the users of perf event open() that want to monitor this graphics stuff normally \keep working after a kernel version bump? I realize that the kernel is very much against breaking userspace interfaces, but if userspace has already decided to break itself after every update, we might as well take advantage of that...</blasphemy>",technical
,
"Not at all. perf is fully ABI compatible. Yes Ubuntu/Debian make you do it, but there is no reason for it other than their ignorance. Other sane distributions don't. Usually the first step when I'm forced to use one of those machine is to remove the useless wrapper and call the perf binary directly.",technical
,
"Ah, I guess the answer is ""0"", since you want to see data about what other users are doing. Does the i915 PMU expose sampling events, counting events, or both? The thing about sampling events is that they AFAIK always let the user pick arbitrary data to collect - like register contents, or userspace stack memory -, and independent of the performance counter being monitored, this kind of access should not be permitted to other contexts. (But it might be that I misunderstand how perf works - I'm not super familiar with its API.)",technical
,
"And why so? You can keep the original functionality around with the existing restrictions without breaking any existing user space. That existing functionality does not require new knobs. It stays as is. So if you want to use the enhanced version with per PMU permissions based on file descriptors you need a new version of perf. That's nothing new, if the kernel adds new features to any syscall, then you need new tools, new libraries etc. The only guarantee the kernel makes is not to break existing user space, but there is no guarantee that you can utilize new features with existing userspace.",technical
,
"Hello, Sounds like a plan. Thanks! ",technical
,
"Hello, There are usages in production where perf event open() syscall accompanied with read(), mmap() etc. is embedded into application on per-thread basis and is used for self monitoring and dynamic execution tuning. There are also other Perf tools around that, for example, are statically linked and then used as on Linux as on Android. Backward compatibility does matter in these cases.",technical
,
"Hell, Currently *core* paranoid >= 1 (per-process mode) prevents simultaneous sampling on CPU events (perf record) and reading of uncore HW counters (perf stat -I), because uncore counters count system wide and that is allowed only when *core* paranoid <= 0.Uncore counts collected simultaneously with CPU event samples can be correlated using timestamps taken from some common system clock e.g.CLOCK MONOTONIC RAW. Could it be secure enough to still allow reading of system wide uncore HW counters when sampling of CPU events is limited to specific processes by *core* paranoid >= 1?",technical
,
"Well, it's nothing fundamentally new, that new features require changes to applications, libraries etc. It's nice if it can be avoided of course. From a design POV, Jann's idea to have a per PMU special file which you need to open for getting access is way better than the extra knobs. It allows to use all existing security mechanisms to be used. Peter and I discussed that and we came up with the idea that the file descriptor is not even required, i.e. you could make it backward compatible.perf event open() knows which PMU is associated with the event the caller tries to open. So perf event open() can try to access/open the special per PMU file on behalf of the caller. That should get the same security treatment like a regular open() from user space. If that succeeds, access is granted. The magic file could still be writeable for root to give general restrictions aside of the file based ones similar to what you are proposing. The analysis and documentation requirements still remain of course.",technical
,
"(That was Mark's idea, not mine, I just agree with his idea a lot.)",technical
,
"Hello,<SNIP>Let me wrap up all the requirements and ideas that have been captured so far.1. A file [1] is added so that it can belong to a group of users allowed to use {PMU},   something like this Modifications of file content are allowed to those who can   modify setting.2. Semantics and content of the introduced paranoid file is   similar to this   The perf event paranoid file can be set to restrict access   to the performance counters.   2   allow only user-space measurements (default since Linux 4.6).   1   allow both kernel and user measurements (default before Linux 4.6).   0   allow access to CPU-specific data but not raw trace point samples.  -1  no restrictions.   The existence of the perf event paranoid file is the official method   for determining if a kernel supports perf event open().3. Every time an event for {PMU} is created over perf event open():   a) the calling thread's euid is checked to belong to {PMU} users group      and if it does then the event's fd is allocated;   b) then traditional checks against perf event pranoid content are applied;   c) if the file doesn't exist the access is governed by global setting, 4. Documentation/admin-guide/perf-security.rst file is introduced that:   a) contains general explanation for fine grained access control;   b) contains a section with guidance about scope and risk for each PMU      which is enabled for fine grained access control;   c) file is extended when more PMUs are enabled for fine grain control;Security analysis for uncore IMC, QPI/UPI, PC Ie PMUs is still required to be enabled for fine grain control.",technical
,
"Right, though I personally prefer something like 'access control' as filename, but that's bike shed painting realm. Not only the user group, it really should do the full security checks which are done on open().Hmm, not sure about that because that might be conflicting. Correct.     0) Better documentation of this.",technical
,
"Hello, I expect it is already implemented by some internal kernel API so that it could be reused. Well, possible contradictions could be converged to some reasonable point during technical review stage. Current perf event paranoid semantics is still required for PMUs that are governed by global setting at. Exactly. perf event open man7 [1] requires update as well, however this is not a part of kernel source tree so these docs changes are to be mailed TO:",technical
,
"You'll also have to make sure that this thing in kernel/events/core.c doesn't have any bad effect:    /*    * Special case software events and allow them to be part of    * any hardware group.    */As in, make sure that you can't smuggle in arbitrary software events by attaching them to a whitelisted hardware event. And you can't whitelist anything that permits using sampling events with arbitrary sample type.",technical
,
"Hi,<SNIP>Yes, makes sense. Please see and comment below.<SNIP>It appears that there is a dependency on the significance of data that PMUs captures for later analysis. Currently there are following options for data being captured (please correct or extend if something is missing from the list below):1) Monitored process details:   - system information on a process as a container (of threads, memory data and     IDs (e.g. open fds) from process specific namespaces and etc.);   - system information on threads as containers (of execution context details);2) Execution context details:   - memory addresses;   - memory data;   - calculation results;   - calculation state in HW;3) Monitored process and execution context telemetry data, used for building   various performance metrics and can come from:   - user mode code and OS kernel;   - various parts of HW e.g. core, uncore, peripheral and etc. Group 2) is the potential leakage source of sensitive process data so if a PMU, at some mode, samples execution context details then the PMU, working in that mode, is the subject for *access* and *scope* control. On the other hand if captured data contain only the monitored process details and/or associated execution telemetry, there is probably no sensitive data leakage thru that captured data. For example, if cpu PMU samples PC addresses overtime, e.g. for providing hotspots-by-function profile, then this requires to be controlled as from access as from scope perspective, because PC addresses is execution context details that can contain sensitive data. However, if cpu PMU does counting of some metric value, or if software PMU reads value of thread active time from the OS, possibly overtime, for later building some rating profile, or reading of some HW counter value without attribution to any execution context details, that is probably not that risky as in the case of PC address sampling. Uncore PMUs e.g. memory controller (IMC), interconnect (QPI/UPI) and peripheral (PCIe) currently only read counters values that are captured system wide by HW, and provide no attribution to any specific execution context details, thus, sensitive process data. Based on that, A) paranoid knob is required for a PMU if it can capture data from group 2)B) paranoid knob limits scope of capturing sensitive data:   -3 - *scope* is defined by some high level setting   -2 - disabled - no allowed *scope*   -1 - no restrictions - max *scope*    0 - system wide    1 - process user and kernel space    2 - process user space onlyC) paranoid knob has to be checked every time the PMU is going to start   capturing sensitive data to avoid capturing beyond the allowed scope. PMU *access* semantics is derived from fs ACLs and could look like this:r - read PMU architectural and configuration details, read PMU *access* settingsw - modify PMU *access* settingsx - modify PMU configuration and collect data So levels of *access* to PMU could look like this:root=rwx, {PMU} users=r-x, other=r--.Possible examples of *scope* control settings could look like this:1) system wide user+kernel mode CPU sampling with context switches   and uncore counting: Please share more thought so that it eventually could go into Documentation/admin-guide/perf-security.rst.",technical
,
I missed this and do the wrong thing. I'm really sorry for this.,technical
,
This patch doesn't apply to cryptodev because the bug has already been fixed by another patch.,technical
,
"While I agree that it is polling anyway, this change can add significant burden when debugging and trace is enabled for cpu idle, if idle state 0is used often. For example: Phoronix dbench test, 96 clients: 900 second trace:Kernel 4.20-rc1:idle state 0 entry exits: 686,724 Does trace being enabled effect the system under test: Yes. Kernel 4.20-rc1 with this patch reverted: idle state 0 entry exits: 66,185Does trace being enabled effect the system under test: No, or minimal....",technical
,
"Thank you for your patch If my understanding was correct, the chance to use BUSIFx is when TDM split mode. And this patch selects it on runtime (= hw param) ?But, I think we can/should select it on probe timing from DT connection. Am I misunderstanding ? I'm not sure how to select, but adding new ssiuX0 - ssiuX7 is realistic idea (parse sound card is not realistic...) ?If so, your rxu/txu DMA can be more simple ?",technical
,
"Yes, only when SSI works in Split/Ex-Split mode, BUSIFx other than 0 is necessary Because, in order to automatically determine BUSIF number, information like SSI mode (non-Split/Split/Ex-Split), runtime channel, are required(in our internal implementation, SSI mode is selected by kctrl)because of this, in this patch, BUSIF is selected on runtime with the above reasoning, BUSIF is selected on runtime. What do you think? ",technical
,
"Thank you for your feedback(snip)I have no objection that you are customizing your kernel locally. But, upstreaming kernel based on it is not acceptable for me. I'm not sure detail of your local implementation, but I don't think we need to select SSI mode by kctrl. If my understanding was correct, it can also be selected automatically somehow. Or, am I misunderstanding ?I could understand what you want to do, and yes, I can agree that we want/need to have it on upstream. Thank you very much to indicating it to me. But we need to consider more how to implement it. Especially, it is related to DT bindings. As you already know, if it is implemented on upstream kernel, we need to keep compatibility in the future, and it is very difficult. So, my opinions for BUSIFn support are	- SSI mode should be selected automatically	- BUSIFn connection should be selected on DT	  (I think we don't want random sound output position ?)	  - To select it, we need to have new ""ssiu"" DT settings,	    or parse sound card. Maybe adding ssiu is realistic.",technical
,
"Thanks for your comments SSI can work in following modes1. Basic Mode: (channel 1, 2, 4, 6, 8, 16)2. TDM Extended Mode: (channel 6, 8)3. TDM Split Mode: (channel 1, 2)4. TDM Ex-Split mode: (Channel 2, 4, 6, 8, 10) for example user asks dai-link0 to playback 2ch audio stream, driver can't determine which mode to work, as it can be Basic mode ,Split mode or Ex-Split mode. Yes, I agree with you, upstream need to consider lots of things can you give me your idea, how to automatically determine working mode, when user plays 2 channel stream on playback dai-link since which BUSIFx is used during audio data transfer, is not consideration of user, I think your previous suggestion, (automatically select BUSIFx) makes more sense ",technical
,
" If my understanding was correct, we can do like this If DT indicated sound card has dai-link x N, tdm-slots = <M>,	If (N, M) = (1, 2) : Basic mode	If (N, M) = (1, >2): TDM mode	If (N, M) = (2, 4) : TDM Split mode	If (N, M) = (2, >4): TDM Ex-Split mode	If (N, M) = (>2, 8): TDM Split mode	...Maybe some combination was wrong, but we can do something like this ?Why do we need to use Basic mode if HW has TDM Split mode connection? If user playbacks 2ch audio in such situation, we can use TDM Split mode (= only 2ch has sound, other channel has no sound ?) user might start to playback for other channels. I'm not sure how it works...I'm not yet sure detail, but in your idea, does it mean,BUSIFx connection might be exchanged runtime ?I think BUSIFx connection shouldn't exchanged runtime IMO. Otherwise, sound position can't be fixed, and user can't control sound, I think...",technical
,
"Thanks for your comment The idea to consider tdm slot when determine SSI mode makes sense to me, by checking runtime channel and tdm slots combination, I think SSI mode can be automatically selected like following: it shouldn't be changed during runtime, my idea is BUSIFx can be automatically selected when corresponding dai-link is not active The reason I added rsnd ssi select busif(io, chan) in rsnd hw params()in patch ASoC: rsnd: add busif property to dai stream of v2 patch-set,is because runtime channel is necessary information to determine which BUSIFx to select,(which is mentioned in above) and at this stage (rsnd hw params()), all other control settings(register setting, dma address calculation etc) haven't been done, so corresponding dai-link can be considered to be not active at this timing but maybe you have better suggestion when to automatically select BUSIFx What is your opinion? ",technical
,
"Thanks for your feedback Sorry, but I couldn't understand what this table means ? For example, what does ""1ch"" mean ?It looks like ""1ch playback by TDM""...My image is like this.	",technical
,
"Thanks for your feedback sorry for vague explanation, by ""1ch"" I mean runtime 1 channel playback for example: if user plays 1 channel stream, by checking tdm slots value, if it is < 4, then it can't be working in Split mode, so driver will automatically set SSI to work in Basic mode, otherwise, SSI will work in TDM Split mode. After re-think about BUSIFx selection, I agree with you, it shouldn't be random, and select it via device-tree, as you have already demonstrated is a good idea Based on our discussion so far, I think we both agree on the following points1. Driver select SSI mode automatically (by checking tdm slots, runtime channel, etc)2. Driver parse BUSIFx for each dai-link from device-tree I will review my v2 patch-set, drop changes violate to above two points ",technical
,
"Hmm... ??Maybe, we are misunderstanding each other...In my understanding, if platform can use TDM 8ch Split mode, user interface will be for example ser can playback like this sound will be converted to 2ch by alsalib, and daiX will receive converted 2ch sound. SSI always playbacks it as part of TDM 8ch Split mode. In this platform, it can handle stereo sound only on each daiX, and always works as TDM Split mode, never works as Basic Mode / TDM Ex-Split mode. If DAI was dai0, dai1 only, it will be TDM Ex-Split mode. It depends on tdm-slots, I think. If tdm-slots was  something like this, is my understanding. Thank you for understanding my idea. Nice to know ",technical
,
"I believe this hasn't addressed my questions in  Namely ""It is the more general idea that I am not really sure about. First of all. Does it make  any  sense to randomize 4MB blocks by default? Why cannot we simply have it disabled? Then and more concerning question is, does it even make sense to have this randomization applied to higher orders than 0? Attacker might fragment the memory and keep recycling the lowest order and get the predictable behavior that we have right now.",technical
,
This is the biggest portion of the series and I am wondering why do we need it at all. Why it isn't sufficient to rely on the patch 3 here? Pages freed from the bootmem allocator go via the same path so they might be shuffled at that time. Or is there any problem with that? Not enough entropy at the time when this is called or the final result is s not randomized enough (some numbers would be helpful).,technical
,
"I'm not aware of any CVE that this would directly preclude, but that said the entropy injected at 4MB boundaries raises the bar on heap attacks. Environments that want more can adjust that with the boot parameter. Given the potential benefits I think it would only make sense to default disable it if there was a significant runtime impact, from what I have seen there isn't. Certainly I expect there are attacks that can operate within a 4MB window, as I expect there are attacks that could operate within a 4K window that would need sub-page randomization to deter. In fact I believe that is the motivation for CONFIG SLAB FREELIST RANDOM.Combining that with page allocator randomization makes the kernel less predictable.Is that enough justification for this patch on its own? It's debatable. Combine that though with the wider availability of platforms with memory-side-cache and I think it's a reasonable default behavior for the kernel to deploy.",technical
,
"In fact we started with only patch3 and it had no measurable impact on the cache conflict rate. So the reason front-back randomization is not enough is due to the in-order initial freeing of pages. At the start of that process putting page1 in front or behind page0 still keeps them close together, page2 is still near page1 and has a high chance of being adjacent. As more pages are added ordering diversity improves, but there is still high page locality for the low address pages and this leads to no significant impact to the cache conflict rate. Patch3 is enough to keep the entropy sustained over time, but it's not enough initially.",technical
,
Does the above address your concerns? v4.20 is perhaps the last upstream kernel release in advance of wider hardware availability.,technical
,
That should be in the changelog IMHO. ,technical
,
"I am sorry but this hasn't explained anything (at least to me). I can still see a way to bypass this randomization by fragmenting the memory. With that possibility in place this doesn't really provide the promised additional security. So either I am missing something or the per-order threshold is simply a wrong interface to a broken security misfeature. I do not think so from what I have heard so far.OK, this sounds a bit more interesting. I am going to speculate because memory-side-cache is way too generic of a term for me to imagine anything specific. Many years back while at a university I was playing with page coloring as a method to reach a more stable performance results due to reduced cache conflicts. It was not always a performance gain but it definitely allowed for more stable run-to-run comparable results. I can imagine that a randomization might lead to a similar effect although I am not sure how much and it would be more interesting to hear about that effect. If this is really the case then I would assume on/off knob to control the randomization without something as specific as order. ",technical
,
"I think a similar argument can be made against CONFIG SLAB FREELIST RANDOM the randomization benefits can be defeated with more effort, and more effort is the entire point. I'm missing what bar you are judging the criteria for these patches, my bar is increased protection against allocation ordering attacks as seconded by Kees, and the memory side caching effects. That said I don't have a known CVE in my mind that would be mitigated by 4MB page shuffling. No need to imagine, a memory side cache shipped on a previous product as Robert linked in his comments. Cache coloring is effective up until your workload no longer fits in that color. Randomization helps to attenuate the cache conflict rate when that happens. For workloads that may fit in the cache, and/or environments that need more explicit cache control we have the recent changes to numa emulation [1] to arrange for cache sized numa nodes. Are we only debating the enabling knob at this point? I'm not opposed to changing that, but I do think we want to keep the rest of the infrastructure to allow for shuffling on a variable page size boundary in case there is enhanced security benefits at smaller buddy-page sizes.",technical
,
"Fair enough, I'll fold that in when I rebase on top of -next.",technical
,
"If there is relatively simple way to achieve that (which I dunno about the slab free list randomization because I am not familiar with the implementation) then the feature is indeed questionable. I would understand an argument about feasibility if bypassing was extremely hard but fragmenting the memory is relatively a simple task. As said above, if it is quite easy to bypass the randomization then calling and advertizing this as a security feature is a dubious. Not enough to outright nak it of course but also not something I would put my stamp on. And arguments would be much more solid if they were backed by some numbers (not only for the security aspect but also the side caching effects).Could you make this a part of the changelog? I would really appreciate  to see justification based on actual numbers rather than quite hand wavy ""it helps"". Yes, that was my observation back then more or less. But even when you do not fit into the cache a color aware strategy (I was playing with bin hoping as well) produced a more deterministic/stable results. But that is just a side note as it doesn't directly relate to your change. I can imagine that. Do we have any numbers to actually back that claim though? Could you point me to some more documentation. My google-fu is failing me and ""5.2.27.5 Memory Side Cache Information Structure"" doesn't point to anything official (except for your patch referencing it).I am still trying to understand the benefit of this change. If the caching effects are actually the most important part and there is a reasonable cut in allocation order to keep the randomization effective during the runtime then I would like to understand the thinking behind that. In other words does the randomization at smaller orders than biggest order still visible in actual benchmarks? If not then on/off knob should be sufficient with potential auto tuning based on actual HW rather than to expect poor admin to google for RANDOM ORDER to use on a specific HW and all the potential cargo cult that will grow around it. As I've said before, I am not convinced about the security argument but even if I am wrong here then I am still quite sure that you do not want to expose the security aspect as ""chose an order to randomize from ""because admins will have no real way to know what is the RANDOM ORDER to set. So even then it should be on/off thing. You are going to pay some of the performance because you would lose some page allocator optimizations (e.g. pcp lists) but that is unavoidable AFAICS. With all that being said, I think the overall idea makes sense but you should try much harder to explain  why  we need it and back your justification by actual  data  before I would consider my ack. ",technical
,
"In fact you don't even need to fragment since you'll have 4MB contiguous targets by default, but that's not the point. We'll now have more entropy in the allocation order to compliment the entropy introduced at the per-SLAB level with CONFIG SLAB FREELIST RANDOM....and now that I've made that argument I think I've come around to your point about the shuffle page order parameter. The only entity that might have a better clue about ""safer"" shuffle orders than MAX ORDER is the distribution provider. I'll cut a v4 to move all of this under a configuration symbol and make the shuffle order a compile time setting. I put in the changelog that these patches reduced the cache conflict rate by 2.5X on a Java benchmark. I specifically did not put KNL data directly into the changelog because that is not a general purpose server platform. Note, you can also think about this just on pure architecture terms.I.e. that for a direct mapped cache anywhere in a system you can have a near zero cache conflict rate on a first run of a workload and high conflict rate on a second run based on how lucky you are with memory allocation placement relative to the first run. Randomization keeps you out of such performance troughs and provides more reliable average performance.  With the numa emulation patch I referenced an administrator could constrain a workload to run in a cache-sized subset of the available memory if they really know what they are doing and need firmer guarantees. The risk if Linux does not have this capability is unstable hacks like zone sort and rebooting, as referenced in that KNL article, which are not suitable for a general purpose kernel / platform. Yes, 2.5X cache conflict rate reduction, in the change log. So, I've come around to your viewpoint on this. Especially when we have CONFIG SLAB FREELIST RANDOM the security benefit of smaller than MAX ORDER shuffling is hard to justify and likely does not need kernel parameter based control. I don't have a known CVE, I only have the ack of people more knowledgeable about security than myself like Kees to say in effect, ""yes, this complicates attacks"". If you won't take Kees' word for it, I'm not sure what other justification I can present on the security aspect.2.5X cache conflict reduction on a Java benchmark workload that the exceeds the cache size by multiple factors is the data I can provide today. Post launch it becomes easier to share more precise data, but that's post 4.20. The hope of course is to have this capability available in an upstream released kernel in advance of wider hardware availability.",technical
,
"And how is somebody providing a kernel for large variety of workloads supposed to know?[...]I am not disagreeing here. That reliable average might be worse than what you get with the non-randomized case. And that might be a fair deal for some workloads. You are, however, providing a functionality which is enabled by default without any actual numbers (well except for a java  workload that seems to benefit) so you should really do your homework stop hand waving and give us some numbers and/or convincing arguments please. Then mention how and what you can achieve by that in the changelog. We could have lived without those for quite some time so this doesn't seem to be anything super urgent to push through without a proper justification. Which is a single benchmark result which is not even described in detail to be able to reproduce that measurement. I am sorry for nagging here but I would expect something less obscure. How does this behave for usual workloads that we test cache sensitive workloads. I myself am not a benchmark person but I am pretty sure there are people who can help  you to find proper ones to run and evaluate. Thanks! In general (nothing against Kees here of course), I prefer a stronger justification than ""somebody said it will make attacks harder"". At least my concern about fragmented memory which is not really hard to achieve at all should be reasonably clarified. I am fully aware there is no absolute measure here but making something harder under ideal conditions doesn't really help for common attack strategies which can prepare the system into an actual state to exploit allocation predictability. I amno expert here but if an attacker can deduce the allocation pattern then fragmenting the memory is one easy step to overcome what people would consider a security measure. So color me unconvinced for now. I will not comment on timing but in general, any performance related changes should come with numbers for a wider variety of workloads. In any case, I believe the change itself is not controversial as long it is opt-in (potentially autotuned based on specific HW) with a reasonable API. And no I do not consider RANDOM ORDER a good interface. ",technical
,
"True, this would be a much easier discussion with a wider / deeper data set. The latest version of the patches no longer enable it by default. I'm giving you the data I can give with respect to pre-production hardware.The numa emulation aspect is orthogonal to the randomization implementation. It does not belong in the randomization changelog. We lived without them previously because memory-side-caches were limited to niche hardware, now this is moving into general purpose server platforms and the urgency / impact goes up accordingly. No need to apologize. I wouldn't pick benchmarks that are cpu-cache sensitive since those are small number of MBs in size, a memory-side cache is on the order of 10s of GBs.Another way to attack heap randomization without fragmentation is to just perform heap spraying and hope that lands the data the attacker needs in the right place. I still think that allocation entropy > 0 is positive benefit, but I don't know how to determine the curve of security benefit relative to shuffle order. That's fair. Do you mean disable shuffling on systems that don't have a memory-side-cache unless / until we can devise a security benefit curve relative to shuffle-order? The former I can do, the latter, I'm  at a loss. I think the current v4 proposal of compile-time setting is reasonable once we have consensus / guidance on the default shuffle-order.",technical
,
"Yes, enable when the HW requires that for whatever reason and make add a global knob to enable it for those that might find it useful for security reasons with a clear cost/benefit description. Not ""this is the security thingy enable and feel safe(r)"" ",technical
,
"Hello, Can anyone please confirm this bug and apply the patch? Thanks!",technical
,
"I assume that this co-processor only deals with the routing itself, and doesn't need to be talked to during interrupt processing, right? I don't really see the point of making this user-selectable. If you're compiling support for a given platform, this platform configuration fragment should itself select the necessary dependencies for the system to work as expected. Here, you are leaving the choice to the user, with a 50% chance of getting a system that doesn't boot...nit: s/(HWIRQ)/(hwirq)/g Oh great. So this is reinventing the GICv3 ITS, only for SPIs. :-(Now, this structure seems completely useless, see below. Maybe it would make sense to have a macro that hides this:      	       *hwirq = FWSPEC TO HWIRQ(fwspec);This looks horrible. Why doesn't your firmware interface have a helper functions that hides this? Something like:	;and you could even add some error checking. And put this where it belongs (in the helper function).I don't think this structure serves any purpose. src id and src index are just a decomposition of hwirq. dst irq is the GIC interrupt, which is stored... by the GIC driver. Also, it is worth realising that you're allocating per-interrupt data, but none of the per-interrupt callbacks are using it. In my book, that's a sure sign that this structure is pointless. Am I missing anything here? Same remarks about the horrible interface. Please address this. But it also worth realising that this code will never be called with nr irqs!=1 (that's only for things like PCIMulti-MSI).Do you expect other drivers to require similar resource request? If so, It might be worth getting the firmware interface to do that work. Specially the ""give me my SCI"" part.",technical
,
"I would drop the GIC here, and replace it by ""parent interrupt controller"", as nothing here is GIC specific. Are all trigger types supported? Why that constraint? From what I can see, the two are fairly independent, and the constraint looks more of a Linux driver issue than a DT constraint.",technical
,
"Rob, DT maintainers, I'd like a feedback from DT maintainers on this 'range' topic.TISCI Firmware [1] currently seems to define a type corresponding to a device ID[2]. in AM6 device, for example, this is different, however have a 1 to 1 correspondence. However, there is expectation that type will end up as device ID in a future SoC.While this is subject to much debate internally, I'd like some feedback if this is OK from Device tree representation - it is true that Firmware does look at it as type, however in some future SoC, it could be that the values themselves may correspond one to one with a device id -> The original wish was that types might be something reusable across SoCs,but that is turning out to be more of a theoretical wish than any thing practical.",technical
,
"Nope, only level interrupts are supported. Will fix it in v2.Driver when calling irq domain alloc irqs parent(), the fwspec node that gets passed assumes that parent is gic. parameters are filled in with such assumption. Do you suggest anything to make it more generic? Thanks a lot for the review. Also, I need a suggestion regarding one more interrupt controller(Interrupt Aggregator) on the same SoC controlled by TISCI PROTOCOL.The Interrupt Aggregator (INTA) provides a centralized machine which handles the termination of system events to that they can be coherently processed by the host(s) in the system. Integration looks something similar  .Configuration of the Int map registers that maps global events to vint is done by a system controller (like the Device Memory and Security Controller on K3AM654 SoC). Driver should request the system controller to get the range of global events and vints assigned to the requesting host. Management of these requested resources should be handled by driver and requests system controller to map specific global event to vint, bit pair. There can be cases such that IRQ routes can involve both INTR and INTA like below:	IP ---> INTA ---> INTR ----> GIC.In these cases TISCI involves only one message with parameters (source id, source offset, inta id, dst id) for configuring IRQ route till the destination. Coprocessor will detect there is INTR in the IRQ path and configure that as well. Right now I kind of differentiated this scenario in INTA driver by passing a flag(TI SCI EVENT) to INTR driver. If such flag comes, INTR driver should avoid calling ti sci api for configuring. Do you think this is the right direction or do you suggest a better solution. If I am not clear in the above description, I can post an RFC for INTA driver for continuing this discussion.",technical
,
"Yes, that's right. There are 2 reasons why I made it tristate:- Not all interrupts go through this irqchip(At least in the AM6 SoC using this). Most of the legacy peripherals still are directly connected to GIC- TI SCI PROTOCOL is defined as tristate. If you still feel I should not make it user-selectable, I can drop it.okay.okay.All existing TISCI users follow the same convention, so I did not bother adding any such wrapper. Will update TISCI with these wrappers and see what firmware maintainer says.hmm..you are right, these 3 fields can be dropped completely. Will fix it in v2.I tried to consolidate sci resource part under devm ti sci get of resource() api but dst-id is something that is used by irqchip driver. So couldn't consolidate it and had to get it from dt in the driver probe.",technical
,
"But as you said, these are ""legacy"" interrupts, and most of the interesting stuff is routed through the system controller. We also try not to have core interrupt controllers as modules. As for having the firmware interface as a module, I wonder what the use-case is. I really wonder what the added value is for the user. Frankly, exposing all kind of data structures to the world is a pretty poor form of abstraction, which is what the firmware is supposed to provide. I'd strongly suggest that include/linux/soc/ti/ti sci protocol.h gets cleaned up, and that the whole ti sci ops disappears from the that file. Nobody outside of the firmware *implementation* needs to know about its, and it would be much better served by a set of helpers. Finally, please make the TISCI interrupt management part of this series, so that I can review it as part of the code that uses it.",technical
,
"As I said, that's a Linux driver issue, not a DT specification at all. It is not worth it trying to generalize it in the driver implementation, but the DT spec it self should be generic enough. I'm sorry, but I really have no idea what the global events and the vints are. Maybe you should describe what this is all about, and maybe provide a pointer to some documentation...Frankly, it mostly indicates that the firmware does too much, and should be more flexible. That'd be preferable, IMO. Please provide definitions for all the above jargon, as well as pointers to publicly available documentation, if any.",technical
,
"okay, will not make it use configurable in v2.Sure, my next version will include TISCI interrupt management as well. Thanks and regards, Lokesh",technical
,
"okay, will fix it in next version. Sorry I should have done that earlier. TRM is available here[1], Section 9.3 talks about Interrupt Router, Section 10.2.7 talks about Interrupt aggregator. Documentation for TISCI IRQ management is available here Sure, will try to post the consolidated series asap. Thanks a lot for the help.",technical
,
" DT maintainers, Any help on this topic?",technical
,
"I'm not sure I follow all the terminology here of type, subtype, ""dsthost irq"", etc. It looks to me like you should be using interrupt-map property. Rob",technical
,
"I agree with you in principle Peter and have tweaked the patch description to make it clearer that we are doing this to make GCC static analysis more helpful (suppressing a false warning is a worthwhile if you are dealing with lots of them).However, nice though it is to have elegant comment structure I think we should still have this patch in place.  This effort to 'fix' these warnings has already identified a few places where it was wrong so I'm keen to see it applied by default even if it isn't perfect.",technical
,
"Thanks. Below are some examples of cases in which the fall-through warning turned out to be an actual bug So, yeah. This effort is worth it.",technical
,
Done the first of the above…,technical
,
"Indeed. I meant to respond earlier, but then forgot... Thank you!",technical
,
"x86 64 is our case, I should have documented it more clearly. Right, as you said, this is easy to fix. I found recent discussion about why x86-64 is using generic string function here:",technical
,
"Repeating my comment on version 1:My understanding of the concern behind this change is that we should be able to use an email address for the current development practices, such as Reported-by, Suggested-by, etc tags when the email address was provided in what is a public space for the project.  The public space is visible to anyone in the world who desires to access  do not understand how ""ordinarily collected by the project"" is equivalent to ""an email address that was provided in a public space for the project"". Ordinarily collected could include activities that can be expected to be private and not visible to any arbitrary person in the world. My issue is with the word choice.  I agree with the underlying concept.",technical
,
"I don't think it is ... or should be.  This section is specifically enumerating unacceptable behaviours.  The carve out ""email address not ordinarily collected by the project"" means that adding someone's email address in a tag isn't immediately sanctionable in the code of conduct as unacceptable behaviour if a question about whether you asked explicit permission arises.  Equally, a carve out from unacceptable behaviours doesn't make the action always acceptable, so it's not a licence to publish someone's email address regardless of context. It's not a blanket permission, it's an exclusion from being considered unacceptable behaviour.  I would be interested to know what information we ordinarily collect in the course of building linux that should be considered private because I might have missed something about the implications here.",technical
,
Acked-by : Shuah Khan,technical
,
"does that include bugzilla.kernel.org, or should we think of those email addresses (of bug submitters) as private?  They look public to me.",technical
,
"No, that's not my desired goal.   The section is not about giving permission it's about making sure listed unacceptable behaviours don't overlap what we normally do.  The goal is to exclude email the project ordinarily collects from immediate sanction under the unacceptable behaviours clause.  I deliberately didn't add anything about permission because that's up to the project to define in its more standard contribution documents. I agree, but, as I said, my goal wasn't to provide explicit permission(because the list is too long and too dependent on the way the project operates) it was to carve out an exclusion from sanction for stuff the kernel normally does.  The carve out doesn't translate into explicit permission because the project can define other standards for the way email addresses are added to the tags. I think the crux of the disagreement is that you think the carve out equates to a permission which is not specific enough and I think it doesn't equate to a permission at all, which is why there's no need to make it more explicit.  Is that a fair characterisation?",technical
,
"Hello, What about properly formatted patches (with From and SoB) sent to them maintainer, without copying any mailing lists? To me, a patch sent to a maintainer is obviously sent for inclusion in the kernel.",technical
,
"OK.  I am fine with the goal of wording that excludes certain things from unacceptable behavior instead providing permissions for certain things.  I think me phrasing as permission instead of carve out is creating a lot of the miscommunication. Please re-read my comments, but in every place where I state things in a way of providing permissions, re-state it in your mind as the same sentence  except  phrased as excluding from unacceptable behavior.  (I started to do that explicitly, but it looked like I was just going to create a whole lot of distracting text.)Nope.  That is a big place where I was not transferring my thoughts to clear communication.  I agree that what I wrote should have been written in terms of carve out instead of permission. Nope.  My concern is ""which email addresses"".",technical
,
"[...]The idea here was because it's a carve out that doesn't give permission and because the permission is ruled by the project contribution documents, the carve out should be broad enough to cover anything they might say hence ""email addresses not ordinarily collected by the project"" are still included as unacceptable behaviour. Perhaps if you propose the wording you'd like to see it would help because there still looks to be some subtlety I'm not getting.",technical
,
"This ends up reading like so:----Examples of unacceptable behavior by participants include:...* Publishing others ™ private information, such as a physical or electronic address that has been provided in a public space for the project, without explicit permission.----I think that in context, you want a 'not' in there.  That is: unacceptable behavior includes publishing others' private information... that has *not*been provided in a public space.  So, I think the suggested text needs some fixing, IMHO.I looked at this issue upstream, and decided to leave the wording in the CoC itself alone - favoring instead to add a clarifying addition to the upstream CoC FAQ, about some email addresses not being private information. The reason I took that approach, rather than try to change the wording inside the CoC, is that the current wording seems to me to be sufficient. The thing that is unacceptable is publishing private information.  The ""such as..."" clause is intended to convey examples of the types of thing that might usually be considered private information.  But it is not exhaustive, nor is it necessarily correct, depending on the circumstances.  In particular, email addresses are sometimes private information and sometimes not. In the context of kernel development, many email addresses are not private. I am sympathetic to the argument that we use emails as public information so much in kernel development processes, that it makes sense to omit this or qualify it more. My own views are that:1) if we change this line at all, we should simply omit the ""such as..."" part of the phrase, and leave it at:* Publishing others ™ private information without explicit permission but also2) I'm OK with leaving the phrase as is and handling the concerns in an clarifying document. Just my 2 cents. --",technical
,
"You beat me to this one.  However, there is another issue that I did touch on but perhaps not in this sub thread: For those of us who live in the US, our addresses (that's physical and sometimes email) are actually provided in a public space because they're available in the public property records.  That's actually why I chose ""not ordinarily collected by the project"" as opposed to ""not previously provided in the public space"" or an equivalent because doxxing in the US is mostly finding this information from public sources and broadcasting  think that's the sense of the people who acked this, yes.  Personally I'm happy with a separate clarification in another document, but I can also see the argument that we do need our single CoC to be consistent with our operational method, which is why I proposed the patch. This looks OK to me too ... the problem with the original is that the additional qualification overlaps our normal project method of operation, this solves the issue as well.",technical
,
"Yes, thank you. That clarification helps a  lot  in understanding what you have said previously in this thread.  Thanks.  :-) Looks good to me.",technical
,
"James, and our other friends, More than one ambiguity. This whole file needs to go. Who decides what is trolling, and what is a technique for raising awareness or sparking discussion on an issue? Why should this last bit remain?  Any literate person with access to a dictionary should know how ambiguous the word professional is.  As an amateur contributor to the FOSS ecosystem I am more than a bit offended by the decision to use such divisive, politically charged, and financially discriminatory language in a project of such massive technical importance.  This entire file should be expunged from the repository and replaced by well defined minimalistic guidelines for maintaining order on the mailing lists, rather than a set of ambiguous codes that force maintainers to take politically motivated actions against contributors for undefined reasons. Using words like professional is a distressing red flag because it doesn't add any clarification on the issue (what was the issue again?), it only raises more questions.  I can't think of any reason that word would be needed unless you're trying to push out unpaid contributors.  Why should someone's employment status be held against them when contributing ideas or code to a technical project that has benefited greatly from amateur contributions? I fear for the kernels future now that irrational politics are beginning to creep.",technical
,
"No any reason I know of.  It must be just an old convention. Yes, please.",technical
,
These kind of issues are usually fixed by fixing the network driver's shutdown routine to ensure that MSI interrupts are cleared there.,technical
,
"I'm not sure shutdown handlers for drivers are called in panic exec (I remember of an old experiment I did, loading a kernel with ""kexec -p"" didn't trigger the handlers).But this case is even worse, because the NICs were in PCI passthrough mode, using vfio. So, they were completely unaware of what happened in the host kernel. Also, this is spec compliant - system reset events should guarantee the bits are cleared (although kexec is not exactly a system reset, it's similar)",technical
,
"AFAIK, all shutdown (not remove) routines are called before launching the next kernel even in crash scenario. It is not safe to start the new kernel while hardware is doing a DMA to the system memory and triggering interrupts. Shutdown routine in PCI core used to disable MSI/MSI-x on behalf of all endpoints but it was later decided that this is the responsibility of the endpoint driver. ",technical
,
"I don't want to expand the early quirk infrastructure unless there is absolutely no other way to solve this.  The early quirk stuff isx86-specific, and it's not obvious that this problem is x86-only.This patch scans buses 0-255, but still only in domain 0, so it won't help with even more complicated systems that use other domains. I'm not an IRQ expert, but it seems wrong to me that we are enabling this interrupt before we're ready for it.  The MSI should target anIOAPIC.  Can't that IOAPIC entry be masked until later?  I guess the kdump kernel doesn't know what MSI address the device might be using. Could the IRQ core be more tolerant of this somehow, e.g., if it notices incoming interrupts with no handler, could it disable the IOAPIC entry and fall back to polling periodically until a handler is added?",technical
,
"I agree with you, it's definitely not safe to start a new kernel within-flight DMA transactions, but in the crash scenario I think the rationale was that running kernel is broken so it's even more unreliable to try gracefully shutdown the devices than hope-for-the-best and start the kdump kernel right away heheh Fact is that the shutdown handlers are not called in the crash scenario. They come from device shutdown(), the code paths are as follow. To validate this, one can load a kernel with ""initcall debug"" parameter, and performs a kexec - if the shutdown handlers are called, there's adev info() call that shows a message per device. This may be a good idea, using the pci layer to disable MSIs in the quiesce path of the broken kernel. I'll follow-up this discussion in Bjorn's reply.",technical
,
"thanks for your quick reply. I understand your point, but I think this is inherently an architecture problem. No matter what solution we decide for, it'll need to be applied in early boot time, like before the PCI layer gets initialized.So, I think a first step would be to split the solution ""timing"" in 2 possibilities: a) We could try to disable MSIs or whatever approach we take in the quiesce path of crash kexec(), before the bootstrap of the kdump kernel. The pro is we could use PCI handlers to do it generically. The con is it'd touch that delicate shutdown path, from a broken kernel, and this is unreliable. Also, I've noticed changes in those crash paths usually gain huge amount of criticism by community, seems nobody wants to change a bit of this code, if not utterly necessary.b) Continue using an early boot approach. IMO, this would be per-arch by nature. Currently, powerpc for example does not suffer this issue due to their arch code performing a FW-aided PCI fundamental reset in the devices[0].On the other hand, x86 has no generic fundamental reset infrastructure to my knowledge (we tried some alternatives, like a Bridge reset[1] that didn't work, or zeroing the the command register, which worked), but if we go with the IOAPIC way of handling this (which we tried a bit and failed too), it'll be even more arch-dependent, since IOAPIC is x86 concept. After discussing here internally, an alternative way for this MSI approach work without requiring the change in the early PCI infrastructure is to check if we're in kdump kernel and perform manually the full scan in that case, instead of changing the generic case as proposed here. This would still be x86-only, but again, it's difficult if not impossible to fix all archs using the same code here. Finally, about multi-domain PCI topologies, I've never saw it on x86, I wasn't aware that such things existed in x86 - but if required we can quickly extend the logic to contemplate it too. Thanks again, looking , adapted to work in early boot time.",technical
,
"Thank you for the review.  Basically we use these prints to get a notification when a system is having thermal issues.  It's easy to look in dmesg and see the prints and know that something temperature related is going on. However, I agree that the current solution is a bit hacky, and in looking at it a bit further we don't even cover all the paths that we need to.  The processor set cur state()  function in drivers/acpi/processor thermal.c, for example, is used on the x86 64systems I'm testing with and wasn't augmented with prints. I'm going to take a step back and try and find another solution.  The info you added to sysfs looks very promising, thank you for pointing it out.",technical
,
"Thank you for the patch. Nonetheless, I've just applied similar Liviu's patch [0], since it arrived one week ago already. I'll send it upstream with LED fixes for 4-20-rc2.",technical
,
"If people are hitting it this often, maybe it is time to push it early? Linus should not have problem taking two pull requests in a merge window.",technical
,
"Thank you for your patch! It would be much better if you can send it using traditional tools,i.e. `git send-email ...`.",technical
,
"Hi, It's up to Rob of course, but IMO it seems a nicer way forward to include both the SoC-specific string and the ""version"" string in all cases.  I'd write this for the full text: - compatible.  NOTE that some old device tree files may be floating around that only   have the string ""qcom,sdhci-msm-v4"" without the SoC compatible string   but doing that should be considered a deprecated practice.",technical
,
Fine by me if you update all the dts files. I assume you meant to append '-sdhci' here?,technical
,
"HI, Done and done.",technical
,
Thank you. Will update the documentation.,technical
,
"Assuming this is for arm64, I'm somewhat surprised that memset() could be that much faster than clear page(), since they should effectively amount to the same thing (a DC ZVA loop). What hardware is this on? Profiling to try and see exactly where the extra time goes would be interesting too. Or just mask it out in   iommu dma alloc pages()?What if the pages came from highmem? I know that doesn't happen on arm64 today, but the point of this code *is* to be generic, and other users will arrive eventually.",technical
,
"I am running with tegra 186-p2771-0000.dtb so it's arm64 yes. I re-ran the test to get some accuracy within the function and got. Note: new memset takes about 164 usec, resulting in 400 usec diff      for the entire iommu dma alloc() function call. It looks like this might be more than the diff between clear page and memset, and might be related to mapping and cache. Any idea? Yea, the change here would be neater then. Hmm, so it probably should use sg miter start/stop() too? Looking at the flush routine doing in PAGE SIZE for each iteration, would be possible to map and memset contiguous pages together? Actually the flush routine might be also optimized if we can map contiguous pages. ",technical
,
And in what case does dma alloc * performance even matter?,technical
,
"FYI, I have patches I plan to submit soon that gets rid of the struct scatter list use in this code to simplify it:",technical
,
...and I have some significant objections to that simplification which I plan to respond with ;)(namely that it defaults the whole higher-order page allocation business which will have varying degrees of performance impact on certain cases),technical
,
"Hmm, I guess it might not be so much clear page() itself as all the gubbins involved in getting there from prep new page(). I could perhaps make some vague guesses about how the A57 cores might get tickled by the different code patterns, but the Denver cores are well beyond my ability to reason about. Out of even further curiosity, how does the quick hack below compare? I suppose the ideal point at which to do it would be after the remapping when we have the entire buffer contiguous in vmalloc space and can make best use of prefetchers etc. - DMA ATTR NO KERNEL MAPPING is a bit of a spanner in the works, but we could probably accommodate a special case for that. As Christoph points out, this isn't really the place to be looking for performance anyway (unless it's pathologically bad as per the DMA ATTR ALLOC SINGLE PAGES fun), but if we're looking at pulling the remapping out of the arch code, maybe we could aim to rework the zeroing completely as part of that.",technical
,
"Honestly, this was amplified by running a local iommu benchmark test. Practically dma alloc/free() should not be that stressful, but we cannot say the performance doesn't matter at all, right? Though many device drivers pre-allocate memory for DMA usage, it could matter where a driver dynamically allocates and releases. And actually I have a related question for you: I saw that the dma direct alloc() cancels the   GFP ZERO flag and does manual memset() after allocation. Might that be possibly related to a performance concern? Though I don't see any performance keyword for that part of code, especially seems that memset() was there from the beginning.",technical
,
"I tried out that change. And the results are as followings: a. Routine (1) reduced from 422 usec to 55 usecb. Routine (2) increased from 441 usec to 833 usecc. Overall, it seems to remain the same: 900+ usecI would understand the point. So probably it'd be more plausible to have the change if it reflects on some practical benchmark. I might need to re-run some tests with heavier use cases. That'd be nice. I believe it'd be good to have.",technical
,
"Well, please place your objection there.  The behavior does match what every other iommu-based dma ops implementation outside of arm/arm64 does, so there is some precedent for it to say the least.  But if the only current users objects I'll surely find a way to accommodate it, but a good rationale including numbers would be useful to document it.",technical
,
It's better to not have a mixture of nodes at a level with and without unit-addresses. So I'd move all the i2c nodes under an 'i2c-mux' node.,technical
,
"<Top posting for new topic>I'm replying here rather than spam the IRC channel with a big paste. It's also a useful description to the probe sequence, so I've kept it with the driver posting. I hope the following helps illustrate the sequences which are involved So yes sensors are only communicated with once the link is brought up as much as possible. Because the sensors are i2c devices on the i2c mux - they are not probed until their adapters are created and added. At this stage the i2c-mux core framework will iterate all the devices described by the DT for that adapter. As each one is probed - the i2c mux framework will call max9286 i2c mux select() and enable only the single link. This allows us to configure each camera independently (which is essential because they are all configured to the same i2caddress by default at power on) Hope this helps, and feel free to ask if you have any more questions.",technical
,
"thanks for the clarification. One additional note below. For the records, an additional bit of explanation I got from Kieran via IRC.The fact that link is already up when the sensors are probed is due to the fact that the power regulator has a delay of *8 seconds*. This is intended, because there's an MCU on the camera modules that talks on the I2C bus during that time, and thus the drivers need to wait after it's done. This delay happens before max9286 setup() is called.",technical
,
"Sorry to jump up, but I feel this should be clarified. The 8sec delay is due to the fact an integrated MCU on the remote camera module programs the local sensor and the serializer integrated in the module in to some default configuration state. At power up, we just want to let it finish, with all reverse channels closed(camera module -> SoC direction) not to have the MCU transmitted messages repeated to the local side (our remote serializer does repeat messages not directed to it on it's remote side, as our local deserializer does).The ""link up"" thing is fairly more complicated for GMSL than just having a binary ""on"" or ""off"" mode. This technology defines two different ""channels"", a 'configuration-channel' for transmitting control messages on the serial link (i2c messages for the deserializer/serializer pair this patches support) and a 'video-channel' for transmission of high-speed data, such as, no surprise, video and images :)GMSL also defines two ""link modes"": a clock-less ""configuration link ""and an high-speed ""video link"". The ""configuration link"" is available a few msec after power up (roughly), while the ""video link"" needs a pixel clock to be supplied to the serializer for it to enter this mode and be able to lock the status between itself and the deserializer. Then it can begin serializing video data. The 'control channel' is available both when the link is in 'configuration' and 'video' mode, while the 'video' channel is available only when the link is in 'video' mode (or, to put it more simply: you can send i2c configuration messages while the link is serializing video).Our implementation uses the link in 'configuration mode' during the remote side programming phase, at 'max9286 i2c mux init()' time, with the 'max9286 i2c mux select()' function enabling selectively the 'configuration link' of each single remote end. It probes the remote device by instantiating a new i2c adapter connected to the mux, one for each remote end, and performs the device configuration by initially using its default power up i2c address (it is safe to do so, all other links are closed), then changes the remote devices address to an unique one(as our devices allows us to do so, otherwise you should use the deserializer address translation feature to mask and translate the remote addresses).Now all remote devices have an unique i2c address, and we can operate with all 'configuration links' open with no risk of i2c addresses collisions. At this point when we want to start the video stream, we send a control message to the remote device, which enables the pixel clock output from the image sensor, and activate the 'video channel' on the remote serializer. The local deserializer makes sure all 'video links 'are locked (see 'max9286 check video links()') and at this point we can begin serializing/deserializing video data. As you can see, the initial delay only plays a role in avoiding collision before we properly configure the channels and the i2c addresses. The link setup phase is instead an integral part of the system configuration, and there are no un-necessary delays used to work around it setup procedure. Does this help clarifying the system start-up procedure?",technical
,
"Yes, that's very informative, thank you very much. Given the complexity of the driver and the non-obviousness of some workarounds to ""unfortunate hardware design choices"", I think [some of]this explanation should be committed together with the driver, in order to make it more understandable to other people. Even more since you've already taken time to write it.",technical
,
"All, sorry for joining this late... See below my considerations. I find this kind of address mapping is the weak point in this patchset. The ser-desert chipset splits the world in ""local"" and ""remote"" side. The camera node belongs to the remote side, but the 0x51 and 0x61 addresses belong to the local side. Think about supporting N different main boards and M remote boards. 0x51 might be available on some main boards but not all. IMO under the camera@51 (even the i2c@0) node there should be only remote hardware description. To support the N*M possible combinations, there should be: * a DT for the main board mentioning only addresses for the   local i2c bus, down to the i2c@0 with address-cells, size-cells and   reg properties * a DT overlay for each remote board, mentioning the remote i2c   chips with their physical addresses, but no local addresses The only way I could devise to be generic is to bind each physical remote address to a local address at runtime. Also, to be implemented reliably, an address translation feature is required on the local (de)ser chip. So the question is: can the  chip do i2c address translation?",technical
,
"All, below a few minor questions, and a big one at the bottom.[...]5 pads, 4 formats. Why does the source node have no fmt? , even though using device tree I think this won't matter in the current kernel code. However I think ""max9286->sd.flags =..."" is more correct here, and it's also what most other drivers do. According to the docs MEDIA ENT F VID IF BRIDGE appears more fitting. I can't manage to like this initialization sequence, sorry. If at all possible, each max9286 should initialize itself independently from each other, like any normal driver. First, it requires that each chip on the remote side can configure its own slave address. Not all chips do. Second, using a static i2c address map does not scale well and limits hot plugging, as I discussed in my reply to patch 1/4. The problem should be solvable cleanly if the MAX9286 supports address translation like the TI chips.",technical
,
"I'd say you're on time - not late, Thanks for joining :)Well, in our use case - in fact the camera has a set of fixed addresses for each camera - and these are the addresses we are requesting the camera to be updated to. Once the camera is communicated with - the first step is to reprogram the device to respond to the addresses specified here. Of course - well in fact all of our I2C addresses across our two max9286 instances, and 8 camera devices share the same bus 'address space'. It's crucial to provide this address on a per board level, which is why it is specified in the DT.I wonder if perhaps it was a mistake to include the camera description in this part of the example, as it's not related to the max9286 specifically. Rob has already suggested moving these to a lower 'i2c-node' level which I like the sound of, and might make this separation more clear. Yes, The max9286 (desert) can do i2c address translation - but so too can the max9271 (serialiser)We do our address translation on the camera (serialiser) side. The cameras *all* boot with the same i2c address (and thus all conflict) - We disable all links - We enable /one/ link - We initialise and reprogram the address of that camera to the address   specified in the camera node. - Then we move to the next camera. The reality is we 'just need' a spare address on the I2C bus - but as yet - there is no mechanism in I2C core to request a spare address. Thus it is the responsibility of the DT node to ensure there is no conflict. For an example, here is our DT overlay file for our max9286 expansion board:",technical
,
"Thank you for your review, The source pad is a CSI2 link - so a 'frame format' would be inappropriate. A quick glance looks like you're right. That looks like a good catch! I've updated locally ready for v5.Yes, I agree. We recently updated the adv748x to this too. Also updated locally to add to v5.Yes, I think we're in agreement here, but unfortunately this section is a workaround for the fact that our devices share a common address space. We (currently) *must* disable both devices before we start the initialisation process for either on our platform currently...That said - I think this section needs to be removed from the upstream part at least for now. I think we should probably carry this 'workaround' separately. This part is the core issue that I talked about in my presentation at ALS-Japan [0] don't think we can treat GMSL as hot-pluggable currently ... But as we discussed - I see that we should think about this for FPD-Link Also as a further aside here, we use ""device is bound"" which is not exported, and means that this driver won't compile successfully as a module currently (thanks to the kbuild test robot for highlighting that)",technical
,
"Yes, the way it works is clear. Interesting point. In my case I'm thinking DT overlays, they help me a lot in finding a proper generalization. With some generalization, camera modules [the same would happen with display modules] are similar to beaglebone capes or rpi hats: 1. there can be different camera modules being designed over time 2. there can be different base boards being designed over time 3. there is a standard inter connection between them (mechanical,    electrical, communication bus) 4. camera modules and base boards are designed and sold independently    (thanks to point 3)Overlays are a natural choice in this case. Even bootloader-time overlays will suffice for my reasoning, let's remove the hot plug mess from this discussion. Now, in this patch you are modeling the remote camera as if it were a""normal"" I2C device, except: a) it has 2 slave addresses (no problem with this) b) the 2 slave addresses in DT are not the physical ones With this model it seems natural to write ""camera@51/reg = <0x51 0x61>""in the camera DT overlay. Except 0x51 and 0x61 do not exist on the camera module, those numbers come from the base board, since you know those two addresses are not used on the bus where gmsl-deserializer@2cis. But it works. Then one year later a random SBC vendor starts selling a new base board that has on the same i2c bus a GMSL desert and a random i2c chip, unrelated to cameras, at address 0x51. Bang, the camera sensor does not work anymore, but there is no hardware reason for it not to work. Well, easy to fix, find an address that is unused on all known base boards and replace, say, 0x51->0x71 in the camera overlay. (OK, I violated the ""DT as a stable ABI"" principle) But then other boards appear and, taking this to an extreme, you can get to a situation where every i2c address is used on at least one board. How do you fix that? Maybe this scenario is a bit too apocalyptic, and maybe too much for current automotive uses, but I think it illustrates how the current model is not generic enough. Since there is no existing code in the kernel yet, I think we should strive to do better in order to minimize future problems. My approach is instead to clearly split the local and remote domain. The latter is what could be moved to an overlay. For example: The core difference is that I split the camera@51/reg property in two: * sensor@50/reg: the remote side (camera overlay);   carries the physical i2c address (note both sensors are at 0x50) * serializer@3d/i2c-alias-pool: the local side (base board);   lists a pool of addresses that are not used on the i2c bus See how there is no mixing between local and remote. The pool will differ from one base board to another. To implement this, I developed an ""i2c address translator"" that maps physical remote addresses to local addresses from the pool at runtime. It still needs some work, but address translation it is working. Good!By ""address translation"" I mean the i2c address is changed by some device in the middle between the i2c master and the slave. In this sense you are not doing address translation, you are rather modifying the chip addresses. Then transactions happen with the new (0x51/0x61) address, which does not get modified during subsequent transactions. Not a reliable one, definitely, since there could be i2c devices unknown to the software. This is why I had to introduce the alias pool: the DT writer is required to know which addresses are available and list them in DT",technical
,
"Ok, thanks for the clarification. The model I proposed in my review to patch 1/4 (split remote physical address from local address pool) allows to avoid this workaround. Oh, interesting, I hadn't noticed that you gave this talk -- at the same conference as Vladimir's talk! No video recording apparently, but are slides available at least? I've been mixing hot plug and DT overlays and that generated confusion, sorry. My point exists even with no hot plug, see the reply to patch 1/4.",technical
,
"I'm happy to see this will be well maintained. :-)A part of this driver looks like a driver for an OV camera sensor. Would there be something that prevents separating the camera sensor driver from this one? Hmm. What are you using g mbus config() for? Do you need a busy loop? Could you use msleep()?return ov...(); ?You could use devm kzalloc().=, as in the other patch. You're missing v4l2 ctrl handler free() here. As well as here. Could you use probe new, so you could remove the i2c ID table? Or do you need that for something?-",technical
,
"No problem. Can the remote (max9271) translate addresses for transactions originating from the local side? This would make it possible to do a proper address translation, although 2 addresses is a quite small amount. BTW all the TI chips I'm looking at can do address translation but, as far as I understand, only when acting as ""slave proxy"", i.e. when attached to the bus master. If the Maxim chips do the same, the ""remote translation"" would be unusable. Sadly, it looks pretty much unavoidable...Thanks. Indeed it would have been! But hey, The FOSDEM CFPs are still open!",technical
,
"Yes, that's true for systems with a single max9286 [1]We have a system with 2 de-serializers, and what happens is the following: The system starts with the following configuration: with a single max9286 it would be easy. We operate on one channel at the time, do the reprogramming (or set up the translation, for the TIchip use case) when adding the adapter for the channel, and then we can talk with all remotes, which now have a different address2)      Of course, to do the reprogramming, we need to initially send messages to the default 0x40 address each max9271 boots with. If we don't close all channels but the one we intend to reprogram, all remotes would receive the same message, and thus will be re-programmed to the same address (not nice). [2]Now, if you have two max9286, installed on the same i2c bus, then you need to make sure all channels of the 'others' are closed, before you can reprogram your remotes, otherwise, you would end up reprogramming all the remotes of the 'others' when trying to reprogram yours, as our local de-serializers, bounces everything they receives, not directed to them, to their remote sides. When addr reprogramming is done, we enter the image streaming phase, with all channels open, as now, all remotes, have a different i2c address assigned. Suggestions on how to better handle this are very welcome. The point here is that, to me, this is a gmsl-specific implementation thing. Do you think for your chips, if they do translations, can you easy mask them with the i2c address you want (being that specified in the remote node or selected from an i2c-addr-pool, or something else) without having to care about others remotes to be accidentally programmed to an i2c address they're not intended to be assigned to. Hope this helps clarify your concerns, and I think the actual issue to discuss, at least on bindings, would be the i2c-address assignment method, as this impacts GMSL, as well as other implementation that would use the same binding style as this patches. Thanks   j[1] I still don't get why 'addr translation' >> 'addr reprogramming'. Even the GMSL application development examples uses addr reprogramming, so I guess this is how those chips are supposed to work.[2] If your local side supports address translation, you don't need to talk with the remote side to 'mask' it, so you don't need this workaround.",technical
,
"This last sentence is the one point that makes things so hard on the GMSL chips. In my previous email(s) I partially forgot about this, so I was hoping  a better implementation could be possible. Thanks for re-focusing me.It would have been lovely if the hardware designers had at least put ani2c mux between the soc and those chatty deserializers... :-\Yes. The TI chips have a ""passthrough-all"" option to propagate all transactions with an unknown address, but it's mostly meant for debugging. In normal usage the local chip will propagate (with addresses translated) only transactions coming with a known slave address, including its own address(es), the remote (de)ser aliases and the remote chip aliases. All aliases are disabled until programmed. Absolutely.",technical
,
"Thank you for your review, Well it means /someone/ should always be able to pick it up :D I didn't know who to put here - so I put all of the current blames ;) We've all put a lot of time and work in to the GMSL bring up and refactoring. If you think it's overkill, I can reduce the names. Same on max9286.I don't think there's anything preventing it - except (a fair bit of) development time. We also have the RDACM21 to support, which uses the max9271 and anOV10640. At that time - this will absolutely have to be split. We shouldn't replicate the max9271 code. I mentioned briefly in the cover letter: But to get more dedicated time to work this - we need to show some progress on GMSL up-streaming, so the max9286 and bindings take priority for now. A little bit catch 22 ... :D I'm sure there will be overlap between GMSL and FPD-Link with the RDACM range [0] of cameras too, which also provide TI-FPD Link serialisers. I currently envisage that we would have an RDACM20 'driver' which would know that it has a max9271 serialiser and an OV10635 sensor, and would handle the links of any subdevices internally. As the RDACM20 is an object itself, I think this makes sense ... unless anyone suggests that each part should be broken down into the DT directly ? (I think that would possibly be a bit too much)[0]Good point here ... I assumed it was passed through up to the VIN - but it's really not applicable here. Or if it is - then it should be describing the GMSL bus link! I'll bet this isn't even getting called and can likely be removed. Checkpatch warns here: WARNING: msleep < 20ms can sleep for up to 20ms; see Documentation/timers/timers-howto.txt#10: FILE: drivers/media/i2c/rdacm20.c:461:+       msleep(10);I think for this context, msleep(10) even with the warning is fine here, but perhaps we can meet that with a usleep range(10000, 20000); too. Yes, that would be nicer. Will change. Bah - yes :) Good spot. Thanks Ack.I believe probe new is probably fine.I should really resurrect my i2c-probe-coccinelle patch and get that conversion task done, so we can get to removing and replacing .probe :)( note to self ... starting projects when unemployed becomes difficult to continue when someone else gives you projects to work on all the time ...) Changes described above made and tested on a *single MAX9286* capturing4 cameras simultaneously, now on my rcar.git gmsl/v5 branch ... :D",technical
,
Yes. Indeed the pool can be seen as a list of physical addresses that: - are unused by other chips on the local bus - and given to the ser/des to use for the remote devices - will physically appear on the local bus to talk to remote devices Whether they are runtime translated or reprogrammed on the remote devices is not much relevant for the local bus. I don't foresee any problem in moving from a large pool at the serializer to small pools at each port.,technical
,
"Apologies for the late reply. Not at all. There are too many drivers that do not receive the attention they'd need. :-I Btw. I think this might be worth a new comment to tell what devices can be found here --- it's not a camera sensor as such really. But I wonder how should it be called. We do have ""Miscellaneous helper chips"" at the end. I'm not sure that'd be better. As-is could be fine, too. Oh, sorry; I missed that. Does the DT currently contain all the necessary information for the drivers to get everything they need, if you separated them? I don't remember all the details, but my understanding is that RDACM20 is much more than just a box that contains the serialiser and the sensors. So very probably it'll need its own driver, too. Powering on the sensors, for instance, seemed hard to make generic. There are two use cases I know for this --- SoC camera and something that changes dynamically. The former is obsolete and the latter is better addressed by the frame descriptors I'd like to see go in for 4.22.usleep range(), then, but just setting the delay to precisely 10 ms is much better than a 10 ms busy loop. That'd be nice!",technical
,
This bit is pretty unsightly. Especially the static in each inline,technical
,
I understood that you mean line static char str[256];This array will be defined several times. I will remove inline form function definition. It's not necessary. Thank you for comment.,technical
,
Why not depend on USB instead of USB XHCI HCD? Need a comma between switch and Host-only. Why depend on Config options to populate resources? The resources should be there regardless. It is a lot simpler that way as it reflects the hardware as-is.dev dbg() for this and all occurrences below?,technical
,
"Is it better to split this patch into 3 parts?1) host support2) gadget support3) DRD support (along with patch 4)how about naming this to cnds3 get current role driver() ?You are changing the role here. Shouldn't it just start whatever role is already in cdns->role? And you have a cnds3 set role() function to set role. Where is the balancing pm runtime put() for this? All role switching code can come as part of DRD driver. Here you are not checking for Kconfig options before getting resources which is the right thing. However this will be broken if you don't get rid of the Kconfig checks when you populate the resources in patch 1.What exactly does role start have to do? Can you start the role before requesting irq? Shouldn't the order bepm runtime get sync();cdns3 remove roles();pm runtime put noidle();pm runtime disable();you didn't call usb phy init() anywhere. Why is a call to host driver init() required? Why is OTG after END? Does OTG have a role driver as well? If not it must not come here. It is a mode, not a role. Why is cdns3 host driver init() required?",technical
,
"I think we need to make a clear distinction between mode and role. Mode is the controller mode. (Host-only, Device-only, dual-role[otg]) Role is the USB controller state should correspond to enum usb dr mode which should be the argument for this function if you want to set mode. Why not just use cdns->dr mode directly? Do you want to check if it is host role here? e.g. if dr mode = USB DR MODE OTG. Looks like this function should be on core.c?CDNS3 ROLE HOST.should this be called cdns3 drd init()?s/HOST/PERIPHERAL?",technical
,
Why does role driver need hook to irq handler? Can't each driver host or gadget handle it's respective irq on its own? If the same IRQ line is used it could be requested as a shared IRQ.,technical
,
If we start with OTG mode and user says change mode to device will we still switch to host based on ID pin change? If it does then this isn't working correctly. We need to stop processing ID interrupts and keep the role static till the user switches it back to otg.,technical
,
"I will replace it with this: Depend on USB SUPPORT && (USB l USB GADGET) && HAS DMA You're right. I will remove these Config options from this code. Ok , I replaced it. Thanks for all comments ",technical
,
"Currently we have:0003-usb-cdns3-Driver-initialization-code.patch - generic initialization common code. I agree that some fragment could be moved to next free patches. It could improve understanding of code.0004-usb-cdns3-Added-DRD-support.patch - it's short file so contains initialization and other related to DRD function.0005-usb-cdns3-Added-Wrapper-to-XCHI-driver.patch - host support - quite short patch0006-usb-cdns3-Initialization-code-for-Device-side - device initialization It's better. I've changed it. The function cdns3 set role currently do nothing. A little explanation. The main author of this file is Peter Chan. We use the same controller, but we have different platforms. I adopt this file to my platform. I tried to keep the concept of his solution and remove only platform specific codes. I assume this approach allows him easily to re-use this code in the feature. In cdns3 set role he makes some platform specific code that allow him to switch between Host and device. I do not need it In the  feature this function probably will call some platform specific function. Currently this function is not used so I will remove  will remove this fragment. It will be implemented later in pm runtime functions. Detecting Host and Device mode can be achieved in different ways depending on platform. I assume that core.c file is generic part that allow to connect different way of detecting modes. In my solution I use OTG registers which are part of this controller. But someone can use  extcon to get information about mode e.g;I treat drd.c file rather as platform specific extension for this driver, patch 1 will be changed. Start the Device or Host mode.  After this operation device/host can handle interrupts. A good point. It's works correct, but I will change this. It's look strange. Yes, I found it too. In next series this will be removed. Also, I will add the phy initialization, but I want to use generic phy instead of usb phy. I'm not sure whether it is proper solution, but we have only generic simple phy driver. No OTG doesn't have role driver. It was the simplest solution. It's used only in debugfs.c  and drd.c files. I'll give it some thought how to change it without big impact to rest codes. It's initialize xhci driver with. Maybe I will move this function to  0005-usb-cdns3-Added-Wrapper-to-XCHI-driver.patch then all code will be in this some patch. Thanks for all comment",technical
,
How often are these invoked? For I/O intensive cases dev dbg() will not be useful as it will affect timing adversely because of which it might prevent the issue from happening when debug is enabled. How about using tracepoints instead?,technical
,
"I agree with you,  In next set patch distinction between mode and role will be clear. The function will be slightly completed. After corrections: dr mode holds the hardware configuration. It will be set during initialization and will not be changed. current dr mode will hold current mode. This mode can be changed by debugfs. This function will look: If DRD mode is set to HOST then always return true, elsewhere it will be based on ID pin. So, for USB DR MODE OTG driver will waiting for appropriate role but when current dr mode == USB DR MODE HOST then we know that only HOST role can be set and driver can return true immediately.> The name will be changed to cdns3 drd update mode and will be called during initialization, or when user change mode by means of debugfs.I found it too and was corrected. I changed the name cdns3 drd init().  I call this cdns3 drd probe, because I was not sure whether DRD will be part of whole driver or will be separate driver. Thanks for all your comments",technical
